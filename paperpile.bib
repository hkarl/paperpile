@MISC{noauthor_undated-xd,
  title       = "pcie-model",
  abstract    = "Contribute to pcie-bench/pcie-model development by creating an
                 account on GitHub.",
  institution = "Github",
  keywords    = "ProgrammableNetworks",
  language    = "en"
}

@MISC{noauthor_undated-ps,
  title = "underwater-vehicles.pdf",
  file  = "All Papers/Other/underwater-vehicles.pdf - underwater-vehicles.pdf"
}

@ARTICLE{Clementi2011-ve,
  title         = "Information Spreading in Stationary Markovian Evolving
                   Graphs",
  author        = "Clementi, Andrea and Monti, Angelo and Pasquale, Francesco
                   and Silvestri, Riccardo",
  abstract      = "Markovian evolving graphs are dynamic-graph models where the
                   links among a fixed set of nodes change during time
                   according to an arbitrary Markovian rule. They are extremely
                   general and they can well describe important dynamic-network
                   scenarios. We study the speed of information spreading in
                   the ``stationary phase'' by analyzing the completion time of
                   the ``flooding mechanism''. We prove a general theorem that
                   establishes an upper bound on flooding time in any
                   stationary Markovian evolving graph in terms of its
                   node-expansion properties. We apply our theorem in two
                   natural and relevant cases of such dynamic graphs.
                   ``Geometric Markovian evolving graphs'' where the Markovian
                   behaviour is yielded by ``n'' mobile radio stations, with
                   fixed transmission radius, that perform independent random
                   walks over a square region of the plane. ``Edge-Markovian
                   evolving graphs'' where the probability of existence of any
                   edge at time ``t'' depends on the existence (or not) of the
                   same edge at time ``t-1''. In both cases, the obtained upper
                   bounds hold ``with high probability'' and they are nearly
                   tight. In fact, they turn out to be tight for a large range
                   of the values of the input parameters. As for geometric
                   Markovian evolving graphs, our result represents the first
                   analytical upper bound for flooding time on a class of
                   concrete mobile networks.",
  month         =  mar,
  year          =  2011,
  file          = "All Papers/C/Clementi et al. 2011 - Information Spreading in Stationary Markovian Evolving Graphs.pdf",
  archivePrefix = "arXiv",
  eprint        = "1103.0741",
  primaryClass  = "cs.DM",
  arxivid       = "1103.0741"
}

@ARTICLE{Liang2017-ez,
  title         = "{RLlib}: Abstractions for Distributed Reinforcement Learning",
  author        = "Liang, Eric and Liaw, Richard and Moritz, Philipp and
                   Nishihara, Robert and Fox, Roy and Goldberg, Ken and
                   Gonzalez, Joseph E and Jordan, Michael I and Stoica, Ion",
  abstract      = "Reinforcement learning (RL) algorithms involve the deep
                   nesting of highly irregular computation patterns, each of
                   which typically exhibits opportunities for distributed
                   computation. We argue for distributing RL components in a
                   composable way by adapting algorithms for top-down
                   hierarchical control, thereby encapsulating parallelism and
                   resource requirements within short-running compute tasks. We
                   demonstrate the benefits of this principle through RLlib: a
                   library that provides scalable software primitives for RL.
                   These primitives enable a broad range of algorithms to be
                   implemented with high performance, scalability, and
                   substantial code reuse. RLlib is available at
                   https://rllib.io/.",
  month         =  dec,
  year          =  2017,
  file          = "All Papers/L/Liang et al. 2017 - RLlib - Abstractions for Distributed Reinforcement Learning.pdf",
  archivePrefix = "arXiv",
  eprint        = "1712.09381",
  primaryClass  = "cs.AI",
  arxivid       = "1712.09381"
}

@ARTICLE{Rodrigues2018-qb,
  title         = "{Fine-Grained} Energy and Performance Profiling framework
                   for Deep Convolutional Neural Networks",
  author        = "Rodrigues, Crefeda Faviola and Riley, Graham and Lujan,
                   Mikel",
  abstract      = "There is a huge demand for on-device execution of deep
                   learning algorithms on mobile and embedded platforms. These
                   devices present constraints on the application due to
                   limited resources and power. Hence, developing
                   energy-efficient solutions to address this issue will
                   require innovation in algorithmic design, software and
                   hardware. Such innovation requires benchmarking and
                   characterization of Deep Neural Networks based on
                   performance and energy-consumption alongside accuracy.
                   However, current benchmarks studies in existing deep
                   learning frameworks (for example, Caffe, Tensorflow, Torch
                   and others) are based on performance of these applications
                   on high-end CPUs and GPUs. In this work, we introduce a
                   benchmarking framework called ``SyNERGY'' to measure the
                   energy and time of 11 representative Deep Convolutional
                   Neural Networks on embedded platforms such as NVidia Jetson
                   TX1. We integrate ARM's Streamline Performance Analyser with
                   standard deep learning frameworks such as Caffe and CuDNNv5,
                   to study the execution behaviour of current deep learning
                   models at a fine-grained level (or specific layers) on image
                   processing tasks. In addition, we build an initial
                   multi-variable linear regression model to predict energy
                   consumption of unseen neural network models based on the
                   number of SIMD instructions executed and main memory
                   accesses of the CPU cores of the TX1 with an average
                   relative test error rate of 8.04 +/- 5.96 \%. Surprisingly,
                   we find that it is possible to refine the model to predict
                   the number of SIMD instructions and main memory accesses
                   solely from the application's Multiply-Accumulate (MAC)
                   counts, hence, eliminating the need for actual measurements.
                   Our predicted results demonstrate 7.08 +/- 6.0 \% average
                   relative error over actual energy measurements of all 11
                   networks tested, except MobileNet. By including MobileNet
                   the average relative test error increases to 17.33 +/- 12.2
                   \%.",
  month         =  mar,
  year          =  2018,
  file          = "All Papers/R/Rodrigues et al. 2018 - Fine-Grained Energy and Performance Profiling framework for Deep Convolutional Neural Networks.pdf",
  archivePrefix = "arXiv",
  eprint        = "1803.11151",
  primaryClass  = "cs.PF",
  arxivid       = "1803.11151"
}

@ARTICLE{Schaarschmidt2018-qg,
  title         = "{LIFT}: Reinforcement Learning in Computer Systems by
                   Learning From Demonstrations",
  author        = "Schaarschmidt, Michael and Kuhnle, Alexander and Ellis, Ben
                   and Fricke, Kai and Gessert, Felix and Yoneki, Eiko",
  abstract      = "Reinforcement learning approaches have long appealed to the
                   data management community due to their ability to learn to
                   control dynamic behavior from raw system performance. Recent
                   successes in combining deep neural networks with
                   reinforcement learning have sparked significant new interest
                   in this domain. However, practical solutions remain elusive
                   due to large training data requirements, algorithmic
                   instability, and lack of standard tools. In this work, we
                   introduce LIFT, an end-to-end software stack for applying
                   deep reinforcement learning to data management tasks. While
                   prior work has frequently explored applications in
                   simulations, LIFT centers on utilizing human expertise to
                   learn from demonstrations, thus lowering online training
                   times. We further introduce TensorForce, a TensorFlow
                   library for applied deep reinforcement learning exposing a
                   unified declarative interface to common RL algorithms, thus
                   providing a backend to LIFT. We demonstrate the utility of
                   LIFT in two case studies in database compound indexing and
                   resource management in stream processing. Results show LIFT
                   controllers initialized from demonstrations can outperform
                   human baselines and heuristics across latency metrics and
                   space usage by up to 70\%.",
  month         =  aug,
  year          =  2018,
  file          = "All Papers/S/Schaarschmidt et al. 2018 - LIFT - Reinforcement Learning in Computer Systems by Learning From Demonstrations.pdf",
  archivePrefix = "arXiv",
  eprint        = "1808.07903",
  primaryClass  = "cs.LG",
  arxivid       = "1808.07903",
  doi           = "10.1109/ICAC.2006.1662383"
}

@ARTICLE{Jay2018-pe,
  title         = "Internet Congestion Control via Deep Reinforcement Learning",
  author        = "Jay, Nathan and Rotman, Noga H and Brighten Godfrey, P and
                   Schapira, Michael and Tamar, Aviv",
  abstract      = "We present and investigate a novel and timely application
                   domain for deep reinforcement learning (RL): Internet
                   congestion control. Congestion control is the core
                   networking task of modulating traffic sources'
                   data-transmission rates to efficiently utilize network
                   capacity, and is the subject of extensive attention in light
                   of the advent of Internet services such as live video,
                   virtual reality, Internet-of-Things, and more. We show that
                   casting congestion control as RL enables training deep
                   network policies that capture intricate patterns in data
                   traffic and network conditions, and leverage this to
                   outperform the state-of-the-art. We also highlight
                   significant challenges facing real-world adoption of
                   RL-based congestion control, including fairness, safety, and
                   generalization, which are not trivial to address within
                   conventional RL formalism. To facilitate further research
                   and reproducibility of our results, we present a test suite
                   for RL-guided congestion control based on the OpenAI Gym
                   interface.",
  month         =  oct,
  year          =  2018,
  file          = "All Papers/J/Jay et al. 2018 - Internet Congestion Control via Deep Reinforcement Learning.pdf",
  keywords      = "MLAspects;CongestionControl",
  archivePrefix = "arXiv",
  eprint        = "1810.03259",
  primaryClass  = "cs.NI",
  arxivid       = "1810.03259"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gawlowicz2018-dh,
  title         = "ns3-gym: Extending {OpenAI} Gym for Networking Research",
  author        = "Gaw{\l}owicz, Piotr and Zubow, Anatolij",
  abstract      = "OpenAI Gym is a toolkit for reinforcement learning (RL)
                   research. It includes a large number of well-known problems
                   that expose a common interface allowing to directly compare
                   the performance results of different RL algorithms. Since
                   many years, the ns-3 network simulation tool is the de-facto
                   standard for academic and industry research into networking
                   protocols and communications technology. Numerous scientific
                   papers were written reporting results obtained using ns-3,
                   and hundreds of models and modules were written and
                   contributed to the ns-3 code base. Today as a major trend in
                   network research we see the use of machine learning tools
                   like RL. What is missing is the integration of a RL
                   framework like OpenAI Gym into the network simulator ns-3.
                   This paper presents the ns3-gym framework. First, we discuss
                   design decisions that went into the software. Second, two
                   illustrative examples implemented using ns3-gym are
                   presented. Our software package is provided to the community
                   as open source under a GPL license and hence can be easily
                   extended.",
  month         =  oct,
  year          =  2018,
  file          = "All Papers/G/Gawłowicz and Zubow 2018 - ns3-gym - Extending OpenAI Gym for Networking Research.pdf",
  archivePrefix = "arXiv",
  eprint        = "1810.03943",
  primaryClass  = "cs.NI",
  arxivid       = "1810.03943"
}

@ARTICLE{Sarwar_Murshed2019-yr,
  title         = "Machine Learning at the Network Edge: A Survey",
  author        = "Sarwar Murshed, M G and Murphy, Christopher and Hou, Daqing
                   and Khan, Nazar and Ananthanarayanan, Ganesh and Hussain,
                   Faraz",
  abstract      = "Resource-constrained IoT devices, such as sensors and
                   actuators, have become ubiquitous in recent years. This has
                   led to the generation of large quantities of data in
                   real-time, which is an appealing target for AI systems.
                   However, deploying machine learning models on such
                   end-devices is nearly impossible. A typical solution
                   involves offloading data to external computing systems (such
                   as cloud servers) for further processing but this worsens
                   latency, leads to increased communication costs, and adds to
                   privacy concerns. To address this issue, efforts have been
                   made to place additional computing devices at the edge of
                   the network, i.e close to the IoT devices where the data is
                   generated. Deploying machine learning systems on such edge
                   computing devices alleviates the above issues by allowing
                   computations to be performed close to the data sources. This
                   survey describes major research efforts where machine
                   learning systems have been deployed at the edge of computer
                   networks, focusing on the operational aspects including
                   compression techniques, tools, frameworks, and hardware used
                   in successful applications of intelligent edge systems.",
  month         =  jul,
  year          =  2019,
  file          = "All Papers/S/Sarwar Murshed et al. 2019 - Machine Learning at the Network Edge - A Survey.pdf",
  keywords      = "MLAspects",
  archivePrefix = "arXiv",
  eprint        = "1908.00080",
  primaryClass  = "cs.LG",
  arxivid       = "1908.00080"
}

@ARTICLE{Niknam2019-iz,
  title         = "Federated Learning for Wireless Communications: Motivation,
                   Opportunities and Challenges",
  author        = "Niknam, Solmaz and Dhillon, Harpreet S and Reed, Jeffery H",
  abstract      = "There is a growing interest in the wireless communications
                   community to complement the traditional model-based design
                   approaches with data-driven machine learning (ML)-based
                   solutions. While conventional ML approaches rely on the
                   assumption of having the data and processing heads in a
                   central entity, this is not always feasible in wireless
                   communications applications because of the inaccessibility
                   of private data and large communication overhead required to
                   transmit raw data to central ML processors. As a result,
                   decentralized ML approaches that keep the data where it is
                   generated are much more appealing. Owing to its
                   privacy-preserving nature, federated learning is
                   particularly relevant for many wireless applications,
                   especially in the context of fifth generation (5G) networks.
                   In this article, we provide an accessible introduction to
                   the general idea of federated learning, discuss several
                   possible applications in 5G networks, and describe key
                   technical challenges and open problems for future research
                   on federated learning in the context of wireless
                   communications.",
  month         =  jul,
  year          =  2019,
  file          = "All Papers/N/Niknam et al. 2019 - Federated Learning for Wireless Communications - Motivation, Opportunities and Challenges.pdf",
  archivePrefix = "arXiv",
  eprint        = "1908.06847",
  primaryClass  = "eess.SP",
  arxivid       = "1908.06847"
}

@ARTICLE{Hoffman2020-hr,
  title         = "Acme: A Research Framework for Distributed Reinforcement
                   Learning",
  author        = "Hoffman, Matthew W and Shahriari, Bobak and Aslanides, John
                   and Barth-Maron, Gabriel and Momchev, Nikola and
                   Sinopalnikov, Danila and Sta{\'n}czyk, Piotr and Ramos,
                   Sabela and Raichuk, Anton and Vincent, Damien and Hussenot,
                   L{\'e}onard and Dadashi, Robert and Dulac-Arnold, Gabriel
                   and Orsini, Manu and Jacq, Alexis and Ferret, Johan and
                   Vieillard, Nino and Ghasemipour, Seyed Kamyar Seyed and
                   Girgin, Sertan and Pietquin, Olivier and Behbahani, Feryal
                   and Norman, Tamara and Abdolmaleki, Abbas and Cassirer,
                   Albin and Yang, Fan and Baumli, Kate and Henderson, Sarah
                   and Friesen, Abe and Haroun, Ruba and Novikov, Alex and
                   Colmenarejo, Sergio G{\'o}mez and Cabi, Serkan and Gulcehre,
                   Caglar and Le Paine, Tom and Srinivasan, Srivatsan and
                   Cowie, Andrew and Wang, Ziyu and Piot, Bilal and de Freitas,
                   Nando",
  abstract      = "Deep reinforcement learning (RL) has led to many recent and
                   groundbreaking advances. However, these advances have often
                   come at the cost of both increased scale in the underlying
                   architectures being trained as well as increased complexity
                   of the RL algorithms used to train them. These increases
                   have in turn made it more difficult for researchers to
                   rapidly prototype new ideas or reproduce published RL
                   algorithms. To address these concerns this work describes
                   Acme, a framework for constructing novel RL algorithms that
                   is specifically designed to enable agents that are built
                   using simple, modular components that can be used at various
                   scales of execution. While the primary goal of Acme is to
                   provide a framework for algorithm development, a secondary
                   goal is to provide simple reference implementations of
                   important or state-of-the-art algorithms. These
                   implementations serve both as a validation of our design
                   decisions as well as an important contribution to
                   reproducibility in RL research. In this work we describe the
                   major design decisions made within Acme and give further
                   details as to how its components can be used to implement
                   various algorithms. Our experiments provide baselines for a
                   number of common and state-of-the-art algorithms as well as
                   showing how these algorithms can be scaled up for much
                   larger and more complex environments. This highlights one of
                   the primary advantages of Acme, namely that it can be used
                   to implement large, distributed RL algorithms that can run
                   at massive scales while still maintaining the inherent
                   readability of that implementation. This work presents a
                   second version of the paper which coincides with an increase
                   in modularity, additional emphasis on offline, imitation and
                   learning from demonstrations algorithms, as well as various
                   new agents implemented as part of Acme.",
  month         =  jun,
  year          =  2020,
  file          = "All Papers/H/Hoffman et al. 2020 - Acme - A Research Framework for Distributed Reinforcement Learning.pdf",
  archivePrefix = "arXiv",
  eprint        = "2006.00979",
  primaryClass  = "cs.LG",
  arxivid       = "2006.00979"
}

@ARTICLE{Carrascosa2020-zl,
  title         = "{Cloud-gaming:Analysis} of Google Stadia traffic",
  author        = "Carrascosa, Marc and Bellalta, Boris",
  abstract      = "Interactive, real-time, and high-quality cloud video games
                   pose a serious challenge to the Internet due to simultaneous
                   high-throughput and low round trip delay requirements. In
                   this paper, we investigate the traffic characteristics of
                   Stadia, the cloud-gaming solution from Google, which is
                   likely to become one of the dominant players in the gaming
                   sector. To do that, we design several experiments, and
                   perform an extensive traffic measurement campaign to obtain
                   all required data. Our first goal is to gather a deep
                   understanding of Stadia traffic characteristics by
                   identifying the different protocols involved for both
                   signalling and video/audio contents, the traffic generation
                   patterns, and the packet size and inter-packet time
                   probability distributions. Then, our second goal is to
                   understand how different Stadia games and configurations,
                   such as the video codec and the video resolution selected,
                   impact on the characteristics of the generated traffic.
                   Finally, we aim to evaluate the ability of Stadia to adapt
                   to different link capacity conditions, including those cases
                   where the capacity drops suddenly. Our results and findings,
                   besides illustrating the characteristics of Stadia traffic,
                   are also valuable for planning and dimensioning future
                   networks, as well as for designing new resource management
                   strategies.",
  month         =  sep,
  year          =  2020,
  file          = "All Papers/C/Carrascosa and Bellalta 2020 - Cloud-gaming - Analysis of Google Stadia traffic.pdf",
  keywords      = "NetworkTraffic",
  archivePrefix = "arXiv",
  eprint        = "2009.09786",
  primaryClass  = "cs.NI",
  arxivid       = "2009.09786"
}

@ARTICLE{Banbury2021-kt,
  title         = "{MLPerf} Tiny Benchmark",
  author        = "Banbury, Colby and Reddi, Vijay Janapa and Torelli, Peter
                   and Holleman, Jeremy and Jeffries, Nat and Kiraly, Csaba and
                   Montino, Pietro and Kanter, David and Ahmed, Sebastian and
                   Pau, Danilo and Thakker, Urmish and Torrini, Antonio and
                   Warden, Peter and Cordaro, Jay and Di Guglielmo, Giuseppe
                   and Duarte, Javier and Gibellini, Stephen and Parekh, Videet
                   and Tran, Honson and Tran, Nhan and Wenxu, Niu and Xuesong,
                   Xu",
  abstract      = "Advancements in ultra-low-power tiny machine learning
                   (TinyML) systems promise to unlock an entirely new class of
                   smart applications. However, continued progress is limited
                   by the lack of a widely accepted and easily reproducible
                   benchmark for these systems. To meet this need, we present
                   MLPerf Tiny, the first industry-standard benchmark suite for
                   ultra-low-power tiny machine learning systems. The benchmark
                   suite is the collaborative effort of more than 50
                   organizations from industry and academia and reflects the
                   needs of the community. MLPerf Tiny measures the accuracy,
                   latency, and energy of machine learning inference to
                   properly evaluate the tradeoffs between systems.
                   Additionally, MLPerf Tiny implements a modular design that
                   enables benchmark submitters to show the benefits of their
                   product, regardless of where it falls on the ML deployment
                   stack, in a fair and reproducible manner. The suite features
                   four benchmarks: keyword spotting, visual wake words, image
                   classification, and anomaly detection.",
  month         =  jun,
  year          =  2021,
  file          = "All Papers/B/Banbury et al. 2021 - MLPerf Tiny Benchmark.pdf",
  archivePrefix = "arXiv",
  eprint        = "2106.07597",
  primaryClass  = "cs.LG",
  arxivid       = "2106.07597"
}

@ARTICLE{Lacava2022-va,
  title         = "Programmable and Customized Intelligence for Traffic
                   Steering in {5G} Networks Using Open {RAN} Architectures",
  author        = "Lacava, Andrea and Polese, Michele and Sivaraj, Rajarajan
                   and Soundrarajan, Rahul and Bhati, Bhawani Shanker and
                   Singh, Tarunjeet and Zugno, Tommaso and Cuomo, Francesca and
                   Melodia, Tommaso",
  abstract      = "5G and beyond mobile networks will support heterogeneous use
                   cases at an unprecedented scale, thus demanding automated
                   control and optimization of network functionalities
                   customized to the needs of individual users. Such
                   fine-grained control of the Radio Access Network (RAN) is
                   not possible with the current cellular architecture. To fill
                   this gap, the Open RAN paradigm and its specification
                   introduce an open architecture with abstractions that enable
                   closed-loop control and provide data-driven, and intelligent
                   optimization of the RAN at the user level. This is obtained
                   through custom RAN control applications (i.e., xApps)
                   deployed on near-real-time RAN Intelligent Controller
                   (near-RT RIC) at the edge of the network. Despite these
                   premises, as of today the research community lacks a sandbox
                   to build data-driven xApps, and create large-scale datasets
                   for effective AI training. In this paper, we address this by
                   introducing ns-O-RAN, a software framework that integrates a
                   real-world, production-grade near-RT RIC with a 3GPP-based
                   simulated environment on ns-3, enabling the development of
                   xApps and automated large-scale data collection and testing
                   of Deep Reinforcement Learning-driven control policies for
                   the optimization at the user-level. In addition, we propose
                   the first user-specific O-RAN Traffic Steering (TS)
                   intelligent handover framework. It uses Random Ensemble
                   Mixture, combined with a state-of-the-art Convolutional
                   Neural Network architecture, to optimally assign a serving
                   base station to each user in the network. Our TS xApp,
                   trained with more than 40 million data points collected by
                   ns-O-RAN, runs on the near-RT RIC and controls its base
                   stations. We evaluate the performance on a large-scale
                   deployment, showing that the xApp-based handover improves
                   throughput and spectral efficiency by an average of 50\%
                   over traditional handover heuristics, with less mobility
                   overhead.",
  month         =  sep,
  year          =  2022,
  file          = "All Papers/L/Lacava et al. 2022 - Programmable and Customized Intelligence for Traffic Steering in 5G Networks Using Open RAN Architectures.pdf",
  archivePrefix = "arXiv",
  eprint        = "2209.14171",
  primaryClass  = "cs.NI",
  arxivid       = "2209.14171"
}

@ARTICLE{Lacava2023-ri,
  title         = "{ns-O-RAN}: Simulating {O-RAN} {5G} Systems in ns-3",
  author        = "Lacava, Andrea and Bordin, Matteo and Polese, Michele and
                   Sivaraj, Rajarajan and Zugno, Tommaso and Cuomo, Francesca
                   and Melodia, Tommaso",
  abstract      = "O-RAN is radically shifting how cellular networks are
                   designed, deployed and optimized through network
                   programmability, disaggregation, and virtualization.
                   Specifically, RAN Intelligent Controllers (RICs) can
                   orchestrate and optimize the Radio Access Network (RAN)
                   operations, allowing fine-grained control over the network.
                   RICs provide new approaches and solutions for classical use
                   cases such as on-demand traffic steering, anomaly detection,
                   and Quality of Service (QoS) management, with an
                   optimization that can target single User Equipments (UEs),
                   slices, cells, or entire base stations. While this comes
                   with the potential to enable intelligent, programmable RANs,
                   there are still significant challenges to be faced,
                   primarily related to data collection at scale, development
                   and testing of custom control logic for the RICs, and
                   availability of Open RAN simulation and experimental tools
                   for the research and development communities. To address
                   this, we introduce ns-O-RAN, a software integration between
                   a real-world near-real-time RIC and an ns-3 simulated RAN
                   which provides a platform for researchers and telco
                   operators to build, test and integrate xApps. ns-O-RAN
                   extends a popular Open RAN experimental framework (OpenRAN
                   Gym) with simulation capabilities that enable the generation
                   of realistic datasets without the need for experimental
                   infrastructure. We implement it as a new open-source ns-3
                   module that uses the E2 interface to connect different
                   simulated 5G base stations with the RIC, enabling the
                   exchange of E2 messages and RAN KPMs to be consumed by
                   standard xApps. Furthermore, we test ns-O-RAN with the OSC
                   and OpenRAN Gym RICs, simplifying the onboarding from a test
                   environment to production with real telecom hardware
                   controlled without major reconfigurations required. ns-O-RAN
                   is open source and publicly available, together with
                   quick-start tutorials and documentation.",
  month         =  may,
  year          =  2023,
  file          = "All Papers/L/Lacava et al. 2023 - ns-O-RAN - Simulating O-RAN 5G Systems in ns-3.pdf",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "2305.06906",
  primaryClass  = "cs.NI",
  arxivid       = "2305.06906"
}

@ARTICLE{Bou2023-yy,
  title         = "{TorchRL}: A data-driven decision-making library for
                   {PyTorch}",
  author        = "Bou, Albert and Bettini, Matteo and Dittert, Sebastian and
                   Kumar, Vikash and Sodhani, Shagun and Yang, Xiaomeng and De
                   Fabritiis, Gianni and Moens, Vincent",
  abstract      = "Striking a balance between integration and modularity is
                   crucial for a machine learning library to be versatile and
                   user-friendly, especially in handling decision and control
                   tasks that involve large development teams and complex,
                   real-world data, and environments. To address this issue, we
                   propose TorchRL, a generalistic control library for PyTorch
                   that provides well-integrated, yet standalone components.
                   With a versatile and robust primitive design, TorchRL
                   facilitates streamlined algorithm development across the
                   many branches of Reinforcement Learning (RL) and control. We
                   introduce a new PyTorch primitive, TensorDict, as a flexible
                   data carrier that empowers the integration of the library's
                   components while preserving their modularity. Hence replay
                   buffers, datasets, distributed data collectors,
                   environments, transforms and objectives can be effortlessly
                   used in isolation or combined. We provide a detailed
                   description of the building blocks, supporting code examples
                   and an extensive overview of the library across domains and
                   tasks. Finally, we show comparative benchmarks to
                   demonstrate its computational efficiency. TorchRL fosters
                   long-term support and is publicly available on GitHub for
                   greater reproducibility and collaboration within the
                   research community. The code is opensourced on
                   https://github.com/pytorch/rl.",
  month         =  jun,
  year          =  2023,
  file          = "All Papers/B/Bou et al. 2023 - TorchRL - A data-driven decision-making library for PyTorch.pdf",
  archivePrefix = "arXiv",
  eprint        = "2306.00577",
  primaryClass  = "cs.LG",
  arxivid       = "2306.00577"
}

@MISC{Willinger_undated-hh,
  title        = "Where mathematics meets the internet",
  author       = "Willinger, Walter and Paxson, Vern",
  howpublished = "\url{http://www.ams.org/notices/199808/paxson.pdf}",
  note         = "Accessed: 2021-1-22",
  file         = "All Papers/W/Willinger and Paxson - Where mathematics meets the internet.pdf",
  keywords     = "NetworkTraffic",
  doi          = "10.1.1.173.1903"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Boutaba2021-ob,
  title     = "Managing Virtualized Networks and Services with Machine Learning",
  booktitle = "Communication Networks and Service Management in the Era of
               Artificial Intelligence and Machine Learning",
  author    = "Boutaba, Raouf and Shahriar, Nashid and Salahuddin, Mohammad A
               and Limam, Noura",
  abstract  = "Virtualization is instigating a paradigm shift in the networking
               industry, to keep up with emerging application's quality of
               service requirements, massive growth in traffic volume, and to
               reduce capital and operational expenditures. Network
               virtualization coupled with function virtualization enables
               network providers to offer on‐demand virtualized networks and
               services. Network slicing goes a step further by facilitating a
               new business model, namely Network‐as‐a‐Service, to offer
               dedicated and customized network slices (i.e. partitions of
               physical infrastructure) to multiple tenants, while ensuring
               proper isolation. However, this shift introduces new challenges
               for network providers and calls for intelligent and automated
               management. Artificial intelligence and machine learning are
               considered as enablers for the automated deployment and
               management of virtualized networks and services. This chapter
               exposes the state‐of‐the‐art research that leverages artificial
               intelligence and machine learning to address complex problems in
               deploying and managing virtualized networks and services. It
               also delineates open, prominent research challenges and
               opportunities to realize automated management of virtualized
               networks and services.",
  publisher = "IEEE",
  pages     = "33--68",
  year      =  2021,
  keywords  = "Servers;Hardware;Virtualization;Virtual private networks;Network
               slicing;Network function virtualization;Computer industry",
  isbn      = "9781119675440",
  doi       = "10.1002/9781119675525.ch3"
}

@MISC{Shannon1948-nc,
  title    = "A Mathematical Theory of Communication",
  author   = "Shannon, C E",
  journal  = "Bell System Technical Journal",
  volume   =  27,
  number   =  4,
  pages    = "623--656",
  year     =  1948,
  keywords = "GDS",
  doi      = "10.1002/j.1538-7305.1948.tb00917.x"
}

@ARTICLE{Mealy1955-bu,
  title    = "A method for synthesizing sequential circuits",
  author   = "Mealy, George H",
  abstract = "The theoretical basis of sequential circuit synthesis is
              developed, with particular reference to the work of D. A. Huffman
              and E. F. Moore. A new method of synthesis is developed which
              emphasizes formal procedures rather than the more familiar
              intuitive ones. Familiarity is assumed with the use of switching
              algebra in the synthesis of combinational circuits.",
  journal  = "The Bell System Technical Journal",
  volume   =  34,
  number   =  5,
  pages    = "1045--1079",
  month    =  sep,
  year     =  1955,
  keywords = "GDS",
  issn     = "0005-8580",
  doi      = "10.1002/j.1538-7305.1955.tb03788.x"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Nino-Mora2009-io,
  title     = "Stochastic schedulingStochastic Scheduling",
  booktitle = "Encyclopedia of Optimization",
  author    = "Ni{\~n}o-Mora, Jos{\'e}",
  editor    = "Floudas, Christodoulos A and Pardalos, Panos M",
  abstract  = "Introduction",
  publisher = "Springer US",
  pages     = "3818--3824",
  year      =  2009,
  file      = "All Papers/N/Niño-Mora 2009 - Stochastic schedulingStochastic Scheduling.pdf",
  address   = "Boston, MA",
  isbn      = "9780387747590",
  doi       = "10.1007/978-0-387-74759-0\_665"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Nino-Mora2009-iw,
  title     = "Stochastic schedulingStochastic Scheduling",
  booktitle = "Encyclopedia of Optimization",
  author    = "Ni{\~n}o-Mora, Jos{\'e}",
  editor    = "Floudas, Christodoulos A and Pardalos, Panos M",
  abstract  = "Introduction",
  publisher = "Springer US",
  pages     = "3818--3824",
  year      =  2009,
  file      = "All Papers/N/Niño-Mora 2009 - Stochastic schedulingStochastic Scheduling.pdf",
  address   = "Boston, MA",
  keywords  = "SurveysTutorials",
  isbn      = "9780387747590",
  doi       = "10.1007/978-0-387-74759-0\_665"
}

@BOOK{Mario:Amazing_Journey_Reason:2020,
  title     = "The Amazing Journey of Reason: from {DNA} to Artificial
               Intelligence",
  author    = "Alemi, Mario",
  abstract  = "The Amazing Journey analyzes the latest results in chemistry,
               biology, neuroscience, anthropology and sociology under the
               light of the evolution of intelligence, seen as the ability of
               processing information.",
  publisher = "Springer International Publishing",
  series    = "SpringerBriefs in Computer Science",
  year      =  2020,
  file      = "All Papers/A/Alemi 2020 - The Amazing Journey of Reason - from DNA to Artificial Intelligence.pdf",
  address   = "Cham, Switzerland",
  keywords  = "Read;Misc",
  isbn      = "9783030259617",
  doi       = "10.1007/978-3-030-25962-4"
}

@INPROCEEDINGS{Abdellah2020-hu,
  title     = "Deep Learning with Long {Short-Term} Memory for {IoT} Traffic
               Prediction",
  booktitle = "Internet of Things, Smart Spaces, and Next Generation Networks
               and Systems",
  author    = "Abdellah, Ali R and Koucheryavy, Andrey",
  abstract  = "5G network is new wireless mobile communication technology
               beyond 4G networks. These days, many network applications have
               been emerged and have led to an enormous amount of network
               traffic. Numerous studies have been conducted for enhancing the
               prediction accuracy of network traffic applications. Network
               traffic management and monitoring require technology for traffic
               prediction without the need for network operators. It is
               expected that each of the 5G networks and the Internet of things
               technologies to spread widely in the next few years. On the
               practical level, 5G uses the Internet of Things (IoT) for
               working in high-traffic networks with multiple sensors sending
               their packets to a destination simultaneously, which is an
               advantage of IoT applications. 5G presents wide bandwidth, low
               delay, and extremely high data throughput. Predicting network
               traffic has a great influence on IoT networks which results in
               reliable communication. A fully functional 5G network will not
               occur without artificial intelligence (AI) that can learn and
               make decisions on its own. Deep learning has been successfully
               applied to traffic prediction where it promotes traffic
               predictions via powerful fair representation learning. In this
               paper, we perform the prediction of IoT traffic in time series
               using LSTM - deep learning. the prediction accuracy has been
               evaluated using the RMSE as a merit function and mean absolute
               percentage error (MAPE).",
  publisher = "Springer International Publishing",
  pages     = "267--280",
  year      =  2020,
  file      = "All Papers/A/Abdellah and Koucheryavy 2020 - Deep Learning with Long Short-Term Memory for IoT Traffic Prediction.pdf",
  doi       = "10.1007/978-3-030-65726-0\_24"
}

@INPROCEEDINGS{Abraham2004-zb,
  title     = "Routing with Improved {Communication-Space} {Trade-Off}",
  booktitle = "Distributed Computing",
  author    = "Abraham, Ittai and Gavoille, Cyril and Malkhi, Dahlia",
  abstract  = "Given a weighted undirected network with arbitrary node names,
               we present a family of routing schemes characterized by an
               integral parameter $\kappa$ $\geq$ 1. The scheme uses
               $\widetilde\{O\}(n^\{1/\kappa\} \log D)$space routing table at
               each node, and routes along paths of linear stretch O($\kappa$),
               where D is the normalized diameter of the network. When D is
               polynomial in n, the scheme has asymptotically optimal stretch
               factor. With the same memory bound, the best previous results
               obtained stretch O($\kappa$2).",
  publisher = "Springer Berlin Heidelberg",
  pages     = "305--319",
  year      =  2004,
  file      = "All Papers/A/Abraham et al. 2004 - Routing with Improved Communication-Space Trade-Off.pdf",
  keywords  = "ComputerNetworks",
  doi       = "10.1007/978-3-540-30186-8\_22"
}

@INPROCEEDINGS{Schulz2008-qb,
  title     = "Stochastic Online Scheduling Revisited",
  booktitle = "Combinatorial Optimization and Applications",
  author    = "Schulz, Andreas S",
  abstract  = "We consider the problem of minimizing the total weighted
               completion time on identical parallel machines when jobs have
               stochastic processing times and may arrive over time. We give
               randomized as well as deterministic online and off-line
               algorithms that have the best known performance guarantees in
               either setting, deterministic and off-line or randomized and
               online. Our analysis is based on a novel linear programming
               relaxation for stochastic scheduling problems, which can be
               solved online.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "448--457",
  year      =  2008,
  file      = "All Papers/S/Schulz 2008 - Stochastic Online Scheduling Revisited.pdf",
  doi       = "10.1007/978-3-540-85097-7\_42"
}

@INCOLLECTION{Casares-Giner2011-bb,
  title     = "Mobility Models for Mobility Management",
  booktitle = "Network Performance Engineering: A Handbook on Convergent
               {Multi-Service} Networks and Next Generation Internet",
  author    = "Casares-Giner, Vicente and Pla, Vicent and Escalle-Garcia, Pablo",
  editor    = "Kouvatsos, Demetres D",
  abstract  = "The main goals of today's wireless mobile telecommunication
               systems are to provide both, mobility and ubiquity to mobile
               terminals (MTs) with a required quality of service. By ubiquity
               we understand the ability of a MT to be connected to the network
               anytime, anywhere, regardless of the access channel's
               characteristics. In this chapter we deal with mobility aspects.
               We provide some basic background on mobility models that are
               being used in performance evaluation of relevant mobility
               management procedures, such as handover and location update. For
               handover, and consequently for channel holding time, we revisit
               the characterization of the cell residence time. Then, based on
               those previous results, models for the location area residence
               time are built. Cell residence time can be seen as a
               micro-mobility parameter while the latter can be considered as a
               macro-mobility parameter; and both have a significant impact on
               the handover and location update algorithms.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "716--745",
  year      =  2011,
  file      = "All Papers/C/Casares-Giner et al. 2011 - Mobility Models for Mobility Management.pdf",
  address   = "Berlin, Heidelberg",
  keywords  = "Mobility",
  isbn      = "9783642027420",
  doi       = "10.1007/978-3-642-02742-0\_30"
}

@INPROCEEDINGS{Chen2009-gq,
  title     = "Approximating Matches Made in Heaven",
  booktitle = "Proceedings of the 36th International Colloquium on Automata,
               Languages and Programming: Part {I}",
  author    = "Chen, Ning and Immorlica, Nicole and Karlin, Anna R and Mahdian,
               Mohammad and Rudra, Atri",
  abstract  = "Motivated by applications in online dating and kidney exchange,
               we study a stochastic matching problem in which we have a random
               graph G given by a node set V and probabilities p ( i , j ) on
               all pairs i , j *** V representing the probability that edge ( i
               , j ) exists. Additionally, each node has an integer weight t (
               i ) called its patience parameter. Nodes represent agents in a
               matching market with dichotomous preferences, i.e., each agent
               finds every other agent either acceptable or unacceptable and is
               indifferent between all acceptable agents. The goal is to
               maximize the welfare, or produce a matching between acceptable
               agents of maximum size. Preferences must be solicited based on
               probabilistic information represented by p ( i , j ), and agent
               i can be asked at most t ( i ) questions regarding his or her
               preferences.A stochastic matching algorithm iteratively probes
               pairs of nodes i and j with positive patience parameters. With
               probability p ( i , j ), an edge exists and the nodes are
               irrevocably matched. With probability 1 *** p ( i , j ), the
               edge does not exist and the patience parameters of the nodes are
               decremented. We give a simple greedy strategy for selecting
               probes which produces a matching whose cardinality is, in
               expectation, at least a quarter of the size of this optimal
               algorithm's matching. We additionally show that variants of our
               algorithm (and our analysis) can handle more complicated
               constraints, such as a limit on the maximum number of rounds, or
               the number of pairs probed in each round.",
  publisher = "Springer-Verlag",
  pages     = "266--278",
  series    = "ICALP '09",
  month     =  jul,
  year      =  2009,
  file      = "All Papers/C/Chen et al. 2009 - Approximating Matches Made in Heaven.pdf",
  address   = "Berlin, Heidelberg",
  keywords  = "Algorithms\_Theory",
  location  = "Rhodes, Greece",
  isbn      = "9783642029264",
  doi       = "10.1007/978-3-642-02927-1\_23"
}

@INCOLLECTION{Birman2010-zf,
  title     = "A History of the Virtual Synchrony Replication Model",
  booktitle = "Replication: Theory and Practice",
  author    = "Birman, Ken",
  editor    = "Charron-Bost, Bernadette and Pedone, Fernando and Schiper,
               Andr{\'e}",
  abstract  = "In this chapter, we discuss a widely used fault-tolerant data
               replication model called virtual synchrony. The model responds
               to two kinds of needs. First, there is the practical question of
               how best to embed replication into distributed systems. Virtual
               synchrony defines dynamic process groups that have self-managed
               membership. Applications can join or leave groups at will: a
               process group is almost like a replicated variable that lives in
               the network. The second need relates to performance. Although
               state machine replication is relatively easy to understand,
               protocols that implement state machine replication in the
               standard manner are too slow to be useful in demanding settings,
               and are hard to deploy in very large data centers of the sort
               seen in today's cloud-computing environments. Virtual synchrony
               implementations, in contrast, are able to deliver updates at the
               same data rates (and with the same low latencies) as IP
               multicast: the fast (but unreliable) Internet multicast
               protocol, often supported directly by hardware. The trick that
               makes it possible to achieve these very high levels of
               performance is to hide overheads by piggybacking extra
               information on regular messages that carry updates. The virtual
               synchrony replication model has been very widely adopted, and
               was used in everything from air traffic control and stock market
               systems to data center management platforms marketed by
               companies like IBM and Microsoft. Moreover, in recent years,
               state machine protocols such as those used in support of Paxos
               have begun to include elements of the virtual synchrony model,
               such as self-managed and very dynamic membership. Our
               exploration of the model takes the form of a history. We start
               by exploring the background, and then follow evolution of the
               model over time.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "91--120",
  year      =  2010,
  file      = "All Papers/B/Birman 2010 - A History of the Virtual Synchrony Replication Model.pdf",
  address   = "Berlin, Heidelberg",
  isbn      = "9783642112942",
  doi       = "10.1007/978-3-642-11294-2\_6"
}

@MISC{Haeupler2012-tb,
  title   = "Lower Bounds on Information Dissemination in Dynamic Networks",
  author  = "Haeupler, Bernhard and Kuhn, Fabian",
  journal = "Lecture Notes in Computer Science",
  pages   = "166--180",
  year    =  2012,
  file    = "All Papers/H/Haeupler and Kuhn 2012 - Lower Bounds on Information Dissemination in Dynamic Networks.pdf",
  doi     = "10.1007/978-3-642-33651-5\_12"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Meinel2015-kw,
  title     = "Mathematische Grundlagen der Informatik: Mathematisches Denken
               und Beweisen Eine Einf{\"u}hrung",
  author    = "Meinel, Christoph and Mundhenk, Martin",
  abstract  = "Die mathematischen Grundlagen der Informatik werden anhand von
               Definitionen und Beispielen anschaulich eingef{\"u}hrt. Ziel des
               Buches ist es, systematisch die f{\"u}r die Informatik typischen
               und grundlegenden mathematischen Denkweisen vorzustellen -- ohne
               dabei auf besondere, die {\"u}bliche Schulmathematik
               {\"u}bersteigende Vorkenntnisse aufzubauen.",
  publisher = "Springer Fachmedien Wiesbaden",
  month     =  oct,
  year      =  2015,
  file      = "All Papers/M/Meinel and Mundhenk 2015 - Mathematische Grundlagen der Informatik - Mathematisches Denken und Beweisen Eine Einführung.pdf",
  keywords  = "GDS",
  language  = "de",
  isbn      = "9783658098858",
  doi       = "10.1007/978-3-658-09886-5"
}

@ARTICLE{Xiao2020-by,
  title    = "Workload-driven coordination between virtual machine allocation
              and task scheduling",
  author   = "Xiao, Zheng and Wang, Bangyong and Li, Xing and Du, Jiayi",
  abstract = "The current task scheduling is separated from the virtual machine
              (VM) allocation, which, to some extent, wastes resources or
              degrades application performance. The scheduling algorithm
              influences the demand of VMs in terms of service-level agreement,
              while the number of VMs determines the performance of task
              scheduling. Workload plays an indispensable role in both dynamic
              VM allocation and task scheduling. To address this problem, we
              coordinate task scheduling and VM allocation based on workload
              characteristics. Workload is empirically time-varying and
              stochastic. We demonstrate that the acquired workload data set
              has Markov property which can be modeled as a Markov chain. Then,
              three workload characteristic operators are extracted:
              persistence, recurrence and entropy, which quantify the relative
              stability, burstiness, and unpredictability of the workload,
              respectively. Experiments indicate that the persistence and
              recurrence of workloads has a direct bearing on the average
              response time and resource utilization of the system. A nonlinear
              model between the load characteristic operators and the number of
              VMs is established. In order to test the performance of the
              collaborative framework, we design a scheduling algorithm based
              on genetic algorithm (GA), which takes the estimated number of
              VMs as input and the task completion time as the optimization
              target. Simulation experiments have been performed on the
              CloudSim platform, testifying that the estimated average absolute
              VMs error is only 2.6\%. The GA-based task scheduling algorithm
              could improve resource utilization and reduce task completion
              time compared with the first come first serve and greedy
              algorithm. The proposed coordination mechanism in this paper has
              proved able to find the optimal match and reduce the resource
              cost by utilizing the interaction between VM allocation and task
              scheduling.",
  journal  = "Neural Comput. Appl.",
  volume   =  32,
  number   =  10,
  pages    = "5535--5551",
  month    =  may,
  year     =  2020,
  file     = "All Papers/X/Xiao et al. 2020 - Workload-driven coordination between virtual machine allocation and task scheduling.pdf",
  keywords = "MLNetworking",
  issn     = "0941-0643, 1433-3058",
  doi      = "10.1007/s00521-019-04022-1"
}

@ARTICLE{Hernandez-Leal2019-kb,
  title    = "A survey and critique of multiagent deep reinforcement learning",
  author   = "Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E",
  abstract = "Deep reinforcement learning (RL) has achieved outstanding results
              in recent years. This has led to a dramatic increase in the
              number of applications and methods. Recent works have explored
              learning beyond single-agent scenarios and have considered
              multiagent learning (MAL) scenarios. Initial results report
              successes in complex multiagent domains, although there are
              several challenges to be addressed. The primary goal of this
              article is to provide a clear overview of current multiagent deep
              reinforcement learning (MDRL) literature. Additionally, we
              complement the overview with a broader analysis: (i) we revisit
              previous key components, originally presented in MAL and RL, and
              highlight how they have been adapted to multiagent deep
              reinforcement learning settings. (ii) We provide general
              guidelines to new practitioners in the area: describing lessons
              learned from MDRL works, pointing to recent benchmarks, and
              outlining open avenues of research. (iii) We take a more critical
              tone raising practical challenges of MDRL (e.g., implementation
              and computational demands). We expect this article will help
              unify and motivate future research to take advantage of the
              abundant literature that exists (e.g., RL and MAL) in a joint
              effort to promote fruitful research in the multiagent community.",
  journal  = "Auton. Agent. Multi. Agent. Syst.",
  volume   =  33,
  number   =  6,
  pages    = "750--797",
  month    =  nov,
  year     =  2019,
  file     = "All Papers/H/Hernandez-Leal et al. 2019 - A survey and critique of multiagent deep reinforcement learning.pdf",
  issn     = "1387-2532, 1573-7454",
  doi      = "10.1007/s10458-019-09421-1"
}

@ARTICLE{Liao2019-il,
  title    = "{AI-based} software-defined virtual network function scheduling
              with delay optimization",
  author   = "Liao, Dan and Wu, Yulong and Wu, Ziyang and Zhu, Zeyuan and
              Zhang, Wanting and Sun, Gang and Chang, Victor",
  abstract = "AI-based network function virtualization (NFV) is an emerging
              technique that separates network control functionality from
              dedicated hardware middleboxes and is virtualized to reduce
              capital and operational costs. With the advances of NFV and
              AI-based software-defined networks, dynamic network service
              demands can be flexibly and effectively accomplished by
              connecting multiple virtual network functions (VNFs) running on
              virtual machines. However, such promising technology also
              introduces several new research challenges. Due to resource
              constraints, service providers may have to deploy different
              service function chains (SFCs) to share the same physical
              resources. Such sharing inevitably forces the scheduling of the
              SFCs and resources, which consumes computational time and
              introduces problems associated with reducing the response delay.
              In this paper, we address this challenge by developing two
              dynamic priority methods for queuing AI-based VNFs/services to
              improve the user experience. We account for both transmission and
              processing delays in our proposed algorithms and achieve a new
              processing order (scheduler) for VNFs to minimize the overall
              scheduling delay. The simulation results indicate that the
              proposed scheme can promote the performance of AI-based
              VNFs/services to meet strict latency requirements.",
  journal  = "Cluster Comput.",
  volume   =  22,
  number   =  6,
  pages    = "13897--13909",
  month    =  nov,
  year     =  2019,
  file     = "All Papers/L/Liao et al. 2019 - AI-based software-defined virtual network function scheduling with delay optimization.pdf",
  keywords = "NFV;MLNetworking",
  issn     = "1386-7857, 1573-7543",
  doi      = "10.1007/s10586-018-2124-0"
}

@ARTICLE{Hester2013-fb,
  title    = "{TEXPLORE}: real-time sample-efficient reinforcement learning for
              robots",
  author   = "Hester, Todd and Stone, Peter",
  abstract = "The use of robots in society could be expanded by using
              reinforcement learning (RL) to allow robots to learn and adapt to
              new situations online. RL is a paradigm for learning sequential
              decision making tasks, usually formulated as a Markov Decision
              Process (MDP). For an RL algorithm to be practical for robotic
              control tasks, it must learn in very few samples, while
              continually taking actions in real-time. In addition, the
              algorithm must learn efficiently in the face of noise,
              sensor/actuator delays, and continuous state features. In this
              article, we present texplore, the first algorithm to address all
              of these challenges together. texplore is a model-based RL method
              that learns a random forest model of the domain which generalizes
              dynamics to unseen states. The agent explores states that are
              promising for the final policy, while ignoring states that do not
              appear promising. With sample-based planning and a novel parallel
              architecture, texplore can select actions continually in
              real-time whenever necessary. We empirically evaluate the
              importance of each component of texplore in isolation and then
              demonstrate the complete algorithm learning to control the
              velocity of an autonomous vehicle in real-time.",
  journal  = "Mach. Learn.",
  volume   =  90,
  number   =  3,
  pages    = "385--429",
  month    =  mar,
  year     =  2013,
  file     = "All Papers/H/Hester and Stone 2013 - TEXPLORE - real-time sample-efficient reinforcement learning for robots.pdf",
  keywords = "MLNetworking",
  issn     = "0885-6125, 1573-0565",
  doi      = "10.1007/s10994-012-5322-7"
}

@ARTICLE{Dulac-Arnold2021-uz,
  title    = "Challenges of real-world reinforcement learning: definitions,
              benchmarks and analysis",
  author   = "Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J and
              Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd",
  abstract = "Reinforcement learning (RL) has proven its worth in a series of
              artificial domains, and is beginning to show some successes in
              real-world scenarios. However, much of the research advances in
              RL are hard to leverage in real-world systems due to a series of
              assumptions that are rarely satisfied in practice. In this work,
              we identify and formalize a series of independent challenges that
              embody the difficulties that must be addressed for RL to be
              commonly deployed in real-world systems. For each challenge, we
              define it formally in the context of a Markov Decision Process,
              analyze the effects of the challenge on state-of-the-art learning
              algorithms, and present some existing attempts at tackling it. We
              believe that an approach that addresses our set of proposed
              challenges would be readily deployable in a large number of real
              world problems. Our proposed challenges are implemented in a
              suite of continuous control environments called realworldrl-suite
              which we propose an as an open-source benchmark.",
  journal  = "Mach. Learn.",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/D/Dulac-Arnold et al. 2021 - Challenges of real-world reinforcement learning - definitions, benchmarks and analysis.pdf",
  keywords = "MLNetworking;ML",
  issn     = "0885-6125",
  doi      = "10.1007/s10994-021-05961-4"
}

@ARTICLE{Sarker2020-aq,
  title    = "{BehavDT}: A Behavioral Decision Tree Learning to Build
              {User-Centric} {Context-Aware} Predictive Model",
  author   = "Sarker, Iqbal H and Colman, Alan and Han, Jun and Khan, Asif
              Irshad and Abushark, Yoosef B and Salah, Khaled",
  abstract = "This paper formulates the problem of building a context-aware
              predictive model based on user diverse behavioral activities with
              smartphones. In the area of machine learning and data science, a
              tree-like model as that of decision tree is considered as one of
              the most popular classification techniques, which can be used to
              build a data-driven predictive model. The traditional decision
              tree model typically creates a number of leaf nodes as decision
              nodes that represent context-specific rigid decisions, and
              consequently may cause overfitting problem in behavior modeling.
              However, in many practical scenarios within the context-aware
              environment, the generalized outcomes could play an important
              role to effectively capture user behavior. In this paper, we
              propose a behavioral decision tree, ``BehavDT'' context-aware
              model that takes into account user behavior-oriented
              generalization according to individual preference level. The
              BehavDT model outputs not only the generalized decisions but also
              the context-specific decisions in relevant exceptional cases. The
              effectiveness of our BehavDT model is studied by conducting
              experiments on individual user real smartphone datasets. Our
              experimental results show that the proposed BehavDT context-aware
              model is more effective when compared with the traditional
              machine learning approaches, in predicting user diverse behaviors
              considering multi-dimensional contexts.",
  journal  = "Mobile Networks and Applications",
  volume   =  25,
  number   =  3,
  pages    = "1151--1161",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/S/Sarker et al. 2020 - BehavDT - A Behavioral Decision Tree Learning to Build User-Centric Context-Aware Predictive Model.pdf",
  keywords = "GeneralNetworking",
  issn     = "1572-8153",
  doi      = "10.1007/s11036-019-01443-z"
}

@ARTICLE{Kaviri2021-ez,
  title    = "A cooperative control framework of multiple unmanned aerial
              vehicles for dynamic oil spill cleanup",
  author   = "Kaviri, Samane and Tahsiri, Ahmadreza and Taghirad, Hamid D",
  abstract = "Oil spills within the marine environment are undesirable events
              caused by unavoidable economic activities of huge sea lane
              traffics. A tremendous effort has been made to tackle this
              problem within academia and industries; among them, the concept
              of autonomous vehicles for oil spill combating seems to provide a
              promising solution. This paper mainly proposes a cooperative
              deployment framework for unmanned aerial vehicles (UAVs) to
              perform oil spill cleanup missions with dispersant spraying. An
              appropriate oil density function is introduced by applying a
              Gaussian mixture model based on NOAA's advanced oil spill model
              (General NOAA Operational Modeling Environment). UAVs are then
              deployed to cover oil spills for spraying operations. This
              deployment problem is formulated as a coverage problem based on
              centroidal Voronoi tessellation to determine UAVs' optimal
              location. By transforming the coverage control problem into a
              target tracking one, an Integral Terminal Super Twisting Sliding
              Mode Control is provided to drive the UAVs to the optimal
              configuration. Furthermore, a novel spraying adjustment strategy
              is also designed to target the oil spills more accurately at the
              appropriate dosage. The effectiveness of the proposed framework
              is studied through a case study in the Abuzar oil field in the
              Persian Gulf. The results verify the performance of the
              cooperative oil spill cleanup framework in conjunction with
              advanced oil spill modeling. Moreover, it is concluded that the
              oil spill can be more effectively dispersed if the proposed
              spraying adjustment strategy is implemented.",
  journal  = "J. Brazil. Soc. Mech. Sci. Eng.",
  volume   =  43,
  number   =  6,
  pages    = "289",
  month    =  may,
  year     =  2021,
  file     = "All Papers/K/Kaviri et al. 2021 - A cooperative control framework of multiple unmanned aerial vehicles for dynamic oil spill cleanup.pdf",
  issn     = "1678-5878, 1806-3691",
  doi      = "10.1007/s40430-021-03005-5"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bose1960-br,
  title    = "On a class of error correcting binary group codes",
  author   = "Bose, R C and Ray-Chaudhuri, D K",
  abstract = "A general method of constructing error correcting binary group
              codes is obtained. A binary group code with n places, k of which
              are information places is called an (n,k) code. An explicit
              method of constructing t-error correcting (n,k) codes is given
              for n = 2m−1 and k = 2m−1−R(m,t) ≧ 2m−1−mt where R(m,t) is a
              function of m and t which cannot exceed mt. An example is worked
              out to illustrate the method of construction.",
  journal  = "Information and Control",
  volume   =  3,
  number   =  1,
  pages    = "68--79",
  month    =  mar,
  year     =  1960,
  file     = "All Papers/B/Bose and Ray-Chaudhuri 1960 - On a class of error correcting binary group codes.pdf",
  keywords = "ComputerNetworks",
  issn     = "0019-9958",
  doi      = "10.1016/S0019-9958(60)90287-4"
}

@ARTICLE{Zhou2004-ma,
  title    = "{CDS}: a code distribution scheme for active networks",
  author   = "Zhou, Yuezhi and Zhang, Yaoxue and Lu, Jianhua",
  abstract = "Active networking may process packets in a customized way by
              injecting active programs into network elements. How to deliver
              active codes to a network node to perform customized computation
              on packets is quite important and essential in active networks
              (ANs). This paper investigates fundamental problems regarding the
              active code distribution protocol (CDP) [IEEE International
              Conference on Networks (ICON), Bangkok Thailand October (2001)]
              and presents some design considerations for achieving a practical
              CDP, including naming and description method of active
              applications, store and transport of active codes, as well as
              security issues, etc.. Moreover, we present a new common and
              flexible code distribution scheme for ANs, named as code
              distribution scheme (CDS). CDS employs a unique active
              application identifier to name an active application, which maps
              active code through a two-level mapping mechanism based on XML,
              achieving distributed deployment of active codes and application
              composition. In addition, a revised trivial file transfer
              protocol (TFTP) and a digital signature based on public key
              infrastructure (PKI) are introduced into CDS in order to
              accomplish security. An implemented prototype of CDS for
              effectiveness verification shows many advantages over the
              existing CDP, such as its flexible code distribution, composition
              ability, and quite acceptable system performance.",
  journal  = "Comput. Commun.",
  volume   =  27,
  number   =  3,
  pages    = "315--321",
  month    =  feb,
  year     =  2004,
  keywords = "Active networks; Active application; Code distribution protocol;
              Code distribution scheme",
  issn     = "0140-3664",
  doi      = "10.1016/S0140-3664(03)00237-8"
}

@ARTICLE{Karatza2001-bv,
  title    = "Job scheduling in heterogeneous distributed systems",
  author   = "Karatza, Helen D",
  abstract = "This paper investigates scheduling policies in a heterogeneous
              distributed system, where half of the total processors have
              double the speed of the others. Processor performance is examined
              and compared under a variety of workloads. Two job classes are
              considered. Programs of the first class are dedicated to fast
              processors, while second class programs are generic in the sense
              that they can be allocated to any processor. It was our intention
              to find a policy that increases overall system throughput by
              increasing the throughput of the generic jobs without seriously
              degrading performance of the dedicated jobs. However, simulation
              results indicate that each scheduling policy considered has its
              merits and the best performer tended to depend on the degree of
              multiprogramming.",
  journal  = "J. Syst. Softw.",
  volume   =  56,
  number   =  3,
  pages    = "203--212",
  month    =  mar,
  year     =  2001,
  file     = "All Papers/K/Karatza 2001 - Job scheduling in heterogeneous distributed systems.pdf",
  keywords = "Heterogeneous distributed systems; Performance; Scheduling",
  issn     = "0164-1212",
  doi      = "10.1016/S0164-1212(00)00098-4"
}

@BOOK{Ross1993-id,
  title     = "Introduction to Probability Models",
  author    = "Ross, Sheldon M",
  abstract  = "The text is particularly well suited for those wanting to apply
               probability theory to the study of phenomena in fields such as
               engineering, management science, the physical and social
               sciences, and operations research. This fifth edition features
               updated examples and exercises, with an emphasis wherever
               possible on real data.",
  publisher = "Academic Press",
  year      =  1993,
  keywords  = "GDS",
  language  = "en",
  isbn      = "9780125984553",
  doi       = "10.1016/c2013-0-11417-1"
}

@ARTICLE{Jiao2019-ik,
  title    = "A new approach to oil spill detection that combines deep learning
              with unmanned aerial vehicles",
  author   = "Jiao, Zeyu and Jia, Guozhu and Cai, Yingjie",
  abstract = "This study presents a novel approach to automatic oil spill
              detection, using unmanned aerial vehicle (UAV) images to realize
              intelligent control in oil production. Despite considerable
              effort, oil spills still cannot be detected automatically and
              effectively due to the complexity of the real production
              environment, which forces oil enterprises to manually inspect
              facilities and detect oil spills. To solve the problem, we
              propose an approach consisting of UAVs, deep learning and
              traditional algorithms---an approach which divides the oil spill
              detection task into three independent sub-tasks. First, we
              constructed a model based on the deep convolutional neural
              network, which can quickly detect the suspected oil spill area in
              images to ensure there are no omissions. Second, to remove other
              obstacles in the images, we adjusted the Otsu algorithm to filter
              the detection results, which improves precision while not
              affecting the recall rate. Third, the Maximally Stable Extremal
              Regions algorithm was used to obtain the detail polygon region
              from the detection box, thus automatically evaluating the
              severity of the oil spill. Experiments showed that our method
              could solve problems effectively, reducing the cost of oil spill
              detection by 57.2\% when compared with the traditional manual
              inspection process.",
  journal  = "Comput. Ind. Eng.",
  volume   =  135,
  pages    = "1300--1311",
  month    =  sep,
  year     =  2019,
  keywords = "Oil spill; Unmanned aerial vehicle; Deep convolutional neural
              network; Otsu algorithm; Maximally stable extremal regions;
              Intelligent control",
  issn     = "0360-8352",
  doi      = "10.1016/j.cie.2018.11.008"
}

@ARTICLE{Babarczi2020-mi,
  title    = "A mathematical framework for measuring network flexibility",
  author   = "Babarczi, P{\'e}ter and Kl{\"u}gel, Markus and Mart{\'\i}nez
              Alba, Alberto and He, Mu and Zerwas, Johannes and Kalmbach,
              Patrick and Blenk, Andreas and Kellerer, Wolfgang",
  abstract = "In the field of networking research, increased flexibility of new
              system architecture proposals, protocols, or algorithms is often
              stated to be a competitive advantage over its existing
              counterparts. However, this advantage is usually claimed only on
              an argumentative level and neither formally supported nor
              thoroughly investigated due to the lack of a unified flexibility
              framework. As we will show in this paper, the flexibility
              achieved by a system implementation can be measured, which
              consequently can be used to make different networking solutions
              quantitatively comparable with each other. The idea behind our
              mathematical model is to relate network flexibility to the
              achievable subset of the set of all possible demand changes, and
              to use measure theory to quantify it. As increased flexibility
              might come with additional system complexity and cost, our
              framework provides a cost model which measures how expensive it
              is to operate a flexible system. The introduced flexibility
              framework contains different normalization strategies to provide
              intuitive meaning to the network flexibility value as well, and
              also provides guidelines for generating demand changes with
              (non-)uniform demand utilities. Finally, our network flexibility
              framework is applied on two different use-cases, and the benefits
              of a quantitative flexibility analysis compared to pure intuitive
              arguments are demonstrated.",
  journal  = "Comput. Commun.",
  volume   =  164,
  pages    = "13--24",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/B/Babarczi et al. 2020 - A mathematical framework for measuring network flexibility.pdf",
  keywords = "Network flexibility; Cost of flexibility; Measure theory; Demand
              changes; Softwarized networks;GeneralNetworking",
  issn     = "0140-3664",
  doi      = "10.1016/j.comcom.2020.09.014"
}

@ARTICLE{Zheng2020-xy,
  title    = "{5G} network-oriented hierarchical distributed cloud computing
              system resource optimization scheduling and allocation",
  author   = "Zheng, Guang and Zhang, Hao and Li, Yanling and Xi, Lei",
  abstract = "As the core technology of the next generation mobile
              communication system, the development of 5G key technologies
              needs to be able to efficiently and effectively support massive
              data services. Aiming at the impact of massive data traffic on
              mobile communication networks in 5G communication systems, this
              paper proposes a 5G-oriented hierarchical distributed cloud
              service mobile communication system architecture. The model
              consists of a cloud access layer, a distributed micro-cloud
              system, and a core cloud data center. The distributed micro cloud
              system consists of multiple micro clouds that are deployed to the
              edge of the network. The service content in the core cloud data
              center can be deployed and cached to the local micro cloud server
              in advance to reduce repeated redundant transmission of user
              requested content in the network. Aiming at the problem of how to
              determine the migration object when dynamically optimizing the
              resource structure, a heuristic function-based dynamic
              optimization algorithm for cloud resources is proposed. The
              experimental results show that the dynamic expansion algorithm of
              cloud resources based on dynamic programming ideas can better
              improve the performance of virtual resources, and the dynamic
              optimization algorithm of cloud resources based on heuristic
              functions can effectively and quickly optimize the resource
              structure, thereby improving the operating efficiency of user
              virtual machine groups. An efficient resource allocation scheme
              based on cooperative Q (Quality) learning is proposed. The
              environmental knowledge obtained by the base station learning and
              exchanging information is used for distributed resource block
              allocation. This resource allocation scheme can obtain the
              optimal resource allocation strategy in a short learning time,
              and can terminate the learning process at any time according to
              the delay requirements of different services. Compared with
              traditional resource allocation schemes, it can effectively
              improve system throughput.",
  journal  = "Comput. Commun.",
  volume   =  164,
  pages    = "88--99",
  month    =  dec,
  year     =  2020,
  keywords = "5G network; Cloud computing; Dynamic resource scheduling;
              Resource allocation;EdgeFogCloudIoT",
  issn     = "0140-3664",
  doi      = "10.1016/j.comcom.2020.10.005"
}

@ARTICLE{Miles2020-xc,
  title    = "A study of {LoRaWAN} protocol performance for {IoT} applications
              in smart agriculture",
  author   = "Miles, Badreddine and Bourennane, El-Bay and Boucherkha, Samia
              and Chikhi, Salim",
  abstract = "The use of Internet of Things (IoT) is becoming increasingly
              common in agribusiness to increase food production capacity for
              the expanding global population. Recently, low-power wide-area
              networks (LPWANs) have been used in the development of IoT
              applications that require low power consumption and low data
              transmission rates. LoRaWAN is considered the most suitable
              communication network for LPWANs for IoT applications in smart
              agriculture. In this paper, we present an in-depth study of the
              performance of the LoRaWAN communication network in the context
              of an IoT application for a pilot farm. We consider several
              scenarios and analyze simulation results by using Network
              Simulator 3. We then propose a mathematical model that precisely
              predicts the successful packet delivery rate for this type of
              network considering the number of nodes and the transmission
              interval duration. Finally, we validate the results of our model
              by comparing them with other simulation results under different
              scenarios.",
  journal  = "Comput. Commun.",
  volume   =  164,
  pages    = "148--157",
  month    =  dec,
  year     =  2020,
  keywords = "Internet of Things (IoT); LoRa; LoRaWAN; Modeling; Network
              Simulator 3 (NS-3); Performance; Simulation; Smart agriculture",
  issn     = "0140-3664",
  doi      = "10.1016/j.comcom.2020.10.009"
}

@ARTICLE{Zaidi2021-ux,
  title    = "Internet of Flying Things ({IoFT)}: A Survey",
  author   = "Zaidi, Sofiane and Atiquzzaman, Mohammed and Calafate, Carlos T",
  abstract = "Unmanned Aerial Vehicles (UAVs) have recently received
              significant attention by the civilian and military community,
              mostly due to the fast growth of UAV technologies supported by
              wireless communications and networking. UAVs can be used to
              improve the efficiency and performance of the Internet of Things
              (IoT) in terms of connectivity, coverage, reliability, stability,
              etc. In particular, to support IoT applications in an efficient
              manner, UAVs should be organized as a Flying Ad-hoc NETwork
              (FANET). FANET is a subclass of Mobile Ad-hoc Network (MANET)
              where nodes are Unmanned Artifact Systems (UAS). However, the
              deployment of UAVs in IoT is limited by several constraints, such
              as limited resource capacity of UAVs and ground devices, signal
              collision and interference, intermittent availability of the IoT
              infrastructure, etc. In the Internet of Flying Things (IoFT)
              literature, there are no survey or study that exhaustively covers
              and discusses all key concepts and recent works on IoFT. In this
              paper a comprehensive survey on the IoFT is presented, covering
              the state of the art in flying things with a focus on IoFT. A
              taxonomy of related literature on IoFT is proposed, including a
              classification, description and comparative study of different
              work on IoFT. Furthermore, the paper presents IoFT applications,
              IoFT challenges and future perspectives. This survey aims to
              provide the basic concepts and a complete overview of the recent
              studies on IoFT for the scientific researchers.",
  journal  = "Comput. Commun.",
  volume   =  165,
  pages    = "53--74",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/Z/Zaidi et al. 2021 - Internet of Flying Things (IoFT) - A Survey.pdf",
  keywords = "Internet of Flying Things; Unmanned Aerial Vehicle; Unmanned
              Artifact System; Internet of Things; Flying Ad-hoc NETwork;5G6G",
  issn     = "0140-3664",
  doi      = "10.1016/j.comcom.2020.10.023"
}

@ARTICLE{Huang2021-gc,
  title    = "Market-based dynamic resource allocation in Mobile Edge Computing
              systems with multi-server and multi-user",
  author   = "Huang, Xiaowen and Zhang, Wenjie and Yang, Jingmin and Yang,
              Liwei and Yeo, Chai Kiat",
  abstract = "Mobile Edge Computing (MEC) is critical to the development of the
              Internet of things (IoTs) and 5G networks. However, the
              computation and communication resources of edge servers are
              limited, so it is challenging to perform resource allocation
              especially when the competition among edge servers is also taken
              into consideration. In this paper, we propose a trading model to
              investigate both the computation and communication resources
              allocation in MEC systems with multi-server and multi-user. We
              model the dynamic behavior of mobile users (MUs) using an
              evolutionary game, and then we build the deterministic and
              stochastic models to study the evolution of MUs where the
              evolutionary equilibrium is considered as the solution. We
              propose an evolution algorithm to obtain the evolutionary
              equilibrium. Furthermore, we analyze the competition among edge
              cloud servers (ECSs) by a noncooperative game, and propose an
              iteration algorithm to obtain Nash equilibrium where the ECSs can
              adjust the amount of resources provided to MUs and the
              corresponding price charged in order to attract more MUs. The
              existences of evolutionary equilibrium and Nash equilibrium are
              validated in performance evaluation.",
  journal  = "Comput. Commun.",
  volume   =  165,
  pages    = "43--52",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/H/Huang et al. 2021 - Market-based dynamic resource allocation in Mobile Edge Computing systems with multi-server and multi-user.pdf",
  keywords = "Resource allocation; Mobile Edge Computing; Evolutionary game;
              Noncooperative game;EdgeFogCloudIoT",
  issn     = "0140-3664",
  doi      = "10.1016/j.comcom.2020.11.001"
}

@ARTICLE{Chen2021-uh,
  title    = "Long-term optimization for {MEC-enabled} {HetNets} with
              device--edge--cloud collaboration",
  author   = "Chen, Long and Wu, Jigang and Zhang, Jun",
  abstract = "For effective computation offloading with multi-access edge
              computing (MEC), both communication and computation resources
              should be properly managed, considering the dynamics of mobile
              users such as the time-varying demands and user mobility. Most
              existing works regard the remote cloud server as a special edge
              server. However, service quality cannot be met when some of the
              edge servers cannot be connected. Besides, the computation
              capability of the cloud has not been fully exploited especially
              when edge servers are congested. We develop an on-line offloading
              decision and computational resource management algorithm with
              joint consideration of collaborations between device--cloud,
              edge--edge and edge--cloud. The objective is to minimize the
              total energy consumption of the system, subject to computational
              capability and task buffer stability constraints. Lyapunov
              optimization technique is used to jointly deal with the
              delay-energy trade-off optimization and load balancing. The
              optimal CPU-cycle frequencies, best transmission powers and
              offloading scheduling policies are jointly handled in the
              three-layer system. Extensive simulation results demonstrate
              that, with V varies in [0.1,5]$\times$109, the proposed algorithm
              can save more than 50\% energy and over 120\% task processing
              time than three existing benchmark algorithms averagely.",
  journal  = "Comput. Commun.",
  volume   =  166,
  pages    = "66--80",
  month    =  jan,
  year     =  2021,
  keywords = "Long-term; Offloading; Edge computing; HetNet; Lyapunov;
              Collaboration;5G6G",
  issn     = "0140-3664",
  doi      = "10.1016/j.comcom.2020.11.011"
}

@ARTICLE{Touch2011-xo,
  title     = "A Dynamic Recursive Unified Internet Design ({DRUID})",
  author    = "Touch, Joe and Baldine, Ilia and Dutta, Rudra and Finn, Gregory
               G and Ford, Bryan and Jordan, Scott and Massey, Dan and Matta,
               Abraham and Papadopoulos, Christos and Reiher, Peter and
               Rouskas, George",
  abstract  = "The Dynamic Recursive Unified Internet Design (DRUID) is a
               future Internet design that unifies overlay networks with
               conventional layered network architectures. DRUID is based on
               the fundamental concept of recursion, enabling a simple and
               direct network architecture that unifies the data, control,
               management, and security aspects of the current Internet,
               leading to a more trustworthy network. DRUID's architecture is
               based on a single recursive block that can adapt to support a
               variety of communication functions, including parameterized
               mechanisms for hard/soft state, flow and congestion control,
               sequence control, fragmentation and reassembly, compression,
               encryption, and error recovery. This recursion is guided by the
               structure of a graph of translation tables that help
               compartmentalize the scope of various functions and identifier
               spaces, while relating these spaces for resource discovery,
               resolution, and routing. The graph also organizes persistent
               state that coordinates behavior between individual data events
               (e.g., coordinating packets as a connection), among different
               associations (e.g., between connections), as well as helping
               optimize the recursive discovery process through caching, and
               supporting prefetching and distributed pre-coordination. This
               paper describes the DRUID architecture composed of these three
               parts (recursive block, translation tables, persistent state),
               and highlights its goals and benefits, including unifying the
               data, control, management, and security planes currently
               considered orthogonal aspects of network architecture.",
  journal   = "Comput. Netw.",
  publisher = "Elsevier North-Holland, Inc.",
  volume    =  55,
  number    =  4,
  pages     = "919--935",
  month     =  mar,
  year      =  2011,
  file      = "All Papers/T/Touch et al. 2011 - A Dynamic Recursive Unified Internet Design (DRUID).pdf",
  address   = "USA",
  keywords  = "Network architecture, Dynamic stacks, Recursive networks, Future
               internet;ServicesDescription",
  issn      = "1389-1286",
  doi       = "10.1016/j.comnet.2010.12.016"
}

@ARTICLE{Lin2018-yy,
  title    = "Flow-level traffic model for adaptive streaming services in
              mobile networks",
  author   = "Lin, Yu-Ting and Bonald, Thomas and Elayoubi, Salah Eddine",
  abstract = "This paper proposes a traffic model for mobile networks carrying
              adaptive streaming. The proposed model takes into account the
              flow dynamics and some application-level parameters influencing
              the performance of adaptive streaming, such as the presence of a
              playout buffer, chunk duration configuration and the video bit
              rate configuration. Paper shows based on flow-level queueing
              model how to compute performance metrics for a streaming user
              such as average video bit rate, average service time, average
              deficit rate, defined as the probability of having its
              instantaneous throughput lower than the chosen video bit rate and
              average buffer surplus, related to the amount of video
              accumulated in the buffer. Since heterogeneous radio conditions
              and the coexistence of elastic traffic and streaming services
              make the exact solution intractable, we propose a simple yet
              accurate approximation that reduces the computational complexity
              and makes system dimensioning easier for mobile network
              operators. Our numerical results show the performance impacts of
              video chunk duration and amount of available video bit rate.
              These performance impacts obtained by our models give an insight
              for operators to properly dimension their adaptive streaming
              service by well configuring both HTTP-level and MAC-level
              parameters.",
  journal  = "Computer Networks",
  volume   =  137,
  pages    = "1--16",
  month    =  jun,
  year     =  2018,
  file     = "All Papers/L/Lin et al. 2018 - Flow-level traffic model for adaptive streaming services in mobile networks.pdf",
  keywords = "Video quality-of-Experience; Mobile networks; Adaptive streaming;
              Flow-level dynamics; Video chunk duration;NetworkTraffic",
  issn     = "1389-1286",
  doi      = "10.1016/j.comnet.2018.01.027"
}

@ARTICLE{Bonati2023-xs,
  title    = "{OpenRAN} Gym: {AI/ML} development, data collection, and testing
              for {O-RAN} on {PAWR} platforms",
  author   = "Bonati, Leonardo and Polese, Michele and D'Oro, Salvatore and
              Basagni, Stefano and Melodia, Tommaso",
  abstract = "Open Radio Access Network (RAN) architectures will enable
              interoperability, openness and programmable data-driven control
              in next generation cellular networks. However, developing and
              testing efficient solutions that generalize across heterogeneous
              cellular deployments and scales, and that optimize network
              performance in such diverse environments is a complex task that
              is still largely unexplored. In this paper, we present OpenRAN
              Gym, a unified, open, and O-RAN-compliant experimental toolbox
              for data collection, design, prototyping and testing of
              end-to-end data-driven control solutions for next generation Open
              RAN systems. OpenRAN Gym extends and combines into a unique
              solution several software frameworks for data collection of RAN
              statistics and RAN control, and a lightweight O-RAN
              near-real-time RAN Intelligent Controller (RIC) tailored to run
              on experimental wireless platforms. We first provide an overview
              of the various architectural components of OpenRAN Gym and
              describe how it is used to collect data and design, train and
              test artificial intelligence and machine learning O-RAN-compliant
              applications (xApps) at scale. We then describe in detail how to
              test the developed xApps on softwarized RANs and provide an
              example of two xApps developed with OpenRAN Gym that are used to
              control a network with 7 base stations and 42 users deployed on
              the Colosseum testbed. Finally, we show how solutions developed
              with OpenRAN Gym on Colosseum can be exported to real-world,
              heterogeneous wireless platforms, such as the Arena testbed and
              the POWDER and COSMOS platforms of the PAWR program. OpenRAN Gym
              and its software components are open-source and
              publicly-available to the research community. By guiding the
              readers from instantiating the components of OpenRAN Gym, to
              running experiments in a softwarized RAN with an O-RAN-compliant
              near-RT RIC and xApps, we aim at providing a key reference for
              researchers and practitioners working on experimental Open RAN
              systems.",
  journal  = "Computer Networks",
  volume   =  220,
  pages    = "109502",
  month    =  jan,
  year     =  2023,
  file     = "All Papers/B/Bonati et al. 2023 - OpenRAN Gym - AI - ML development, data collection, and testing for O-RAN on PAWR platforms.pdf",
  keywords = "5G/6G; O-RAN; AI/ML",
  issn     = "1389-1286",
  doi      = "10.1016/j.comnet.2022.109502"
}

@ARTICLE{Pacini2014-vt,
  title    = "Distributed job scheduling based on Swarm Intelligence: A survey",
  author   = "Pacini, Elina and Mateos, Cristian and Garc{\'\i}a Garino, Carlos",
  abstract = "Scientists and engineers need computational power to satisfy the
              increasing resource intensive nature of their simulations. For
              example, running Parameter Sweep Experiments (PSE) involve
              processing many independent jobs, given by multiple initial
              configurations (input parameter values) against the same program
              code. Hence, paradigms like Grid Computing and Cloud Computing
              are employed for gaining scalability. However, job scheduling in
              Grid and Cloud environments represents a difficult issue since it
              is basically NP-complete. Thus, many variants based on
              approximation techniques, specially those from Swarm Intelligence
              (SI), have been proposed. These techniques have the ability of
              searching for problem solutions in a very efficient way. This
              paper surveys SI-based job scheduling algorithms for bag-of-tasks
              applications (such as PSEs) on distributed computing
              environments, and uniformly compares them based on a derived
              comparison framework. We also discuss open problems and future
              research in the area.",
  journal  = "Comput. Electr. Eng.",
  volume   =  40,
  number   =  1,
  pages    = "252--269",
  month    =  jan,
  year     =  2014,
  file     = "All Papers/P/Pacini et al. 2014 - Distributed job scheduling based on Swarm Intelligence - A survey.pdf",
  issn     = "0045-7906",
  doi      = "10.1016/j.compeleceng.2013.11.023"
}

@ARTICLE{Pontzer2022-bf,
  title    = "Balancing growth, reproduction, maintenance, and activity in
              evolved energy economies",
  author   = "Pontzer, Herman and McGrosky, Amanda",
  abstract = "Economic models predominate in life history research, which
              investigates the allocation of an organism's resources to growth,
              reproduction, and maintenance. These approaches typically employ
              a heuristic Y model of resource allocation, which predicts
              trade-offs among tasks within a fixed budget. The common currency
              among tasks is not always specified, but most models imply that
              metabolic energy, either from food or body stores, is the
              critical resource. Here, we review the evidence for metabolic
              energy as the common currency of growth, reproduction, and
              maintenance, focusing on studies in humans and other vertebrates.
              We then discuss the flow of energy to competing physiological
              tasks (physical activity, maintenance, and reproduction or
              growth) and its effect on life history traits. We propose a
              $\Psi$ model of energy flow to these tasks, which provides an
              integrative framework for examining the influence of
              environmental factors and the expansion and contraction of energy
              budgets in the evolution of life history strategies.",
  journal  = "Curr. Biol.",
  volume   =  32,
  number   =  12,
  pages    = "R709--R719",
  month    =  jun,
  year     =  2022,
  file     = "All Papers/P/Pontzer and McGrosky 2022 - Balancing growth, reproduction, maintenance, and activity in evolved energy economies.pdf",
  language = "en",
  issn     = "0960-9822, 1879-0445",
  pmid     = "35728556",
  doi      = "10.1016/j.cub.2022.05.018"
}

@MISC{Liu2022-ho,
  title    = "The {SOLIDS} {6G} Mobile Network Architecture: Driving Forces,
              Features, and Functional Topology",
  author   = "Liu, Guangyi and Li, Na and Deng, Juan and Wang, Yingying and
              Sun, Junshuai and Huang, Yuhong",
  journal  = "Engineering",
  volume   =  8,
  pages    = "42--59",
  year     =  2022,
  file     = "All Papers/L/Liu et al. 2022 - The SOLIDS 6G Mobile Network Architecture - Driving Forces, Features, and Functional Topology.pdf",
  keywords = "5G6G",
  doi      = "10.1016/j.eng.2021.07.013"
}

@ARTICLE{Yao2020-mp,
  title    = "Joint optimization of function mapping and preemptive scheduling
              for service chains in network function virtualization",
  author   = "Yao, Hong and Xiong, Muzhou and Li, Hui and Gu, Lin and Zeng,
              Deze",
  abstract = "The idea of Network Function Virtualization (NFV) is to decouple
              of network functions from dedicated hardwares to obtain higher
              flexibility in terms of network management and maintenance.
              Although the problem of function placement and scheduling in NFV
              has drawn much attention in recent years, existing studies only
              consider the precedence constraints between network functions
              within a service but lacking for the interactions among services.
              In this paper, under the premise of satisfying the precedence
              conditions of forming the service chain, we study how to minimize
              the completion time of the whole system through efficient mapping
              and preemptive scheduling of functions for multiple service
              chains. We firstly model this issue as an Integer Linear
              Programming (ILP) problem. To avoid the computational complexity
              of ILP, an online preemptive algorithm is designed. Extensive
              simulations are conducted to validate the effectiveness of the
              proposed algorithm. The simulation results indicate that our
              algorithm is suitable for placement and scheduling of functions
              for multiple service chains in NFV.",
  journal  = "Future Gener. Comput. Syst.",
  volume   =  108,
  pages    = "1112--1118",
  month    =  jul,
  year     =  2020,
  keywords = "Network function virtualization; Function mapping; Preemptive
              scheduling; Service chain;NFV",
  issn     = "0167-739X",
  doi      = "10.1016/j.future.2017.12.021"
}

@ARTICLE{Khan2019-fe,
  title     = "Edge computing: A survey",
  author    = "Khan, Wazir Zada and Ahmed, Ejaz and Hakak, Saqib and Yaqoob,
               Ibrar and Ahmed, Arif",
  abstract  = "In recent years, the Edge computing paradigm has gained
               considerable popularity in academic and industrial circles. It
               serves as a key enabler for many future technologies like 5G,
               Internet of Things (IoT), augmented reality and
               vehicle-to-vehicle communications by connecting cloud computing
               facilities and services to the end users. The Edge computing
               paradigm provides low latency, mobility, and location awareness
               support to delay-sensitive applications. Significant research
               has been carried out in the area of Edge computing, which is
               reviewed in terms of latest developments such as Mobile Edge
               Computing, Cloudlet, and Fog computing, resulting in providing
               researchers with more insight into the existing solutions and
               future applications. This article is meant to serve as a
               comprehensive survey of recent advancements in Edge computing
               highlighting the core applications. It also discusses the
               importance of Edge computing in real life scenarios where
               response time constitutes the fundamental requirement for many
               applications. The article concludes with identifying the
               requirements and discuss open research challenges in Edge
               computing.",
  journal   = "Future Gener. Comput. Syst.",
  publisher = "North-Holland",
  volume    =  97,
  pages     = "219--235",
  month     =  aug,
  year      =  2019,
  file      = "All Papers/K/Khan et al. 2019 - Edge computing - A survey.pdf",
  keywords  = "Mobile edge computing; Edge computing; Cloudlets; Fog computing;
               Micro clouds; Cloud computing;EdgeFogCloudIoT;FutureInternet",
  issn      = "0167-739X",
  doi       = "10.1016/j.future.2019.02.050"
}

@ARTICLE{Amiri2017-ql,
  title    = "Survey on prediction models of applications for resources
              provisioning in cloud",
  author   = "Amiri, Maryam and Mohammad-Khanli, Leyli",
  abstract = "According to the dynamic nature of cloud and the rapid growth of
              the resources demand in it, the resource provisioning is one of
              the challenging problems in the cloud environment. The resources
              should be allocated dynamically according to the demand changes
              of the application. Over-provisioning increases energy wasting
              and costs. On the other hand, under-provisioning causes Service
              Level Agreements (SLA) violation and Quality of Service (QoS)
              dropping. Therefore the allocated resources should be close to
              the current demand of applications as much as possible.
              Furthermore, the speed of response to the workload changes to
              achieve the desired performance level is a critical issue for
              cloud elasticity. For this purpose, the future demand of
              applications should be determined. Thus, the prediction of the
              application in different aspects (workload, performance) is an
              essential step before the resource provisioning. According to the
              prediction results, the sufficient resources are allocated to the
              applications in the appropriate time in a way that QoS is ensured
              and SLA violation is avoided. This paper reviews the state of the
              art application prediction methods in different aspects. Through
              a meticulous literature review of the state of the art
              application prediction schemes, a taxonomy for the application
              prediction models is presented that investigates main
              characteristics and challenges of the different models. Finally,
              open research issues and future trends of the application
              prediction are discussed.",
  journal  = "Journal of Network and Computer Applications",
  volume   =  82,
  pages    = "93--113",
  month    =  mar,
  year     =  2017,
  keywords = "Cloud Computing; Prediction; Application; Workload; Resources
              Provisioning;EdgeFogCloudIoT",
  issn     = "1084-8045",
  doi      = "10.1016/j.jnca.2017.01.016"
}

@ARTICLE{Silva_Filho2018-lr,
  title    = "Approaches for optimizing virtual machine placement and migration
              in cloud environments: A survey",
  author   = "Silva Filho, Manoel C and Monteiro, Claudio C and In{\'a}cio,
              Pedro R M and Freire, M{\'a}rio M",
  abstract = "Cloud computing is a model for providing computing resources as a
              utility which faces several challenges on management of
              virtualized resources. Accordingly, virtual machine placement and
              migration are crucial to achieve multiple and conflicting goals.
              Regarding the complexity of these tasks and plethora of existing
              proposals, this work surveys the state-of-the-art in the area. It
              presents a cloud computing background, a review of several
              proposals, a discussion of problem formulations, advantages and
              shortcomings of reviewed works. Furthermore, it highlights the
              challenges for new solutions and provides several open issues,
              showing the relevancy of the topic in an increasing and demanding
              market.",
  journal  = "J. Parallel Distrib. Comput.",
  volume   =  111,
  pages    = "222--250",
  month    =  jan,
  year     =  2018,
  keywords = "Cloud computing; Resource allocation; VM migration; VM placement;
              Server consolidation; Service Level Agreement (SLA)",
  issn     = "0743-7315",
  doi      = "10.1016/j.jpdc.2017.08.010"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Garcia-Martin2019-cz,
  title    = "Estimation of energy consumption in machine learning",
  author   = "Garc{\'\i}a-Mart{\'\i}n, Eva and Rodrigues, Crefeda Faviola and
              Riley, Graham and Grahn, H{\aa}kan",
  abstract = "Energy consumption has been widely studied in the computer
              architecture field for decades. While the adoption of energy as a
              metric in machine learning is emerging, the majority of research
              is still primarily focused on obtaining high levels of accuracy
              without any computational constraint. We believe that one of the
              reasons for this lack of interest is due to their lack of
              familiarity with approaches to evaluate energy consumption. To
              address this challenge, we present a review of the different
              approaches to estimate energy consumption in general and machine
              learning applications in particular. Our goal is to provide
              useful guidelines to the machine learning community giving them
              the fundamental knowledge to use and build specific energy
              estimation methods for machine learning algorithms. We also
              present the latest software tools that give energy estimation
              values, together with two use cases that enhance the study of
              energy consumption in machine learning.",
  journal  = "J. Parallel Distrib. Comput.",
  volume   =  134,
  pages    = "75--88",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/G/García-Martín et al. 2019 - Estimation of energy consumption in machine learning.pdf",
  keywords = "Machine learning; GreenAI; Energy consumption; Deep learning;
              High performance computing",
  issn     = "0743-7315",
  doi      = "10.1016/j.jpdc.2019.07.007"
}

@ARTICLE{Nance_Hall2021-ym,
  title    = "A Survey of Reconfigurable Optical Networks",
  author   = "Nance Hall, Matthew and Foerster, Klaus-Tycho and Schmid, Stefan
              and Durairajan, Ramakrishnan",
  abstract = "Reconfigurable optical networks have emerged as a promising
              technology to efficiently serve the fast-growing traffic produced
              by the digital society. This paper provides a survey of the
              field. We first review enabling optical hardware technologies in
              general and then consider technologies that are specific to data
              center networks and wide-area networks in more detail. We further
              provide an overview of the cost models used in the literature as
              well as the algorithmic problems introduced by these
              technologies, their first solutions, and discuss systems and
              implementation aspects. We conclude with a discussion of open
              challenges.",
  journal  = "Optical Switching and Networking",
  volume   =  41,
  pages    = "100621",
  month    =  sep,
  year     =  2021,
  file     = "All Papers/N/Nance Hall et al. 2021 - A Survey of Reconfigurable Optical Networks.pdf",
  keywords = "Reconfigurable optical networks; Data center networks; Wide area
              networks; Networked Systems; Algorithms;FutureInternet",
  issn     = "1573-4277",
  doi      = "10.1016/j.osn.2021.100621"
}

@ARTICLE{Desislavov2023-co,
  title    = "Trends in {AI} inference energy consumption: Beyond the
              performance-vs-parameter laws of deep learning",
  author   = "Desislavov, Radosvet and Mart{\'\i}nez-Plumed, Fernando and
              Hern{\'a}ndez-Orallo, Jos{\'e}",
  abstract = "The progress of some AI paradigms such as deep learning is said
              to be linked to an exponential growth in the number of
              parameters. There are many studies corroborating these trends,
              but does this translate into an exponential increase in energy
              consumption? In order to answer this question we focus on
              inference costs rather than training costs, as the former account
              for most of the computing effort, solely because of the
              multiplicative factors. Also, apart from algorithmic innovations,
              we account for more specific and powerful hardware (leading to
              higher FLOPS) that is usually accompanied with important energy
              efficiency optimisations. We also move the focus from the first
              implementation of a breakthrough paper towards the consolidated
              version of the techniques one or two year later. Under this
              distinctive and comprehensive perspective, we analyse relevant
              models in the areas of computer vision and natural language
              processing: for a sustained increase in performance we see a much
              softer growth in energy consumption than previously anticipated.
              The only caveat is, yet again, the multiplicative factor, as
              future AI increases penetration and becomes more pervasive.",
  journal  = "Sustainable Computing: Informatics and Systems",
  volume   =  38,
  pages    = "100857",
  month    =  apr,
  year     =  2023,
  file     = "All Papers/D/Desislavov et al. 2023 - Trends in AI inference energy consumption - Beyond the performance-vs-parameter laws of deep learning.pdf",
  keywords = "Artificial Intelligence; Deep learning; Inference; Energy
              consumption; Performance analysis; Performance evaluation; AI
              progress",
  issn     = "2210-5379",
  doi      = "10.1016/j.suscom.2023.100857"
}

@ARTICLE{Nomikos2020-un,
  title    = "A {UAV-based} moving {5G} {RAN} for massive connectivity of
              mobile users and {IoT} devices",
  author   = "Nomikos, Nikolaos and Michailidis, Emmanouel T and Trakadas,
              Panagiotis and Vouyioukas, Demosthenes and Karl, Holger and
              Martrat, Josep and Zahariadis, Theodore and Papadopoulos,
              Konstantinos and Voliotis, Stamatis",
  abstract = "Currently, the coexistence of multiple users and devices
              challenges the network's ability to reliably connect them. This
              article proposes a novel communication architecture that
              satisfies the requirements of fifth-generation (5G) mobile
              network applications. In particular, this architecture extends
              and combines ultra-dense networking (UDN), multi-access edge
              computing (MEC), and virtual infrastructure manager (VIM)
              concepts to provide a flexible network of moving radio access
              (RA) nodes, flying or moving to areas where users and devices
              struggle for connectivity and data rate. Furthermore, advances in
              radio communications and non-orthogonal multiple access (NOMA),
              virtualization technologies and energy-awareness mechanisms are
              integrated towards a mobile UDN that not only allows RA nodes to
              follow the user but also enables the virtualized network
              functions (VNFs) to adapt to user mobility by migrating from one
              node to another. Performance evaluation shows that the underlying
              network improves connectivity of users and devices through the
              flexible deployment of moving RA nodes and the use of NOMA.",
  journal  = "Vehicular Communications",
  volume   =  25,
  pages    = "100250",
  month    =  oct,
  year     =  2020,
  keywords = "UAVs; NOMA; Relays; Resource allocation; Moving cells; Network
              architecture",
  issn     = "2214-2096",
  doi      = "10.1016/j.vehcom.2020.100250"
}

@ARTICLE{Searle1980-tg,
  title     = "Minds, brains, and programs",
  author    = "Searle, John R",
  abstract  = "This article can be viewed as an attempt to explore the
               consequences of two propositions. (1) Intentionality in human
               beings (and animals) is a product of causal features of the
               brain. I assume this is an empirical fact about the actual
               causal relations between mental processes and brains. It says
               simply that certain brain processes are sufficient for
               intentionality. (2) Instantiating a computer program is never by
               itself a sufficient condition of intentionality. The main
               argument of this paper is directed at establishing this claim.
               The form of the argument is to show how a human agent could
               instantiate the program and still not have the relevant
               intentionality. These two propositions have the following
               consequences: (3) The explanation of how the brain produces
               intentionality cannot be that it does it by instantiating a
               computer program. This is a strict logical consequence of 1 and
               2. (4) Any mechanism capable of producing intentionality must
               have causal powers equal to those of the brain. This is meant to
               be a trivial consequence of 1. (5) Any attempt literally to
               create intentionality artificially (strong AI) could not succeed
               just by designing programs but would have to duplicate the
               causal powers of the human brain. This follows from 2 and
               4.``Could a machine think?'' On the argument advanced here only
               a machine could think, and only very special kinds of machines,
               namely brains and machines with internal causal powers
               equivalent to those of brains. And that is why strong AI has
               little to tell us about thinking, since it is not about machines
               but about programs, and no program by itself is sufficient for
               thinking.",
  journal   = "Behav. Brain Sci.",
  publisher = "Cambridge University Press",
  volume    =  3,
  number    =  3,
  pages     = "417--424",
  month     =  sep,
  year      =  1980,
  file      = "All Papers/S/Searle 1980 - Minds, brains, and programs.pdf",
  keywords  = "artificial intelligence; brain; intentionality; mind;GDS",
  issn      = "0140-525X, 1469-1825",
  doi       = "10.1017/S0140525X00005756"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Turing1950-zj,
  title     = "{I.---COMPUTING} {MACHINERY} {AND} {INTELLIGENCE}",
  author    = "Turing, A M",
  abstract  = "I propose to consider the question, `Can machines think?' This
               should begin with definitions of the meaning of the terms
               `machine' and `think'. The definitions",
  journal   = "Mind",
  publisher = "Oxford Academic",
  volume    = "LIX",
  number    =  236,
  pages     = "433--460",
  month     =  oct,
  year      =  1950,
  file      = "All Papers/T/Turing 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf",
  keywords  = "GDS",
  language  = "en",
  issn      = "0026-4423",
  doi       = "10.1093/mind/LIX.236.433"
}

@ARTICLE{Shamma2020-ww,
  title     = "Game theory, learning, and control systems",
  author    = "Shamma, Jeff S",
  abstract  = "Game theory is the study of interacting decision makers, whereas
               control systems involve the design of intelligent
               decision-making devices. When many control systems are
               interconnected, the result can be viewed through the lens of
               game theory. This article discusses both long standing
               connections between these fields as well as new connections
               stemming from emerging applications.",
  journal   = "Natl Sci Rev",
  publisher = "Oxford Academic",
  volume    =  7,
  number    =  7,
  pages     = "1118--1119",
  month     =  jul,
  year      =  2020,
  file      = "All Papers/S/Shamma 2020 - Game theory, learning, and control systems.pdf",
  keywords  = "MLNetworking",
  issn      = "2095-5138",
  doi       = "10.1093/nsr/nwz163"
}

@ARTICLE{Zhang1997-sj,
  title    = "Secure code distribution",
  author   = "Zhang, X N",
  abstract = "The Java Virtual Machine does not offer a way for code obtained
              from trusted sources to be granted extra rights. The article
              describes two approaches to authentification for code
              distribution: one extends the JVM to include a digital signature
              in applets; the other uses MIME encapsulation to take advantage
              of available security infrastructures. The signed-applet approach
              gives a programmer more flexibility because it addresses the
              security issues at a more fundamental level. However,
              signed-applet security mechanisms may vary for different code
              distribution schemes, making integration difficult. The
              MIME-based approach provides a unified security interface. It is
              more efficient in the sense that all classes can be encapsulated
              in one multipart attachment, and a single signature or
              verification operation will cover all classes. The approaches can
              also be combined and tailored to satisfy various requirements.
              Ultimately, operating systems must support the concept of a
              secure compartment so that separate resource management policies
              can be implemented for the secure compartment and the rest of the
              system.",
  journal  = "Computer",
  volume   =  30,
  number   =  6,
  pages    = "76--79",
  month    =  jun,
  year     =  1997,
  keywords = "
              Security;Authentication;Java;Cryptography;Protection;Safety;Virtual
              machining;Digital signatures;Terminology;Authorization",
  issn     = "0018-9162, 1558-0814",
  doi      = "10.1109/2.587552"
}

@ARTICLE{Wang1993-cf,
  title    = "Implementing precise interruptions in pipelined {RISC} processors",
  author   = "Wang, C-J and Emnett, F",
  abstract = "Pipelining is an implementation technique that exploits
              parallelism among instructions. Imprecise interruption problems
              arise when a pipelined processor has multiple multicycle
              functional units because instruction completion might be out of
              order. An early issued, long-running instruction might generate
              an interruption after the completion of several short-running
              instructions issued later, resulting in an imprecise
              interruption. Four methods of providing precise interruptions
              with regard to performance degradation and cost of implementation
              are compared from the VLSI silicon resources perspective. Results
              provide valuable information for VLSI processor designers to
              consider if they include the precise interruption in their
              designs. The four methods are in-order instruction completion,
              reorder buffer, history file, and future file.>",
  journal  = "IEEE Micro",
  volume   =  13,
  number   =  4,
  pages    = "36--43",
  month    =  aug,
  year     =  1993,
  keywords = "Reduced instruction set computing;Pipeline
              processing;Registers;Computer aided instruction;Parallel
              processing;Out of order;Lab-on-a-chip;Computer
              architecture;Computational modeling;Throughput;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/40.229713"
}

@ARTICLE{Diefendorff1994-yt,
  title    = "Evolution of the {PowerPC} architecture",
  author   = "Diefendorff, K and Oehler, R and Hochsprung, R",
  abstract = "The PowerPC is a new RISC architecture derived from IBM's POWER
              architecture. The changes made to POWER simplify implementations,
              increase clock rates, enable a higher degree of superscalar
              execution, extend the architecture to 64 bits, and add
              multiprocessor support. For compatibility with existing software,
              the developers retained POWER's basic instruction set, opcode
              assignments, and programming model.>",
  journal  = "IEEE Micro",
  volume   =  14,
  number   =  2,
  pages    = "34--49",
  month    =  apr,
  year     =  1994,
  keywords = "Computer architecture;Reduced instruction set
              computing;Pipelines;Delay;Clocks;Software performance;Power
              system modeling;Power generation;Computer aided instruction;High
              performance computing;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/40.272836"
}

@ARTICLE{Crawford1990-pz,
  title    = "The i486 {CPU}: executing instructions in one clock cycle",
  author   = "Crawford, J H",
  abstract = "The author discusses the design goals of the i486 development
              program, which were to ensure binary compatibility with the 386
              microprocessor and the 387 math coprocessor, increase performance
              by two to three times over a 386/387 processor system at the same
              clock rate, and extend the IBM PC standard architecture of the
              386 CPU with features suitable for minicomputers. A cache
              integrated into the instruction pipeline lets this 386-compatible
              processor achieve minicomputer performance levels. The design and
              performance of the on-chip cache and the instruction pipeline are
              examined in detail.>",
  journal  = "IEEE Micro",
  volume   =  10,
  number   =  1,
  pages    = "27--36",
  month    =  feb,
  year     =  1990,
  keywords = "Clocks;Application software;Pipelines;Hardware;Computer
              architecture;Microcomputers;Multitasking;Logic;Protection;Microprocessors;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/40.46766"
}

@ARTICLE{Edenfield1990-tf,
  title    = "The 68040 processor. I. Design and implementation",
  author   = "Edenfield, R W and Gallup, M G and Ledbetter, W B and McGarity, R
              C and Quintana, E E and Reininger, R A",
  abstract = "The design of the 68040, a third-generation, full-32-b
              microprocessor in the Motorola 68000 family, is presented. The
              68040 integrates over 1.2 million transistors on one chip and can
              execute the complete 68020 microprocessor and 68882
              floating-point coprocessor instruction sets. Pipelined integer
              and floating-point execution units that operate concurrently with
              separate internal memory controllers and an autonomous bus
              controller contribute to its high performance level. Physical
              caches of 4 kB each for instruction and data reside on chip.
              Separate address-translation caches of 64 entries apiece operate
              in parallel with the instruction and data caches. This
              arrangement provides complete memory management in a virtual,
              demand-paged operating system. The design team explains its total
              approach and the workings of the integer and floating-point
              units.>",
  journal  = "IEEE Micro",
  volume   =  10,
  number   =  1,
  pages    = "66--78",
  month    =  feb,
  year     =  1990,
  keywords = "Process design;Operating systems;Microprocessors;Instruction
              sets;Clocks;Memory
              management;Silicon;System-on-a-chip;Coprocessors;Frequency;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/40.46770"
}

@ARTICLE{Christie1996-wx,
  title    = "Developing the {AMD-K5} architecture",
  author   = "Christie, D",
  abstract = "AMD engineers developed the K5 microarchitecture for AMD's first
              home grown x86-compatible microprocessor. We designed it to not
              only compete with Intel Pentium-class processors but, more
              importantly, to establish a baseline microarchitecture and
              implementation for ongoing development of a family of
              leading-edge x86-compatible microprocessors. The resulting
              AMD5K86 products can decode and issue up to four x86 instructions
              per cycle, with full out-of-order dependency-driven execution
              that takes place speculatively beyond unresolved branches. As a
              first-time, independent x86 implementation, the K5 microprocessor
              presented many unique challenges and constraints to its
              designers.",
  journal  = "IEEE Micro",
  volume   =  16,
  number   =  2,
  pages    = "16--27",
  month    =  apr,
  year     =  1996,
  keywords = "Timing;Uncertainty;Silicon;Microprocessors;Computer
              architecture;Design methodology;Circuit synthesis;Information
              resources;Documentation;Programming;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/40.491459"
}

@ARTICLE{Yeager1996-dt,
  title    = "The Mips {R10000} superscalar microprocessor",
  author   = "Yeager, K C",
  abstract = "The Mips R10000 is a dynamic, superscalar microprocessor that
              implements the 64-bit Mips 4 instruction set architecture. It
              fetches and decodes four instructions per cycle and dynamically
              issues them to five fully-pipelined, low-latency execution units.
              Instructions can be fetched and executed speculatively beyond
              branches. Instructions graduate in order upon completion.
              Although execution is out of order, the processor still provides
              sequential memory consistency and precise exception handling. The
              R10000 is designed for high performance, even in large,
              real-world applications with poor memory locality. With
              speculative execution, it calculates memory addresses and
              initiates cache refills early. Its hierarchical, nonblocking
              memory system helps hide memory latency with two levels of
              set-associative, write-back caches.",
  journal  = "IEEE Micro",
  volume   =  16,
  number   =  2,
  pages    = "28--41",
  month    =  apr,
  year     =  1996,
  keywords = "Microprocessors;Delay;Logic arrays;CMOS logic circuits;Out of
              order;Logic design;Adaptive arrays;Pipelines;Registers;Design
              optimization;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/40.491460"
}

@ARTICLE{Jacob1998-nm,
  title    = "Virtual memory in contemporary microprocessors",
  author   = "Jacob, B and Mudge, T",
  abstract = "Here, we consider the memory management designs of a sampling of
              six recent processors, focusing primarily on their architectural
              differences, and hint at optimizations that someone designing or
              porting system software might want to consider. We selected
              examples from the most popular commercial microarchitectures: the
              MIPS R10000, Alpha 21164, PowerPC 604, PA-8000, UltraSPARC-I, and
              Pentium II. This survey describes how each processor architecture
              supports the common features of virtual memory: address space
              protection, shared memory, and large address spaces.",
  journal  = "IEEE Micro",
  volume   =  18,
  number   =  4,
  pages    = "60--75",
  month    =  jul,
  year     =  1998,
  file     = "All Papers/J/Jacob and Mudge 1998 - Virtual memory in contemporary microprocessors.pdf",
  keywords = "Microprocessors;Memory
              management;Clocks;Pipelines;Protection;Hardware;Microarchitecture;Power
              system management;Resource management;Physics computing;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/40.710872"
}

@ARTICLE{Walden1999-xr,
  title    = "Analog-to-digital converter survey and analysis",
  author   = "Walden, R H",
  abstract = "Analog-to-digital converters (ADCs) are ubiquitous, critical
              components of software radio and other signal processing systems.
              This paper surveys the state-of-the-art of ADCs, including
              experimental converters and commercially available parts. The
              distribution of resolution versus sampling rate provides insight
              into ADC performance limitations. At sampling rates below 2
              million samples per second (Gs/s), resolution appears to be
              limited by thermal noise. At sampling rates ranging from /spl
              sim/2 Ms/s to /spl sim/4 giga samples per second (Gs/s),
              resolution falls off by /spl sim/1 bit for every doubling of the
              sampling rate. This behavior may be attributed to uncertainty in
              the sampling instant due to aperture jitter. For ADCs operating
              at multi-Gs/s rates, the speed of the device technology is also a
              limiting factor due to comparator ambiguity. Many ADC
              architectures and integrated circuit technologies have been
              proposed and implemented to push back these limits. The trend
              toward single-chip ADCs brings lower power dissipation. However,
              technological progress as measured by the product of the ADC
              resolution (bits) times the sampling rate is slow. Average
              improvement is only /spl sim/1.5 bits for any given sampling
              frequency over the last six-eight years.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  17,
  number   =  4,
  pages    = "539--550",
  month    =  apr,
  year     =  1999,
  file     = "All Papers/W/Walden 1999 - Analog-to-digital converter survey and analysis.pdf",
  keywords = "Analog-digital conversion;Sampling methods;Integrated circuit
              technology;Signal resolution;Signal sampling;Software
              radio;Signal processing;Uncertainty;Apertures;Jitter;GDS",
  issn     = "1558-0008",
  doi      = "10.1109/49.761034"
}

@ARTICLE{Leland1994-ih,
  title    = "On the self-similar nature of Ethernet traffic (extended version)",
  author   = "Leland, W E and Taqqu, M S and Willinger, W and Wilson, D V",
  abstract = "Demonstrates that Ethernet LAN traffic is statistically
              self-similar, that none of the commonly used traffic models is
              able to capture this fractal-like behavior, that such behavior
              has serious implications for the design, control, and analysis of
              high-speed, cell-based networks, and that aggregating streams of
              such traffic typically intensifies the self-similarity
              (``burstiness'') instead of smoothing it. These conclusions are
              supported by a rigorous statistical analysis of hundreds of
              millions of high quality Ethernet traffic measurements collected
              between 1989 and 1992, coupled with a discussion of the
              underlying mathematical and statistical properties of
              self-similarity and their relationship with actual network
              behavior. The authors also present traffic models based on
              self-similar stochastic processes that provide simple, accurate,
              and realistic descriptions of traffic scenarios expected during
              B-ISDN deployment.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  2,
  number   =  1,
  pages    = "1--15",
  month    =  feb,
  year     =  1994,
  file     = "All Papers/L/Leland et al. 1994 - On the self-similar nature of Ethernet traffic (extended version).pdf",
  keywords = "telecommunication traffic;statistical analysis;B-ISDN;local area
              networks;fractals;packet switching;stochastic
              processes;self-similar nature;Ethernet traffic;LAN
              traffic;fractal-like behavior;high-speed cell-based
              networks;burstiness;statistical analysis;self-similar stochastic
              processes;B-ISDN;Ethernet networks;Traffic control;Communication
              system traffic control;Telecommunication traffic;Local area
              networks;Fractals;Smoothing methods;Statistical
              analysis;Stochastic processes;B-ISDN;NetworkTraffic",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/90.282603"
}

@ARTICLE{Crovella1997-gz,
  title     = "Self-similarity in World Wide Web traffic: evidence and possible
               causes",
  author    = "Crovella, Mark E and Bestavros, Azer",
  journal   = "IEEE/ACM Trans. Netw.",
  publisher = "IEEE Press",
  volume    =  5,
  number    =  6,
  pages     = "835--846",
  month     =  dec,
  year      =  1997,
  file      = "All Papers/C/Crovella and Bestavros 1997 - Self-similarity in World Wide Web traffic - evidence and possible causes.pdf",
  keywords  = "World Wide Web, self-similarity, file sizes, Internet, heavy
               tails;NetworkTraffic",
  issn      = "1063-6692",
  doi       = "10.1109/90.650143"
}

@ARTICLE{Wang2016-vr,
  title    = "Joint Optimization of Service Function Chaining and Resource
              Allocation in Network Function Virtualization",
  author   = "Wang, L and Lu, Z and Wen, X and Knopp, R and Gupta, R",
  abstract = "Network function virtualization (NFV) has already been a new
              paradigm for network architectures. By migrating NFs from
              dedicated hardware to virtualization platform, NFV can
              effectively improve the flexibility to deploy and manage service
              function chains (SFCs). However, resource allocation for
              requested SFC in NFV-based infrastructures is not trivial as it
              mainly consists of three phases: virtual network functions (VNFs)
              chain composition, VNFs forwarding graph embedding, and VNFs
              scheduling. The decision of these three phases can be mutually
              dependent, which also makes it a tough task. Therefore, a
              coordinated approach is studied in this paper to jointly optimize
              NFV resource allocation in these three phases. We apply a general
              cost model to consider both network costs and service
              performance. The coordinate NFV-RA is formulated as a
              mixed-integer linear programming, and a heuristic-based algorithm
              (JoraNFV) is proposed to get the near optimal solution. To make
              the coordinated NFV-RA more tractable, JoraNFV is divided into
              two sub-algorithms, one-hop optimal traffic scheduling and a
              multi-path greedy algorithm for VNF chain composition and VNF
              forwarding graph embedding. Last, extensive simulations are
              performed to evaluate the performance of JoraNFV, and results
              have shown that JoraNFV can get a solution within 1.25 times of
              the optimal solution with reasonable execution time, which
              indicates that JoraNFV can be used for online NFV planning.",
  journal  = "IEEE Access",
  volume   =  4,
  pages    = "8084--8094",
  year     =  2016,
  file     = "All Papers/W/Wang et al. 2016 - Joint Optimization of Service Function Chaining and Resource Allocation in Network Function Virtualization.pdf",
  keywords = "computer networks;graph theory;integer programming;linear
              programming;resource allocation;telecommunication
              scheduling;telecommunication traffic;virtualisation;online NFV
              planning;JoraNFV performance;multipath greedy algorithm;one-hop
              optimal traffic scheduling;heuristic-based
              algorithm;mixed-integer linear programming;VNF scheduling;VNF
              forwarding graph embedding;virtual network function;SFC;NFV-based
              infrastructure;network function virtualization;joint service
              function chaining and resource allocation optimization;Resource
              management;Noise measurement;Telecommunication
              traffic;Scheduling;Mix integer linear programming;Virtual
              function placement;NFV;resource allocation;service function
              chain;traffic scheduling;virtual function placement;NFV",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2016.2629278"
}

@ARTICLE{Liu2018-sl,
  title    = "Content Popularity Prediction and Caching for {ICN}: A Deep
              Learning Approach With {SDN}",
  author   = "Liu, W and Zhang, J and Liang, Z and Peng, L and Cai, J",
  abstract = "In information-centric networking, accurately predicting content
              popularity can improve the performance of caching. Therefore,
              based on software defined network (SDN), this paper proposes
              Deep-Learning-based Content Popularity Prediction (DLCPP) to
              achieve the popularity prediction. DLCPP adopts the switch's
              computing resources and links in the SDN to build a distributed
              and reconfigurable deep learning network. For DLCPP, we initially
              determine the metrics that can reflect changes in content
              popularity. Second, each network node collects the
              spatial-temporal joint distribution data of these metrics. Then,
              the data are used as input to stacked auto-encoders (SAE) in
              DLCPP to extract the spatiotemporal features of popularity.
              Finally, we transform the popularity prediction into a
              multi-classification problem through discretizing the content
              popularity into multiple classifications. The Softmax classifier
              is used to achieve the content popularity prediction. Some
              challenges for DLCPP are also addressed, such as determining the
              structure of SAE, realizing the neuron function on an SDN switch,
              and deploying DLCPP on an OpenFlow-based SDN. At the same time,
              we propose a lightweight caching scheme that integrates cache
              placement and cache replacement-caching based on popularity
              prediction and cache capacity (CPC). Abundant experiments
              demonstrate good performance of DLCPP, and it achieves close to
              2.1\% 15\% and 5.2\% 40\% accuracy improvements over neural
              networks and auto regressive, respectively. Benefitting from
              DLCPP's better prediction accuracy, CPC can yield a steady
              improvement of caching performance over other dominant cache
              management frameworks.",
  journal  = "IEEE Access",
  volume   =  6,
  pages    = "5075--5089",
  year     =  2018,
  file     = "All Papers/L/Liu et al. 2018 - Content Popularity Prediction and Caching for ICN - A Deep Learning Approach With SDN.pdf",
  keywords = "cache storage;Internet;learning (artificial intelligence);neural
              nets;DLCPP;SDN;cache placement;cache
              replacement-caching;prediction accuracy;content popularity
              prediction;information-centric networking;software defined
              network;distributed learning network;reconfigurable deep learning
              network;stacked auto-encoders;SAE;Softmax classifier;popularity
              prediction;cache capacity;cache management frameworks;Machine
              learning;Correlation;Switches;Feature extraction;Predictive
              models;Artificial neural networks;Data mining;Information-centric
              networking;SDN;deep learning;content popularity
              prediction;caching scheme;GeneralNetworking",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2017.2781716"
}

@ARTICLE{Naha2018-zj,
  title    = "Fog Computing: Survey of Trends, Architectures, Requirements, and
              Research Directions",
  author   = "Naha, Ranesh Kumar and Garg, Saurabh and Georgakopoulos,
              Dimitrios and Jayaraman, Prem Prakash and Gao, Longxiang and
              Xiang, Yong and Ranjan, Rajiv",
  abstract = "Emerging technologies such as the Internet of Things (IoT)
              require latency-aware computation for real-time application
              processing. In IoT environments, connected things generate a huge
              amount of data, which are generally referred to as big data. Data
              generated from IoT devices are generally processed in a cloud
              infrastructure because of the on-demand services and scalability
              features of the cloud computing paradigm. However, processing IoT
              application requests on the cloud exclusively is not an efficient
              solution for some IoT applications, especially time-sensitive
              ones. To address this issue, Fog computing, which resides in
              between cloud and IoT devices, was proposed. In general, in the
              Fog computing environment, IoT devices are connected to Fog
              devices. These Fog devices are located in close proximity to
              users and are responsible for intermediate computation and
              storage. One of the key challenges in running IoT applications in
              a Fog computing environment are resource allocation and task
              scheduling. Fog computing research is still in its infancy, and
              taxonomy-based investigation into the requirements of Fog
              infrastructure, platform, and applications mapped to current
              research is still required. This survey will help the industry
              and research community synthesize and identify the requirements
              for Fog computing. This paper starts with an overview of Fog
              computing in which the definition of Fog computing, research
              trends, and the technical differences between Fog and cloud are
              reviewed. Then, we investigate numerous proposed Fog computing
              architectures and describe the components of these architectures
              in detail. From this, the role of each component will be defined,
              which will help in the deployment of Fog computing. Next, a
              taxonomy of Fog computing is proposed by considering the
              requirements of the Fog computing paradigm. We also discuss
              existing research works and gaps in resource allocation and
              scheduling, fault tolerance, simulation tools, and Fog-based
              microservices. Finally, by addressing the limitations of current
              research works, we present some open issues, which will determine
              the future research direction for the Fog computing paradigm.",
  journal  = "IEEE Access",
  volume   =  6,
  pages    = "47980--48009",
  year     =  2018,
  file     = "All Papers/N/Naha et al. 2018 - Fog Computing - Survey of Trends, Architectures, Requirements, and Research Directions.pdf",
  keywords = "Edge computing;Cloud computing;Computer architecture;Market
              research;Internet of Things;Resource management;Taxonomy;Fog
              computing;Internet of Things (IoT);fog devices;fault
              tolerance;IoT application;microservices;EdgeFogCloudIoT",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2018.2866491"
}

@ARTICLE{Zhang2019-se,
  title    = "Mobility Prediction: A Survey on {State-of-the-Art} Schemes and
              Future Applications",
  author   = "Zhang, H and Dai, L",
  abstract = "Recently, mobility has gathered tremendous interest as the users'
              desire for consecutive connections and better quality of service
              has increased. An accurate prediction of user mobility in mobile
              networks provides efficient resource and handover management,
              which can avoid unacceptable degradation of the perceived
              quality. Therefore, mobility prediction in wireless networks is
              of great importance and many works have been dedicated to this
              issue. In this paper, the necessity of mobility prediction,
              together with its intrinsic characteristics in terms of movement
              predictability, prediction outputs, and performance metrics is
              discussed. Moreover, the learning perspective of solutions to
              mobility prediction has been studied. Specifically, an overview
              of the state-of-the-art approaches is provided, including Markov
              chain, hidden Markov model, artificial neural network, Bayesian
              network, and data mining based on different kinds of knowledge.
              At last, this paper also explores the open research challenges
              due to the advent of the fifth-generation mobile system and puts
              forward some potential trends in the near future.",
  journal  = "IEEE Access",
  volume   =  7,
  pages    = "802--822",
  year     =  2019,
  file     = "All Papers/Z/Zhang and Dai 2019 - Mobility Prediction - A Survey on State-of-the-Art Schemes and Future Applications.pdf",
  keywords = "Handover;Quality of service;History;Hidden Markov
              models;Prediction algorithms;Measurement;Mobility
              prediction;quality of service (QoS);resource reservation;handover
              management;the fifth-generation mobile system
              (5G);GeneralNetworking",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2018.2885821"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Lopez_Garcia2020-ul,
  title    = "A {Cloud-Based} Framework for Machine Learning Workloads and
              Applications",
  author   = "L{\'o}pez Garc{\'\i}a, {\'A}lvaro and De Lucas, Jes{\'u}s Marco
              and Antonacci, Marica and Zu Castell, Wolfgang and David, Mario
              and Hardt, Marcus and Lloret Iglesias, Lara and Molt{\'o},
              Germ{\'a}n and Plociennik, Marcin and Tran, Viet and Alic, Andy S
              and Caballer, Miguel and Plasencia, Isabel Campos and Costantini,
              Alessandro and Dlugolinsky, Stefan and Duma, Doina Cristina and
              Donvito, Giacinto and Gomes, Jorge and Heredia Cacha, Ignacio and
              Ito, Keiichi and Kozlov, Valentin Y and Nguyen, Giang and Orviz
              Fern{\'a}ndez, Pablo and {\v S}ustr, Zd{\v e}nek and Wolniewicz,
              Pawel",
  abstract = "In this paper we propose a distributed architecture to provide
              machine learning practitioners with a set of tools and cloud
              services that cover the whole machine learning development cycle:
              ranging from the models creation, training, validation and
              testing to the models serving as a service, sharing and
              publication. In such respect, the DEEP-Hybrid-DataCloud framework
              allows transparent access to existing e-Infrastructures,
              effectively exploiting distributed resources for the most
              compute-intensive tasks coming from the machine learning
              development cycle. Moreover, it provides scientists with a set of
              Cloud-oriented services to make their models publicly available,
              by adopting a serverless architecture and a DevOps approach,
              allowing an easy share, publish and deploy of the developed
              models.",
  journal  = "IEEE Access",
  volume   =  8,
  pages    = "18681--18692",
  year     =  2020,
  file     = "All Papers/L/López García et al. 2020 - A Cloud-Based Framework for Machine Learning Workloads and Applications.pdf",
  keywords = "Cloud computing;Computational modeling;Task analysis;Deep
              learning;Tools;Computer architecture;Cloud computing;computers
              and information processing;deep learning;distributed
              computing;machine learning;serverless architectures",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2020.2964386"
}

@ARTICLE{Wang2020-lh,
  title    = "Deep Learning for Edge Computing Applications: A
              {State-of-the-Art} Survey",
  author   = "Wang, Fangxin and Zhang, Miao and Wang, Xiangxiang and Ma,
              Xiaoqiang and Liu, Jiangchuan",
  abstract = "With the booming development of Internet-of-Things (IoT) and
              communication technologies such as 5G, our future world is
              envisioned as an interconnected entity where billions of devices
              will provide uninterrupted service to our daily lives and the
              industry. Meanwhile, these devices will generate massive amounts
              of valuable data at the network edge, calling for not only
              instant data processing but also intelligent data analysis in
              order to fully unleash the potential of the edge big data. Both
              the traditional cloud computing and on-device computing cannot
              sufficiently address this problem due to the high latency and the
              limited computation capacity, respectively. Fortunately, the
              emerging edge computing sheds a light on the issue by pushing the
              data processing from the remote network core to the local network
              edge, remarkably reducing the latency and improving the
              efficiency. Besides, the recent breakthroughs in deep learning
              have greatly facilitated the data processing capacity, enabling a
              thrilling development of novel applications, such as video
              surveillance and autonomous driving. The convergence of edge
              computing and deep learning is believed to bring new
              possibilities to both interdisciplinary researches and industrial
              applications. In this article, we provide a comprehensive survey
              of the latest efforts on the deep-learning-enabled edge computing
              applications and particularly offer insights on how to leverage
              the deep learning advances to facilitate edge applications from
              four domains, i.e., smart multimedia, smart transportation, smart
              city, and smart industry. We also highlight the key research
              challenges and promising research directions therein. We believe
              this survey will inspire more researches and contributions in
              this promising field.",
  journal  = "IEEE Access",
  volume   =  8,
  pages    = "58322--58336",
  year     =  2020,
  file     = "All Papers/W/Wang et al. 2020 - Deep Learning for Edge Computing Applications - A State-of-the-Art Survey.pdf",
  keywords = "Edge computing;Deep learning;Cloud computing;Data
              processing;Servers;Computational modeling;Autonomous
              vehicles;Internet of Things;edge computing;deep
              learning;intelligent edge applications;ML4Net;MLAspects;ATOS",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2020.2982411"
}

@ARTICLE{Khorov2020-oh,
  title    = "Current Status and Directions of {IEEE} 802.11be, the Future
              {Wi-Fi} 7",
  author   = "Khorov, E and Levitsky, I and Akyildiz, I F",
  abstract = "While customers rivet their eyes on Wi-Fi 6, in the bowels of the
              IEEE 802.11 Working Group that creates Wi-Fi standards, the next
              generation Wi-Fi is being developed. At the very first sight, the
              new IEEE 802.11be amendment to the Wi-Fi standard is nothing but
              scaled 11ax with doubled bandwidth and the increased number of
              spatial streams, which together provide data rates as high as 40
              Gbps. A bit deeper dive into the 802.11 activities reveals that
              11be will support real-time applications. In reality, 11be
              introduces many more revolutionary changes to Wi-Fi, which will
              form a basement for further Wi-Fi evolution. Although by now (May
              2020), the development process is at the very early phase without
              any draft specification, the analysis of the discussion in the
              802.11 Working Group gives insights into the main innovations of
              11be. In addition to the ones above, they include native
              multi-link operation, channel sounding optimization that opens
              the door for massive MIMO, advanced PHY and MAC techniques, the
              cooperation of various access points. The paper analyzes hundreds
              of features proposed for the new technology, focusing on the open
              problems that can be solved by the researchers who want to
              contribute to the development of 802.11be.",
  journal  = "IEEE Access",
  volume   =  8,
  pages    = "88664--88688",
  year     =  2020,
  file     = "All Papers/K/Khorov et al. 2020 - Current Status and Directions of IEEE 802.11be, the Future Wi-Fi 7.pdf",
  keywords = "access protocols;MIMO communication;next generation
              networks;wireless LAN;IEEE 802.11 Working Group;next generation
              Wi-Fi;Wi-Fi standard;IEEE 802.11be amendment;multilink
              operation;channel sounding optimization;massive MIMO;MAC
              techniques;PHY techniques;Wireless fidelity;IEEE 802.11
              Standard;MIMO communication;Throughput;Media Access
              Protocol;OFDM;802.11be;extremely high throughput;4096 QAM;320
              MHz;MU-MIMO;time-sensitive networking;multi-link
              operation;implicit sounding;distributed MU-MIMO;Wireless",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2020.2993448"
}

@ARTICLE{Fuller2020-wj,
  title    = "Digital Twin: Enabling Technologies, Challenges and Open Research",
  author   = "Fuller, Aidan and Fan, Zhong and Day, Charles and Barlow, Chris",
  abstract = "Digital Twin technology is an emerging concept that has become
              the centre of attention for industry and, in more recent years,
              academia. The advancements in industry 4.0 concepts have
              facilitated its growth, particularly in the manufacturing
              industry. The Digital Twin is defined extensively but is best
              described as the effortless integration of data between a
              physical and virtual machine in either direction. The challenges,
              applications, and enabling technologies for Artificial
              Intelligence, Internet of Things (IoT) and Digital Twins are
              presented. A review of publications relating to Digital Twins is
              performed, producing a categorical review of recent papers. The
              review has categorised them by research areas: manufacturing,
              healthcare and smart cities, discussing a range of papers that
              reflect these areas and the current state of research. The paper
              provides an assessment of the enabling technologies, challenges
              and open research for Digital Twins.",
  journal  = "IEEE Access",
  volume   =  8,
  pages    = "108952--108971",
  year     =  2020,
  file     = "All Papers/F/Fuller et al. 2020 - Digital Twin - Enabling Technologies, Challenges and Open Research.pdf",
  keywords = "Smart cities;Data analysis;Manufacturing;Data models;Internet of
              Things;Computational modeling;Digital twins;applications;enabling
              technologies;industrial Internet of Things (IIoT);Internet of
              Things (IoT);machine learning;deep learning;literature review",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2020.2998358"
}

@ARTICLE{Pham2020-ss,
  title    = "A Survey of {Multi-Access} Edge Computing in {5G} and Beyond:
              Fundamentals, Technology Integration, and {State-of-the-Art}",
  author   = "Pham, Quoc-Viet and Fang, Fang and Ha, Vu Nguyen and Piran, Md
              Jalil and Le, Mai and Le, Long Bao and Hwang, Won-Joo and Ding,
              Zhiguo",
  abstract = "Driven by the emergence of new compute-intensive applications and
              the vision of the Internet of Things (IoT), it is foreseen that
              the emerging 5G network will face an unprecedented increase in
              traffic volume and computation demands. However, end users mostly
              have limited storage capacities and finite processing
              capabilities, thus how to run compute-intensive applications on
              resource-constrained users has recently become a natural concern.
              Mobile edge computing (MEC), a key technology in the emerging
              fifth generation (5G) network, can optimize mobile resources by
              hosting compute-intensive applications, process large data before
              sending to the cloud, provide the cloud-computing capabilities
              within the radio access network (RAN) in close proximity to
              mobile users, and offer context-aware services with the help of
              RAN information. Therefore, MEC enables a wide variety of
              applications, where the real-time response is strictly required,
              e.g., driverless vehicles, augmented reality, robotics, and
              immerse media. Indeed, the paradigm shift from 4G to 5G could
              become a reality with the advent of new technological concepts.
              The successful realization of MEC in the 5G network is still in
              its infancy and demands for constant efforts from both academic
              and industry communities. In this survey, we first provide a
              holistic overview of MEC technology and its potential use cases
              and applications. Then, we outline up-to-date researches on the
              integration of MEC with the new technologies that will be
              deployed in 5G and beyond. We also summarize testbeds and
              experimental evaluations, and open source activities, for edge
              computing. We further summarize lessons learned from
              state-of-the-art research works as well as discuss challenges and
              potential future directions for MEC research.",
  journal  = "IEEE Access",
  volume   =  8,
  pages    = "116974--117017",
  year     =  2020,
  file     = "All Papers/P/Pham et al. 2020 - A Survey of Multi-Access Edge Computing in 5G and Beyond - Fundamentals, Technology Integration, and State-of-the-Art.pdf",
  keywords = "Cloud computing;5G mobile communication;Edge computing;Internet
              of Things;Radio access networks;NOMA;Wireless communication;5G
              and beyond network;heterogeneous networks;Internet of
              Things;machine learning;edge computing;non-orthogonal multiple
              access;testbeds;unmanned aerial vehicle;wireless power transfer
              and energy harvesting;EdgeFogCloudIoT;5G6G;EdgeFogCloudIoT;Cloud",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2020.3001277"
}

@ARTICLE{Filali2020-ia,
  title    = "{Multi-Access} Edge Computing: A Survey",
  author   = "Filali, Abderrahime and Abouaomar, Amine and Cherkaoui, Soumaya
              and Kobbane, Abdellatif and Guizani, Mohsen",
  abstract = "Multi-access Edge Computing (MEC) is a key solution that enables
              operators to open their networks to new services and IT
              ecosystems to leverage edge-cloud benefits in their networks and
              systems. Located in close proximity from the end users and
              connected devices, MEC provides extremely low latency and high
              bandwidth while always enabling applications to leverage cloud
              capabilities as necessary. In this article, we illustrate the
              integration of MEC into a current mobile networks' architecture
              as well as the transition mechanisms to migrate into a standard
              5G network architecture. We also discuss SDN, NFV, SFC and
              network slicing as MEC enablers. Then, we provide a
              state-of-the-art study on the different approaches that optimize
              the MEC resources and its QoS parameters. In this regard, we
              classify these approaches based on the optimized resources and
              QoS parameters (i.e., processing, storage, memory, bandwidth,
              energy and latency). Finally, we propose an architectural
              framework for a MEC-NFV environment based on the standard SDN
              architecture.",
  journal  = "IEEE Access",
  volume   =  8,
  pages    = "197017--197046",
  year     =  2020,
  file     = "All Papers/F/Filali et al. 2020 - Multi-Access Edge Computing - A Survey.pdf",
  keywords = "5G mobile communication;Network slicing;Computer
              architecture;Optimization;Quality of service;Edge
              computing;Bandwidth;5G;multi-access edge computing;network
              function virtualization;network slicing;service function
              chaining;software defined networking;EdgeFogCloudIoT",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2020.3034136"
}

@ARTICLE{Dong2021-gq,
  title    = "{Next-Generation} Data Center Network Enabled by Machine
              Learning: Review, Challenges, and Opportunities",
  author   = "Dong, Haiwei and Munir, Ali and Tout, Hanine and Ganjali, Yashar",
  abstract = "Data center network (DCN) is the backbone of many emerging
              applications from smart connected homes to smart traffic control
              and is continuously evolving to meet the diverse and
              ever-increasing computing requirements of these applications. The
              data centers often have tens of thousands of components such as
              servers and switches/routers that work together to achieve a
              common objective and serve these applications. Managing such
              large data centers is a tedious process and demands automation,
              intelligent control and decision making within the data center.
              Recently both the industry and academia have focused on bringing
              intelligence to the control, automation, and management of DCNs.
              Despite the variety of works that surveyed ML for networking, to
              the best of our knowledge, none has focused on DCN, which makes
              this survey original. Readers in the academic and industrial
              communities will all benefit from a comprehensive discussion of
              the ML solutions applied in DCN to address critical essential
              problems, including workload forecasting, traffic flow control,
              traffic classification and scheduling, topology management,
              network state prediction, root cause analysis, and network
              security. Furthermore, this article outlines the challenges and
              concludes with the future research venues in adopting ML for
              automatic, intelligent and autonomous DCNs.",
  journal  = "IEEE Access",
  volume   =  9,
  pages    = "136459--136475",
  year     =  2021,
  file     = "All Papers/D/Dong et al. 2021 - Next-Generation Data Center Network Enabled by Machine Learning - Review, Challenges, and Opportunities.pdf",
  keywords = "Data centers;Topology;Network topology;Machine
              learning;Servers;Supervised learning;Process control;Data center
              network;machine learning applications;survey",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2021.3117763"
}

@ARTICLE{Zeydan2022-ya,
  title    = "Recent Advances in Data Engineering for Networking",
  author   = "Zeydan, Engin and Mangues-Bafalluy, Josep",
  abstract = "This tutorial paper examines recent advances in data engineering,
              focusing on aspects of network management and orchestration. We
              provide a comprehensive analysis of standardization efforts as
              well as platform development activities related to data
              engineering driven network design. We then focus on the
              integration aspects of the data engineering ecosystem and
              telecommunication networks. The results of our tutorial
              investigation show that despite various efforts towards
              standardization and network management and orchestration
              platforms, there is still a significant gap in applying recent
              developments in the evolving data engineering world to the
              telecommunication domain. New advanced functionalities in data
              engineering as well as clear separations between the building
              blocks of data engineering pipelines within the proposed
              standardized architectures have been overlooked or not explored
              in detail by the standardization or platform development bodies
              in the telecommunication domain. Therefore, at the end of the
              paper, we discuss these gaps and research challenges in the
              context of future development processes for data
              engineering-driven network design and applications of data
              engineering concepts in telecommunication networks. We also
              propose several recommendations for early adoption of these
              technologies and frameworks in telecommunication infrastructures
              and platforms.",
  journal  = "IEEE Access",
  pages    = "1--1",
  year     =  2022,
  file     = "All Papers/Z/Zeydan and Mangues-Bafalluy 2022 - Recent Advances in Data Engineering for Networking.pdf",
  keywords = "Communications technology;Data engineering;Distributed
              databases;Artificial intelligence;Pipelines;Big
              Data;Tutorials;data engineering;network
              management;orchestration;tutorial;Distributed Systems",
  issn     = "2169-3536",
  doi      = "10.1109/ACCESS.2022.3162863"
}

@INPROCEEDINGS{Huang2018-yq,
  title     = "Kubebench: A Benchmarking Platform for {ML} Workloads",
  booktitle = "2018 First International Conference on Artificial Intelligence
               for Industries ({AI4I})",
  author    = "Huang, Xinyuan and Saha, Amit Kumar and Dutta, Debojyoti and
               Gao, Ce",
  abstract  = "Machine Learning (ML) workloads are becoming mainstream in the
               enterprise but the plethora of choices around ML toolkits and
               multi-cloud infrastructure make it difficult to compare their
               performance and costs. In this paper, we motivate the need for
               benchmarking ML systems in a consistent way, discuss the
               requirements of an ML benchmarking platform, and propose a
               design that satisfies the requirements. We present Kubebench, an
               example open-source implementation of an ML benchmarking
               platform based on Kubeflow, itself an open-source project for
               managing any ML stack on Kubernetes, a widely used container
               management platform.",
  pages     = "73--76",
  month     =  sep,
  year      =  2018,
  keywords  = "Benchmark testing;Containers;Open source software;Cloud
               computing;Machine
               learning;Production;Training;machine-learning;benchmarking;kubebench;kubeflow;kubernetes",
  doi       = "10.1109/AI4I.2018.8665688"
}

@INPROCEEDINGS{Amri2017-ip,
  title     = "{Inter-VM} Interference in Cloud Environments: A Survey",
  booktitle = "2017 {IEEE/ACS} 14th International Conference on Computer
               Systems and Applications ({AICCSA})",
  author    = "Amri, Sabrine and Hamdi, Hedi and Brahmi, Zaki",
  abstract  = "Cloud Computing paradigm has been a trend in the computational
               world. Thus, many service providers today are competing to
               enhance their features and attract more customers as they are
               offering them a bunch of services through a pay-asyou-go pricing
               model. However, despite their huge fame, cloud environments
               still suffer from some issues that are being studied by
               researchers from various perspectives. One of the controversial
               cloud issues nowadays is interference among virtual machines
               (VMs) sharing the same hardware platform called also physical
               machine (PM). This problem occurs due to contention on shared
               resources between co-hosted VMs which results in performance
               degradation. Resource contention happens when demand for shared
               resources exceeds the supply due to VMs cohosting. The
               co-hosting of VMs on the same PM, emerges from the ambition of
               server consolidation that cloud providers aim to reach in order
               to improve power efficiency and optimize resource utilization.
               Therefore, the key factor of successful server consolidation is
               to minimize performance interference among colocated VMs. In
               this paper, we are going to reveal the interference issue as
               well as its major cause, survey existing researchers' visions to
               detect and/or predict such issue and exhibit a comparative study
               between them. Then we will reveal some challenging points that
               require further consideration. Our comparative study may be
               helpful for people aiming to be involved in cloud environments
               research field, as it gathers different existing approaches
               dealing with performance interference detection and prediction
               in cloud environments.",
  pages     = "154--159",
  month     =  oct,
  year      =  2017,
  file      = "All Papers/A/Amri et al. 2017 - Inter-VM Interference in Cloud Environments - A Survey.pdf",
  keywords  = "Interference;Cloud
               computing;Virtualization;Servers;Measurement;Hardware;Degradation;cloud
               computing;virtualization;physical machine (PM);performance
               interference;resource utilization;SLA violation",
  issn      = "2161-5330",
  doi       = "10.1109/AICCSA.2017.122"
}

@INPROCEEDINGS{Dunner2017-gp,
  title     = "Understanding and optimizing the performance of distributed
               machine learning applications on apache spark",
  booktitle = "2017 {IEEE} International Conference on Big Data (Big Data)",
  author    = "D{\"u}nner, Celestine and Parnell, Thomas and Atasu, Kubilay and
               Sifalakis, Manolis and Pozidis, Haralampos",
  abstract  = "In this paper we explore the performance limits of Apache Spark
               for machine learning applications. We begin by analyzing the
               characteristics of a state-of-the-art distributed machine
               learning algorithm implemented in Spark and compare it to an
               equivalent reference implementation using the high performance
               computing framework MPI. We identify critical bottlenecks of the
               Spark framework and carefully study their implications on the
               performance of the algorithm. In order to improve Spark
               performance we then propose a number of practical techniques to
               alleviate some of its overheads. However, optimizing
               computational efficiency and framework related overheads is not
               the only key to performance - we demonstrate that in order to
               get the best performance out of any implementation it is
               necessary to carefully tune the algorithm to the respective
               trade-off between computation time and communication latency.
               The optimal trade-off depends on both the properties of the
               distributed algorithm as well as infrastructure and
               framework-related characteristics. Finally, we apply these
               technical and algorithmic optimizations to three different
               distributed linear machine learning algorithms that have been
               implemented in Spark. We present results using five large
               datasets and demonstrate that by using the proposed
               optimizations, we can achieve a reduction in the performance
               difference between Spark and MPI from 20x to 2x.",
  pages     = "331--338",
  month     =  dec,
  year      =  2017,
  keywords  = "Sparks;Machine learning algorithms;Computational
               modeling;Algorithm design and analysis;Partitioning
               algorithms;Programming;Optimization",
  doi       = "10.1109/BigData.2017.8257942"
}

@INPROCEEDINGS{Spell2017-os,
  title     = "Flux: Groupon's automated, scalable, extensible machine learning
               platform",
  booktitle = "2017 {IEEE} International Conference on Big Data (Big Data)",
  author    = "Spell, Derrick C and Zeng, Xiao-Han T and Chung, Jae Young and
               Nooraei, Bahador and Shomer, Richard T and Wang, Ling-Yong and
               Gibson, James C and Kirsche, Daniel",
  abstract  = "As machine learning becomes the driving force of the daily
               operation of companies within the information technology sector,
               infrastructure that enables automated, scalable machine learning
               is a core component of the systems of many large companies.
               Various systems and products are being built, offered, and open
               sourced. As an e-commerce company, numerous aspects of Groupon's
               business is driven by machine learning. To solve the scalability
               issue and provide a seamless collaboration between data
               scientists and engineers, we built Flux, a system that expedites
               the deployment, execution, and monitoring of machine learning
               models. Flux focuses on enabling data scientists to build model
               prototypes with languages and tools they are most proficient in,
               and integrating the models into the enterprise production
               system. It manages the life cycle of deployed models, and
               executes them in distributed batch mode, or exposes them as
               micro-services for real-time use cases. Its design focuses on
               automation and easy management, scalability, and extensibility.
               Flux is the central system for supervised machine learning tasks
               at Groupon and has been supporting multiple teams across the
               company.",
  pages     = "1554--1559",
  month     =  dec,
  year      =  2017,
  keywords  = "Data models;Engines;Libraries;Real-time systems;Predictive
               models;Computational modeling;Tools;machine learning
               system;machine learning automation;machine learning
               infrastructure",
  doi       = "10.1109/BigData.2017.8258089"
}

@INPROCEEDINGS{Assefi2017-kg,
  title     = "Big data machine learning using apache spark {MLlib}",
  booktitle = "2017 {IEEE} International Conference on Big Data (Big Data)",
  author    = "Assefi, Mehdi and Behravesh, Ehsun and Liu, Guangchi and Tafti,
               Ahmad P",
  abstract  = "Artificial intelligence, and particularly machine learning, has
               been used in many ways by the research community to turn a
               variety of diverse and even heterogeneous data sources into high
               quality facts and knowledge, providing premier capabilities to
               accurate pattern discovery. However, applying machine learning
               strategies on big and complex datasets is computationally
               expensive, and it consumes a very large amount of logical and
               physical resources, such as data file space, CPU, and memory. A
               sophisticated platform for efficient big data analytics is
               becoming more important these days as the data amount generated
               in a daily basis exceeds over quintillion bytes. Apache Spark
               MLlib is one of the most prominent platforms for big data
               analysis which offers a set of excellent functionalities for
               different machine learning tasks ranging from regression,
               classification, and dimension reduction to clustering and rule
               extraction. In this contribution, we explore, from the
               computational perspective, the expanding body of the Apache
               Spark MLlib 2.0 as an open-source, distributed, scalable, and
               platform independent machine learning library. Specifically, we
               perform several real world machine learning experiments to
               examine the qualitative and quantitative attributes of the
               platform. Furthermore, we highlight current trends in big data
               machine learning research and provide insights for future work.",
  pages     = "3492--3498",
  month     =  dec,
  year      =  2017,
  keywords  = "Sparks;Big Data;Libraries;Data models;Computer
               architecture;Machine learning algorithms;Apache Spark MLlib;Big
               Data Machine Learning;Big Data Analytics;Machine Learning",
  doi       = "10.1109/BigData.2017.8258338"
}

@INPROCEEDINGS{Ross2019-mx,
  title     = "{EdgeInsight}: Characterizing and Modeling the Performance of
               Machine Learning Inference on the Edge and Cloud",
  booktitle = "2019 {IEEE} International Conference on Big Data (Big Data)",
  author    = "Ross, Philipp and Luckow, Andre",
  abstract  = "The Internet-of-Things (IoT) is growing in importance enabling
               an increasing number of scientific, industrial, and societal
               applications. At the same time, the computational capabilities
               of IoT and edge devices are rapidly improving making them viable
               for machine learning. Thus, the deployment of machine learning
               models on the edge is becoming a critical capability. However,
               such deployments are challenging as edge devices are more
               resource-constrained than clouds and cannot elastically scale on
               demand. Moving the application to the cloud can provide more
               computational power, but raises other challenges, such as
               security, reliability, and bandwidth. The integration of edge
               and cloud computing resources is often essential for many
               applications allowing them to choose the best configuration with
               respect to their requirements and characteristics, such as data
               rates and computational complexity. To understand the trade-offs
               between edge and cloud computing the assessment of different
               cloud and edge configurations is required. In this paper, we
               present EdgeInsight, a framework for characterizing and modeling
               of the inference performance of edge and cloud infrastructures.
               EdgeInsight enables system builders to size and fine-tune
               infrastructure parameters for different workloads and
               applications. We use EdgeInsight to qualitatively and
               quantitatively study edge and cloud deployment configuration for
               deep learning inference. Our evaluation shows, that edge
               inference can outperform cloud inference when model architecture
               and accuracy, inference framework and pre-processing parameters
               are carefully selected.",
  pages     = "1897--1906",
  month     =  dec,
  year      =  2019,
  keywords  = "Image edge detection;Machine learning;Computational
               modeling;Cloud computing;Throughput;Hardware;Edge
               computing;cloud computing;edge computing;deep learning;machine
               learning",
  doi       = "10.1109/BigData47090.2019.9005455"
}

@INPROCEEDINGS{Fu2020-gm,
  title     = "Automatic Information Infrastructure Planning",
  booktitle = "2020 Chinese Automation Congress ({CAC})",
  author    = "Fu, Jiayi",
  abstract  = "In order to improve the information infrastructure planning
               efficiency, it's necessary to develop an intelligent system to
               realize automatic planning. This work presents a smart planning
               system which includes three modules: the geographic information
               recognition module, which is able to identify different
               categories of land using maps, photos and satellite images; the
               scoring module, which calculates the location and various
               parameters of the required urban information infrastructures,
               such as the base stations, the pipelines, etc., and generates a
               certain number of candidate location coordinates to provide the
               scores of each facility to be planned, which arise from certain
               technical, economical, principles; The optimization module,
               which uses the optimization algorithms like gradient descent
               method, Newton method, Levenberg-Marquardt method, simulated
               annealing method, genetic algorithm, etc. to optimize the
               infrastructure locations and choose the best plan combination
               eventually.",
  pages     = "7211--7213",
  month     =  nov,
  year      =  2020,
  keywords  = "Planning;Optimization;Base stations;Simulated annealing;Long
               Term Evolution;Genetic algorithms;Satellites;base station
               planning;machine learning;Newton's method;gradient descent",
  issn      = "2688-0938",
  doi       = "10.1109/CAC51589.2020.9326992"
}

@INPROCEEDINGS{Sharkh2020-re,
  title     = "{CloudMach}: Cloud Computing Application Performance Improvement
               through Machine Learning",
  booktitle = "2020 {IEEE} Canadian Conference on Electrical and Computer
               Engineering ({CCECE})",
  author    = "Sharkh, Mohamed Abu and Xu, Yong and Leyder, Eric",
  abstract  = "Cloud computing is rapidly becoming the standard through which
               enterprises of all sizes fulfill their computing infrastructure
               demands. This work aims at exploring the impact that machine
               learning algorithms can have on Cloud application behavior
               profiling and prediction. Although classic machine learning
               algorithms have been used in Cloud Computing context before,
               cutting-edge algorithms like deep learning (DL) and
               reinforcement learning (RL) are yet to be convincingly exploited
               for this specific problem. Despite being a revelation with
               fields like image processing and speech recognition, these
               algorithms (deep neural networks for instance) face adoption
               challenges outside certain topics. There is a high demand for
               timely research work that dissects these algorithms and develops
               novel techniques to facilitate seamless adoption for Cloud
               providers and clients. In this work, we evaluate the efficiency
               of machine learning algorithms in the Cloud context by applying
               them to a large scale application resource utilization data set
               (TU Delft Bitbrains traces). The objective is to design a Cloud
               application behavior prediction technique based on machine
               learning predictors. Any improvement on prediction precision has
               direct impact on key performance indicators for both Cloud
               providers and Cloud tenants/clients. Experimental results show
               the potential of our approach to improve Cloud resource
               scheduling in a Cloud data center.",
  pages     = "1--6",
  month     =  aug,
  year      =  2020,
  keywords  = "Cloud computing;Machine learning algorithms;Resource
               management;Data centers;Time series analysis;Predictive
               models;Throughput;Cloud Computing;Deep Learning;Scalability",
  issn      = "2576-7046",
  doi       = "10.1109/CCECE47787.2020.9255686"
}

@INPROCEEDINGS{Duong2018-qk,
  title     = "Distributed Machine Learning on {IAAS} Clouds",
  booktitle = "2018 5th {IEEE} International Conference on Cloud Computing and
               Intelligence Systems ({CCIS})",
  author    = "Duong, Ta Nguyen Binh and Sang, Nguyen Quang",
  abstract  = "Training complex machine learning (ML) models with large
               datasets requires powerful computing infrastructure, which is
               costly to acquire and maintain. As a result, ML researchers turn
               to the cloud for on-demand and elastic resource provisioning
               capabilities. Two issues have arisen from this trend: 1) if not
               configured properly, training ML models on the cloud could incur
               significant cost and time, and 2) many researchers in ML tend to
               focus more on model and algorithm development, so they may not
               have enough time or skills to deal with system setup, resource
               selection and configuration. In this work, we propose and
               implement FC2: a web service for fast, convenient and
               cost-effective distributed ML model training over public cloud
               resource. Central to the effectiveness of FC2 is the ability to
               recommend an appropriate resource configuration in terms of cost
               and execution time for a given ML training task. Extensive
               experiments with real-world deep neural network models and
               datasets demonstrate the effectiveness of our solution.",
  pages     = "58--62",
  month     =  nov,
  year      =  2018,
  keywords  = "Distributed machine learning;Public clouds;Resource
               selection;Cluster deployment",
  doi       = "10.1109/CCIS.2018.8691150"
}

@INPROCEEDINGS{Min_Cao2007-by,
  title     = "Power control and transmission scheduling for network utility
               maximization in wireless networks",
  booktitle = "2007 46th {IEEE} Conference on Decision and Control",
  author    = "{Min Cao} and Raghunathan, V and Hanly, S and Sharma, V and
               Kumar, P R",
  abstract  = "We consider a joint power control and transmission scheduling
               problem in wireless networks with average power constraints.
               While the capacity region of a wireless network is convex, a
               characterization of this region is a hard problem. We formulate
               a network utility optimization problem involving time-sharing
               across different ``transmission modes,'' where each mode
               corresponds to the set of power levels used in the network. The
               structure of the optimal solution is a time-sharing across a
               small set of such modes. We use this structure to develop an
               efficient heuristic approach to finding a suboptimal solution
               through column generation iterations. This heuristic approach
               converges quite fast in simulations, and provides a tool for
               wireless network planning.",
  pages     = "5215--5221",
  month     =  dec,
  year      =  2007,
  file      = "All Papers/M/Min Cao et al. 2007 - Power control and transmission scheduling for network utility maximization in wireless networks.pdf",
  keywords  = "power control;radio networks;telecommunication
               control;telecommunication network management;telecommunication
               network planning;power control;transmission scheduling,
               problem;network utility optimization problem;wireless network
               planning;Power control;Utility programs;Wireless
               networks;Optimal scheduling;Routing;Contracts;Interference
               constraints;Time sharing computer systems;Broadcasting;Wireless
               mesh networks;Network utility maximization;power
               control;transmission scheduling;column generation;Wireless",
  issn      = "0191-2216",
  doi       = "10.1109/CDC.2007.4434829"
}

@INPROCEEDINGS{Bojovic2012-kp,
  title     = "A Cognitive scheme for Radio Admission Control in {LTE} systems",
  booktitle = "2012 3rd International Workshop on Cognitive Information
               Processing ({CIP})",
  author    = "Bojovic, B and Baldo, N and Dini, P",
  abstract  = "In order to provide QoS requirements in high speed future
               communication networks, such as LTE, the operator has to provide
               a Radio Admission Control algorithm which will guarantee the QoS
               of different service types (e.g. voice, data, video, ftp) while
               maximizing radio resource utilization. In this paper we propose
               a Cognitive Radio Admission Control Scheme based on a Multilayer
               Feed-forward Neural Network. According to our scheme, the eNodeB
               performs Radio Admission Control using a cognitive engine that
               learns from the past experience how the admission of a new
               session would affect the QoS of all sessions in the future. We
               implemented our Radio Admission Control Scheme using a LTE-EPC
               simulator. Since our scheme is based on learning from the past
               experience, we expect that it will be able to satisfy QoS
               requirements, in a variety of realistic scenarios, which can not
               be accounted for in analytical models.",
  pages     = "1--3",
  month     =  may,
  year      =  2012,
  file      = "All Papers/B/Bojovic et al. 2012 - A Cognitive scheme for Radio Admission Control in LTE systems.pdf",
  keywords  = "cognitive radio;feedforward neural nets;Long Term
               Evolution;quality of service;telecommunication
               computing;telecommunication congestion control;cognitive
               scheme;LTE systems;QoS requirements;high speed future
               communication networks;radio resource utilization;cognitive
               radio admission control scheme;multilayer feedforward neural
               network;eNodeB;cognitive engine;Neural networks;Admission
               control;Measurement;Resource management;Wireless
               communication;Training;Boring;MLNetworking",
  issn      = "2327-1698",
  doi       = "10.1109/CIP.2012.6232936"
}

@INPROCEEDINGS{Stage2009-cd,
  title     = "Network-aware migration control and scheduling of differentiated
               virtual machine workloads",
  booktitle = "2009 {ICSE} Workshop on Software Engineering Challenges of Cloud
               Computing",
  author    = "Stage, A and Setzer, T",
  abstract  = "Server virtualization enables dynamic workload management for
               data centers. However, especially live migrations of virtual
               machines (VM) induce significant overheads on physical hosts and
               the shared network infrastructure possibly leading to host
               overloads and SLA violations of co-hosted applications. While
               some recent work addresses the impact of live migrations on CPUs
               of physical hosts, little attention has been given to the
               control and optimization of migration algorithms and
               migration-related network bandwidth consumption. In this paper
               we introduce network topology aware scheduling models for VM
               live migrations. We propose a scheme for classifying VMs based
               on their workload characteristics and propose adequate resource
               and migration scheduling models for each class, taking network
               bandwidth requirements of migrations and network topologies into
               account. We also underline the necessity for additional
               migration control parameters for efficient migration scheduling.",
  pages     = "9--14",
  month     =  may,
  year      =  2009,
  file      = "All Papers/S/Stage and Setzer 2009 - Network-aware migration control and scheduling of differentiated virtual machine workloads.pdf",
  keywords  = "computer centres;virtual machines;network-aware migration
               control;differentiated virtual machine workloads;dynamic
               workload management;data centers;host overloads;SLA
               violations;network bandwidth consumption;network topology aware
               scheduling;migration scheduling;Virtual machining;Virtual
               manufacturing;Bandwidth;Switches;Network topology;Network
               servers;Voice mail;Platform virtualization;Load
               management;Control systems;Datacentre",
  doi       = "10.1109/CLOUD.2009.5071527"
}

@INPROCEEDINGS{Huang2011-jo,
  title     = "Power Consumption of Virtual Machine Live Migration in Clouds",
  booktitle = "2011 Third International Conference on Communications and Mobile
               Computing",
  author    = "Huang, Qiang and Gao, Fengqian and Wang, Rui and Qi, Zhengwei",
  abstract  = "Virtualization Technology has been employed increasingly widely
               in modern data centers in order to improve its energy
               efficiency. In particular, the capability of virtual machine(VM)
               migration brings multiple benefits for such as resources(CPU,
               memory, et al.) distribution, energy aware consolidation.
               However, the migration of virtual machines itself brings extra
               power consumption. For this reason, a better understanding of
               its effect on system power consumption is highly desirable. In
               this paper, we present a power consumption evaluation on the
               effects of live migration of VMs. Results show that the power
               overhead of migration is much less in the scenario of employing
               the strategy of consolidation than the regular deployment
               without using consolidation. Our results are based on the
               typical physical server, the power of which is linear model of
               CPU utilization percentage.",
  pages     = "122--125",
  month     =  apr,
  year      =  2011,
  keywords  = "Servers;Power demand;Virtual machining;USA
               Councils;Computers;Quality of service;Computational
               modeling;data center;power consumption;live migration",
  doi       = "10.1109/CMC.2011.62"
}

@ARTICLE{Alsheikh2014-lk,
  title    = "Machine Learning in Wireless Sensor Networks: Algorithms,
              Strategies, and Applications",
  author   = "Alsheikh, M A and Lin, S and Niyato, D and Tan, H",
  abstract = "Wireless sensor networks (WSNs) monitor dynamic environments that
              change rapidly over time. This dynamic behavior is either caused
              by external factors or initiated by the system designers
              themselves. To adapt to such conditions, sensor networks often
              adopt machine learning techniques to eliminate the need for
              unnecessary redesign. Machine learning also inspires many
              practical solutions that maximize resource utilization and
              prolong the lifespan of the network. In this paper, we present an
              extensive literature review over the period 2002-2013 of machine
              learning methods that were used to address common issues in WSNs.
              The advantages and disadvantages of each proposed algorithm are
              evaluated against the corresponding problem. We also provide a
              comparative guide to aid WSN designers in developing suitable
              machine learning solutions for their specific application
              challenges.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  16,
  number   =  4,
  pages    = "1996--2018",
  year     =  2014,
  file     = "All Papers/A/Alsheikh et al. 2014 - Machine Learning in Wireless Sensor Networks - Algorithms, Strategies, and Applications.pdf",
  keywords = "learning (artificial intelligence);wireless sensor
              networks;machine learning;wireless sensor networks;resource
              utilization;network lifespan;AD 2002-13;Wireless sensor
              networks;Routing;Machine learning algorithms;Clustering
              algorithms;Algorithm design and analysis;Principal component
              analysis;Classification algorithms;Wireless sensor
              networks;machine learning;data
              mining;security;localization;clustering;data aggregation;event
              detection;query processing;data integrity;fault detection;medium
              access control;compressive sensing;ML;MLNetworking",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2014.2320099"
}

@ARTICLE{Al-Fuqaha2015-gi,
  title    = "Internet of Things: A Survey on Enabling Technologies, Protocols,
              and Applications",
  author   = "Al-Fuqaha, A and Guizani, M and Mohammadi, M and Aledhari, M and
              Ayyash, M",
  abstract = "This paper provides an overview of the Internet of Things (IoT)
              with emphasis on enabling technologies, protocols, and
              application issues. The IoT is enabled by the latest developments
              in RFID, smart sensors, communication technologies, and Internet
              protocols. The basic premise is to have smart sensors collaborate
              directly without human involvement to deliver a new class of
              applications. The current revolution in Internet, mobile, and
              machine-to-machine (M2M) technologies can be seen as the first
              phase of the IoT. In the coming years, the IoT is expected to
              bridge diverse technologies to enable new applications by
              connecting physical objects together in support of intelligent
              decision making. This paper starts by providing a horizontal
              overview of the IoT. Then, we give an overview of some technical
              details that pertain to the IoT enabling technologies, protocols,
              and applications. Compared to other survey papers in the field,
              our objective is to provide a more thorough summary of the most
              relevant protocols and application issues to enable researchers
              and application developers to get up to speed quickly on how the
              different protocols fit together to deliver desired
              functionalities without having to go through RFCs and the
              standards specifications. We also provide an overview of some of
              the key IoT challenges presented in the recent literature and
              provide a summary of related research work. Moreover, we explore
              the relation between the IoT and other emerging technologies
              including big data analytics and cloud and fog computing. We also
              present the need for better horizontal integration among IoT
              services. Finally, we present detailed service use-cases to
              illustrate how the different protocols presented in the paper fit
              together to deliver desired IoT services.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  17,
  number   =  4,
  pages    = "2347--2376",
  year     =  2015,
  file     = "All Papers/A/Al-Fuqaha et al. 2015 - Internet of Things - A Survey on Enabling Technologies, Protocols, and Applications.pdf",
  keywords = "Internet of Things;mobile communication;protocols;radiofrequency
              identification;Internet of Things;RFID;smart
              sensors;communication technologies;Internet
              protocols;machine-to-machine;M2M technologies;intelligent
              decision making;fog computing;big data analytics;cloud
              computing;Internet of things;Computer architecture;Radiofrequency
              identification;Intelligent sensors;Mobile communication;Internet
              of things;IoT;CoAP;MQTT;AMQP;XMPP;DDS;mDNS;IoT Gateway;Internet
              of Things (IoT);CoAP;MQTT;AMQP;XMPP;DDS;mDNS;IoT
              gateway;5G6G;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2015.2444095"
}

@ARTICLE{Chen2017-ie,
  title    = "{Machine-to-Machine} Communications in {Ultra-Dense}
              {Networks---A} Survey",
  author   = "Chen, S and Ma, R and Chen, H and Zhang, H and Meng, W and Liu, J",
  abstract = "To achieve 1000-fold capacity increase in 5G wireless
              communications, ultradense network (UDN) is believed to be one of
              the key enabling technologies. Most of the previous research
              activities on UDNs were based very much on human-to-human
              communications. However, to provide ubiquitous Internet of Things
              services, machine-to-machine (M2M) communications will play a
              critical role in 5G systems. As the number of machine-oriented
              connections increases, it is expected that supporting M2M
              communications is an essential requirement in all future UDNs. In
              this paper, we aim to bridge the gaps between M2M communications
              and UDNs, which were commonly considered as two separate issues
              in the literature. The paper begins with a brief introduction on
              M2M communications and UDNs, and then will discuss the issues on
              the roles of M2M communications in future UDNs. We will identify
              different ways to implement M2M communications in the UDNs from
              the perspectives of layered architecture, including physical,
              media access control, network, and application layers. Other two
              important issues, i.e., security and network virtualization, will
              also be addressed. Before the end of this paper, we will give a
              summary on identified research topics for future studies.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  19,
  number   =  3,
  pages    = "1478--1503",
  year     =  2017,
  keywords = "5G mobile communication;Internet of Things;machine-to-machine
              communication;ubiquitous computing;machine-to-machine
              communication;ultra-dense network;5G wireless
              communication;human-to-human communication;ubiquitous Internet of
              Things services;application layers;physical layers;media access
              control layers;network layers;network virtualization;network
              security;5G mobile communications;Internet of Things;Microcell
              networks;5G;machine-type communication;ultra-dense
              network;Internet of Things;small
              cell;5G6G;EdgeFogCloudIoT;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2017.2678518"
}

@ARTICLE{Bui2017-nr,
  title    = "A Survey of Anticipatory Mobile Networking: {Context-Based}
              Classification, Prediction Methodologies, and Optimization
              Techniques",
  author   = "Bui, N and Cesana, M and Hosseini, S A and Liao, Q and
              Malanchini, I and Widmer, J",
  abstract = "A growing trend for information technology is to not just react
              to changes, but anticipate them as much as possible. This
              paradigm made modern solutions, such as recommendation systems, a
              ubiquitous presence in today's digital transactions. Anticipatory
              networking extends the idea to communication technologies by
              studying patterns and periodicity in human behavior and network
              dynamics to optimize network performance. This survey collects
              and analyzes recent papers leveraging context information to
              forecast the evolution of network conditions and, in turn, to
              improve network performance. In particular, we identify the main
              prediction and optimization tools adopted in this body of work
              and link them with objectives and constraints of the typical
              applications and scenarios. Finally, we consider open challenges
              and research directions to make anticipatory networking part of
              next generation networks.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  19,
  number   =  3,
  pages    = "1790--1821",
  year     =  2017,
  file     = "All Papers/B/Bui et al. 2017 - A Survey of Anticipatory Mobile Networking - Context-Based Classification, Prediction Methodologies, and Optimization Techniques.pdf",
  keywords = "mobile radio;next generation networks;optimisation;pattern
              classification;anticipatory mobile networking;context-based
              classification;prediction methodology;optimization
              technique;information technology;digital
              transactions;communication technologies;optimization tools;next
              generation networks;Optimization;Context;Predictive
              models;Context modeling;Mobile computing;Mobile
              communication;Anticipatory;prediction;optimization;5G;mobile
              networks;GeneralNetworking",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2017.2694140"
}

@ARTICLE{Mao2017-dl,
  title    = "A Survey on Mobile Edge Computing: The Communication Perspective",
  author   = "Mao, Y and You, C and Zhang, J and Huang, K and Letaief, K B",
  abstract = "Driven by the visions of Internet of Things and 5G
              communications, recent years have seen a paradigm shift in mobile
              computing, from the centralized mobile cloud computing toward
              mobile edge computing (MEC). The main feature of MEC is to push
              mobile computing, network control and storage to the network
              edges (e.g., base stations and access points) so as to enable
              computation-intensive and latency-critical applications at the
              resource-limited mobile devices. MEC promises dramatic reduction
              in latency and mobile energy consumption, tackling the key
              challenges for materializing 5G vision. The promised gains of MEC
              have motivated extensive efforts in both academia and industry on
              developing the technology. A main thrust of MEC research is to
              seamlessly merge the two disciplines of wireless communications
              and mobile computing, resulting in a wide-range of new designs
              ranging from techniques for computation offloading to network
              architectures. This paper provides a comprehensive survey of the
              state-of-the-art MEC research with a focus on joint
              radio-and-computational resource management. We also discuss a
              set of issues, challenges, and future research directions for MEC
              research, including MEC system deployment, cache-enabled MEC,
              mobility management for MEC, green MEC, as well as privacy-aware
              MEC. Advancements in these directions will facilitate the
              transformation of MEC from theory to practice. Finally, we
              introduce recent standardization efforts on MEC as well as some
              typical MEC application scenarios.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  19,
  number   =  4,
  pages    = "2322--2358",
  year     =  2017,
  file     = "All Papers/M/Mao et al. 2017 - A Survey on Mobile Edge Computing - The Communication Perspective.pdf;All Papers/M/Mao et al. 2017 - A Survey on Mobile Edge Computing - The Communication Perspective.pdf",
  keywords = "5G mobile communication;cloud computing;Internet of Things;mobile
              computing;mobility management (mobile radio);computation
              offloading;state-of-the-art MEC research;radio;MEC system
              deployment;mobility management;green MEC;privacy-aware
              MEC;typical MEC application scenarios;mobile edge
              computing;centralized mobile cloud;resource-limited mobile
              devices;latency energy consumption;mobile energy
              consumption;computational resource management;Cloud
              computing;Edge computing;5G mobile communication;Mobile
              computing;Wireless communication;Mobile edge computing;fog
              computing;mobile cloud computing;computation offloading;resource
              management;green computing;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2017.2745201"
}

@ARTICLE{Noormohammadpour2018-rk,
  title     = "Datacenter Traffic Control: Understanding Techniques and
               Tradeoffs",
  author    = "Noormohammadpour, Mohammad and Raghavendra, Cauligi S",
  abstract  = "Datacenters provide cost-effective and flexible access to
               scalable compute and storage resources necessary for today's
               cloud computing needs. A typical datacenter is made up of
               thousands of servers connected with a large network and usually
               managed by one operator. To provide quality access to the
               variety of applications and services hosted on datacenters and
               maximize performance, it deems necessary to use datacenter
               networks effectively and efficiently. Datacenter traffic is
               often a mix of several classes with different priorities and
               requirements. This includes user-generated interactive traffic,
               traffic with deadlines, and long-running traffic. To this end,
               custom transport protocols and traffic management techniques
               have been developed to improve datacenter network performance.
               In this tutorial paper, we review the general architecture of
               datacenter networks, various topologies proposed for them, their
               traffic properties, general traffic control challenges in
               datacenters and general traffic control objectives. The purpose
               of this paper is to bring out the important characteristics of
               traffic control in datacenters and not to survey all existing
               solutions (as it is virtually impossible due to massive body of
               existing research). We hope to provide readers with a wide range
               of options and factors while considering a variety of traffic
               control mechanisms. We discuss various characteristics of
               datacenter traffic control, including management schemes,
               transmission control, traffic shaping, prioritization, load
               balancing, multipathing, and traffic scheduling. Next, we point
               to several open challenges as well as new and interesting
               networking paradigms. At the end of this paper, we briefly
               review inter-datacenter networks that connect geographically
               dispersed datacenters, which have been receiving increasing
               attention recently and pose interesting and novel research
               problems. To measure the performance of datacenter networks,
               different performance metrics have been used, such as flow
               completion times, deadline miss rate, throughput, and fairness.
               Depending on the application and user requirements, some metrics
               may need more attention. While investigating different traffic
               control techniques, we point out the tradeoffs involved in terms
               of costs, complexity, and performance. We find that a
               combination of different traffic control techniques may be
               necessary at particular entities and layers in the network to
               improve the variety of performance metrics. We also find that
               despite significant research efforts, there are still open
               problems that demand further attention from the research
               community.",
  journal   = "IEEE Communications Surveys \& Tutorials",
  publisher = "IEEE",
  volume    =  20,
  number    =  2,
  pages     = "1492--1525",
  year      =  2018,
  keywords  = "DataCenter",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/COMST.2017.2782753"
}

@ARTICLE{Zhang2018-mj,
  title    = "A Survey on Virtual Machine Migration: Challenges, Techniques,
              and Open Issues",
  author   = "Zhang, Fei and Liu, Guangming and Fu, Xiaoming and Yahyapour,
              Ramin",
  abstract = "When users flood in cloud data centers, how to efficiently manage
              hardware resources and virtual machines (VMs) in a data center to
              both lower economical cost and ensure a high service quality
              becomes an inevitable work for cloud providers. VM migration is a
              cornerstone technology for the majority of cloud management
              tasks. It frees a VM from the underlying hardware. This feature
              brings a plenty of benefits to cloud providers and users. Many
              researchers are focusing on pushing its cutting edge. In this
              paper, we first give an overview of VM migration and discuss both
              its benefits and challenges. VM migration schemes are classified
              from three perspectives: 1) manner; 2) distance; and 3)
              granularity. The studies on non-live migration are simply
              reviewed, and then those on live migration are comprehensively
              surveyed based on the three main challenges it faces: 1) memory
              data migration; 2) storage data migration; and 3) network
              connection continuity. The works on quantitative analysis of VM
              migration performance are also elaborated. With the development
              and evolution of cloud computing, user mobility becomes an
              important motivation for live VM migration in some scenarios
              (e.g., fog computing). Thus, the studies regarding linking VM
              migration to user mobility are summarized as well. At last, we
              list the open issues which are waiting for solutions or further
              optimizations on live VM migration.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  20,
  number   =  2,
  pages    = "1206--1243",
  year     =  2018,
  file     = "All Papers/Z/Zhang et al. 2018 - A Survey on Virtual Machine Migration - Challenges, Techniques, and Open Issues.pdf",
  keywords = "Cloud computing;Servers;Hardware;Virtual machining;Electronic
              mail;Taxonomy;Cloud computing;data center;virtual machine
              migration;pre-copy;post-copy;hybrid-copy;network connection;user
              mobility;performance analysis",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2018.2794881"
}

@ARTICLE{Mukherjee2018-sj,
  title    = "Survey of Fog Computing: Fundamental, Network Applications, and
              Research Challenges",
  author   = "Mukherjee, Mithun and Shu, Lei and Wang, Di",
  abstract = "Fog computing is an emerging paradigm that extends computation,
              communication, and storage facilities toward the edge of a
              network. Compared to traditional cloud computing, fog computing
              can support delay-sensitive service requests from end-users (EUs)
              with reduced energy consumption and low traffic congestion.
              Basically, fog networks are viewed as offloading to core
              computation and storage. Fog nodes in fog computing decide to
              either process the services using its available resource or send
              to the cloud server. Thus, fog computing helps to achieve
              efficient resource utilization and higher performance regarding
              the delay, bandwidth, and energy consumption. This survey starts
              by providing an overview and fundamental of fog computing
              architecture. Furthermore, service and resource allocation
              approaches are summarized to address several critical issues such
              as latency, and bandwidth, and energy consumption in fog
              computing. Afterward, compared to other surveys, this paper
              provides an extensive overview of state-of-the-art network
              applications and major research aspects to design these networks.
              In addition, this paper highlights ongoing research effort, open
              challenges, and research trends in fog computing.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  20,
  number   =  3,
  pages    = "1826--1857",
  year     =  2018,
  keywords = "Edge computing;Cloud computing;Task analysis;Companies;Computer
              architecture;Delays;Sensors;Fog computing;fog networks;fog-radio
              access networks;edge computing;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2018.2814571"
}

@ARTICLE{Larsen2019-ot,
  title    = "A Survey of the Functional Splits Proposed for {5G} Mobile
              Crosshaul Networks",
  author   = "Larsen, Line M P and Checko, Aleksandra and Christiansen, Henrik
              L",
  abstract = "Pacing the way toward 5G has lead researchers and industry in the
              direction of centralized processing known from Cloud-Radio Access
              Networks (C-RAN). In C-RAN research, a variety of different
              functional splits is presented by different names and focusing on
              different directions. The functional split determines how many
              base station functions to leave locally, close to the user, with
              the benefit of relaxing fronthaul network bitrate and delay
              requirements, and how many functions to centralize with the
              possibility of achieving greater processing benefits. This paper
              presents for the first time a comprehensive overview
              systematizing the different work directions for both research and
              industry, while providing a detailed description of each
              functional split option and an assessment of the advantages and
              disadvantages. This paper gives an overview of where the most
              effort has been directed in terms of functional splits, and where
              there is room for further studies. The standardization currently
              taking place is also considered and mapped into the research
              directions. It is investigated how the fronthaul network will be
              affected by the choice of functional split, both in terms of
              bitrates and latency, and as the different functional splits
              provide different advantages and disadvantages, the option of
              flexible functional splits is also looked into.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  21,
  number   =  1,
  pages    = "146--172",
  year     =  2019,
  file     = "All Papers/L/Larsen et al. 2019 - A Survey of the Functional Splits Proposed for 5G Mobile Crosshaul Networks.pdf",
  keywords = "Physical layer;Protocols;3GPP;Long Term Evolution;Bit
              rate;Antennas;Computer architecture;Functional
              split;crosshaul;X-haul;C-RAN;fronthaul;standardization;industry;network
              architecture;Mobile\_Wireless",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2018.2868805"
}

@ARTICLE{Abdullah2019-sa,
  title     = "Segment Routing in Software Defined Networks: A Survey",
  author    = "Abdullah, Zahraa N and Ahmad, Imtiaz and Hussain, Iftekhar",
  abstract  = "Segment routing (SR) has emerged as a promising source-routing
               methodology to overcome the challenges in the current routing
               schemes. It has received noticeable attention both in industry
               and academia, due to its flexibility, scalability, and
               applicability, especially in software defined networks. The
               emerging cloud services require strict service level agreements
               such as packet loss, delay, and jitter. Studies have shown that
               traditional network architectures lack the essential flexibility
               and scalability to offer these services. To combat this, a more
               flexible and agile routing paradigm of SR enables a source node
               to steer an incoming packet along a performance engineered path
               represented as an ordered list of instructions called segment
               list. This is encoded as a multiprotocol label switching label
               stack or an IPv6 address list in the packet header. This paper
               provides a comprehensive review of the novel SR technology by
               describing its architecture, operations, and key applications to
               date. SR paradigm can be effectively applied to a wide range of
               network applications, such as traffic engineering, network
               resiliency, network monitoring, and service function chaining,
               to achieve efficient network solutions. Furthermore, this paper
               identifies an interesting set of future research directions and
               open issues that can help realize the full potential of the
               emergent SR paradigm.",
  journal   = "IEEE Communications Surveys \& Tutorials",
  publisher = "IEEE",
  volume    =  21,
  number    =  1,
  pages     = "464--486",
  year      =  2019,
  keywords  = "FutureInternet",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/COMST.2018.2869754"
}

@ARTICLE{Khorov2019-er,
  title     = "A Tutorial on {IEEE} 802.11ax High Efficiency {WLANs}",
  author    = "Khorov, E and Kiryanov, A and Lyakhov, A and Bianchi, G",
  abstract  = "While celebrating the 21st year since the very first IEEE 802.11
               ``legacy'' 2 Mbit/s wireless local area network standard, the
               latest Wi-Fi newborn is today reaching the finish line, topping
               the remarkable speed of 10 Gbit/s. IEEE 802.11ax was launched in
               May 2014 with the goal of enhancing throughput-per-area in
               high-density scenarios. The first 802.11ax draft versions,
               namely, D1.0 and D2.0, were released at the end of 2016 and
               2017. Focusing on a more mature version D3.0, in this tutorial
               paper, we help the reader to smoothly enter into the several
               major 802.11ax breakthroughs, including a brand new orthogonal
               frequency-division multiple access-based random access approach
               as well as novel spatial frequency reuse techniques. In
               addition, this tutorial will highlight selected significant
               improvements (including physical layer enhancements, multi-user
               multiple input multiple output extensions, power saving
               advances, and so on) which make this standard a very significant
               step forward with respect to its predecessor 802.11ac.",
  journal   = "IEEE Communications Surveys Tutorials",
  publisher = "IEEE",
  volume    =  21,
  number    =  1,
  pages     = "197--216",
  year      =  2019,
  file      = "All Papers/K/Khorov et al. 2019 - A Tutorial on IEEE 802.11ax High Efficiency WLANs.pdf",
  keywords  = "frequency allocation;frequency division multiple access;MIMO
               communication;next generation networks;OFDM modulation;wireless
               channels;wireless LAN;brand new orthogonal frequency-division
               multiple access-based random access approach;multiuser multiple
               input multiple output extensions;IEEE 802.11ax high
               efficiency;wireless local area network standard;latest
               Wi-Fi;finish line;throughput-per-area;high-density
               scenarios;802.11ax draft versions;spatial frequency reuse
               techniques;bit rate 2.0 Mbit/s;IEEE 802.11
               Standard;Tutorials;Wireless fidelity;Throughput;MIMO
               communication;Wireless LAN;quality of service;OFDM;IEEE
               802.11ax;high efficiency WLANs;Wi-Fi;dense deployment;OFDMA;UL
               MU-MIMO;Good;Done;Wireless",
  issn      = "1553-877X",
  doi       = "10.1109/COMST.2018.2871099"
}

@ARTICLE{Sharma2020-oj,
  title    = "Toward Massive Machine Type Communications in {Ultra-Dense}
              Cellular {IoT} Networks: Current Issues and Machine
              {Learning-Assisted} Solutions",
  author   = "Sharma, S K and Wang, X",
  abstract = "The ever-increasing number of resource-constrained machine-type
              communication (MTC) devices is leading to the critical challenge
              of fulfilling diverse communication requirements in dynamic and
              ultra-dense wireless environments. Among different application
              scenarios that the upcoming 5G and beyond cellular networks are
              expected to support, such as enhanced mobile broadband (eMBB),
              massive machine type communications (mMTCs), and ultra-reliable
              and low latency communications (URLLCs), the mMTC brings the
              unique technical challenge of supporting a huge number of MTC
              devices in cellular networks, which is the main focus of this
              paper. The related challenges include quality of service (QoS)
              provisioning, handling highly dynamic and sporadic MTC traffic,
              huge signalling overhead, and radio access network (RAN)
              congestion. In this regard, this paper aims to identify and
              analyze the involved technical issues, to review recent advances,
              to highlight potential solutions and to propose new research
              directions. First, starting with an overview of mMTC features and
              QoS provisioning issues, we present the key enablers for mMTC in
              cellular networks. Along with the highlights on the inefficiency
              of the legacy random access (RA) procedure in the mMTC scenario,
              we then present the key features and channel access mechanisms in
              the emerging cellular IoT standards, namely, LTE-M and narrowband
              IoT (NB-IoT). Subsequently, we present a framework for the
              performance analysis of transmission scheduling with the QoS
              support along with the issues involved in short data packet
              transmission. Next, we provide a detailed overview of the
              existing and emerging solutions toward addressing RAN congestion
              problem, and then identify potential advantages, challenges, and
              use cases for the applications of emerging machine learning (ML)
              techniques in ultra-dense cellular networks. Out of several ML
              techniques, we focus on the application of low-complexity
              Q-learning approach in the mMTC scenario along with the recent
              advances toward enhancing its learning performance and
              convergence. Finally, we discuss some open research challenges
              and promising future research directions.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  1,
  pages    = "426--471",
  year     =  2020,
  file     = "All Papers/S/Sharma and Wang 2020 - Toward Massive Machine Type Communications in Ult ... IoT Networks - Current Issues and Machine Learning-Assisted Solutions.pdf",
  keywords = "5G mobile communication;cellular radio;Internet of
              Things;learning (artificial intelligence);Long Term
              Evolution;machine-to-machine communication;quality of
              service;radio access networks;telecommunication network
              reliability;telecommunication scheduling;telecommunication
              traffic;ultra-dense wireless environments;ultra-reliable and low
              latency communications;unique technical challenge;MTC
              devices;quality of service provisioning;sporadic MTC traffic;huge
              signalling overhead;radio access network congestion;legacy random
              access procedure;mMTC scenario;channel access mechanisms;emerging
              cellular IoT standards;narrowband IoT;NB-IoT;QoS support;machine
              learning techniques;ultra-dense cellular networks;massive machine
              type communications;ultra-dense cellular IoT networks;machine
              learning-assisted solutions;resource-constrained machine-type
              communication devices;diverse communication
              requirements;low-complexity Q-learning approach;Internet of
              Things;Quality of service;Cellular networks;Wireless
              communication;Protocols;Tutorials;5G mobile
              communication;Cellular IoT;mMTC;5G and beyond wireless;RAN
              congestion;machine learning;Q-learning;LTE-M;NB-IoT;ML;5G6G",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2019.2916177"
}

@ARTICLE{Das2020-pf,
  title    = "A Survey on Controller Placement in {SDN}",
  author   = "Das, T and Sridharan, V and Gurusamy, M",
  abstract = "In recent years, Software Defined Networking (SDN) has emerged as
              a pivotal element not only in data-centers and wide-area
              networks, but also in next generation networking architectures
              such as Vehicular ad hoc network and 5G. SDN is characterized by
              decoupled data and control planes, and logically centralized
              control plane. The centralized control plane in SDN offers
              several opportunities as well as challenges. A key design choice
              of the SDN control plane is placement of the controller(s), which
              impacts a wide range of network issues ranging from latency to
              resiliency, from energy efficiency to load balancing, and so on.
              In this paper, we present a comprehensive survey on the
              controller placement problem (CPP) in SDN. We introduce the CPP
              in SDN and highlight its significance. We present the classical
              CPP formulation along with its supporting system model. We also
              discuss a wide range of the CPP modeling choices and associated
              metrics. We classify the CPP literature based on the objectives
              and methodologies. Apart from the primary use-cases of the CPP in
              data-center networks and wide area networks, we also examine the
              recent application of the CPP in several new domains such as
              mobile/cellular networks, 5G, named data networks, wireless mesh
              networks and VANETs. We conclude our survey with discussion on
              open issues and future scope of this topic.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  1,
  pages    = "472--503",
  year     =  2020,
  file     = "All Papers/D/Das et al. 2020 - A Survey on Controller Placement in SDN.pdf",
  keywords = "computer centres;next generation networks;resource
              allocation;software defined networking;telecommunication
              control;telecommunication network topology;wide area
              networks;software defined networking;data-centers;wide-area
              networks;generation networking architectures;decoupled
              data;control planes;logically centralized control plane;SDN
              control plane;network issues;controller placement
              problem;classical CPP formulation;CPP modeling choices;CPP
              literature;data-center networks;wide area networks;named data
              networks;wireless mesh networks;Resilience;Quality of
              service;Load management;Protocols;Optical switches;Software
              defined networking;controller placement problem;next generation
              networking;NFV\_SDN",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2019.2935453"
}

@ARTICLE{Rodrigues2020-fm,
  title    = "Machine Learning Meets Computation and Communication Control in
              Evolving Edge and Cloud: Challenges and Future Perspective",
  author   = "Rodrigues, T K and Suto, K and Nishiyama, H and Liu, J and Kato,
              N",
  abstract = "Mobile Edge Computing (MEC) is considered an essential future
              service for the implementation of 5G networks and the Internet of
              Things, as it is the best method of delivering computation and
              communication resources to mobile devices. It is based on the
              connection of the users to servers located on the edge of the
              network, which is especially relevant for real-time applications
              that demand minimal latency. In order to guarantee a
              resource-efficient MEC (which, for example, could mean improved
              Quality of Service for users or lower costs for service
              providers), it is important to consider certain aspects of the
              service model, such as where to offload the tasks generated by
              the devices, how many resources to allocate to each user
              (specially in the wired or wireless device-server communication)
              and how to handle inter-server communication. However, in the MEC
              scenarios with many and varied users, servers and applications,
              these problems are characterized by parameters with exceedingly
              high levels of dimensionality, resulting in too much data to be
              processed and complicating the task of finding efficient
              configurations. This will be particularly troublesome when 5G
              networks and Internet of Things roll out, with their massive
              amounts of devices. To address this concern, the best solution is
              to utilize Machine Learning (ML) algorithms, which enable the
              computer to draw conclusions and make predictions based on
              existing data without human supervision, leading to quick
              near-optimal solutions even in problems with high dimensionality.
              Indeed, in scenarios with too much data and too many parameters,
              ML algorithms are often the only feasible alternative. In this
              paper, a comprehensive survey on the use of ML in MEC systems is
              provided, offering an insight into the current progress of this
              research area. Furthermore, helpful guidance is supplied by
              pointing out which MEC challenges can be solved by ML solutions,
              what are the current trending algorithms in frontier ML research
              and how they could be used in MEC. These pieces of information
              should prove fundamental in encouraging future research that
              combines ML and MEC.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  1,
  pages    = "38--67",
  year     =  2020,
  file     = "All Papers/R/Rodrigues et al. 2020 - Machine Learning Meets Computation and Communica ... ontrol in Evolving Edge and Cloud - Challenges and Future Perspective.pdf;All Papers/R/Rodrigues et al. 2020 - Machine_Learning_Meets_Computation_and_Communication_Control_in_Evolving_Edge_and_Cloud_Challenges_and_Future_Perspective.pdf",
  keywords = "5G mobile communication;cloud computing;learning (artificial
              intelligence);mobile computing;demand minimal;resource-efficient
              MEC;service providers;service model;wired device-server
              communication;wireless device-server communication;inter-server
              communication;MEC scenarios;efficient configurations;Internet of
              Things;near-optimal solutions;ML algorithms;MEC systems;MEC
              challenges;ML solutions;current trending algorithms;frontier ML
              research;Machine learning;communication control;future
              perspective;essential future service;communication
              resources;mobile devices;real-time applications;mobile edge
              computing;5G networks;Cloud computing;Servers;Edge
              computing;Mobile handsets;5G mobile communication;Task
              analysis;Internet of Things;Mobile edge
              computing;cloudlet;scalability;communication/computation resource
              management;ML;artificial
              intelligence;ML;EdgeFogCloudIoT;MLAspects",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2019.2943405"
}

@ARTICLE{Niemiec2020-xw,
  title    = "A Survey on {FPGA} Support for the Feasible Execution of
              Virtualized Network Functions",
  author   = "Niemiec, G S and Batista, L M S and Schaeffer-Filho, A E and
              Nazar, G L",
  abstract = "Network Functions Virtualization (NFV) has received considerable
              attention in the past few years, both from industry and academia,
              due to its potential for reducing capital and operational
              expenditures, thus enabling faster innovation in networks. NFV
              proposes decoupling network functions from fixed hardware
              platforms and implementing them as virtual machines on
              off-the-shelf servers. Although potentially able to provide the
              mentioned benefits, software-based implementations of
              compute-intensive network functions still struggle to perform at
              the desired speed, especially when requiring line-rate processing
              for ever-faster communication links. To address this, hardware
              acceleration can be used to improve the throughput and latency of
              virtualized network functions (VNFs). In order to provide the
              advantages foreseen for NFV, however, such accelerators cannot be
              fixed and application-specific hardware, since they need to cope
              with new VNFs as well as fluctuations in demand. In this context,
              Field-Programmable Gate Arrays (FPGAs) are particularly suitable,
              since they are able to provide high-performance implementations
              of network functions and they are completely reprogrammable,
              thereby being able to implement different VNFs even after
              deployment. There have been many recent efforts to enable the use
              of FPGAs in the NFV context, including efficient implementations
              of network functions on FPGAs, platforms to manage the
              integration and coexistence of multiple VNFs on an FPGA, and
              high-level synthesis tools especially tailored to ease the
              programming of VNFs for FPGAs. In this work we survey previous
              work covering these aspects, and discuss the main open research
              challenges that must be addressed before FPGA adoption in NFV
              infrastructures becomes effectively seamless and efficient.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  1,
  pages    = "504--525",
  year     =  2020,
  file     = "All Papers/N/Niemiec et al. 2020 - A Survey on FPGA Support for the Feasible Execution of Virtualized Network Functions.pdf",
  keywords = "field programmable gate arrays;high level languages;program
              compilers;programmable packet parsing;field programmable gate
              array;packet classification function;packet security
              function;high-level programmability;PP high-level language;Xilinx
              Virtex-7 FPGA device;Pipeline processing;Field programmable gate
              arrays;Protocols;Throughput;Program processors;Pipelines;Computer
              architecture;High-speed packet processing;FPGA-based parallel
              processing;Domain-specific languages and compilers; virtual
              machines; virtualisation; hardware platforms; FPGAs; NFV; network
              functions virtualization; field-programmable gate arrays;
              Virtualization; Tutorials; Hardware acceleration; Servers;
              Industries; FPGA;NFV\_SDN",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2019.2943690"
}

@ARTICLE{Shanmuga_Sundaram2020-ys,
  title    = "A Survey on {LoRa} Networking: Research Problems, Current
              Solutions, and Open Issues",
  author   = "Shanmuga Sundaram, J P and Du, W and Zhao, Z",
  abstract = "Wireless networks have been widely deployed for many
              Internet-of-Things (IoT) applications, like smart cities and
              precision agriculture. Low Power Wide Area Networking (LPWAN) is
              an emerging IoT networking paradigm to meet three key
              requirements of IoT applications, i.e., low cost, large scale
              deployment and high energy efficiency. Among all available LPWAN
              technologies, LoRa networking has attracted much attention from
              both academia and industry, since it specifies an open standard
              and allows us to build autonomous LPWAN networks without any
              third-party infrastructure. Many LoRa networks have been
              developed recently, e.g., managing solar plants in Carson City,
              Nevada, USA and power monitoring in Lyon and Grenoble, France.
              However, there are still many research challenges to develop
              practical LoRa networks, e.g., link coordination, resource
              allocation, reliable transmissions and security. This article
              provides a comprehensive survey on LoRa networks, including the
              technical challenges of deploying LoRa networks and recent
              solutions. Based on our detailed analysis of current solutions,
              some open issues of LoRa networking are discussed. The goal of
              this survey paper is to inspire more works on improving the
              performance of LoRa networks and enabling more practical
              deployments.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  1,
  pages    = "371--388",
  year     =  2020,
  file     = "All Papers/S/Shanmuga Sundaram et al. 2020 - A Survey on LoRa Networking - Research Problems, Current Solutions, and Open Issues.pdf",
  keywords = "Internet of Things;resource allocation;wide area networks;LPWAN
              technologies;autonomous LPWAN networks;LoRa networking;emerging
              IoT networking paradigm;Low Power Wide Area Networking;wireless
              networks;Internet of Things;Chirp;Downlink;Authentication;Logic
              gates;OFDM;Bandwidth;Internet-of-Things;low powered wide area
              networking;LoRa;taxonomy;Done;Low quality;Wireless",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2019.2949598"
}

@ARTICLE{Barakabitze2020-mf,
  title    = "{QoE} Management of Multimedia Streaming Services in Future
              Networks: A Tutorial and Survey",
  author   = "Barakabitze, A A and Barman, N and Ahmad, A and Zadtootaghaj, S
              and Sun, L and Martini, M G and Atzori, L",
  abstract = "The highly demanding Over-The-Top (OTT) multimedia applications
              pose increased challenges to Internet Service Providers (ISPs)
              for assuring a reasonable Quality of Experience (QoE) to their
              customers due to lack of flexibility, agility and scalability in
              traditional networks. The future networks are shifting towards
              the cloudification of the network resources via Software Defined
              Networks (SDN) and Network Function Virtualization (NFV). This
              will equip ISPs with cutting-edge technologies to provide service
              customization during service delivery and offer QoE which meets
              customers' needs via intelligent QoE control and management
              approaches. Towards this end, we provide in this paper a tutorial
              and a comprehensive survey of QoE management solutions in current
              and future networks. We start with a high-level description of
              QoE management for multimedia services, which integrates QoE
              modelling, monitoring, and optimization. This followed by a
              discussion of HTTP Adaptive Streaming (HAS) solutions as the
              dominant technique for streaming videos over the best-effort
              Internet. We then summarize the key elements in SDN/NFV along
              with an overview of ongoing research projects, standardization
              activities and use cases related to SDN, NFV, and other emerging
              applications. We provide a survey of the state-of-the-art of QoE
              management techniques categorized into three different groups: a)
              QoE-aware/driven strategies using SDN and/or NFV; b)
              QoE-aware/driven approaches for adaptive streaming over emerging
              architectures such as multi-access edge computing, cloud/fog
              computing, and information-centric networking; and c) extended
              QoE management approaches in new domains such as immersive
              augmented and virtual reality, mulsemedia and video gaming
              applications. Based on the review, we present a list of
              identified future QoE management challenges regarding emerging
              multimedia applications, network management and orchestration,
              network slicing and collaborative service management in
              softwarized networks. Finally, we provide a discussion on future
              research directions with a focus on emerging research areas in
              QoE management, such as QoE-oriented business models, QoE-based
              big data strategies, and scalability issues in QoE optimization.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  1,
  pages    = "526--565",
  year     =  2020,
  file     = "All Papers/B/Barakabitze et al. 2020 - QoE Management of Multimedia Streaming Services in Future Networks - A Tutorial and Survey.pdf",
  keywords = "Big Data;cloud computing;computer network management;quality of
              experience;software defined networking;video
              streaming;virtualisation;Internet Service Providers;ISP;network
              resources;network function virtualization;service
              customization;service delivery;QoE management
              solutions;multimedia services;HTTP adaptive streaming;QoE
              management techniques;information-centric networking;network
              slicing;collaborative service management;softwarized
              networks;QoE-oriented business models;QoE-based big data
              strategies;QoE optimization;software defined networks;multimedia
              streaming services;over-the-top multimedia applications;quality
              of experience;NFV;SDN;intelligent QoE control;HAS
              solutions;Quality of experience;Streaming
              media;Videos;Tutorials;Optimization;Cloud computing;Adaptation
              models;QoE;network management;OTT;ISP;5G;SDN;NFV;OTT and ISP
              collaboration;5G6G",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2019.2958784"
}

@ARTICLE{Hussain2020-eg,
  title    = "Machine Learning for Resource Management in Cellular and {IoT}
              Networks: Potentials, Current Solutions, and Open Challenges",
  author   = "Hussain, F and Hassan, S A and Hussain, R and Hossain, E",
  abstract = "Internet-of-Things (IoT) refers to a massively heterogeneous
              network formed through smart devices connected to the Internet.
              In the wake of disruptive IoT with a huge amount and variety of
              data, Machine Learning (ML) and Deep Learning (DL) mechanisms
              will play a pivotal role to bring intelligence to the IoT
              networks. Among other aspects, ML and DL can play an essential
              role in addressing the challenges of resource management in
              large-scale IoT networks. In this article, we conduct a
              systematic and in-depth survey of the ML- and DL-based resource
              management mechanisms in cellular wireless and IoT networks. We
              start with the challenges of resource management in cellular IoT
              and low-power IoT networks, review the traditional resource
              management mechanisms for IoT networks, and motivate the use of
              ML and DL techniques for resource management in these networks.
              Then, we provide a comprehensive survey of the existing ML- and
              DL-based resource management techniques in wireless IoT networks
              and the techniques specifically designed for HetNets, MIMO and
              D2D communications, and NOMA networks. To this end, we also
              identify the future research directions in using ML and DL for
              resource allocation and management in IoT networks.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  2,
  pages    = "1251--1275",
  year     =  2020,
  file     = "All Papers/H/Hussain et al. 2020 - Machine Learning for Resource Management in Cellular and IoT Networks - Potentials, Current Solutions, and Open Challenges.pdf",
  keywords = "cellular radio;Internet;Internet of Things;learning (artificial
              intelligence);resource allocation;Machine Learning;massively
              heterogeneous network;disruptive IoT;large-scale IoT
              networks;low-power IoT networks;traditional resource management
              mechanisms;wireless IoT networks;resource allocation;DL-based
              resource management mechanisms;cellular wireless network;ML-based
              resource management mechanisms;HetNets;D2D communications;MIMO
              communications;NOMA networks;smart devices;Internet of
              Things;Resource management;Wireless communication;Communication
              system security;Computer architecture;Quality of
              service;Mathematical model;Internet-of-Things (IoT);wireless
              IoT;machine learning;deep learning;resource allocation;resource
              management;D2D;MIMO;HetNets;NOMA;ML;5G6G",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.2964534"
}

@ARTICLE{Wang2020-hz,
  title    = "Thirty Years of Machine Learning: The Road to {Pareto-Optimal}
              Wireless Networks",
  author   = "Wang, J and Jiang, C and Zhang, H and Ren, Y and Chen, K-C and
              Hanzo, L",
  abstract = "Future wireless networks have a substantial potential in terms of
              supporting a broad range of complex compelling applications both
              in military and civilian fields, where the users are able to
              enjoy high-rate, low-latency, low-cost and reliable information
              services. Achieving this ambitious goal requires new radio
              techniques for adaptive learning and intelligent decision making
              because of the complex heterogeneous nature of the network
              structures and wireless services. Machine learning (ML)
              algorithms have great success in supporting big data analytics,
              efficient parameter estimation and interactive decision making.
              Hence, in this article, we review the thirty-year history of ML
              by elaborating on supervised learning, unsupervised learning,
              reinforcement learning and deep learning. Furthermore, we
              investigate their employment in the compelling applications of
              wireless networks, including heterogeneous networks (HetNets),
              cognitive radios (CR), Internet of Things (IoT), machine to
              machine networks (M2M), and so on. This article aims for
              assisting the readers in clarifying the motivation and
              methodology of the various ML algorithms, so as to invoke them
              for hitherto unexplored services as well as scenarios of future
              wireless networks.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  3,
  pages    = "1472--1514",
  year     =  2020,
  file     = "All Papers/W/Wang et al. 2020 - Thirty Years of Machine Learning - The Road to Pareto-Optimal Wireless Networks.pdf",
  keywords = "Big Data;cognitive radio;data analysis;decision
              making;information services;Internet;Internet of Things;learning
              (artificial intelligence);radio networks;unsupervised
              learning;Pareto-optimal wireless networks;future wireless
              networks;substantial potential;complex compelling
              applications;military fields;civilian fields;reliable information
              services;ambitious goal;radio techniques;adaptive
              learning;intelligent decision making;complex heterogeneous
              nature;network structures;wireless services;machine learning
              algorithms;big data analytics;efficient parameter
              estimation;interactive decision making;supervised
              learning;unsupervised learning;reinforcement learning;deep
              learning;heterogeneous networks;machine networks;ML
              algorithms;unexplored services;Wireless networks;Wireless sensor
              networks;Internet of Things;Ad hoc networks;Maximum likelihood
              estimation;Clustering algorithms;Machine learning (ML);future
              wireless network;deep
              learning;regression;classification;clustering;network
              association;resource allocation;Wireless;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.2965856"
}

@ARTICLE{Wang2020-iv,
  title    = "Convergence of Edge Computing and Deep Learning: A Comprehensive
              Survey",
  author   = "Wang, X and Han, Y and Leung, V C M and Niyato, D and Yan, X and
              Chen, X",
  abstract = "Ubiquitous sensors and smart devices from factories and
              communities are generating massive amounts of data, and
              ever-increasing computing power is driving the core of
              computation and services from the cloud to the edge of the
              network. As an important enabler broadly changing people's lives,
              from face recognition to ambitious smart factories and cities,
              developments of artificial intelligence (especially deep
              learning, DL) based applications and services are thriving.
              However, due to efficiency and latency issues, the current cloud
              computing service architecture hinders the vision of ``providing
              artificial intelligence for every person and every organization
              at everywhere''. Thus, unleashing DL services using resources at
              the network edge near the data sources has emerged as a desirable
              solution. Therefore, edge intelligence, aiming to facilitate the
              deployment of DL services by edge computing, has received
              significant attention. In addition, DL, as the representative
              technique of artificial intelligence, can be integrated into edge
              computing frameworks to build intelligent edge for dynamic,
              adaptive edge maintenance and management. With regard to mutually
              beneficial edge intelligence and intelligent edge, this paper
              introduces and discusses: 1) the application scenarios of both;
              2) the practical implementation methods and enabling
              technologies, namely DL training and inference in the customized
              edge computing framework; 3) challenges and future trends of more
              pervasive and fine-grained intelligence. We believe that by
              consolidating information scattered across the communication,
              networking, and DL areas, this survey can help readers to
              understand the connections between enabling technologies while
              promoting further discussions on the fusion of edge intelligence
              and intelligent edge, i.e., Edge DL.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  2,
  pages    = "869--904",
  year     =  2020,
  file     = "All Papers/W/Wang et al. 2020 - Convergence of Edge Computing and Deep Learning - A Comprehensive Survey.pdf",
  keywords = "cloud computing;data mining;face recognition;learning (artificial
              intelligence);ubiquitous computing;deep learning;computing
              power;ambitious smart factories;artificial intelligence;DL
              services;network edge;intelligent edge;adaptive edge
              maintenance;mutually beneficial edge intelligence;customized edge
              computing framework;pervasive intelligence;fine-grained
              intelligence;face recognition;cloud computing service
              architecture;ubiquitous computing;Edge computing;Cloud
              computing;Training;Computational modeling;Reliability;Wireless
              communication;Internet of Things;Edge computing;deep
              learning;wireless communication;computation offloading;artificial
              intelligence;ATOS;EdgeFogCloudIoT;ML;MLAspects",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.2970550"
}

@ARTICLE{Navarro-Ortiz2020-bi,
  title    = "A Survey on {5G} Usage Scenarios and Traffic Models",
  author   = "Navarro-Ortiz, J and Romero-Diaz, P and Sendra, S and Ameigeiras,
              P and Ramos-Munoz, J J and Lopez-Soler, J M",
  abstract = "The fifth-generation mobile initiative, 5G, is a tremendous and
              collective effort to specify, standardize, design, manufacture,
              and deploy the next cellular network generation. 5G networks will
              support demanding services such as enhanced Mobile Broadband,
              Ultra-Reliable and Low Latency Communications and massive
              Machine-Type Communications, which will require data rates of
              tens of Gbps, latencies of few milliseconds and connection
              densities of millions of devices per square kilometer. This
              survey presents the most significant use cases expected for 5G
              including their corresponding scenarios and traffic models.
              First, the paper analyzes the characteristics and requirements
              for 5G communications, considering aspects such as traffic
              volume, network deployments, and main performance targets.
              Secondly, emphasizing the definition of performance evaluation
              criteria for 5G technologies, the paper reviews related proposals
              from principal standards development organizations and industry
              alliances. Finally, well-defined and significant 5G use cases are
              provided. As a result, these guidelines will help and ease the
              performance evaluation of current and future 5G innovations, as
              well as the dimensioning of 5G future deployments.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  2,
  pages    = "905--929",
  year     =  2020,
  file     = "All Papers/N/Navarro-Ortiz et al. 2020 - A Survey on 5G Usage Scenarios and Traffic Models.pdf",
  keywords = "5G mobile communication;cellular radio;telecommunication
              traffic;machine-type communications;5G future deployments;future
              5G innovations;performance evaluation criteria;performance
              targets;network deployments;traffic volume;ultra-reliable and low
              latency communications;enhanced Mobile Broadband;cellular network
              generation;fifth-generation mobile communication;traffic
              models;5G usage scenarios;5G mobile communication;Performance
              evaluation;Biological system
              modeling;Industries;Organizations;ITU;5G;IMT-2020;usage
              scenarios;traffic models;5G6G;NetworkTraffic",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.2971781"
}

@ARTICLE{Liu2020-ez,
  title    = "Blockchain and Machine Learning for Communications and Networking
              Systems",
  author   = "Liu, Y and Yu, F R and Li, X and Ji, H and Leung, V C M",
  abstract = "Recently, with the rapid development of information and
              communication technologies, the infrastructures, resources, end
              devices, and applications in communications and networking
              systems are becoming much more complex and heterogeneous. In
              addition, the large volume of data and massive end devices may
              bring serious security, privacy, services provisioning, and
              network management challenges. In order to achieve decentralized,
              secure, intelligent, and efficient network operation and
              management, the joint consideration of blockchain and machine
              learning (ML) may bring significant benefits and have attracted
              great interests from both academia and industry. On one hand,
              blockchain can significantly facilitate training data and ML
              model sharing, decentralized intelligence, security, privacy, and
              trusted decision-making of ML. On the other hand, ML will have
              significant impacts on the development of blockchain in
              communications and networking systems, including energy and
              resource efficiency, scalability, security, privacy, and
              intelligent smart contracts. However, some essential open issues
              and challenges that remain to be addressed before the widespread
              deployment of the integration of blockchain and ML, including
              resource management, data processing, scalable operation, and
              security issues. In this paper, we present a survey on the
              existing works for blockchain and ML technologies. We identify
              several important aspects of integrating blockchain and ML,
              including overview, benefits, and applications. Then we discuss
              some open issues, challenges, and broader perspectives that need
              to be addressed to jointly consider blockchain and ML for
              communications and networking systems.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  2,
  pages    = "1392--1431",
  year     =  2020,
  file     = "All Papers/L/Liu et al. 2020 - Blockchain and Machine Learning for Communications and Networking Systems.pdf",
  keywords = "cryptography;data privacy;decision making;learning (artificial
              intelligence);networking systems;communication
              technologies;massive end devices;network management;efficient
              network operation;machine learning;ML model;decentralized
              intelligence;resource efficiency;resource management;information
              and communication technologies;trusted
              decision-making;intelligent smart contracts;data
              processing;Machine learning;Data privacy;Contracts;Data
              models;Blockchain;machine learning (ML);distributed ledger
              technology (DLT);wireless communications;wireless networks;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.2975911"
}

@ARTICLE{Elbayoumi2020-fh,
  title    = "{NOMA-Assisted} {Machine-Type} Communications in {UDN}:
              {State-of-the-Art} and Challenges",
  author   = "Elbayoumi, M and Kamel, M and Hamouda, W and Youssef, A",
  abstract = "The inevitable next era of smart living requires unprecedented
              advances in enabling technologies. The building blocks of this
              era are devices such as sensors, actuators, and Internet of
              Things (IoT) devices. Machine-Type Communication (MTC), also
              known as Machine-to-Machine (M2M) communication, constitutes the
              main enabling technology to support communications among such
              devices. The massive number of these devices and the immense
              amount of traffic generated by them require a paramount shift in
              cellular and non-cellular wireless technologies to achieve the
              required connectivity. Ultra-Dense Network (UDN) is intuitively
              one of the most convenient approaches to tackle such requirements
              of massive connectivity and high throughput found in MTC. In
              UDNs, small cells are deployed in large densities compared to
              Human-Type Communication Users (HTCUs) like smartphones and
              tablets. In a different scope, multiple access techniques also
              require a paradigm shift to cope with the increasing density of
              required connections while confined by limited resources.
              Recently, Non-Orthogonal Multiple Access (NOMA) has received a
              great attention as an efficacious candidate to enhance the
              network's performance compared to traditional Orthogonal Multiple
              Access (OMA) techniques. For this sake, in this paper, we provide
              a comprehensive survey and illustrative simulation results on the
              application of NOMA to support MTC in a UDN environment. First,
              we give a brief discussion on MTC and its different applications
              and supporting technologies. Then, we focus our effort on
              investigating the main challenges facing MTC along with the
              existing opportunities found in both NOMA and UDN, respectively.
              Finally, we show via simulations the possible gains of both
              technologies and conclude by discussing the open problems for
              future research directions.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  2,
  pages    = "1276--1304",
  year     =  2020,
  file     = "All Papers/E/Elbayoumi et al. 2020 - NOMA-Assisted Machine-Type Communications in UDN - State-of-the-Art and Challenges.pdf",
  keywords = "cellular radio;Internet of Things;multi-access
              systems;telecommunication traffic;MTC;Machine-to-Machine
              communication;main enabling technology;noncellular wireless
              technologies;Ultra-Dense Network;Human-Type Communication
              Users;NOMA;traditional Orthogonal Multiple Access techniques;UDN
              environment;supporting technologies;Machine-Type
              communications;Internet of Things devices;cellular wireless
              technologies;NOMA;Internet of Things;OFDM;Wireless
              communication;MIMO communication;Radio access
              networks;Tutorials;CD-NOMA;computation-offloading;grant-free
              access;IoT;M2M;MEC;MTC;PD-NOMA;random access;UDN;Done;Low
              quality;Wireless;5G6G",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.2977845"
}

@ARTICLE{Chen2020-lv,
  title    = "Energy and Information Management of Electric Vehicular Network:
              A Survey",
  author   = "Chen, Nan and Wang, Miao and Zhang, Ning and Shen, Xuemin",
  abstract = "The connected vehicle paradigm empowers vehicles with the
              capability to communicate with neighboring vehicles and
              infrastructure, shifting the role of vehicles from a
              transportation tool to an intelligent service platform.
              Meanwhile, the transportation electrification pushes forward the
              electric vehicle (EV) commercialization to reduce the greenhouse
              gas emission by petroleum combustion. The unstoppable trends of
              connected vehicle and EVs transform the traditional vehicular
              system to an electric vehicular network (EVN), a clean, mobile,
              and safe system. However, due to the mobility and heterogeneity
              of the EVN, improper management of the network could result in
              charging overload and data congestion. Thus, energy and
              information management of the EVN should be carefully studied. In
              this paper, we provide a comprehensive survey on the deployment
              and management of EVN considering all three aspects of energy
              flow, data communication, and computation. We first introduce the
              management framework of EVN. Then, research works on the EV
              aggregator (AG) deployment are reviewed to provide energy and
              information infrastructure for the EVN. Based on the deployed
              AGs, we present the research work review on EV scheduling that
              includes both charging and vehicle-to-grid (V2G) scheduling.
              Moreover, related works on information communication and
              computing are surveyed under each scenario. Finally, we discuss
              open research issues in the EVN.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  2,
  pages    = "967--997",
  year     =  2020,
  file     = "All Papers/C/Chen et al. 2020 - Energy and Information Management of Electric Vehicular Network - A Survey.pdf",
  keywords = "Vehicle-to-grid;Cellular networks;Connected vehicles;Electric
              vehicle charging;Satellites;Commercialization;Information
              management;Electric vehicular network;connected vehicle;electric
              vehicle;energy scheduling;communication;computing;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.2982118"
}

@ARTICLE{Lim2020-jd,
  title    = "Federated Learning in Mobile Edge Networks: A Comprehensive
              Survey",
  author   = "Lim, W Y B and Luong, N C and Hoang, D T and Jiao, Y and Liang,
              Y-C and Yang, Q and Niyato, D and Miao, C",
  abstract = "In recent years, mobile devices are equipped with increasingly
              advanced sensing and computing capabilities. Coupled with
              advancements in Deep Learning (DL), this opens up countless
              possibilities for meaningful applications, e.g., for medical
              purposes and in vehicular networks. Traditional cloud-based
              Machine Learning (ML) approaches require the data to be
              centralized in a cloud server or data center. However, this
              results in critical issues related to unacceptable latency and
              communication inefficiency. To this end, Mobile Edge Computing
              (MEC) has been proposed to bring intelligence closer to the edge,
              where data is produced. However, conventional enabling
              technologies for ML at mobile edge networks still require
              personal data to be shared with external parties, e.g., edge
              servers. Recently, in light of increasingly stringent data
              privacy legislations and growing privacy concerns, the concept of
              Federated Learning (FL) has been introduced. In FL, end devices
              use their local data to train an ML model required by the server.
              The end devices then send the model updates rather than raw data
              to the server for aggregation. FL can serve as an enabling
              technology in mobile edge networks since it enables the
              collaborative training of an ML model and also enables DL for
              mobile edge network optimization. However, in a large-scale and
              complex mobile edge network, heterogeneous devices with varying
              constraints are involved. This raises challenges of communication
              costs, resource allocation, and privacy and security in the
              implementation of FL at scale. In this survey, we begin with an
              introduction to the background and fundamentals of FL. Then, we
              highlight the aforementioned challenges of FL implementation and
              review existing solutions. Furthermore, we present the
              applications of FL for mobile edge network optimization. Finally,
              we discuss the important challenges and future research
              directions in FL.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  3,
  pages    = "2031--2063",
  year     =  2020,
  file     = "All Papers/L/Lim et al. 2020 - Federated Learning in Mobile Edge Networks - A Comprehensive Survey.pdf;All Papers/L/Lim et al. 2020 - Federated_Learning_in_Mobile_Edge_Networks_A_Comprehensive_Survey.pdf",
  keywords = "cloud computing;data privacy;learning (artificial
              intelligence);legislation;mobile computing;mobile radio;resource
              allocation;data center;cloud server;Traditional cloud-based
              Machine;vehicular networks;increasingly advanced sensing;mobile
              devices;complex mobile edge network;mobile edge network
              optimization;raw data;ML model;Federated Learning;growing privacy
              concerns;increasingly stringent data privacy legislations;edge
              servers;personal data;mobile edge networks;Mobile Edge
              Computing;Training;Servers;Data privacy;Data
              models;Optimization;Privacy;Computational modeling;Federated
              learning;mobile edge networks;resource allocation;communication
              cost;data privacy;data security;ATOS;EdgeFogCloudIoT;ML;MLAspects",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.2986024"
}

@ARTICLE{Lei2020-pm,
  title    = "Deep Reinforcement Learning for Autonomous Internet of Things:
              Model, Applications and Challenges",
  author   = "Lei, L and Tan, Y and Zheng, K and Liu, S and Zhang, K and Shen,
              X",
  abstract = "The Internet of Things (IoT) extends the Internet connectivity
              into billions of IoT devices around the world, where the IoT
              devices collect and share information to reflect status of the
              physical world. The Autonomous Control System (ACS), on the other
              hand, performs control functions on the physical systems without
              external intervention over an extended period of time. The
              integration of IoT and ACS results in a new concept - autonomous
              IoT (AIoT). The sensors collect information on the system status,
              based on which the intelligent agents in the IoT devices as well
              as the Edge/Fog/Cloud servers make control decisions for the
              actuators to react. In order to achieve autonomy, a promising
              method is for the intelligent agents to leverage the techniques
              in the field of artificial intelligence, especially reinforcement
              learning (RL) and deep reinforcement learning (DRL) for decision
              making. In this paper, we first provide a tutorial of DRL, and
              then propose a general model for the applications of RL/DRL in
              AIoT. Next, a comprehensive survey of the state-of-art research
              on DRL for AIoT is presented, where the existing works are
              classified and summarized under the umbrella of the proposed
              general DRL model. Finally, the challenges and open issues for
              future research are identified.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  3,
  pages    = "1722--1760",
  year     =  2020,
  file     = "All Papers/L/Lei et al. 2020 - Deep Reinforcement Learning for Autonomous Internet of Things - Model, Applications and Challenges.pdf",
  keywords = "cloud computing;control engineering computing;decision
              making;Internet of Things;learning (artificial
              intelligence);software agents;cloud servers;Fog servers;edge
              servers;deep reinforcement learning;general DRL model;artificial
              intelligence;control decision making;intelligent
              agents;AIoT;autonomous IoT;ACS results;physical
              systems;autonomous control system;physical world;IoT
              devices;Internet connectivity;autonomous Internet of
              Things;Internet of Things;Servers;Machine
              learning;Actuators;Tutorials;Approximation algorithms;Autonomous
              Internet of Things;deep reinforcement learning;ML;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.2988367"
}

@ARTICLE{Marzouk2020-sr,
  title    = "On Energy Efficient Resource Allocation in Shared {RANs}: Survey
              and Qualitative Analysis",
  author   = "Marzouk, F and Barraca, J P and Radwan, A",
  abstract = "An expansion of services and unprecedented traffic growth is
              anticipated in future networks, aligned with the adoption of the
              long-awaited Fifth Generation (5G) of mobile communications. To
              support this demand, without exposing mobile operators to the
              pressure of CAPEX and OPEX, 5G uses new frequency bands, and
              adopts promising trends, including: densification,
              softwarization, and autonomous management. While the first
              technology is proposed to handle the traffic growth requirements,
              the softwarization and autonomous management are expected to
              play, in synergy, to ensure the desired trade-off between
              reducing the CAPEX and OPEX, while guaranteeing the quality of
              service (QoS). Softwarization is expected to transform the
              network design, from one size fits all, to more demand oriented
              adaptive resource allocation. In this work, we focus on this
              point, by discussing how these technologies act in synergy
              towards enabling RAN sharing. Particularly, we focus on how they
              fit into the issue of energy efficient Multi-Operator Resource
              Allocation (MO-RA). After a survey and classification of schemes
              leveraging this synergy for distinct resource allocation (RA)
              objectives, we present a detailed survey and qualitative
              classification of RA schemes with respect to energy efficiency.
              This work presents an innovative survey, since it concentrates on
              multiple operators, and the enabling of Mobile Virtual Network
              Operators (MVNOs), which will come into play with the complete
              virtualization of mobile networks. Based on the deep literature
              analysis of the different operations that can bring energy
              savings to MO-RA, we conclude the work with listing open
              challenges and future research directions.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  3,
  pages    = "1515--1538",
  year     =  2020,
  file     = "All Papers/M/Marzouk et al. 2020 - On Energy Efficient Resource Allocation in Shared RANs - Survey and Qualitative Analysis.pdf",
  keywords = "cellular radio;energy conservation;mobile communication;mobile
              computing;mobile radio;quality of service;radio access
              networks;resource allocation;telecommunication
              traffic;virtualisation;energy efficient resource
              allocation;shared RANs;qualitative analysis;unprecedented traffic
              growth;5G;mobile communications;mobile operators;frequency
              bands;softwarization;autonomous management;traffic growth
              requirements;desired trade-off;network design;adaptive resource
              allocation;enabling RAN sharing;energy efficient MultiOperator
              Resource Allocation;MO-RA;distinct resource allocation
              objectives;detailed survey;qualitative classification;RA
              schemes;energy efficiency;innovative survey;multiple
              operators;Mobile Virtual Network Operators;mobile networks;deep
              literature analysis;different operations;energy savings;Resource
              management;5G mobile communication;Quality of
              service;Software;Tutorials;Virtualization;Rats;V-RAN
              sharing;resource allocation;energy
              efficiency;SON;ML;NFV;SDN;SDR;QoS;Wireless;5G6G",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3003261"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Slamnik-Krijestorac2020-im,
  title    = "Sharing Distributed and Heterogeneous Resources toward
              {End-to-End} {5G} Networks: A Comprehensive Survey and a Taxonomy",
  author   = "Slamnik-Krije{\v s}torac, N and Kremo, H and Ruffini, M and
              Marquez-Barja, J M",
  abstract = "Regardless of the context to which it is applied, sharing
              resources is well-recognized for its considerable benefits. Since
              5G networks will be service-oriented, on-demand, and highly
              heterogeneous, it is utmost important to approach the design and
              optimization of the network from an end-to-end perspective. In
              addition, in order to ensure end-to-end performance, this
              approach has to entail both wireless and optical domains,
              altogether with the IoT, edge, and cloud paradigms which are an
              indispensable part of the 5G network architecture. Shifting from
              the exclusive ownership of network resources toward sharing
              enables all participants to cope with stringent service
              requirements in 5G networks, gaining significant performance
              improvements and cost savings at the same time. The main
              objective of this paper is to survey the literature on resource
              sharing, providing an in-depth and comprehensive perspective of
              sharing by recognizing the main trends, the techniques which
              enable sharing, and the challenges that need to be addressed. By
              providing a taxonomy which brings the relevant features of a
              comprehensive sharing model into focus, we aim to enable the
              creation of sharing models for more efficient future
              communication networks. We also summarize and discuss the
              relevant issues arising from network sharing, that should be
              properly tackled in the future.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  3,
  pages    = "1592--1628",
  year     =  2020,
  file     = "All Papers/S/Slamnik-Kriještorac et al. 2020 - Sharing Distributed and Heterogeneous ... toward End-to-End 5G Networks - A Comprehensive Survey and a Taxonomy.pdf",
  keywords = "cloud computing;Internet;quality of service;resource
              allocation;optical domains;network resources;sharing enables all
              participants;stringent service requirements;cost savings;resource
              sharing;comprehensive perspective;taxonomy;comprehensive sharing
              model;sharing models;efficient future communication
              networks;network sharing;sharing distributed
              resources;heterogeneous resources;end-to-end 5G networks;sharing
              resources;considerable benefits;service-oriented;end-to-end
              perspective;end-to-end performance;wireless domains;5G mobile
              communication;Resource management;Wireless
              communication;Taxonomy;Optical fiber networks;Cloud
              computing;Sharing resources;end-to-end future communication
              networks;5G;wireless domain;optical domain;IoT;edge;cloud;sharing
              challenges;5G6G",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3003818"
}

@ARTICLE{Gong2020-tr,
  title    = "Toward Smart Wireless Communications via Intelligent Reflecting
              Surfaces: A Contemporary Survey",
  author   = "Gong, S and Lu, X and Hoang, D T and Niyato, D and Shu, L and
              Kim, D I and Liang, Y-C",
  abstract = "This paper presents a literature review on recent applications
              and design aspects of the intelligent reflecting surface (IRS) in
              the future wireless networks. Conventionally, the network
              optimization has been limited to transmission control at two
              endpoints, i.e., end users and network controller. The fading
              wireless channel is uncontrollable and becomes one of the main
              limiting factors for performance improvement. The IRS is composed
              of a large array of scattering elements, which can be
              individually configured to generate additional phase shifts to
              the signal reflections. Hence, it can actively control the signal
              propagation properties in favor of signal reception, and thus
              realize the notion of a smart radio environment. As such, the
              IRS's phase control, combined with the conventional transmission
              control, can potentially bring performance gain compared to
              wireless networks without IRS. In this survey, we first introduce
              basic concepts of the IRS and the realizations of its
              reconfigurability. Then, we focus on applications of the IRS in
              wireless communications. We overview different performance
              metrics and analytical approaches to characterize the performance
              improvement of IRS-assisted wireless networks. To exploit the
              performance gain, we discuss the joint optimization of the IRS's
              phase control and the transceivers' transmission control in
              different network design problems, e.g., rate maximization and
              power minimization problems. Furthermore, we extend the
              discussion of IRS-assisted wireless networks to some emerging use
              cases. Finally, we highlight important practical challenges and
              future research directions for realizing IRS-assisted wireless
              networks in beyond 5G communications.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  4,
  pages    = "2283--2314",
  year     =  2020,
  file     = "All Papers/G/Gong et al. 2020 - Toward Smart Wireless Communications via Intelligent Reflecting Surfaces - A Contemporary Survey.pdf",
  keywords = "Wireless networks;Optimization;Receivers;Scattering;RF
              signals;Communication system security;Wireless communication;5G
              mobile communication;Performance gain;Fading channels;Intelligent
              reflecting surface;smart radio environment;passive
              beamforming;IRS-assisted wireless networks;Wireless",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3004197"
}

@ARTICLE{Shi2020-uh,
  title    = "{Communication-Efficient} Edge {AI}: Algorithms and Systems",
  author   = "Shi, Y and Yang, K and Jiang, T and Zhang, J and Letaief, K B",
  abstract = "Artificial intelligence (AI) has achieved remarkable
              breakthroughs in a wide range of fields, ranging from speech
              processing, image classification to drug discovery. This is
              driven by the explosive growth of data, advances in machine
              learning (especially deep learning), and the easy access to
              powerful computing resources. Particularly, the wide scale
              deployment of edge devices (e.g., IoT devices) generates an
              unprecedented scale of data, which provides the opportunity to
              derive accurate models and develop various intelligent
              applications at the network edge. However, such enormous data
              cannot all be sent to the cloud for processing, due to the
              varying channel quality, traffic congestion and/or privacy
              concerns, and the enormous energy consumption. By pushing
              inference and training processes of AI models to edge nodes, edge
              AI has emerged as a promising alternative. AI at the edge
              requires close cooperation among edge devices, such as smart
              phones and smart vehicles, and edge servers at the wireless
              access points and base stations, which however result in heavy
              communication overheads. In this paper, we present a
              comprehensive survey of the recent developments in various
              techniques for overcoming these communication challenges.
              Specifically, we first identify key communication challenges in
              edge AI systems. We then introduce communication-efficient
              techniques, from both algorithmic and system perspectives for
              training and inference tasks at the network edge. Potential
              future research directions are also highlighted.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  4,
  pages    = "2167--2191",
  year     =  2020,
  file     = "All Papers/S/Shi et al. 2020 - Communication-Efficient Edge AI - Algorithms and Systems.pdf",
  keywords = "Artificial intelligence;Training data;Computational modeling;Data
              models;Servers;Task analysis;Neural networks;Wireless
              communication;Artificial intelligence;edge AI;edge
              intelligence;communication efficiency;ATOS;EdgeFogCloudIoT;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3007787"
}

@ARTICLE{Qiu2020-cy,
  title    = "Edge Computing in Industrial Internet of Things: Architecture,
              Advances and Challenges",
  author   = "Qiu, T and Chi, J and Zhou, X and Ning, Z and Atiquzzaman, M and
              Wu, D O",
  abstract = "The Industrial Internet of Things (IIoT) is a crucial research
              field spawned by the Internet of Things (IoT). IIoT links all
              types of industrial equipment through the network; establishes
              data acquisition, exchange, and analysis systems; and optimizes
              processes and services, so as to reduce cost and enhance
              productivity. The introduction of edge computing in IIoT can
              significantly reduce the decision-making latency, save bandwidth
              resources, and to some extent, protect privacy. This paper
              outlines the research progress concerning edge computing in IIoT.
              First, the concepts of IIoT and edge computing are discussed, and
              subsequently, the research progress of edge computing is
              discussed and summarized in detail. Next, the future architecture
              from the perspective of edge computing in IIoT is proposed, and
              its technical progress in routing, task scheduling, data storage
              and analytics, security, and standardization is analyzed.
              Furthermore, we discuss the opportunities and challenges of edge
              computing in IIoT in terms of 5G-based edge communication, load
              balancing and data offloading, edge intelligence, as well as data
              sharing security. Finally, we introduce some typical application
              scenarios of edge computing in IIoT, such as prognostics and
              health management (PHM), smart grids, manufacturing coordination,
              intelligent connected vehicles (ICV), and smart logistics.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  4,
  pages    = "2462--2488",
  year     =  2020,
  annote   = "Storage, standardization is interessant.",
  file     = "All Papers/Q/Qiu et al. 2020 - Edge Computing in Industrial Internet of Things - Architecture, Advances and Challenges.pdf",
  keywords = "Edge computing;Cloud computing;Internet of Things;Computer
              architecture;Security;Delays;Industrial Internet of Things
              (IIoT);edge computing;reference architecture;advances and
              challenges;application scenarios;Done;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3009103"
}

@ARTICLE{Sun2020-zg,
  title    = "When Machine Learning Meets Privacy in 6G: A Survey",
  author   = "Sun, Y and Liu, J and Wang, J and Cao, Y and Kato, N",
  abstract = "The rapid-developing Artificial Intelligence (AI) technology,
              fast-growing network traffic, and emerging intelligent
              applications (e.g., autonomous driving, virtual reality, etc.)
              urgently require a new, faster, more reliable and flexible
              network form. At this time, researchers in both industry and
              academia have turned their attention to the sixth generation (6G)
              communication networks. In the 6G vision, various intelligent
              application scenarios that utilize Machine Learning (ML)
              technology (the most important branch of AI) will bring rich
              heterogeneous connections, as well as massive information storage
              and operations. When ML meets 6G, new opportunities will emerge
              along with numerous privacy challenges. On one hand, a secure ML
              structure, or the correct application of ML, can protect privacy
              in 6G. On the other hand, ML may be attacked or abused, resulting
              in privacy violation. It is worth noting that the alliance
              between 6G and ML may also be a double-edged sword in many cases,
              rather than absolutely infringe or protect privacy. Therefore,
              based on lots of existing meaningful works, this paper aims to
              provide a comprehensive survey of ML and privacy in 6G, with a
              view to further promoting the development of 6G and privacy
              protection technologies.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  4,
  pages    = "2694--2724",
  year     =  2020,
  file     = "All Papers/S/Sun et al. 2020 - When Machine Learning Meets Privacy in 6G - A Survey.pdf",
  keywords = "6G mobile communication;Data privacy;Big Data;MIMO
              communication;Tutorials;Machine learning;Privacy;machine
              learning;6G;violation;protection;communication;double-edged
              sword;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3011561"
}

@ARTICLE{Deng2020-dd,
  title    = "{IEEE} 802.11be {Wi-Fi} 7: New Challenges and Opportunities",
  author   = "Deng, C and Fang, X and Han, X and Wang, X and Yan, L and He, R
              and Long, Y and Guo, Y",
  abstract = "With the emergence of 4k/8k video, the throughput requirement of
              video delivery will keep grow to tens of Gbps. Other new
              high-throughput and low-latency video applications including
              augmented reality (AR), virtual reality (VR), and online gaming,
              are also proliferating. Due to the related stringent
              requirements, supporting these applications over wireless local
              area network (WLAN) is far beyond the capabilities of the new
              WLAN standard - IEEE 802.11ax. To meet these emerging demands,
              the IEEE 802.11 will release a new amendment standard IEEE
              802.11be - Extremely High Throughput (EHT), also known as
              Wireless-Fidelity (Wi-Fi) 7. This article provides the
              comprehensive survey on the key medium access control (MAC) layer
              techniques and physical layer (PHY) techniques being discussed in
              the EHT task group, including the channelization and tone plan,
              multiple resource units (multi-RU) support, 4096 quadrature
              amplitude modulation (4096-QAM), preamble designs, multiple link
              operations (e.g., multi-link aggregation and channel access),
              multiple input multiple output (MIMO) enhancement, multiple
              access point (multi-AP) coordination (e.g., multi-AP joint
              transmission), enhanced link adaptation and retransmission
              protocols (e.g., hybrid automatic repeat request (HARQ)). This
              survey covers both the critical technologies being discussed in
              EHT standard and the related latest progresses from worldwide
              research. Besides, the potential developments beyond EHT are
              discussed to provide some possible future research directions for
              WLAN.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  4,
  pages    = "2136--2166",
  year     =  2020,
  file     = "All Papers/D/Deng et al. 2020 - IEEE 802.11be Wi-Fi 7 - New Challenges and Opportunities.pdf",
  keywords = "access protocols;augmented reality;automatic repeat request;MIMO
              communication;quadrature amplitude modulation;virtual
              reality;wireless LAN;multiple resource units;multiRU;multiple
              link operations;channel access;multiple input multiple output
              enhancement;multiple access point;multiAP;enhanced link
              adaptation;retransmission protocols;EHT standard;Wi-Fi;throughput
              requirement;video delivery;low-latency video applications;virtual
              reality;online gaming;related stringent requirements;wireless
              local area network;WLAN standard;IEEE 802.11be;Extremely High
              Throughput;Wireless-Fidelity;key medium access control layer
              techniques;physical layer techniques;EHT task
              group;channelization;tone plan;quadrature amplitude
              modulation;IEEE 802.11ax;Throughput;Wireless
              fidelity;Bandwidth;IEEE 802.11 Standard;Media Access
              Protocol;MIMO communication;Wireless communication;IEEE
              802.11be;EHT;Wi-Fi 7;multi-link operation;multi-AP
              coordination;MIMO enhancement;HARQ;Wireless;Mobile\_Wireless;wifi",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3012715"
}

@ARTICLE{Maraqa2020-ps,
  title    = "A Survey of {Rate-Optimal} Power Domain {NOMA} With Enabling
              Technologies of Future Wireless Networks",
  author   = "Maraqa, O and Rajasekaran, A S and Al-Ahmadi, S and
              Yanikomeroglu, H and Sait, S M",
  abstract = "The ambitious high data-rate applications in the envisioned
              future beyond fifth-generation (B5G) wireless networks require
              new solutions, including the advent of more advanced
              architectures than the ones already used in 5G networks, and the
              coalition of different communications schemes and technologies to
              enable these applications requirements. Among the candidate
              communications schemes for future wireless networks are
              non-orthogonal multiple access (NOMA) schemes that allow serving
              more than one user in the same resource block by multiplexing
              users in other domains than frequency or time. In this way, NOMA
              schemes tend to offer several advantages over orthogonal multiple
              access (OMA) schemes such as improved user fairness and spectral
              efficiency, higher cell-edge throughput, massive connectivity
              support, and low transmission latency. With these merits,
              NOMA-enabled transmission schemes are being increasingly looked
              at as promising multiple access schemes for future wireless
              networks. When the power domain is used to multiplex the users,
              it is referred to as the power domain NOMA (PD-NOMA). In this
              paper, we survey the integration of PD-NOMA with the enabling
              communications schemes and technologies that are expected to meet
              the various requirements of B5G networks. In particular, this
              paper surveys the different rate optimization scenarios studied
              in the literature when PD-NOMA is combined with one or more of
              the candidate schemes and technologies for B5G networks including
              multiple-input-single-output (MISO),
              multiple-input-multiple-output (MIMO), massive-MIMO (mMIMO),
              advanced antenna architectures, higher frequency millimeter-wave
              (mmWave) and terahertz (THz) communications, advanced coordinated
              multi-point (CoMP) transmission and reception schemes,
              cooperative communications, cognitive radio (CR), visible light
              communications (VLC), unmanned aerial vehicle (UAV) assisted
              communications and others. The considered system models, the
              optimization methods utilized to maximize the achievable rates,
              and the main lessons learnt on the optimization and the
              performance of these NOMA-enabled schemes and technologies are
              discussed in detail along with the future research directions for
              these combined schemes. Moreover, the role of machine learning in
              optimizing these NOMA-enabled technologies is addressed.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  4,
  pages    = "2192--2235",
  year     =  2020,
  file     = "All Papers/M/Maraqa et al. 2020 - A Survey of Rate-Optimal Power Domain NOMA With Enabling Technologies of Future Wireless Networks.pdf",
  keywords = "NOMA;MIMO communication;Wireless networks;Unmanned aerial
              vehicles;Tutorials;Optimization;Cognitive radio;Wireless
              communication;5G mobile communication;Non-orthogonal multiple
              access (NOMA);beyond 5G (B5G) networks;achievable
              rates;optimization;power allocation;user
              selection;beamforming;multiple-input-single-output
              (MISO);multiple-input-multiple-output (MIMO);massive-MIMO
              (mMIMO);cell-free mMIMO (CF-mMIMO);reconfigurable antenna
              systems;large intelligent surfaces (LIS);3-dimensional MIMO (3-D
              MIMO);millimeter-wave (mmWave);terahertz (THz)
              communications;coordinated multipoint (CoMP);cooperative
              communications;vehicle-to-everything (V2X);cognitive radio
              (CR);visible light communications (VLC);unmanned aerial vehicle
              (UAV);backscatter communications (BackCom);intelligent reflecting
              surfaces (IRS);mobile edge computing (MEC) and edge
              caching;integrated terrestrial-satellite networks;underwater
              communications;machine learning (ML);Wireless",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3013514"
}

@ARTICLE{Wang2020-mc,
  title    = "Architectural Design Alternatives Based on {Cloud/Edge/Fog}
              Computing for Connected Vehicles",
  author   = "Wang, H and Liu, T and Kim, B and Lin, C-W and Shiraishi, S and
              Xie, J and Han, Z",
  abstract = "As vehicles playing an increasingly important role in people's
              daily life, requirements on safer and more comfortable driving
              experience have arisen. Connected vehicles (CVs) can provide
              enabling technologies to realize these requirements and have
              attracted widespread attentions from both academia and industry.
              These requirements ask for a well-designed computing architecture
              to support the Quality-of-Service (QoS) of CV applications.
              Computation offloading techniques, such as cloud, edge, and fog
              computing, can help CVs process computation-intensive and
              large-scale computing tasks. Additionally, different
              cloud/edge/fog computing architectures are suitable for
              supporting different types of CV applications with highly
              different QoS requirements, which demonstrates the importance of
              the computing architecture design. However, most of the existing
              surveys on cloud/edge/fog computing for CVs overlook the
              computing architecture design, where they (i) only focus on one
              specific computing architecture and (ii) lack discussions on
              benefits, research challenges, and system requirements of
              different architectural alternatives. In this article, we provide
              a comprehensive survey on different architectural design
              alternatives based on cloud/edge/fog computing for CVs. The
              contributions of this article are: (i) providing a comprehensive
              literature survey on existing proposed architectural design
              alternatives based on cloud/edge/fog computing for CVs, (ii)
              proposing a new classification of computing architectures based
              on cloud/edge/fog computing for CVs: computation-aided and
              computation-enabled architectures, (iii) presenting a holistic
              comparison among different cloud/edge/fog computing architectures
              for CVs based on functional requirements of CV systems, including
              advantages, disadvantages, and research challenges, (iv)
              presenting a holistic overview on the design of CV systems from
              both academia and industry perspectives, including activities in
              industry, functional requirements, service requirements, and
              design considerations, and (v) proposing several open research
              issues of designing cloud/edge/fog computing architectures for
              CVs.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  4,
  pages    = "2349--2377",
  year     =  2020,
  annote   = "Evtl. relevant Illian??",
  file     = "All Papers/W/Wang et al. 2020 - Architectural Design Alternatives Based on Cloud - Edge - Fog Computing for Connected Vehicles.pdf",
  keywords = "Cloud computing;Computer architecture;Edge
              computing;Servers;Quality of service;Vehicular ad hoc
              networks;Connected vehicles;Mobile edge computing;mobile cloud
              computing;fog computing;connected vehicles;V2X
              communication;architectural design;Done;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3020854"
}

@ARTICLE{Kanj2020-jm,
  title    = "A Tutorial on {NB-IoT} Physical Layer Design",
  author   = "Kanj, M and Savaux, V and Le Guen, M",
  abstract = "The Internet of Things (IoT) is transforming the whole of
              society. It represents the next evolution of the Internet and
              will significantly improve the ability to gather and analyze
              data, as well as the ability to control devices remotely. In this
              respect, the usage of connected devices is continuously growing
              with the expansion of the applications being offered to
              individuals and industries. To address IoT market needs, many
              low-power wide-area (LPWA) technologies have been developed, some
              operating on licensed frequencies (e.g., narrowband-IoT [NB-IoT]
              and Long-Term Evolution-M [LTE-M]), and others on unlicensed
              frequencies (e.g., LoRa, Sigfox, etc.). In this article, we
              address the Release 13 of the NB-IoT 3rd generation partnership
              project (3GPP) standardized LPWA technology and provide a
              tutorial on its physical layer (PHY) design. Specifically, we
              focus on the characteristics and the scheduling of downlink and
              uplink physical channels at the NB-IoT base station side and the
              user equipment (UE) side. The goal is to help readers easily
              understand the NB-IoT system without having to read all the 3GPP
              specifications or the state-of-the-art papers that generally
              describe the system. To this end, each presented concept is
              followed by examples and concrete use-cases to further aid in the
              reader's comprehension. Finally, we briefly describe and
              highlight the new features added to the NB-IoT system in Releases
              14 and 15.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  22,
  number   =  4,
  pages    = "2408--2446",
  year     =  2020,
  file     = "All Papers/K/Kanj et al. 2020 - A Tutorial on NB-IoT Physical Layer Design.pdf",
  keywords = "3GPP;Downlink;Narrowband;Internet of Things;Tutorials;Long Term
              Evolution;Physical layer;Internet of
              Things;NB-IoT;LTE;3GPP;eNB;protocol stack;physical
              layer;scheduling;downlink and uplink channels;LPWA;Wireless",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3022751"
}

@ARTICLE{Nguyen2021-xe,
  title    = "Enabling {AI} in Future Wireless Networks: A Data Life Cycle
              Perspective",
  author   = "Nguyen, D C and Cheng, P and Ding, M and Lopez-Perez, D and
              Pathirana, P N and Li, J and Seneviratne, A and Li, Y and Poor, H
              V",
  abstract = "Recent years have seen rapid deployment of mobile computing and
              Internet of Things (IoT) networks, which can be mostly attributed
              to the increasing communication and sensing capabilities of
              wireless systems. Big data analysis, pervasive computing, and
              eventually artificial intelligence (AI) are envisaged to be
              deployed on top of the IoT and create a new world featured by
              data-driven AI. In this context, a novel paradigm of merging AI
              and wireless communications, called Wireless AI that pushes AI
              frontiers to the network edge, is widely regarded as a key
              enabler for future intelligent network evolution. To this end, we
              present a comprehensive survey of the latest studies in wireless
              AI from the data-driven perspective. Specifically, we first
              propose a novel Wireless AI architecture that covers five key
              data-driven AI themes in wireless networks, including Sensing AI,
              Network Device AI, Access AI, User Device AI and Data-provenance
              AI. Then, for each data-driven AI theme, we present an overview
              on the use of AI approaches to solve the emerging data-related
              problems and show how AI can empower wireless network
              functionalities. Particularly, compared to the other related
              survey papers, we provide an in-depth discussion on the Wireless
              AI applications in various data-driven domains wherein AI proves
              extremely useful for wireless network design and optimization.
              Finally, research challenges and future visions are also
              discussed to spur further research in this promising area.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  1,
  pages    = "553--595",
  year     =  2021,
  keywords = "Artificial intelligence;Wireless sensor networks;Wireless
              networks;Receivers;Communication system security;Internet of
              Things;Wireless networks;artificial intelligence;deep
              learning;machine learning;data-driven AI;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3024783"
}

@ARTICLE{Jaffry2021-au,
  title    = "A Comprehensive Survey on Moving Networks",
  author   = "Jaffry, S and Hussain, R and Gui, X and Hasan, S F",
  abstract = "The unprecedented increase in the demand for mobile data, fuelled
              by new emerging applications and use-cases such as
              high-definition video streaming and heightened online activities
              has caused massive strain on the existing cellular networks. As a
              solution, the fifth generation (5G) of cellular technology has
              been introduced to improve network performance through various
              innovative features such as millimeter-wave spectrum and
              Heterogeneous Networks (HetNets). In essence, HetNets include
              several small cells underlaid within macro-cell to serve densely
              populated regions like stadiums, shopping malls, and so on.
              Recently, a mobile layer of HetNet has been under consideration
              by the research community and is often referred to as moving
              networks. Moving networks comprise of mobile cells that are
              primarily introduced to improve Quality of Service (QoS) for the
              commuting users inside public transport because QoS is
              deteriorated due to vehicular penetration losses and high Doppler
              shift. Furthermore, the users inside fast moving public transport
              also exert excessive load on the core network due to large group
              handovers. To this end, mobile cells will play a crucial role in
              reducing the overall handover count and will help in alleviating
              these problems by decoupling in-vehicle users from the core
              network. This decoupling is achieved by introducing separate
              in-vehicle access link, and out-of-vehicle back-haul links with
              the core network. Additionally side-haul links will connect
              mobile cells with their neighbors. To date, remarkable research
              results have been achieved by the research community in
              addressing challenges linked to moving networks. However, to the
              best of our knowledge, a discussion on moving networks and mobile
              cells in a holistic way is still missing in the current
              literature. To fill the gap, in this article, we comprehensively
              survey moving networks and mobile cells. We cover the
              technological aspects of moving cells and their applications in
              the futuristic applications. We also discuss the use-cases and
              value additions that moving networks may bring to future cellular
              architecture and identify the challenges associated with them.
              Based on the identified challenges, we discuss the future
              research directions.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  1,
  pages    = "110--136",
  year     =  2021,
  file     = "All Papers/J/Jaffry et al. 2021 - A Comprehensive Survey on Moving Networks.pdf",
  keywords = "Quality of service;Handover;Rail transportation;5G mobile
              communication;Cellular networks;Interference;Moving
              networks;mobile cells;moving small cells;moving
              relays;heterogeneous networks (HetNets);5G;vehicular
              networks;5G6G",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2020.3029005"
}

@ARTICLE{Ventre2021-te,
  title     = "Segment Routing: A Comprehensive Survey of Research Activities,
               Standardization Efforts, and Implementation Results",
  author    = "Ventre, Pier Luigi and Salsano, Stefano and Polverini, Marco and
               Cianfrani, Antonio and Abdelsalam, Ahmed and Filsfils, Clarence
               and Camarillo, Pablo and Clad, Francois",
  abstract  = "Fixed and mobile telecom operators, enterprise network operators
               and cloud providers strive to face the challenging demands
               coming from the evolution of IP networks (e.g., huge bandwidth
               requirements, integration of billions of devices and millions of
               services in the cloud). Proposed in the early 2010s, Segment
               Routing (SR) architecture helps face these challenging demands,
               and it is currently being adopted and deployed. SR architecture
               is based on the concept of source routing and has interesting
               scalability properties, as it dramatically reduces the amount of
               state information to be configured in the core nodes to support
               complex services. SR architecture was first implemented with the
               MPLS dataplane and then, quite recently, with the IPv6 dataplane
               (SRv6). IPv6 SR architecture (SRv6) has been extended from the
               simple steering of packets across nodes to a general network
               programming approach, making it very suitable for use cases such
               as Service Function Chaining and Network Function
               Virtualization. In this article, we present a tutorial and a
               comprehensive survey on SR technology, analyzing standardization
               efforts, patents, research activities and implementation
               results. We start with an introduction on the motivations for
               Segment Routing and an overview of its evolution and
               standardization. Then, we provide a tutorial on Segment Routing
               technology, with a focus on the novel SRv6 solution. We discuss
               the standardization efforts and the patents providing details on
               the most important documents and mentioning other ongoing
               activities. We then thoroughly analyze research activities
               according to a taxonomy. We have identified 8 main categories
               during our analysis of the current state of play: Monitoring,
               Traffic Engineering, Failure Recovery, Centrally Controlled
               Architectures, Path Encoding, Network Programming, Performance
               Evaluation and Miscellaneous. We report the current status of SR
               deployments in production networks and of SR implementations
               (including several open source projects). Finally, we report our
               experience from this survey work and we identify a set of future
               research directions related to Segment Routing.",
  journal   = "IEEE Communications Surveys \& Tutorials",
  publisher = "IEEE",
  volume    =  23,
  number    =  1,
  pages     = "182--221",
  year      =  2021,
  file      = "All Papers/V/Ventre et al. 2021 - Segment Routing - A Comprehensive Survey of Research Activities, Standardization Efforts, and Implementation Results.pdf",
  keywords  = "FutureInternet",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/COMST.2020.3036826"
}

@ARTICLE{Wahab2021-sh,
  title    = "Federated Machine Learning: Survey, {Multi-Level} Classification,
              Desirable Criteria and Future Directions in Communication and
              Networking Systems",
  author   = "Wahab, Omar Abdel and Mourad, Azzam and Otrok, Hadi and Taleb,
              Tarik",
  abstract = "The communication and networking field is hungry for machine
              learning decision-making solutions to replace the traditional
              model-driven approaches that proved to be not rich enough for
              seizing the ever-growing complexity and heterogeneity of the
              modern systems in the field. Traditional machine learning
              solutions assume the existence of (cloud-based) central entities
              that are in charge of processing the data. Nonetheless, the
              difficulty of accessing private data, together with the high cost
              of transmitting raw data to the central entity gave rise to a
              decentralized machine learning approach called Federated
              Learning. The main idea of federated learning is to perform an
              on-device collaborative training of a single machine learning
              model without having to share the raw training data with any
              third-party entity. Although few survey articles on federated
              learning already exist in the literature, the motivation of this
              survey stems from three essential observations. The first one is
              the lack of a fine-grained multi-level classification of the
              federated learning literature, where the existing surveys base
              their classification on only one criterion or aspect. The second
              observation is that the existing surveys focus only on some
              common challenges, but disregard other essential aspects such as
              reliable client selection, resource management and training
              service pricing. The third observation is the lack of explicit
              and straightforward directives for researchers to help them
              design future federated learning solutions that overcome the
              state-of-the-art research gaps. To address these points, we first
              provide a comprehensive tutorial on federated learning and its
              associated concepts, technologies and learning approaches. We
              then survey and highlight the applications and future directions
              of federated learning in the domain of communication and
              networking. Thereafter, we design a three-level classification
              scheme that first categorizes the federated learning literature
              based on the high-level challenge that they tackle. Then, we
              classify each high-level challenge into a set of specific
              low-level challenges to foster a better understanding of the
              topic. Finally, we provide, within each low-level challenge, a
              fine-grained classification based on the technique used to
              address this particular challenge. For each category of
              high-level challenges, we provide a set of desirable criteria and
              future research directions that are aimed to help the research
              community design innovative and efficient future solutions. To
              the best of our knowledge, our survey is the most comprehensive
              in terms of challenges and techniques it covers and the most
              fine-grained in terms of the multi-level classification scheme it
              presents.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  2,
  pages    = "1342--1397",
  year     =  2021,
  file     = "All Papers/W/Wahab et al. 2021 - Federated Machine Learning - Survey, Multi-Level Cla ... riteria and Future Directions in Communication and Networking Systems.pdf",
  keywords = "Collaborative work;Machine
              learning;Tutorials;Training;Servers;Data models;Cloud
              computing;Federated learning;federated learning
              tutorial;multi-level classification;statistical
              challenges;transfer learning;machine
              learning;security;communication and networking systems;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3058573"
}

@ARTICLE{Xu2021-ac,
  title    = "A Survey on Resource Allocation for {5G} Heterogeneous Networks:
              Current Research, Future Trends, and Challenges",
  author   = "Xu, Yongjun and Gui, Guan and Gacanin, Haris and Adachi, Fumiyuki",
  abstract = "In the fifth-generation (5G) mobile communication system, various
              service requirements of different communication environments are
              expected to be satisfied. As a new evolution network structure,
              heterogeneous network (HetNet) has been studied in recent years.
              Compared with homogeneous networks, HetNets can increase the
              opportunity in the spatial resource reuse and improve users'
              quality of service by developing small cells into the coverage of
              macrocells. Since there is mutual interference among different
              users and the limited spectrum resource in HetNets, however,
              efficient resource allocation (RA) algorithms are vitally
              important to reduce the mutual interference and achieve spectrum
              sharing. In this article, we provide a comprehensive survey on RA
              in HetNets for 5G communications. Specifically, we first
              introduce the definition and different network scenarios of
              HetNets. Second, RA models are discussed. Then, we present a
              classification to analyze current RA algorithms for the existing
              works. Finally, some challenging issues and future research
              trends are discussed. Accordingly, we provide two potential
              structures for 6G communications to solve the RA problems of the
              next-generation HetNets, such as a learning-based RA structure
              and a control-based RA structure. The goal of this article is to
              provide important information on HetNets, which could be used to
              guide the development of more efficient techniques in this
              research area.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  2,
  pages    = "668--695",
  year     =  2021,
  file     = "All Papers/X/Xu et al. 2021 - A Survey on Resource Allocation for 5G Heterogeneous Networks - Current Research, Future Trends, and Challenges.pdf",
  keywords = "Wireless communication;Macrocell networks;5G mobile
              communication;Interference;Tutorials;Market
              research;Heterogeneous networks;Resource allocation;heterogeneous
              networks;spectrum efficiency;machine learning;5G
              communications;Wireless;5G6G",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3059896"
}

@ARTICLE{Anerousis2021-tl,
  title    = "The Origin and Evolution of Open Programmable Networks and {SDN}",
  author   = "Anerousis, Nikos and Chemouil, Prosper and Lazar, Aurel A and
              Mihai, Nelu and Weinstein, Stephen B",
  abstract = "The term ``SDN'' represents a significant evolution in networking
              technology and draws attention and support from network
              operators, vendors, researchers and industry regulators. Contrary
              to popular belief that SDN is a recent invention, or is related
              only to IP technologies, the concept of network programmability
              and control/data plane separation has its roots in the 1960s and
              1970s when the telephone network started its transition to
              digital. Many iterations followed, leading to applications in
              packet networks and today's Internet. In this article, we review
              a number of early works on network programmability that
              illustrate how several features of SDN emerged progressively over
              several decades. Our review starts from the early concepts of
              network control in the telephone network and continues to examine
              a prolific period of research advancements in the 1990s and early
              2000s that led to a number of startup companies that followed,
              IEEE's own efforts in standardizing network programmability, and
              finally the arrival of the OpenFlow standard. We study the
              importance of this architectural transformation and its
              influences on modern cloud computing and next-generation
              networking.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  3,
  pages    = "1956--1971",
  year     =  2021,
  keywords = "Standards;Control systems;Telephone sets;Industries;Switches;IP
              networks;Hardware;Software-defined networks;programmable
              networks;SDN;SDN history;NFV\_SDN",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3060582"
}

@ARTICLE{Feriani2021-ny,
  title    = "Single and {Multi-Agent} Deep Reinforcement Learning for
              {AI-Enabled} Wireless Networks: A Tutorial",
  author   = "Feriani, Amal and Hossain, Ekram",
  abstract = "Deep Reinforcement Learning (DRL) has recently witnessed
              significant advances that have led to multiple successes in
              solving sequential decision-making problems in various domains,
              particularly in wireless communications. The next generation of
              wireless networks is expected to provide scalable, low-latency,
              ultra-reliable services empowered by the application of
              data-driven Artificial Intelligence (AI). The key enabling
              technologies of future wireless networks, such as intelligent
              meta-surfaces, aerial networks, and AI at the edge, involve more
              than one agent which motivates the importance of multi-agent
              learning techniques. Furthermore, cooperation is central to
              establishing self-organizing, self-sustaining, and decentralized
              networks. In this context, this tutorial focuses on the role of
              DRL with an emphasis on deep Multi-Agent Reinforcement Learning
              (MARL) for AI-enabled wireless networks. The first part of this
              paper will present a clear overview of the mathematical
              frameworks for single-agent RL and MARL. The main idea of this
              work is to motivate the application of RL beyond the model-free
              perspective which was extensively adopted in recent years. Thus,
              we provide a selective description of RL algorithms such as
              Model-Based RL (MBRL) and cooperative MARL and we highlight their
              potential applications in future wireless networks. Finally, we
              overview the state-of-the-art of MARL in fields such as Mobile
              Edge Computing (MEC), Unmanned Aerial Vehicles (UAV) networks,
              and cell-free massive MIMO, and identify promising future
              research directions. We expect this tutorial to stimulate more
              research endeavors to build scalable and decentralized systems
              based on MARL.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  2,
  pages    = "1226--1252",
  year     =  2021,
  file     = "All Papers/F/Feriani and Hossain 2021 - Single and Multi-Agent Deep Reinforcement Learning for AI-Enabled Wireless Networks - A Tutorial.pdf",
  keywords = "Tutorials;Wireless networks;Games;Computational
              modeling;Training;5G mobile communication;Reinforcement
              learning;AI-enabled wireless networks;deep reinforcement learning
              (DRL);multi-agent reinforcement learning (MARL);model-based
              reinforcement learning (MBRL);decentralized
              networks;Models;Mobile\_Wireless",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3063822"
}

@ARTICLE{Jiang2021-cn,
  title    = "A Survey on {Multi-Access} Edge Computing Applied to Video
              Streaming: Some Research Issues and Challenges",
  author   = "Jiang, Xiantao and Yu, F Richard and Song, Tian and Leung, Victor
              C M",
  abstract = "Driven by the quality of experience (QoE) requirement of video
              streaming applications in the smart city, smart education,
              immersive service, and connected vehicle scenarios, the existing
              network poses significant challenges, including ultra-high
              bandwidth, ultra-large storage, and ultra-low latency
              requirements, etc. Multi-access edge computing (MEC) is a
              potential technology, which can provide computation-intensive and
              caching-intensive services for video streaming applications to
              satisfy the requirement of QoE. Thus, focusing on video streaming
              schemes, a comprehensive summary of the state of the art applying
              MEC to video streaming is surveyed. Firstly, the related overview
              and background knowledge are reviewed. Secondly, resource
              allocation issues have been discussed. Thirdly, the enabling
              technologies for video streaming are summarized by taking account
              of caching, computing, and networking. Then, a taxonomy of MEC
              enabled video streaming applications is classified. Finally,
              challenges and future research directions are given.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  2,
  pages    = "871--903",
  year     =  2021,
  keywords = "Streaming media;Resource management;Edge computing;Quality of
              experience;Computer architecture;Bandwidth;Smart
              cities;Multi-access edge computing;video streaming;resource
              allocation;QoE;caching;blockchain;NFV\_SDN;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3065237"
}

@ARTICLE{Tang2021-ve,
  title    = "Survey on Machine Learning for Intelligent {End-to-End}
              Communication Toward 6G: From Network Access, Routing to Traffic
              Control and Streaming Adaption",
  author   = "Tang, Fengxiao and Mao, Bomin and Kawamoto, Yuichi and Kato, Nei",
  abstract = "The end-to-end quality of service (QoS) and quality of experience
              (QoE) guarantee is quite important for network optimization. The
              current 5G and conceived 6G network in the future with ultra high
              density, bandwidth, mobility and large scale brings urgent
              requirement of high efficient end-to-end optimization methods.
              The conventional network optimization methods without learning
              and intelligent decision ability are hard to handle the high
              complexity and dynamic scenarios of 6G. Recently, machine
              learning based QoS and QoE aware network optimization algorithms
              emerge as a hot research area and attract much attention, which
              is widely acknowledged as the potential solution for end-to-end
              optimization in 6G. However, there are still many critical issues
              of employing machine learning in networks, especially in 6G. In
              this paper, we give a comprehensive survey on the recent machine
              learning based network optimization methods to guarantee the
              end-to-end QoS and QoE. To easy to follow, we introduce the
              investigated works following the end-to-end transmission flow
              from network access, routing to network congestion control and
              adaptive steaming control. Then we discuss some open issues and
              potential future research directions.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  3,
  pages    = "1578--1598",
  year     =  2021,
  keywords = "Quality of service;Heuristic algorithms;6G mobile
              communication;Quality of experience;Routing;Machine learning
              algorithms;Reinforcement learning;End-to-end;quality of service
              (QoS);quality of experience (QoE);machine learning (ML);deep
              learning (DL);network access;resource allocation;channel
              assignment;routing;congestion control;adaptive streaming
              control;adaptive bitrate streaming (ABR);5G6G;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3073009"
}

@ARTICLE{Chen2021-ye,
  title    = "Deep Reinforcement Learning for Internet of Things: A
              Comprehensive Survey",
  author   = "Chen, Wuhui and Qiu, Xiaoyu and Cai, Ting and Dai, Hong-Ning and
              Zheng, Zibin and Zhang, Yan",
  abstract = "The incumbent Internet of Things suffers from poor scalability
              and elasticity exhibiting in communication, computing, caching
              and control (4Cs) problems. The recent advances in deep
              reinforcement learning (DRL) algorithms can potentially address
              the above problems of IoT systems. In this context, this paper
              provides a comprehensive survey that overviews DRL algorithms and
              discusses DRL-enabled IoT applications. In particular, we first
              briefly review the state-of-the-art DRL algorithms and present a
              comprehensive analysis on their advantages and challenges. We
              then discuss on applying DRL algorithms to a wide variety of IoT
              applications including smart grid, intelligent transportation
              systems, industrial IoT applications, mobile crowdsensing, and
              blockchain-empowered IoT. Meanwhile, the discussion of each IoT
              application domain is accompanied by an in-depth summary and
              comparison of DRL algorithms. Moreover, we highlight emerging
              challenges and outline future research directions in driving the
              further success of DRL in IoT applications.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  3,
  pages    = "1659--1692",
  year     =  2021,
  keywords = "Internet of Things;Cloud computing;Servers;Security;Reinforcement
              learning;Smart grids;Real-time systems;Deep reinforcement
              learning;Internet of Things;decision making;resource
              allocation;EdgeFogCloudIoT;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3073036"
}

@ARTICLE{Nguyen2021-aj,
  title    = "Federated Learning for Internet of Things: A Comprehensive Survey",
  author   = "Nguyen, Dinh C and Ding, Ming and Pathirana, Pubudu N and
              Seneviratne, Aruna and Li, Jun and Vincent Poor, H",
  abstract = "The Internet of Things (IoT) is penetrating many facets of our
              daily life with the proliferation of intelligent services and
              applications empowered by artificial intelligence (AI).
              Traditionally, AI techniques require centralized data collection
              and processing that may not be feasible in realistic application
              scenarios due to the high scalability of modern IoT networks and
              growing data privacy concerns. Federated Learning (FL) has
              emerged as a distributed collaborative AI approach that can
              enable many intelligent IoT applications, by allowing for AI
              training at distributed IoT devices without the need for data
              sharing. In this article, we provide a comprehensive survey of
              the emerging applications of FL in IoT networks, beginning from
              an introduction to the recent advances in FL and IoT to a
              discussion of their integration. Particularly, we explore and
              analyze the potential of FL for enabling a wide range of IoT
              services, including IoT data sharing, data offloading and
              caching, attack detection, localization, mobile crowdsensing, and
              IoT privacy and security. We then provide an extensive survey of
              the use of FL in various key IoT applications such as smart
              healthcare, smart transportation, Unmanned Aerial Vehicles
              (UAVs), smart cities, and smart industry. The important lessons
              learned from this review of the FL-IoT services and applications
              are also highlighted. We complete this survey by highlighting the
              current challenges and possible directions for future research in
              this booming area.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  3,
  pages    = "1622--1658",
  year     =  2021,
  file     = "All Papers/N/Nguyen et al. 2021 - Federated Learning for Internet of Things - A Comprehensive Survey.pdf",
  keywords = "Internet of Things;Data privacy;Training;Data
              models;Computational modeling;Medical services;Computer
              architecture;Federated learning;Internet of Things;artificial
              intelligence;machine learning;privacy;EdgeFogCloudIoT;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3075439"
}

@ARTICLE{Liu2021-zr,
  title    = "Reconfigurable Intelligent Surfaces: Principles and Opportunities",
  author   = "Liu, Yuanwei and Liu, Xiao and Mu, Xidong and Hou, Tianwei and
              Xu, Jiaqi and Di Renzo, Marco and Al-Dhahir, Naofal",
  abstract = "Reconfigurable intelligent surfaces (RISs), also known as
              intelligent reflecting surfaces (IRSs), or large intelligent
              surfaces (LISs),1 have received significant attention for their
              potential to enhance the capacity and coverage of wireless
              networks by smartly reconfiguring the wireless propagation
              environment. Therefore, RISs are considered a promising
              technology for the sixth-generation (6G) of communication
              networks. In this context, we provide a comprehensive overview of
              the state-of-the-art on RISs, with focus on their operating
              principles, performance evaluation, beamforming design and
              resource management, applications of machine learning to
              RIS-enhanced wireless networks, as well as the integration of
              RISs with other emerging technologies. We describe the basic
              principles of RISs both from physics and communications
              perspectives, based on which we present performance evaluation of
              multiantenna assisted RIS systems. In addition, we systematically
              survey existing designs for RIS-enhanced wireless networks
              encompassing performance analysis, information theory, and
              performance optimization perspectives. Furthermore, we survey
              existing research contributions that apply machine learning for
              tackling challenges in dynamic scenarios, such as random
              fluctuations of wireless channels and user mobility in
              RIS-enhanced wireless networks. Last but not least, we identify
              major issues and research opportunities associated with the
              integration of RISs and other emerging technologies for
              applications to next-generation networks.1Without loss of
              generality, we use the name of RIS in the remainder of this
              paper.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  3,
  pages    = "1546--1577",
  year     =  2021,
  file     = "All Papers/L/Liu et al. 2021 - Reconfigurable Intelligent Surfaces - Principles and Opportunities.pdf",
  keywords = "Wireless networks;Communication system security;Wireless sensor
              networks;Optimization;Array signal
              processing;Tutorials;Performance evaluation;6G;intelligent
              reflecting surfaces (IRSs);large intelligent surfaces
              (LISs);machine learning;performance optimization;reconfigurable
              intelligent surfaces (RISs);wireless networks;Wireless",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3077737"
}

@ARTICLE{Hu2021-ps,
  title    = "Distributed Machine Learning for Wireless Communication Networks:
              Techniques, Architectures, and Applications",
  author   = "Hu, Shuyan and Chen, Xiaojing and Ni, Wei and Hossain, Ekram and
              Wang, Xin",
  abstract = "Distributed machine learning (DML) techniques, such as federated
              learning, partitioned learning, and distributed reinforcement
              learning, have been increasingly applied to wireless
              communications. This is due to improved capabilities of terminal
              devices, explosively growing data volume, congestion in the radio
              interfaces, and increasing concern of data privacy. The unique
              features of wireless systems, such as large scale, geographically
              dispersed deployment, user mobility, and the massive amount of
              data, give rise to new challenges in the design of DML
              techniques. There is a clear gap in the existing literature that
              the DML techniques are yet to be systematically reviewed for
              their applicability to wireless systems. This survey bridges the
              gap by providing a contemporary and comprehensive survey of DML
              techniques with a focus on wireless networks. Specifically, we
              review the latest applications of DML in power control, spectrum
              management, user association, and edge cloud computing. The
              optimality, accuracy, convergence rate, computation cost, and
              communication overhead of DML are analyzed. We also discuss the
              potential adversarial attacks faced by DML applications, and
              describe state-of-the-art countermeasures to preserve privacy and
              security. Last but not least, we point out a number of key issues
              yet to be addressed, and collate potentially interesting and
              challenging topics for future research.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  3,
  pages    = "1458--1493",
  year     =  2021,
  file     = "All Papers/H/Hu et al. 2021 - Distributed Machine Learning for Wireless Communication Networks - Techniques, Architectures, and Applications.pdf",
  keywords = "Wireless communication;Wireless networks;Communication system
              security;Wireless sensor networks;Computer
              architecture;Servers;Security;Distributed machine
              learning;wireless communication networks;convergence;computation
              and communication cost;architecture and platform;data privacy and
              security;ATOS;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3086014"
}

@ARTICLE{Tang2021-az,
  title    = "Comprehensive Survey on Machine Learning in Vehicular Network:
              Technology, Applications and Challenges",
  author   = "Tang, Fengxiao and Mao, Bomin and Kato, Nei and Gui, Guan",
  abstract = "Towards future intelligent vehicular network, the machine
              learning as the promising artificial intelligence tool is widely
              researched to intelligentize communication and networking
              functions. In this paper, we provide a comprehensive survey on
              various machine learning techniques applied to both communication
              and network parts in vehicular network. To benefit reading, we
              first give a preliminary on communication technologies and
              machine learning technologies in vehicular network. Then, we
              detailedly describe the challenges of conventional techniques in
              vehicular network and corresponding machine learning based
              solutions. Finally, we present several open issues and emphasize
              potential directions that are worthy of research for the future
              intelligent vehicular network.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  3,
  pages    = "2027--2057",
  year     =  2021,
  keywords = "Machine learning;Vehicle dynamics;OFDM;Wireless
              networks;Dedicated short range communication;Channel
              estimation;Security;V2X;vehicular network;machine learning;deep
              learning;V2V;Internet of Vehicles (IoV);resource
              allocation;routing;security;mobile cloud computing
              (MEC);Wireless;ML",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3089688"
}

@ARTICLE{Awaysheh2021-fd,
  title    = "Big Data Resource Management amp; Networks: Taxonomy, Survey, and
              Future Directions",
  author   = "Awaysheh, Feras M and Alazab, Mamoun and Garg, Sahil and Niyato,
              Dusit and Verikoukis, Christos",
  abstract = "Big Data (BD) platforms have a long tradition of leveraging
              trends and technologies from the broader computer network and
              communication community. For several years, dedicated servers of
              homogeneous clusters were employed as the dominant paradigm in BD
              networks. In recent years, the BD landscape has changed, porting
              different deployment architectures with various network models.
              This trend has resulted in various associated opportunities and
              challenges that induce BD practitioners to achieve the
              next-generation BD vision. In particular, addressing the BD
              velocity with batch and micro-batch processing. Nevertheless, the
              literature misses an extensive study of the associated impacts of
              adopting these new deployment architectures, giving it holds a
              significant research interest. This study addresses the previous
              concern, offering a comprehensive review of the architectural
              elements of BD batch query deployment models and environments. A
              novel taxonomy is proposed to classify these models based on
              their underlying communication systems. We first discuss the
              batch query processing requirements as comparison criteria of BD
              communication models and compare their salient features. The
              benefits/challenges of these environments away from BD
              traditional on-premise dedicated clusters are presented.
              Thereafter, we provide an extensive survey of the modern BD
              deployment architectures, categorizing them based on their
              underlying infrastructure. Finally, several directions are
              outlined for future research on improving the state-of-the-art of
              BD landscape and provide recommendations for the BD practitioners
              on emerging environments supporting BD applications and the
              general large-scale data analytics.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  4,
  pages    = "2098--2130",
  year     =  2021,
  keywords = "Computer architecture;Task analysis;Resource
              management;Taxonomy;Tutorials;Market research;Data models;Big
              data;batch query systems;resource management and
              communication;computer network comparison;cloud computing;grid
              computing;HPC;decentralized computing;hybrid
              computing;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3094993"
}

@ARTICLE{Sonkoly2021-gz,
  title    = "Survey on Placement Methods in the Edge and Beyond",
  author   = "Sonkoly, Bal{\'a}zs and Czentye, J{\'a}nos and Szalay, M{\'a}rk
              and N{\'e}meth, Bal{\'a}zs and Toka, L{\'a}szl{\'o}",
  abstract = "Edge computing is a (r)evolutionary extension of traditional
              cloud computing. It expands central cloud infrastructure with
              execution environments close to the users in terms of latency in
              order to enable a new generation of cloud applications. This
              paradigm shift has opened the door for telecommunications
              operators, mobile and fixed network vendors: they have joined the
              cloud ecosystem as essential stakeholders considerably
              influencing the future success of the technology. A key problem
              in edge computing is the optimal placement of computational units
              (virtual machines, containers, tasks or functions) of novel
              distributed applications. These components are deployed to a
              geographically distributed virtualized infrastructure and
              heterogeneous networking technologies are invoked to connect them
              while respecting quality requirements. The optimal hosting
              environment should be selected based on multiple criteria by
              novel scheduler algorithms which can cope with the new challenges
              of distributed cloud architecture where networking aspects cannot
              be ignored. The research community has dedicated significant
              efforts to this topic during recent years and a vast number of
              theoretical results have been published addressing different
              variants of the related mathematical problems. However, a
              comprehensive survey focusing on the technical and analytical
              aspects of the placement problem in various edge architectures is
              still missing. This survey provides a comprehensive summary and a
              structured taxonomy of the vast research on placement of
              computational entities in emerging edge infrastructures.
              Following the given taxonomy, the research papers are analyzed
              and categorized according to several dimensions, such as the
              capabilities of the underlying platforms, the structure of the
              supported services, the problem formulation, the applied
              mathematical methods, the objectives and constraints incorporated
              in the optimization problems, and the complexity of the proposed
              methods. We summarize the gained insights and important lessons
              learned, and finally, we reveal some important research gaps in
              the current literature.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  4,
  pages    = "2590--2629",
  year     =  2021,
  file     = "All Papers/S/Sonkoly et al. 2021 - Survey on Placement Methods in the Edge and Beyond.pdf",
  keywords = "Cloud computing;Optimization;Data
              centers;Taxonomy;Tutorials;Telecommunications;Task
              analysis;Edge/fog computing;MEC;cloudlets;resource
              orchestration;function placement
              optimization;offloading;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3101460"
}

@ARTICLE{Xiao2021-il,
  title    = "Leveraging Deep Reinforcement Learning for Traffic Engineering: A
              Survey",
  author   = "Xiao, Yang and Liu, Jun and Wu, Jiawei and Ansari, Nirwan",
  abstract = "After decades of unprecedented development, modern networks have
              evolved far beyond expectations in terms of scale and complexity.
              In many cases, traditional traffic engineering (TE) approaches
              fail to address the quality of service (QoS) requirements of
              modern networks. In recent years, deep reinforcement learning
              (DRL) has proved to be a feasible and effective solution for
              autonomously controlling and managing complex systems. Massive
              growth in the use of DRL applications in various domains is
              beginning to benefit the communications industry. In this paper,
              we firstly provide a comprehensive overview of DRL-based TE.
              Then, we present a detailed literature review on applications of
              DRL for TE including three fundamental issues: routing
              optimization, congestion control, and resource management.
              Finally, we discuss our insights into the challenges and future
              research perspectives of DRL-based TE.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  4,
  pages    = "2064--2097",
  year     =  2021,
  file     = "All Papers/X/Xiao et al. 2021 - Leveraging Deep Reinforcement Learning for Traffic Engineering - A Survey.pdf",
  keywords = "Wireless networks;Routing;Optimization;Reinforcement
              learning;Tutorials;Supervised learning;Wireless sensor
              networks;Deep reinforcement learning;traffic engineering;routing
              optimization;congestion control;resource management;ML;Teaching",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3102580"
}

@ARTICLE{Luo2021-cx,
  title    = "Resource Scheduling in Edge Computing: A Survey",
  author   = "Luo, Quyuan and Hu, Shihong and Li, Changle and Li, Guanghui and
              Shi, Weisong",
  abstract = "With the proliferation of the Internet of Things (IoT) and the
              wide penetration of wireless networks, the surging demand for
              data communications and computing calls for the emerging edge
              computing paradigm. By moving the services and functions located
              in the cloud to the proximity of users, edge computing can
              provide powerful communication, storage, networking, and
              communication capacity. The resource scheduling in edge
              computing, which is the key to the success of edge computing
              systems, has attracted increasing research interests. In this
              paper, we survey the state-of-the-art research findings to know
              the research progress in this field. Specifically, we present the
              architecture of edge computing, under which different
              collaborative manners for resource scheduling are discussed.
              Particularly, we introduce a unified model before summarizing the
              current works on resource scheduling from three research issues,
              including computation offloading, resource allocation, and
              resource provisioning. Based on two modes of operation, i.e.,
              centralized and distributed modes, different techniques for
              resource scheduling are discussed and compared. Also, we
              summarize the main performance indicators based on the surveyed
              literature. To shed light on the significance of resource
              scheduling in real-world scenarios, we discuss several typical
              application scenarios involved in the research of resource
              scheduling in edge computing. Finally, we highlight some open
              research challenges yet to be addressed and outline several open
              issues as the future research direction.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  4,
  pages    = "2131--2165",
  year     =  2021,
  file     = "All Papers/L/Luo et al. 2021 - Resource Scheduling in Edge Computing - A Survey.pdf",
  keywords = "Edge computing;Processor scheduling;Task analysis;Resource
              management;Cloud computing;Job shop scheduling;Internet of
              Things;Internet of things;edge computing;resource
              allocation;computation offloading;resource
              provisioning;EdgeFogCloudIoT",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3106401"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Singh2021-gs,
  title    = "Quantum {Internet---Applications}, Functionalities, Enabling
              Technologies, Challenges, and Research Directions",
  author   = "Singh, Amoldeep and Dev, Kapal and Siljak, Harun and Joshi, Hem
              Dutt and Magarini, Maurizio",
  abstract = "The advanced notebooks, mobile phones, and Internet applications
              in today's world that we use are all entrenched in classical
              communication bits of zeros and ones. Classical Internet has laid
              its foundation originating from the amalgamation of mathematics
              and Claude Shannon's theory of information. However, today's
              Internet technology is a playground for eavesdroppers. This poses
              a serious challenge to various applications that rely on
              classical Internet technology, and it has motivated the
              researchers to switch to new technologies that are fundamentally
              more secure. By exploring the quantum effects, researchers paved
              the way into quantum networks that provide security, privacy, and
              range of capabilities such as quantum computation, communication,
              and metrology. The realization of Quantum Internet (QI) requires
              quantum communication between various remote nodes through
              quantum channels guarded by quantum cryptographic protocols. Such
              networks rely upon quantum bits (qubits) that can simultaneously
              take the value of zeros and ones. Due to the extraordinary
              properties of qubits such as superposition, entanglement, and
              teleportation, it gives an edge to quantum networks over
              traditional networks in many ways. At the same time, transmitting
              qubits over long distances is a formidable task and extensive
              research is going on satellite-based quantum communication, which
              will deliver breakthroughs for physically realizing QI in near
              future. In this paper, QI functionalities, technologies,
              applications and open challenges have been extensively surveyed
              to help readers gain a basic understanding of the infrastructure
              required for the development of the global QI.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  23,
  number   =  4,
  pages    = "2218--2247",
  year     =  2021,
  file     = "All Papers/S/Singh et al. 2021 - Quantum Internet—Applications, Functionalities, Enabling Technologies, Challenges, and Research Directions.pdf",
  keywords = "Qubit;Quantum mechanics;Quantum
              entanglement;Tutorials;Teleportation;Electronic
              mail;Computers;Quantum mechanics;information theory;quantum
              computation;quantum communication and networking;GenericInterest",
  issn     = "1553-877X",
  doi      = "10.1109/COMST.2021.3109944"
}

@ARTICLE{Szott2022-is,
  title     = "{Wi-Fi} Meets {ML}: A Survey on Improving {IEEE} 802.11
               Performance With Machine Learning",
  author    = "Szott, Szymon and Kosek-Szott, Katarzyna and Gaw{\l}owicz, Piotr
               and G{\'o}mez, Jorge Torres and Bellalta, Boris and Zubow,
               Anatolij and Dressler, Falko",
  abstract  = "Wireless local area networks (WLANs) empowered by IEEE 802.11
               (Wi-Fi) hold a dominant position in providing Internet access
               thanks to their freedom of deployment and configuration as well
               as the existence of affordable and highly interoperable devices.
               The Wi-Fi community is currently deploying Wi-Fi 6 and
               developing Wi-Fi 7, which will bring higher data rates, better
               multi-user and multi-AP support, and, most importantly, improved
               configuration flexibility. These technical innovations,
               including the plethora of configuration parameters, are making
               next-generation WLANs exceedingly complex as the dependencies
               between parameters and their joint optimization usually have a
               non-linear impact on network performance. The complexity is
               further increased in the case of dense deployments and
               coexistence in shared bands. While classical optimization
               approaches fail in such conditions, machine learning (ML) is
               able to handle complexity. Much research has been published on
               using ML to improve Wi-Fi performance and solutions are slowly
               being adopted in existing deployments. In this survey, we adopt
               a structured approach to describe the various Wi-Fi areas where
               ML is applied. To this end, we analyze over 250 papers in the
               field, providing readers with an overview of the main trends.
               Based on this review, we identify specific open challenges and
               provide general future research directions.",
  journal   = "IEEE Communications Surveys \& Tutorials",
  publisher = "IEEE",
  volume    =  24,
  number    =  3,
  pages     = "1843--1893",
  year      =  2022,
  file      = "All Papers/S/Szott et al. 2022 - Wi-Fi Meets ML - A Survey on Improving IEEE 802.11 Performance With Machine Learning.pdf",
  keywords  = "Mobile\_Wireless",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/COMST.2022.3179242"
}

@ARTICLE{Yang2023-ak,
  title     = "Semantic Communications for Future Internet: Fundamentals,
               Applications, and Challenges",
  author    = "Yang, Wanting and Du, Hongyang and Liew, Zi Qin and Lim, Wei
               Yang Bryan and Xiong, Zehui and Niyato, Dusit and Chi, Xuefen
               and Shen, Xuemin and Miao, Chunyan",
  abstract  = "With the increasing demand for intelligent services, the
               sixth-generation (6G) wireless networks will shift from a
               traditional architecture that focuses solely on a high
               transmission rate to a new architecture that is based on the
               intelligent connection of everything. Semantic communication
               (SemCom), a revolutionary architecture that integrates user as
               well as application requirements and the meaning of information
               into data processing and transmission, is predicted to become a
               new core paradigm in 6G. While SemCom is expected to progress
               beyond the classical Shannon paradigm, several obstacles need to
               be overcome on the way to a SemCom-enabled smart Internet. In
               this paper, we first highlight the motivations and compelling
               reasons for SemCom in 6G. Then, we provide an overview of
               SemCom-related theory development. After that, we introduce
               three types of SemCom, i.e., semantic-oriented communication,
               goal-oriented communication, and semantic-aware communication.
               Following that, we organize the design of the communication
               system into three dimensions, i.e., semantic information (SI)
               extraction, SI transmission, and SI metrics. For each dimension,
               we review existing techniques and discuss their benefits and
               limitations, as well as the remaining challenges. Then, we
               introduce the potential applications of SemCom in 6G and portray
               the vision of future SemCom-empowered network architecture.
               Finally, we outline future research opportunities. In a
               nutshell, this paper provides a holistic review of the
               fundamentals of SemCom, its applications in 6G networks, and the
               existing challenges and open issues with insights for further
               in-depth investigations.",
  journal   = "IEEE Communications Surveys \& Tutorials",
  publisher = "IEEE",
  volume    =  25,
  number    =  1,
  pages     = "213--250",
  year      =  2023,
  file      = "All Papers/Y/Yang et al. 2023 - Semantic Communications for Future Internet - Fundamentals, Applications, and Challenges.pdf",
  keywords  = "FutureInternet;Mobile\_Wireless",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/COMST.2022.3223224"
}

@ARTICLE{Polese2023-fn,
  title     = "Understanding {O-RAN}: Architecture, Interfaces, Algorithms,
               Security, and Research Challenges",
  author    = "Polese, Michele and Bonati, Leonardo and D'Oro, Salvatore and
               Basagni, Stefano and Melodia, Tommaso",
  abstract  = "The Open Radio Access Network (RAN) and its embodiment through
               the O-RAN Alliance specifications are poised to revolutionize
               the telecom ecosystem. O-RAN promotes virtualized RANs where
               disaggregated components are connected via open interfaces and
               optimized by intelligent controllers. The result is a new
               paradigm for the RAN design, deployment, and operations: O-RAN
               networks can be built with multi-vendor, interoperable
               components, and can be programmatically optimized through a
               centralized abstraction layer and data-driven closed-loop
               control. Therefore, understanding O-RAN, its architecture, its
               interfaces, and workflows is key for researchers and
               practitioners in the wireless community. In this article, we
               present the first detailed tutorial on O-RAN. We also discuss
               the main research challenges and review early research results.
               We provide a deep dive of the O-RAN specifications, describing
               its architecture, design principles, and the O-RAN interfaces.
               We then describe how the O-RAN RAN Intelligent Controllers
               (RICs) can be used to effectively control and manage
               3GPP-defined RANs. Based on this, we discuss innovations and
               challenges of O-RAN networks, including the Artificial
               Intelligence (AI) and Machine Learning (ML) workflows that the
               architecture and interfaces enable, security, and
               standardization issues. Finally, we review experimental research
               platforms that can be used to design and test O-RAN networks,
               along with recent research results, and we outline future
               directions for O-RAN development.",
  journal   = "IEEE Communications Surveys \& Tutorials",
  publisher = "IEEE",
  volume    =  25,
  number    =  2,
  pages     = "1376--1411",
  year      =  2023,
  file      = "All Papers/P/Polese et al. 2023 - Understanding O-RAN - Architecture, Interfaces, Algorithms, Security, and Research Challenges.pdf",
  keywords  = "Mobile\_Wireless",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/COMST.2023.3239220"
}

@ARTICLE{Wang2023-oh,
  title     = "On the Road to 6G: Visions, Requirements, Key Technologies, and
               Testbeds",
  author    = "Wang, Cheng-Xiang and You, Xiaohu and Gao, Xiqi and Zhu, Xiuming
               and Li, Zixin and Zhang, Chuan and Wang, Haiming and Huang,
               Yongming and Chen, Yunfei and Haas, Harald and Thompson, John S
               and Larsson, Erik G and Di Renzo, Marco and Tong, Wen and Zhu,
               Peiying and Shen, Xuemin and Vincent Poor, H and Hanzo, Lajos",
  abstract  = "Fifth generation (5G) mobile communication systems have entered
               the stage of commercial deployment, providing users with new
               services, improved user experiences as well as a host of novel
               opportunities to various industries. However, 5G still faces
               many challenges. To address these challenges, international
               industrial, academic, and standards organizations have commenced
               research on sixth generation (6G) wireless communication
               systems. A series of white papers and survey papers have been
               published, which aim to define 6G in terms of requirements,
               application scenarios, key technologies, etc. Although ITU-R has
               been working on the 6G vision and it is expected to reach a
               consensus on what 6G will be by mid-2023, the related global
               discussions are still wide open and the existing literature has
               identified numerous open issues. This paper first provides a
               comprehensive portrayal of the 6G vision, technical
               requirements, and application scenarios, covering the current
               common understanding of 6G. Then, a critical appraisal of the 6G
               network architecture and key technologies is presented.
               Furthermore, existing testbeds and advanced 6G verification
               platforms are detailed for the first time. In addition, future
               research directions and open challenges are identified to
               stimulate the on-going global debate. Finally, lessons learned
               to date concerning 6G networks are discussed.",
  journal   = "IEEE Communications Surveys \& Tutorials",
  publisher = "IEEE",
  volume    =  25,
  number    =  2,
  pages     = "905--974",
  year      =  2023,
  file      = "All Papers/W/Wang et al. 2023 - On the Road to 6G - Visions, Requirements, Key Technologies, and Testbeds.pdf",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/COMST.2023.3249835"
}

@INPROCEEDINGS{Laurikainen2012-jj,
  title     = "Improving the Efficiency of Deploying Virtual Machines in a
               Cloud Environment",
  booktitle = "2012 International Conference on Cloud and Service Computing",
  author    = "Laurikainen, Risto and Laitinen, Jarno and Lehtovuori, Pekka and
               Nurminen, Jukka K",
  abstract  = "Flexible allocation of resources is one of the main benefits of
               cloud computing. Virtualization is used to achieve this
               flexibility: one or more virtual machines run on a single
               physical machine. These virtual machines can be deployed and
               destroyed as needed. One obstacle to flexibility in current
               cloud systems is that deploying multiple virtual machines
               simultaneously on multiple physical machines is slow due to the
               inefficient usage of available resources. We implemented and
               evaluated three methods of transferring virtual machine images
               for the Open Nebula cloud middleware. One of the implementations
               was based on BitTorrent and the other two were based on
               multicast. Our evaluation results showed that the implemented
               methods were significantly more scalable than the default
               methods available in Open Nebula when tens of virtual machines
               were deployed simultaneously. However, the implemented methods
               were slower than the default unicast methods for deploying only
               one or a few virtual machines at a time due to overhead related
               to managing the transfer process. If the usage pattern of the
               cloud is such that deploying large batches of virtual machines
               at once is common, using the new transfer methods will
               significantly speed up the deployment process and reduce its
               resource usage.",
  pages     = "232--239",
  month     =  nov,
  year      =  2012,
  file      = "All Papers/L/Laurikainen et al. 2012 - Improving the Efficiency of Deploying Virtual Machines in a Cloud Environment.pdf",
  keywords  = "Virtual
               machining;Servers;Unicast;Cloning;Receivers;Software;Testing;cloud
               environment;cloud computing;BitTorrent;multicast;OpenNebula",
  doi       = "10.1109/CSC.2012.43"
}

@INPROCEEDINGS{Wartel2010-hb,
  title     = "Image Distribution Mechanisms in Large Scale Cloud Providers",
  booktitle = "2010 {IEEE} Second International Conference on Cloud Computing
               Technology and Science",
  author    = "Wartel, Romain and Cass, Tony and Moreira, Belmiro and Roche,
               Ewan and Guijarro, Manuel and Goasguen, Sebastien and
               Schwickerath, Ulrich",
  abstract  = "This paper presents the various mechanisms for virtual machine
               image distribution within a large batch farm and between sites
               that offer cloud computing services. The work is presented
               within the context of the Large Hadron Collider Computing Grid
               (LCG), it has two main goals. First it aims at presenting the
               CERN specific mechanisms that have been put in place to test the
               pre-staging of virtual machine images within a large cloud
               infrastructure of several hundred physical hosts. Second it
               introduces the basis of a policy for trusting and distributing
               virtual machine images between sites of the LCG. Finally
               experimental results are shown for the distribution of a 10 GB
               virtual machine image distributed to over 400 physical nodes
               using a binary tree and a Bit Torrent algorithm. Results show
               that images can be pre-staged within 30 minutes.",
  pages     = "112--117",
  month     =  nov,
  year      =  2010,
  keywords  = "Virtual machine monitors;Virtual machining;Cloud
               computing;Indexes;Security;Servers;Bandwidth;virtualization;image;cloud;bittorrent;trust;grid",
  doi       = "10.1109/CloudCom.2010.73"
}

@INPROCEEDINGS{Wu2012-uj,
  title     = "On {P2P} mechanisms for {VM} image distribution in cloud data
               centers: Modeling, analysis and improvement",
  booktitle = "4th {IEEE} International Conference on Cloud Computing
               Technology and Science Proceedings",
  author    = "Wu, Di and Zeng, Yupeng and He, Jian and Liang, Yi and Wen,
               Yonggang",
  abstract  = "To provide elastic cloud services with QoS guarantee, it is
               essential for cloud data centers to provision virtual machines
               rapidly according to user requests. Due to bandwidth bottleneck
               of centralized model, P2P model is recently adopted in data
               centers to relieve server workload by enabling sharing among VM
               instances. In this paper, we develop a simple theoretic model to
               analyze two typical P2P models for VM image distribution,
               namely, isolated-image P2P distribution model and cross-image
               P2P distribution model. We compare their efficiency under
               different parameter settings and derive their corresponding
               optimal server bandwidth allocation strategies. In addition, we
               also propose a practical optimal server bandwidth provisioning
               algorithm for chunk-level cross-image P2P distribution mechanism
               to further improve its efficiency. Extensive simulations are
               conducted to validate the effectiveness of our proposed
               algorithm.",
  pages     = "50--57",
  month     =  dec,
  year      =  2012,
  file      = "All Papers/W/Wu et al. 2012 - On P2P mechanisms for VM image distribution in cloud data centers - Modeling, analysis and improvement.pdf",
  keywords  = "Bandwidth;Servers;Analytical models;Data models;Computational
               modeling;Cloud computing;Optimization;virtual machine (VM);image
               distribution;cloud data centers",
  doi       = "10.1109/CloudCom.2012.6427568"
}

@INPROCEEDINGS{Mehraghdam2014-od,
  title     = "Specifying and placing chains of virtual network functions",
  booktitle = "2014 {IEEE} 3rd International Conference on Cloud Networking
               ({CloudNet})",
  author    = "Mehraghdam, S and Keller, M and Karl, H",
  abstract  = "Network appliances perform different functions on network flows
               and constitute an important part of an operator's network.
               Normally, a set of chained network functions process network
               flows. Following the trend of virtualization of networks,
               virtualization of the network functions has also become a topic
               of interest. We define a model for formalizing the chaining of
               network functions using a context-free language. We process
               deployment requests and construct virtual network function
               graphs that can be mapped to the network. We describe the
               mapping as a Mixed Integer Quadratically Constrained Program
               (MIQCP) for finding the placement of the network functions and
               chaining them together considering the limited network resources
               and requirements of the functions. We have performed a Pareto
               set analysis to investigate the possible trade-offs between
               different optimization objectives.",
  publisher = "ieeexplore.ieee.org",
  pages     = "7--13",
  month     =  oct,
  year      =  2014,
  keywords  = "computer networks;context-free languages;graph theory;integer
               programming;Pareto optimisation;quadratic
               programming;virtualisation;network appliances;chained network
               functions;network function virtualization;network function
               chaining;context-free language;virtual network function
               graphs;mixed integer quadratically constrained
               program;MIQCP;network function placement;Pareto set
               analysis;Context modeling;network function
               virtualization;virtual network functions;network function
               chaining;network service chaining;network function
               placement;MyPapers",
  doi       = "10.1109/CloudNet.2014.6968961"
}

@INPROCEEDINGS{Brumbaugh2019-en,
  title     = "Bighead: A {Framework-Agnostic}, {End-to-End} Machine Learning
               Platform",
  booktitle = "2019 {IEEE} International Conference on Data Science and
               Advanced Analytics ({DSAA})",
  author    = "Brumbaugh, Eli and Bhushan, Mani and Cheong, Andrew and Du,
               Michelle Gu-Qian and Feng, Jeff and Handel, Nick and Hoh, Andrew
               and Hone, Jack and Hunter, Brad and Kale, Atul and Luque,
               Alfredo and Nooraei, Bahador and Park, John and Puttaswamy,
               Krishna and Schiller, Kyle and Shapiro, Evgeny and Shi, Conglei
               and Siegel, Aaron and Simha, Nikhil and Sbrocca, Marie and Yao,
               Shi-Jing and Yoon, Patrick and Zanoyan, Varant and Zeng,
               Xiao-Han T and Zhu, Qiang",
  abstract  = "With the increasing need to build systems and products powered
               by machine learning inside organizations, it is critical to have
               a platform that provides machine learning practitioners with a
               unified environment to easily prototype, deploy, and maintain
               their models at scale. However, due to the diversity of machine
               learning libraries, the inconsistency between environments, and
               various scalability requirement, there is no existing work to
               date that addresses all of these challenges. Here, we introduce
               Bighead, a framework-agnostic, end-to-end platform for machine
               learning. It offers a seamless user experience requiring only
               minimal efforts that span feature set management, prototyping,
               training, batch (offline) inference, real-time (online)
               inference, evaluation, and model lifecycle management. In
               contrast to existing platforms, it is designed to be highly
               versatile and extensible, and supports all major machine
               learning frameworks, rather than focusing on one particular
               framework. It ensures consistency across different environments
               and stages of the model lifecycle, as well as across data
               sources and transformations. It scales horizontally and
               elastically in response to the workload such as dataset size and
               throughput. Its components include a feature management
               framework, a model development toolkit, a lifecycle management
               service with UI, an offline training and inference engine, an
               online inference service, an interactive prototyping
               environment, and a Docker image customization tool. It is the
               first platform to offer a feature management component that is a
               general-purpose aggregation framework with lambda architecture
               and temporal joins. Bighead is deployed and widely adopted at
               Airbnb, and has enabled the data science and engineering teams
               to develop and deploy machine learning models in a timely and
               reliable manner. Bighead has shortened the time to deploy a new
               model from months to days, ensured the stability of the models
               in production, facilitated adoption of cutting-edge models, and
               enabled advanced machine learning based product features of the
               Airbnb platform. We present two use cases of productionizing
               models of computer vision and natural language processing.",
  pages     = "551--560",
  month     =  oct,
  year      =  2019,
  keywords  = "end-to-end machine learning infrastructure;deep
               learning;automated feature engineering;lambda architecture;cloud
               native",
  doi       = "10.1109/DSAA.2019.00070"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Ditzler2017-ho,
  title     = "High Performance Machine Learning ({HPML}) Framework to Support
               {DDDAS} Decision Support Systems: Design Overview",
  booktitle = "2017 {IEEE} 2nd International Workshops on Foundations and
               Applications of Self* Systems ({FAS*W})",
  author    = "Ditzler, Gregory and Hariri, Salim and Akoglu, Ali",
  abstract  = "This paper presents a design for a High Performance Machine
               Learning (HPML) framework to support DDDAS decision processes.
               The HPML framework can provide a high performance computing
               environment to implement large scale machine learning algorithms
               that leverages Big Data tools (e.g., SPARK, Hadoop), parallel
               algorithms, and MapReduce programming paradigm. The framework
               provides the following capabilities: · High Performance Parallel
               Algorithms: For a suite of important ML, we will develop three
               parallel implementations of each algorithm that are based on
               Message Passing Interface (MPI), Shared Memory (SM) and
               MapReduce programming model. · High Performance and Scalable
               Platforms: This will enable us to identify the best high
               performance platform that maximizes performance and scalability
               of the parallel ML methods. We will experiment with and evaluate
               the performance and scalability of different parallel
               architectures (shared memory and message passing), Clusters of
               GPUs, and cloud computing systems. By leveraging the emerging
               Big Data tools and high performance computing algorithms
               (traditional and emerging paradigm such as MapReduce), we will
               be able to achieve the following: 1) reduce significantly the ML
               processing time, 2) enable StreamlinedML users to leverage Big
               Data tools to perform large scale ML tasks over structured and
               non-structured data sets; and 3) enable users to identify the
               best parallel platform and storage allocation and distribution
               that maximize performance and scalability of the selected ML
               algorithms.``",
  pages     = "360--362",
  month     =  sep,
  year      =  2017,
  keywords  = "Machine learning algorithms;Computational
               modeling;Scalability;Programming;Data models;Classification
               algorithms;Machine Learning;High Performance ML;Data
               Analytics;DDDAS",
  doi       = "10.1109/FAS-W.2017.174"
}

@INPROCEEDINGS{Neshatpour2018-dj,
  title     = "Design Space Exploration for Hardware Acceleration of Machine
               Learning Applications in {MapReduce}",
  booktitle = "2018 {IEEE} 26th Annual International Symposium on
               {Field-Programmable} Custom Computing Machines ({FCCM})",
  author    = "Neshatpour, Katayoun and Makrani, Hosein Mohammadi and Sasan,
               Avesta and Ghasemzadeh, Hassan and Rafatirad, Setareh and
               Homayoun, Houman",
  abstract  = "Emerging big data applications heavily rely on machine learning
               algorithms which are computationally intensive. To meet
               computational requirements, and power and scalability
               challenges, FPGA based Hardware accelerators have found their
               way in data centers and cloud infrastructures. Recent efforts on
               HW acceleration of big data mainly attempt to accelerate a
               particular application and deploy it on a specific architecture
               that fits well its performance and power requirements. Given the
               diversity of architectures and ML applications, the important
               research question is which architecture is better suited to meet
               the performance, power and energy-efficiency requirements of a
               diverse range of ML-based analytics applications. In this work,
               we answer this question by investigating how the type of FPGA
               (low-end vs. high-end), and its integration with the CPU
               (on-chip vs. off-chip) along with the choice of CPU (high
               performance big vs. low power little servers) affects the
               speedup yield and power reduction in a CPU+FPGA architecture for
               machine learning applications implemented in MapReduce. We show
               that among the three architectural parameters, the type of CPU
               is the most dominant factor in determining the execution time
               and power in a CPU+FPGA architecture for MapReduce applications.
               The integration technology and FPGA type comes next, with the
               power and performance least sensitive to the FPGA type.",
  pages     = "221--221",
  month     =  apr,
  year      =  2018,
  keywords  = "Field programmable gate arrays;Servers;Computer
               architecture;System-on-chip;Hidden Markov models;Space
               exploration;Machine learning;MapReduce;FPGA acceleration;Machine
               Learning",
  issn      = "2576-2621",
  doi       = "10.1109/FCCM.2018.00055"
}

@INPROCEEDINGS{He2019-yl,
  title     = "Graph Attention {Spatial-Temporal} Network for Deep Learning
               Based Mobile Traffic Prediction",
  booktitle = "2019 {IEEE} Global Communications Conference ({GLOBECOM})",
  author    = "He, K and Huang, Y and Chen, X and Zhou, Z and Yu, S",
  abstract  = "With the rapid development of mobile cellular technologies and
               the popularity of mobile devices, timely mobile traffic
               forecasting with high accuracy becomes more and more critical
               for proactive network service provisioning and efficient network
               resource allocation. Due to the complicated dynamic nature of
               mobile traffic demand, traditional time series methods cannot
               satisfy the requirements of prediction tasks well and often
               neglect the important spatial factors. In addition, while some
               recent approaches model mobile traffic prediction problem using
               temporal and spatial features, they only consider local
               geographical dependency and do not take influential distant
               regions into consideration. In this paper, we propose Graph
               Attention Spatial-Temporal Network (GASTN), a novel deep
               learning framework to tackle the mobile traffic forecasting
               problem. Specifically, GASTN considers spatial correlation
               through the geographical relation graph and utilizes structural
               recurrent neural networks to model the global near-far spatial
               relationships as well as capture the temporal dependencies
               between future demand for mobile traffic and historical traffic
               volume. Besides, two attention mechanisms are proposed to
               integrate different effects in a holistic way. Extensive
               experiments on a large-scale real-world mobile traffic dataset
               demonstrate that our model significantly outperforms the
               state-of-the-art methods.",
  pages     = "1--6",
  month     =  dec,
  year      =  2019,
  file      = "All Papers/H/He et al. 2019 - Graph Attention Spatial-Temporal Network for Deep Learning Based Mobile Traffic Prediction.pdf",
  keywords  = "Correlation;Time series analysis;Forecasting;Predictive
               models;Machine learning;Recurrent neural networks;Heuristic
               algorithms;GeneralNetworking",
  issn      = "2576-6813",
  doi       = "10.1109/GLOBECOM38437.2019.9013136"
}

@INPROCEEDINGS{Souza2019-ug,
  title     = "{MLFV}: {Network-Aware} Orchestration for Placing Chains of
               Virtualized Machine Learning Functions",
  booktitle = "2019 {IEEE} Global Communications Conference ({GLOBECOM})",
  author    = "Souza, Renan and Trois, Celio and Turchetti, Rogerio and
               Martinello, Magnos and Correa, Joao Henrique G and Mafioletti,
               Diego Rossi and Bona, Luis C E and Lima, Joao Carlos D and
               Machado, Alencar",
  abstract  = "Machine Learning as a Service (MLaaS) platforms enables access
               to Machine Learning (ML) processing with scalable
               infrastructure, from anywhere, and at any time, but requires
               sending large amounts of data to the cloud. ML on the edge is
               emerging as an option to reduce latency and bandwidth usage,
               maintaining data privacy. However, the existing edge approaches
               are not aware of the current network state for orchestrating the
               tasks. Network- aware orchestration services are supported by
               the Network Function Virtualization (NFV) architecture, making
               it a promising approach to manage and place ML tasks. In this
               paper, we propose Machine Learning Function Virtualization
               (MLFV), a fully network-aware framework that explores the NFV
               environment to virtualize ML tasks as virtual network functions.
               We describe a novel model for placing chains of ML tasks,
               considering constraints on CPU, memory, the existence of ML
               libraries, and the network overload, aiming to reduce the
               overall execution time of a chain. The results showed that MLFV
               outperformed existing cloud and edge approaches, particularly
               when network connections present instabilities. MLFV was able to
               identify the irregularities, allocating the ML tasks on hosts
               with normal connections, and thus, reducing the time for
               classifying single and multiple concurrent requests.",
  pages     = "1--6",
  month     =  dec,
  year      =  2019,
  keywords  = "Machine learning;Libraries;Task analysis;Computer
               architecture;Cloud computing;Proposals;Network function
               virtualization",
  issn      = "2576-6813",
  doi       = "10.1109/GLOBECOM38437.2019.9013295"
}

@INPROCEEDINGS{Malik2018-ru,
  title     = "Towards Coreless Wireless Mobile Networks",
  booktitle = "2018 {IEEE} Globecom Workshops ({GC} Wkshps)",
  author    = "Malik, Ahsan and Xiao, Xun and Khalili, Ramin and Hecker, Artur",
  abstract  = "Combining two recent trends in wireless mobile systems, the
               small cells and the mobile edge computing, in this paper, we
               first identify new technical requirements for the implementation
               of the future generation of the mobile systems and then present
               architecture, modules and protocols complementing the current
               3GPP Release 15 Reference Model to enable self-organized,
               wireless network of the future, in principle able to operate
               without any topological core network and seamlessly supporting
               hybrid mode operations.",
  pages     = "1--6",
  month     =  dec,
  year      =  2018,
  keywords  = "3GPP;Noise measurement;Wireless communication;Peer-to-peer
               computing;Computer architecture;5G mobile communication;Routing",
  doi       = "10.1109/GLOCOMW.2018.8644192"
}

@INPROCEEDINGS{Waterman2013-qm,
  title     = "The {RISC-V} instruction set",
  booktitle = "2013 {IEEE} Hot Chips 25 Symposium ({HCS})",
  author    = "Waterman, Andrew and Lee, Yunsup and Avizienis, Rimas and Cook,
               Henry and Patterson, David and Asanovic, Krste",
  abstract  = "Presents a collection of slides covering the following topics:
               RISC-V Instruction Set.",
  pages     = "1--1",
  month     =  aug,
  year      =  2013,
  keywords  = "Microarchitecture;Instruction sets;Pipelines;GDS",
  doi       = "10.1109/HOTCHIPS.2013.7478332"
}

@INPROCEEDINGS{Hazelwood2018-nn,
  title     = "Applied Machine Learning at Facebook: A Datacenter
               Infrastructure Perspective",
  booktitle = "2018 {IEEE} International Symposium on High Performance Computer
               Architecture ({HPCA})",
  author    = "Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala,
               Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy,
               Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and
               Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter
               and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong",
  abstract  = "Machine learning sits at the core of many essential products and
               services at Facebook. This paper describes the hardware and
               software infrastructure that supports machine learning at global
               scale. Facebook's machine learning workloads are extremely
               diverse: services require many different types of models in
               practice. This diversity has implications at all layers in the
               system stack. In addition, a sizable fraction of all data stored
               at Facebook flows through machine learning pipelines, presenting
               significant challenges in delivering data to high-performance
               distributed training flows. Computational requirements are also
               intense, leveraging both GPU and CPU platforms for training and
               abundant CPU capacity for real-time inference. Addressing these
               and other emerging challenges continues to require diverse
               efforts that span machine learning algorithms, software, and
               hardware design.",
  pages     = "620--629",
  month     =  feb,
  year      =  2018,
  keywords  = "Facebook;Training;Machine learning algorithms;Neural
               networks;Feeds;Support vector machines;Servers;computer
               architecture;hardware software codesign;machine
               learning;facebook",
  issn      = "2378-203X",
  doi       = "10.1109/HPCA.2018.00059"
}

@INPROCEEDINGS{Wu2019-hb,
  title     = "Machine Learning at Facebook: Understanding Inference at the
               Edge",
  booktitle = "2019 {IEEE} International Symposium on High Performance Computer
               Architecture ({HPCA})",
  author    = "Wu, Carole-Jean and Brooks, David and Chen, Kevin and Chen,
               Douglas and Choudhury, Sy and Dukhan, Marat and Hazelwood, Kim
               and Isaac, Eldad and Jia, Yangqing and Jia, Bill and Leyvand,
               Tommer and Lu, Hao and Lu, Yang and Qiao, Lin and Reagen,
               Brandon and Spisak, Joe and Sun, Fei and Tulloch, Andrew and
               Vajda, Peter and Wang, Xiaodong and Wang, Yanghan and Wasti,
               Bram and Wu, Yiming and Xian, Ran and Yoo, Sungjoo and Zhang,
               Peizhao",
  abstract  = "At Facebook, machine learning provides a wide range of
               capabilities that drive many aspects of user experience
               including ranking posts, content understanding, object detection
               and tracking for augmented and virtual reality, speech and text
               translations. While machine learning models are currently
               trained on customized data-center infrastructure, Facebook is
               working to bring machine learning inference to the edge. By
               doing so, user experience is improved with reduced latency
               (inference time) and becomes less dependent on network
               connectivity. Furthermore, this also enables many more
               applications of deep learning with important features only made
               available at the edge. This paper takes a data-driven approach
               to present the opportunities and design challenges faced by
               Facebook in order to enable machine learning inference locally
               on smart phones and other edge platforms.",
  pages     = "331--344",
  month     =  feb,
  year      =  2019,
  keywords  = "Facebook;Smart phones;Performance evaluation;Graphics processing
               units;Optimization;Hardware;Machine learning;Machine
               learning;Edge Inference;ATOS;MLAspects",
  issn      = "2378-203X",
  doi       = "10.1109/HPCA.2019.00048"
}

@INPROCEEDINGS{Du2017-sm,
  title     = "Cider: a Rapid Docker Container Deployment System through
               Sharing Network Storage",
  booktitle = "2017 {IEEE} 19th International Conference on High Performance
               Computing and Communications; {IEEE} 15th International
               Conference on Smart City; {IEEE} 3rd International Conference on
               Data Science and Systems ({HPCC/SmartCity/DSS})",
  author    = "Du, Lian and Wo, Tianyu and Yang, Renyu and Hu, Chunming",
  abstract  = "Container technology has been prevalent and widely-adopted in
               production environment considering the huge benefits to
               application packing, deploying and management. However, the
               deployment process is relatively slow by using conventional
               approaches. In large-scale concurrent deployments, resource
               contentions on the central image repository would aggravate such
               situation. In fact, it is observable that the image pulling
               operation is mainly responsible for the degraded performance. To
               this end, we propose Cider - a novel deployment system to enable
               rapid container deployment in a high concurrent and scalable
               manner at scale. Firstly, on-demand image data loading is
               proposed by altering the local Docker storage of worker nodes
               into all-nodes-sharing network storage. Also, the local
               copy-on-write layer for containers can ensure Cider to achieve
               the scalability whilst improving the cost-effectiveness during
               the holistic deployment. Experimental results reveal that Cider
               can shorten the overall deployment time by 85\% and 62\% on
               average when deploying one container and 100 concurrent
               containers respectively.",
  pages     = "332--339",
  month     =  dec,
  year      =  2017,
  file      = "All Papers/D/Du et al. 2017 - Cider - a Rapid Docker Container Deployment System through Sharing Network Storage.pdf",
  keywords  = "Containers;Metadata;Cloning;Cows;Scalability;File
               systems;Computer architecture;container; network storage;
               copy-on-write; application deployment",
  doi       = "10.1109/HPCC-SmartCity-DSS.2017.44"
}

@INPROCEEDINGS{Shen2014-dl,
  title     = "{Cost-Effective} Virtual Machine Image Replication Management
               for Cloud Data Centers",
  booktitle = "2014 {IEEE} Intl Conf on High Performance Computing and
               Communications, 2014 {IEEE} 6th Intl Symp on Cyberspace Safety
               and Security, 2014 {IEEE} 11th Intl Conf on Embedded Software
               and Syst ({HPCC,CSS,ICESS})",
  author    = "Shen, Dian and Dong, Fang and Zhang, Junxue and Luo, Junzhou",
  abstract  = "Cloud computing offers infrastructure as a service to deliver
               large amount of computation and storage resources, in which fast
               provisioning of virtual machine(VM) instances has significant
               impacts on the overall system performance and elasticity. In
               this paper, we analyze the characteristics of image provisioning
               by studying the traces collected from the real-world cloud data
               centre. From the analysis results, we observe that the
               overloaded and dynamic requests for some popular images result
               in degradation and fluctuation of performance and availability
               of the system. Addressing this issue, we propose a stochastic
               model based on queueing theory, which captures the main factors
               in image provisioning to optimize the number and placement of
               image replication, so as to manage the VM images in a
               cost-effective manner. We implement our theoretical model based
               on open-source cloud platform and carry out trace driven
               evaluation to validate its effectiveness. The evaluation results
               show that our system is cost-effective and can achieve high and
               stable performance in VM provisioning while remaining high
               availability under different test scenarios.",
  pages     = "229--236",
  month     =  aug,
  year      =  2014,
  keywords  = "Availability;Queueing analysis;Optimization;Image storage;Time
               factors;Data models;Educational institutions",
  doi       = "10.1109/HPCC.2014.41"
}

@INPROCEEDINGS{Samsi2021-hq,
  title     = "The {MIT} Supercloud Dataset",
  booktitle = "2021 {IEEE} High Performance Extreme Computing Conference
               ({HPEC})",
  author    = "Samsi, Siddharth and Weiss, Matthew L and Bestor, David and Li,
               Baolin and Jones, Michael and Reuther, Albert and Edelman,
               Daniel and Arcand, William and Byun, Chansup and Holodnack, John
               and Hubbell, Matthew and Kepner, Jeremy and Klein, Anna and
               McDonald, Joseph and Michaleas, Adam and Michaleas, Peter and
               Milechin, Lauren and Mullen, Julia and Yee, Charles and Price,
               Benjamin and Prout, Andrew and Rosa, Antonio and Vanterpool,
               Allan and McEvoy, Lindsey and Cheng, Anson and Tiwari, Devesh
               and Gadepally, Vijay",
  abstract  = "Artificial intelligence (AI) and Machine learning (ML) workloads
               are an increasingly larger share of the compute workloads in
               traditional High-Performance Computing (HPC) centers and
               commercial cloud systems. This has led to changes in deployment
               approaches of HPC clusters and the commercial cloud, as well as
               a new focus on approaches to optimized resource usage,
               allocations and deployment of new AI frameworks, and
               capabilities such as Jupyter notebooks to enable rapid
               prototyping and deployment. With these changes, there is a need
               to better understand cluster/datacenter operations with the goal
               of developing improved scheduling policies, identifying
               inefficiencies in resource utilization, energy/power
               consumption, failure prediction, and identifying policy
               violations. In this paper we introduce the MIT Supercloud
               Dataset which aims to foster innovative AI/ML approaches to the
               analysis of large scale HPC and datacenter/cloud operations. We
               provide detailed monitoring logs from the MIT Supercloud system,
               which include CPU and GPU usage by jobs, memory usage, file
               system logs, and physical monitoring data. This paper discusses
               the details of the dataset, collection methodology, data
               availability, and discusses potential challenge problems being
               developed using this data. Datasets and future challenge
               announcements will be available via https://dcc.mit.edu.",
  pages     = "1--8",
  month     =  sep,
  year      =  2021,
  file      = "All Papers/S/Samsi et al. 2021 - The MIT Supercloud Dataset.pdf",
  keywords  = "File systems;Conferences;Graphics processing units;Machine
               learning;Rapid prototyping;Resource management;Monitoring",
  issn      = "2643-1971",
  doi       = "10.1109/HPEC49654.2021.9622850"
}

@INPROCEEDINGS{Kalyanasundaram2017-ow,
  title     = "{ARM} Wrestling with Big Data: A Study of Commodity {ARM64}
               Server for Big Data Workloads",
  booktitle = "2017 {IEEE} 24th International Conference on High Performance
               Computing ({HiPC})",
  author    = "Kalyanasundaram, Jayanth and Simmhan, Yogesh",
  abstract  = "ARM processors have dominated the mobile device market in the
               last decade due to their favorable computing to energy ratio. In
               this age of Cloud data centers and Big Data analytics, the focus
               is increasingly on power efficient processing, rather than just
               high throughput computing. ARM's first commodity server-grade
               processor is the recent AMD A1100-series processor, based on a
               64-bit ARM Cortex A57 architecture. In this paper, we study the
               performance and energy efficiency of a server based on this
               ARM64 CPU, relative to a comparable server running an AMD
               Opteron 3300-series x64 CPU, for Big Data workloads.
               Specifically, we study these for Intel's HiBench suite of web,
               query and machine learning benchmarks on Apache Hadoop v2.7 in a
               pseudo-distributed setup, for data sizes up to 20GB files, 5M
               web pages and 500M tuples. Our results show that the ARM64
               server's runtime performance is comparable to the x64 server for
               integer-based workloads like Sort and Hive queries, and only
               lags behind for floating-point intensive benchmarks like
               PageRank, when they do not exploit data parallelism adequately.
               We also see that the ARM64 server takes 1/3rd the energy, and
               has an Energy Delay Product (EDP) that is 50-71\% lower than the
               x64 server. These results hold promise for ARM64 data centers
               hosting Big Data workloads to reduce their operational costs,
               while opening up opportunities for further analysis.",
  pages     = "203--212",
  month     =  dec,
  year      =  2017,
  keywords  = "Servers;Program processors;Big Data;Benchmark testing;Computer
               architecture;Cloud computing;Data centers;Big data
               benchmark;Energy efficient computing;Cloud computing;Map
               Reduce;ARM processor",
  doi       = "10.1109/HiPC.2017.00032"
}

@INPROCEEDINGS{Huang-Fu2021-ni,
  title     = "Comparing the performance of machine learning and deep learning
               algorithms classifying messages in Facebook learning group",
  booktitle = "2021 International Conference on Advanced Learning Technologies
               ({ICALT})",
  author    = "Huang-Fu, Cheng-Yo and Liao, Chen-Hsuan and Wu, Jiun-Yu",
  abstract  = "The use of computer-mediated communication (CMC) has been
               ubiquitous in higher education. To better understand students'
               behaviors and facilitate students' learning through CMC, this
               study aimed to classify messages in Facebook learning group
               which was created as an on-line discussion board. Different
               machine learning and deep learning classification models were
               proposed, trained and testified with corpuses from PTT, one of
               the famous on-line forums in Taiwan. Furthermore, the
               classification of Facebook messages by these well-trained models
               were compared with human coding. Results revealed that recurrent
               neural network (RNN) with word to vector (W2V) for feature
               extraction demonstrated the best performance in accuracy. In
               addition, the combination of RNN and TF-IDF was proved to have
               the highest correlation with human work. Implications for
               artificial intelligence (AI) in education context was discussed.",
  pages     = "347--349",
  month     =  jul,
  year      =  2021,
  keywords  = "Deep learning;Recurrent neural networks;Machine learning
               algorithms;Correlation;Computational modeling;Education;Feature
               extraction;learning analytics;big data;machine learning;deep
               learning;feature extraction",
  issn      = "2161-377X",
  doi       = "10.1109/ICALT52272.2021.00111"
}

@INPROCEEDINGS{Wette2014-qa,
  title     = "Using application layer knowledge in Routing and Wavelength
               Assignment algorithms",
  booktitle = "2014 {IEEE} International Conference on Communications ({ICC})",
  author    = "Wette, Philip and Karl, Holger",
  abstract  = "Preemptive Routing and Wavelength Assignment (RWA) algorithms
               preempt established lightpaths in case not enough resources are
               available to set up a new lightpath in a Wavelength Division
               Multiplexing (WDM) network. The selection of lightpaths to be
               preempted relies on internal decisions of the RWA algorithm.
               Thus, if dedicated properties of the network topology are
               required by the applications running on the network, these
               requirements have to be known to the RWA algorithm. We present a
               family of preemptive RWA algorithms for WDM networks. These
               algorithms have two distinguishing features: a) they can handle
               dynamic traffic by on-the-fly reconfiguration, and b) users can
               give feedback for reconfiguration decisions and thus influence
               the preemption decision of the RWA algorithm, leading to
               networks which adapt directly to application needs. This is
               different from traffic engineering where the network is (slowly)
               adapted to observed traffic patterns. Our algorithms handle
               various WDM network configurations including networks consisting
               of heterogeneous WDM hardware. To this end, we are using the
               layered graph approach together with a newly developed graph
               model that is used to determine conflicting lightpaths.",
  pages     = "3270--3276",
  month     =  jun,
  year      =  2014,
  file      = "All Papers/W/Wette and Karl 2014 - Using application layer knowledge in Routing and Wavelength Assignment algorithms.pdf",
  keywords  = "Heuristic algorithms;Topology;Network topology;Optical
               wavelength conversion;WDM networks;Greedy algorithms;Routing",
  issn      = "1938-1883",
  doi       = "10.1109/ICC.2014.6883825"
}

@INPROCEEDINGS{Taleb2015-qc,
  title     = "User mobility-aware Virtual Network Function placement for
               Virtual {5G} Network Infrastructure",
  booktitle = "2015 {IEEE} International Conference on Communications ({ICC})",
  author    = "Taleb, T and Bagaa, M and Ksentini, A",
  abstract  = "Cloud offerings represent a promising solution for mobile
               network operators to cope with the surging mobile traffic. The
               concept of carrier cloud has therefore emerged as an important
               topic of inquiry. For a successful carrier cloud, algorithms for
               optimal placement of Virtual Network Functions (VNFs) on
               federated cloud are of crucial importance. In this paper, we
               introduce different VNF placement algorithms for carrier cloud
               with two main design goals: i) minimizing path between users and
               their respective data anchor gateways and ii) optimizing their
               sessions' mobility. The two design goals effectively represent
               two conflicting objectives, that we deal with considering the
               mobility features and service usage behavioral patterns of
               mobile users, in addition to the mobile operators' cost in terms
               of the total number of instantiated VNFs to build a Virtual
               Network Infrastructure (VNI). Different solutions are evaluated
               based on different metrics and encouraging results are obtained.",
  pages     = "3879--3884",
  month     =  jun,
  year      =  2015,
  file      = "All Papers/T/Taleb et al. 2015 - User mobility-aware Virtual Network Function placement for Virtual 5G Network Infrastructure.pdf",
  keywords  = "5G mobile communication;cloud computing;mobility management
               (mobile radio);virtualisation;cloud offerings;mobile
               network;mobile traffic;carrier cloud;virtual network
               functions;VNF;federated cloud;data anchor gateways;sessions
               mobility;mobility features;service usage behavioral
               patterns;mobile users;mobile operators cost;VNI;virtual 5G
               network infrastructure;Mobile communication;Mobile
               computing;Delays;Logic gates;Optimization;Games;Wireless
               communication;ToRead;Important;NFV",
  issn      = "1938-1883",
  doi       = "10.1109/ICC.2015.7248929"
}

@INPROCEEDINGS{Tsilimantos2016-cc,
  title     = "Anticipatory radio resource management for mobile video
               streaming with linear programming",
  booktitle = "2016 {IEEE} International Conference on Communications ({ICC})",
  author    = "Tsilimantos, D and Nogales-Gomez, A and Valentin, S",
  abstract  = "In anticipatory networking, channel prediction is used to
               improve communication performance. This paper describes a new
               approach for allocating resources to video streaming traffic
               while accounting for quality of service. The proposed method is
               based on integrating a model of the user's local play-out buffer
               into the radio access network. The linearity of this model
               allows to formulate a Linear Programming problem that optimizes
               the trade-off between the allocated resources and the stalling
               time of the media stream. Our simulation results demonstrate the
               full power of anticipatory optimization in a simple, yet
               representative, scenario. Compared to instantaneous adaptation,
               our anticipatory solution shows impressive gains in spectral
               efficiency and stalling duration at feasible computation time
               while being robust against prediction errors.",
  pages     = "1--6",
  month     =  may,
  year      =  2016,
  file      = "All Papers/T/Tsilimantos et al. 2016 - Anticipatory radio resource management for mobile video streaming with linear programming.pdf",
  keywords  = "linear programming;mobile communication;video
               streaming;anticipatory radio resource management;mobile video
               streaming;channel prediction;local play-out buffer;access
               network;linear programming problem;Streaming media;Resource
               management;Optimization;Mobile communication;Linear
               programming;Wireless communication;Quality of service;Wireless",
  issn      = "1938-1883",
  doi       = "10.1109/ICC.2016.7511099"
}

@INPROCEEDINGS{Muhammad2020-wa,
  title     = "{Delay-Aware} {Multi-Source} Multicast Resource optimization in
               {NFV-Enabled} Network",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Muhammad, A and Qu, L and Assi, C",
  abstract  = "Network Function Virtualization (NFV) is a transformation of
               traditional proprietary network designs to a more agile and
               software based environment. NFV architecture is considered as a
               key enabler for 5G as it offers the flexible deployment, reduced
               setup costs and less-time-to-market for the new services.
               Current studies on NFV in unicast transmission case can not be
               extended to multicast. Owing to the recent popularity and
               growing interest for live video streaming applications,
               efficient multicast solutions in NFV-enabled networks are
               needed. In this paper, we propose an NFV multicast resource
               optimization model as a Mixed Integer Linear Program (MILP)
               exploiting the use of multiple sources and considering the
               end-to-end delay and bandwidth requirements along with two
               heuristics algorithms. We evaluate the performance of the
               proposed algorithms on different network topologies. Simulation
               results prove that the proposed algorithms outperform the
               existing solution in terms of reduced bandwidth consumption and
               the delay values.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/M/Muhammad et al. 2020 - Delay-Aware Multi-Source Multicast Resource optimization in NFV-Enabled Network.pdf",
  keywords  = "5G mobile communication;bandwidth allocation;computer
               networks;integer programming;linear programming;multicast
               communication;resource
               allocation;synchronisation;telecommunication
               computing;telecommunication network topology;video
               streaming;virtualisation;heuristics algorithms;bandwidth
               consumption;network topologies;MILP;mixed integer linear
               program;5G mobile communication;proprietary network
               designs;bandwidth requirements;end-to-end delay;NFV multicast
               resource optimization model;live video streaming
               applications;NFV architecture;software based environment;Network
               Function Virtualization;NFV-enabled network;delay-aware
               multisource multicast resource
               optimization;Delays;Bandwidth;Optimization;Unicast;Routing;Hardware;Servers;NFV;delay;multicast;multi-source;resource
               optimization;NFV",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148590"
}

@INPROCEEDINGS{Carpio2020-bd,
  title     = "Engineering and Experimentally Benchmarking a Container-based
               Edge Computing System",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Carpio, Francisco and Delgado, Marta and Jukan, Admela",
  abstract  = "While edge computing is envisioned to superbly serve latency
               sensitive applications, the implementation-based studies
               benchmarking its performance are few and far between. To address
               this gap, we engineer a modular edge cloud computing system
               architecture that is built on latest advances in
               containerization techniques, including Kafka, for data
               streaming, Docker, as application platform, and Firebase Cloud,
               as realtime database system. We benchmark the performance of the
               system in terms of scalability, resource utilization and latency
               by comparing three scenarios: cloud-only, edge-only and combined
               edge-cloud. The measurements show that edge-only solution
               outperforms other scenarios only when deployed with data located
               at one edge only, i.e., without edge computing wide data
               synchronization. In case of applications requiring data
               synchronization through the cloud, edge-cloud scales around a
               factor 10 times better than cloudonly, until certain number of
               concurrent users in the system, and above this point, cloud-only
               scales better. In terms of resource utilization, we observe that
               whereas the mean utilization increases linearly with the number
               of user requests, the maximum values for the memory and the
               network I/O heavily increase when with an increasing amount of
               data.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/C/Carpio et al. 2020 - Engineering and Experimentally Benchmarking a Container-based Edge Computing System.pdf",
  keywords  = "Cloud computing;Edge computing;Benchmark
               testing;Containers;Scalability;Database systems;edge
               computing;cloud;IoT;networking",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148636"
}

@INPROCEEDINGS{Sun2020-bv,
  title     = "{DeepMigration}: Flow Migration for {NFV} with Graph-based Deep
               Reinforcement Learning",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Sun, Penghao and Lan, Julong and Guo, Zehua and Zhang, Di and
               Chen, Xianfu and Hu, Yuxiang and Liu, Zhi",
  abstract  = "Network Function Virtualization (NFV) enables flexible
               deployment of network services as applications. Network
               operators expect to use a limited number of Network Function
               (NF) instances to handle the fluctuating traffic load and
               provide network services. However, it is a big challenge to
               guarantee the Quality of Service (QoS) under the unpredictable
               network traffic while minimizing the processing resources. One
               typical solution is to realize NF scale-out, scale-in and load
               balancing by elastically migrating the related traffic flows
               with Software-Defined Networking (SDN). However, it is difficult
               to optimally migrate flows since many real-time statuses of NF
               instances should be considered to make accurate decisions. In
               this paper, we propose DeepMigration to solve the problem by
               efficiently and dynamically migrating traffic flows among
               different NF instances. DeepMigration is a Deep Reinforcement
               Learning (DRL)-based solution coupled with Graph Neural Network
               (GNN). By taking advantages of the graph-based relationship
               deduction ability from our customized GNN and the self-evolution
               ability from the experience training of DRL, DeepMigration can
               accurately model the cost (e.g., migration latency) and the
               benefit (e.g., reducing the number of NF instances) of flow
               migration among different NF instances and generate dynamic and
               effective flow migration policies to improve the QoS. Experiment
               results show that DeepMigration requires less migration cost and
               saves up to 71.6\% of the computation time than existing
               solutions.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Noise measurement;Quality of service;Training;Machine
               learning;Load management;Artificial neural networks;Network
               Function Virtualization;Flow Migration;Deep Reinforcement
               Learning;Graph Neural Network;SDN",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148696"
}

@INPROCEEDINGS{Huang2020-op,
  title     = "Reinforcement Learning Based Offloading for Realtime
               Applications in Mobile Edge Computing",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Huang, Hui and Ye, Qiang and Du, Hongwei",
  abstract  = "Energy consumption is one of the most important issues for
               mobile devices such as smartphones and laptops. For mobile
               devices that execute multiple computation-intensive or
               delay-sensitive applications simultaneously, Mobile Edge
               Computing (MEC) based offloading provides a promising solution
               to the energy problem. However, blindly offloading all tasks to
               MEC servers is not the best choice because transferring a simple
               task to a MEC server via wireless networks might consume more
               energy than processing the task locally. In addition, Dynamic
               Voltage and Frequency Scaling (DVFS) could be utilized to reduce
               the energy consumption associated with locally processed tasks
               by appropriately lowering CPU frequency. In this paper, we
               propose a realtime reinforcement learning based offloading
               scheme, RRLO, which is based on both MEC-based offloading and
               DVFS-based energy consumption reduction. Technically, RRLO
               jointly learns the optimal offloading policy and DVFS-based
               scheduling method. Depending on the workload and network
               condition, RRLO not only determines whether a task should be
               offloaded to a MEC server, but also selects the best DVFS method
               used to schedule local tasks. Our simulation results indicate
               that RRLO outperforms the existing MEC-based offloading schemes.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Task analysis;Energy consumption;Mobile
               handsets;Servers;Learning (artificial
               intelligence);Computational modeling;Edge
               computing;Offloading;Mobile Edge Computing;Reinforcement
               Learning;Realtime Applications;EdgeFogCloudIoT",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148748"
}

@INPROCEEDINGS{Ashok_Krishnan2020-yg,
  title     = "Minimizing Age of Information in a Multihop Wireless Network",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Ashok Krishnan, K S and Sharma, Vinod",
  abstract  = "We consider the problem of minimizing age in a multihop wireless
               network. There are multiple source-destination pairs,
               transmitting data through multiple wireless channels, over
               multiple hops. We propose a network control policy which
               consists of a distributed scheduling algorithm, utilizing
               channel state information and queue lengths at each link, in
               combination with a packet dropping rule. Dropping of older
               packets locally at queues is seen to reduce the average age of
               flows, even below what can be achieved by Last Come First Served
               (LCFS) scheduling. Dropping of older packets also allows us to
               use the network without congestion, irrespective of the rate at
               which updates are generated. Furthermore, exploiting system
               state information substantially improves performance. The
               proposed scheduling policy obtains average age values close to a
               theoretical lower bound as well.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/A/Ashok Krishnan and Sharma 2020 - Minimizing Age of Information in a Multihop Wireless Network.pdf",
  keywords  = "Spread spectrum communication;Delays;Schedules;Wireless
               networks;Scheduling;Optimization;Interference;age of
               information;scheduling;multihop networks;Wireless",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148762"
}

@INPROCEEDINGS{Gao2020-qi,
  title     = "Green Offloading in {Fog-Assisted} {IoT} Systems: An Online
               Perspective Integrating Learning and Control",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Gao, Xin and Huang, Xi and Shao, Ziyu and Yang, Yang",
  abstract  = "In fog-assisted IoT systems, it is a common practice to offload
               tasks from IoT devices to their nearby fog nodes to reduce task
               processing latencies and energy consumptions. However, the
               design of online energy-efficient scheme is still an open
               problem because of various uncertainties in system dynamics such
               as processing capacities and transmission rates. Moreover, the
               decision-making process is constrained by resource limits on fog
               nodes and IoT devices, making the design even more complicated.
               In this paper, we formulate such a task offloading problem with
               unknown system dynamics as a combinatorial multi-armed bandit
               (CMAB) problem with long-term constraints on time-average energy
               consumptions. Through an effective integration of online
               learning and online control, we propose a Learning-Aided Green
               Offloading (LAGO) scheme. In LAGO, we employ bandit learning
               methods to handle the exploitation-exploration tradeoff and
               utilize virtual queue techniques to deal with the long-term
               constraints. Our theoretical analysis shows that LAGO can reduce
               the average task latency with an O(1/V + $\surd$(log T)/T)
               regret bound over time horizon T and satisfy the long-term
               time-average energy constraints, where V is a tunable positive
               parameter. We conduct extensive simulations to verify such
               theoretical results.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/G/Gao et al. 2020 - Green Offloading in Fog-Assisted IoT Systems - An Online Perspective Integrating Learning and Control.pdf",
  keywords  = "Task analysis;Energy consumption;Wireless
               communication;Uncertainty;System dynamics;Optimization;Decision
               making;EdgeFogCloudIoT",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148800"
}

@INPROCEEDINGS{Chen2020-ns,
  title     = "Convergence Time Minimization of Federated Learning over
               Wireless Networks",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Chen, Mingzhe and Poor, H Vincent and Saad, Walid and Cui,
               Shuguang",
  abstract  = "In this paper, the convergence time of federated learning (FL),
               when deployed over a realistic wireless network, is studied. In
               particular, with the considered model, wireless users transmit
               their local FL models (trained using their locally collected
               data) to a base station (BS). The BS, acting as a central
               controller, generates a global FL model using the received local
               FL models and broadcasts it back to all users. Due to the
               limited number of resource blocks (RBs) in a wireless network,
               only a subset of users can be selected and transmit their local
               FL model parameters to the BS at each learning step. Meanwhile,
               since each user has unique training data samples and the BS must
               wait to receive all users' local FL models to generate the
               global FL model, the FL performance and convergence time will be
               significantly affected by the user selection scheme. In
               consequence, it is necessary to design an appropriate user
               selection scheme that enables all users to execute an FL scheme
               and efficiently train it. This joint learning, wireless resource
               allocation, and user selection problem is formulated as an
               optimization problem whose goal is to minimize the FL
               convergence time while optimizing the FL performance. To address
               this problem, a probabilistic user selection scheme is proposed
               using which the BS will connect to the users, whose local FL
               models have large effects on its global FL model, with high
               probabilities. Given the user selection policy, the uplink RB
               allocation can be determined. To further reduce the FL
               convergence time, artificial neural networks (ANNs) are used to
               estimate the local FL models of the users that are not allocated
               any RBs for local FL model transmission, which enables the BS to
               include more users' local FL models to generate the global FL
               model so as to improve the FL convergence speed and performance.
               Simulation results show that the proposed ANN-based FL scheme
               can reduce the FL convergence time by up to 53.8\%, compared to
               a standard FL algorithm.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Data models;Convergence;Wireless networks;Training;Training
               data;Resource management;MLNetworking",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148815"
}

@INPROCEEDINGS{Zirwas2020-xc,
  title     = "Profiling of mobile Radio Channels",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Zirwas, Wolfgang and Sternad, Mikael",
  abstract  = "One of the essential challenges for MIMO or massive MIMO and
               cooperative multipoint systems is obtaining accurate channel
               state information (CSI) at the base station side, as the related
               closed loop MIMO precoders are sensitive to channel aging
               effects. For that reason, channel prediction is often seen as
               one of the main enablers to maintain a large part of the
               performance gains achievable with ideal CSI. It has been claimed
               that Kalman filtering provides an upper bound for channel
               prediction performance. In line with this, for real world radio
               channels alternative channel prediction methods based on
               parameter estimation achieved comparably worse prediction
               results. Here, we propose a so called profiling solution as a
               novel parameter estimation method, which promises to improve the
               prediction horizon by a factor of two to three compared to
               Kalman filtering based on autoregressive models. This is
               indicated by first evaluations based on a real world radio
               channel measurement in the NOKIA campus in Munich.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  keywords  = "Channel estimation;Transfer functions;MIMO communication;Kalman
               filters;Parameter estimation;Doppler effect;Frequency-domain
               analysis;Wireless",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148839"
}

@INPROCEEDINGS{Yan2020-cc,
  title     = "Deep Reinforcement Learning Based Offloading for Mobile Edge
               Computing with General Task Graph",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Yan, Jia and Bi, Suzhi and Huang, Liang and Zhang, Ying-Jun
               Angela",
  abstract  = "In this paper, we consider a mobile-edge computing (MEC) system,
               where an access point (AP) assists a mobile device (MD) to
               execute an application consisting of multiple tasks following a
               general task call graph. The objective is to jointly determine
               the offloading decision of each task and the resource allocation
               (e.g., CPU computing power) under time-varying wireless fading
               channels and stochastic edge computing capability, so that the
               energy-time cost (ETC) of the MD is minimized. Solving the
               problem is particularly hard due to the combinatorial offloading
               decisions and the strong coupling among task executions under
               the general dependency model. To address the issue, we propose a
               deep reinforcement learning (DRL) framework based on the
               actor-critic learning structure. In particular, the actor
               network utilizes a deep neural network (DNN) to learn the
               optimal mapping from the input states (i.e., wireless channel
               gains and edge CPU frequency) to the binary offloading decision
               of each task. Meanwhile, for the critic network, we show that
               given the offloading decision, the remaining resource allocation
               problem becomes convex, where we can quickly evaluate the ETC
               performance of the offloading decisions output by the actor
               network. Accordingly, we select the best offloading action and
               store the state-action pair in an experience replay memory as
               the training dataset to continuously improve the action
               generation DNN. Numerical results show that for various types of
               task graphs, the proposed algorithm achieves up to 99.5\% of the
               optimal performance while significantly reducing the
               computational complexity compared to the existing optimization
               methods.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  keywords  = "Task analysis;Servers;Wireless communication;Edge
               computing;Downlink;Wireless sensor networks;Computational
               modeling;MLNetworking;NFV",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148846"
}

@INPROCEEDINGS{Liu2020-tf,
  title     = "{Client-Edge-Cloud} Hierarchical Federated Learning",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Liu, Lumin and Zhang, Jun and Song, S H and Letaief, Khaled B",
  abstract  = "Federated Learning is a collaborative machine learning framework
               to train a deep learning model without accessing clients'
               private data. Previous works assume one central parameter server
               either at the cloud or at the edge. The cloud server can access
               more data but with excessive communication overhead and long
               latency, while the edge server enjoys more efficient
               communications with the clients. To combine their advantages, we
               propose a client-edge-cloud hierarchical Federated Learning
               system, supported with a HierFAVG algorithm that allows multiple
               edge servers to perform partial model aggregation. In this way,
               the model can be trained faster and better
               communication-computation trade-offs can be achieved.
               Convergence analysis is provided for HierFAVG and the effects of
               key parameters are also investigated, which lead to qualitative
               design guidelines. Empirical experiments verify the analysis and
               demonstrate the benefits of this hierarchical architecture in
               different data distribution scenarios. Particularly, it is shown
               that by introducing the intermediate edge servers, the model
               training time and the energy consumption of the end devices can
               be simultaneously reduced compared to cloud-based Federated
               Learning.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/L/Liu et al. 2020 - Client-Edge-Cloud Hierarchical Federated Learning.pdf",
  keywords  = "Servers;Training;Data models;Convergence;Machine learning;Cloud
               computing;Computational modeling;Mobile Edge Computing;Federated
               Learning;Edge Learning;MLNetworking",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148862"
}

@INPROCEEDINGS{Wang2020-yv,
  title     = "Learning Centric Power Allocation for Edge Intelligence",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Wang, Shuai and Wang, Rui and Hao, Qi and Wu, Yik-Chung and
               Poor, H Vincent",
  abstract  = "While machine-type communication (MTC) devices generate massive
               data, they often cannot process this data due to limited energy
               and computation power. To this end, edge intelligence has been
               proposed, which collects distributed data and performs machine
               learning at the edge. However, this paradigm needs to maximize
               the learning performance instead of the communication
               throughput, for which the celebrated water-filling and max-min
               fairness algorithms become inefficient since they allocate
               resources merely according to the quality of wireless channels.
               This paper proposes a learning centric power allocation (LCPA)
               method, which allocates radio resources based on an empirical
               classification error model. To get insights into LCPA, an
               asymptotic optimal solution is derived. The solution shows that
               the transmit powers are inversely proportional to the channel
               gain, and scale exponentially with the learning parameters.
               Experimental results show that the proposed LCPA algorithm
               significantly outperforms other power allocation algorithms.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/W/Wang et al. 2020 - Learning Centric Power Allocation for Edge Intelligence.pdf",
  keywords  = "Support vector machines;Training;Resource management;Machine
               learning;Data models;Performance evaluation;Wireless
               communication;Classification error model;edge
               intelligence;learning centric communication;multiple-input
               multiple-output;MLNetworking;EdgeFogCloudIoT",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148872"
}

@INPROCEEDINGS{Wang2020-vn,
  title     = "{RackNFV}: A {NFV/SFC} System Operating in a Rack",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Wang, Shie-Yuan and Tsai, Ruei-Sheng",
  abstract  = "In this paper, we design, implement, and evaluate the
               performance of RackNFV - a high-performance,
               bandwidth-efficient, and reconfiguration-agile Network Function
               Virtualization (NFV)/Service Function Chain (SFC) system that
               operates in a rack. In RackNFV, multiple servers on which
               Virtual Network Functions (VNFs) are executed and a hardware
               switch that connects these servers are co-located in a rack. The
               switch is dynamically partitioned into multiple slice switches
               that can be flexibly inter-connected to form any desired network
               topology. The VNF servers can be dynamically connected to the
               slice switches to form any desired SFC for packet flows. Packets
               passing through a RackNFV system can be steered through any
               desired SFC by traversing these slice switches and VNF servers.
               RackNFV uses the switching capacity of a hardware switch to
               achieve high-performance and bandwidth-efficient packet
               forwarding along a chain of VNF servers. By using a novel
               port-mapping technique, RackNFV can quickly conFigure a new SFC
               in just 400 milliseconds. Operating in a rack, RackNFV greatly
               saves the bandwidth in a production network and makes the
               deployment and maintenance of NFV/SFC faster and easier.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Switches;Servers;Bandwidth;Wires;Network
               topology;Hardware;NFV;SFC;SDN;NFV",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148921"
}

@INPROCEEDINGS{Eisen2020-ex,
  title     = "Scheduling Low Latency Traffic for Wireless Control Systems in
               {5G} Networks",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Eisen, Mark and Rashid, Mohammad M and Ribeiro, Alejandro and
               Cavalcanti, Dave",
  abstract  = "We consider the problem of allocating 5G radio resources over
               wireless communication links to control a series of independent
               low-latency wireless control systems common in industrial
               settings. Each control system sends state information to the
               base station to compute control signals under tight latency
               requirements. Such latency requirements can be met by
               restricting the uplink traffic to a single subframe in each 5G
               frame, thus ensuring a millisecond latency bound while leaving
               the remaining subframes available for scheduling overhead and
               coexisting broadband traffic. A linear assignment problem can be
               formulated to minimize the expected number of packet drops, but
               this alone is not sufficient to achieve good performance. We
               propose an optimal scheduling with respect to a control
               operation cost that allocates resources based on current control
               system needs. The resulting control-aware scheduling method is
               tested in simulation experiments that show drastically improved
               performance in 5G settings relative to control-agnostic
               scheduling under the proposed time-sliced frame structure.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/E/Eisen et al. 2020 - Scheduling Low Latency Traffic for Wireless Control Systems in 5G Networks.pdf",
  keywords  = "Optimal scheduling;Control systems;Wireless communication;5G
               mobile communication;Job shop scheduling;Wireless sensor
               networks;Broadband communication;wireless control;low
               latency;5G;URLLC",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9148943"
}

@INPROCEEDINGS{Yan2020-gs,
  title     = "Optimizing Mobile Edge Computing {Multi-Level} Task Offloading
               via Deep Reinforcement Learning",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Yan, Peizhi and Choudhury, Salimur",
  abstract  = "In a mobile edge computing (MEC) network, mobile devices could
               selectively offload tasks to the edge server(s) to save time and
               energy. However, we should consider many dynamic factors in task
               offloading optimization, which increases the complexity of this
               problem. Instead of executing the traditional optimization
               algorithm repeatedly, a well-trained empirical model such as an
               artificial neural network could be more efficient in decision
               making. In this research, considering the potential uneven
               spatial distribution of mobile devices in an MEC network with
               multiple wireless edge gateways, we allow an edge gateway to
               offload tasks to a nearby edge gateway further. We propose a
               deep reinforcement learning-based joint optimization approach
               for both device-level and edge-level task offloading.
               Experimental results show that the proposed approach achieves a
               near-optimal task delay performance and a better trade-off
               between the task delay and the energy consumption on tasks.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  keywords  = "Task analysis;Logic gates;Optimization;Wireless
               communication;Servers;Energy consumption;Delays;mobile edge
               computing;task offloading;Q-learning;deep reinforcement
               learning;NFV",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149024"
}

@INPROCEEDINGS{Cao2020-xw,
  title     = "{Energy-Efficient} Mobile Edge Computing in {NOMA-Based}
               Wireless Networks: A Game Theory Approach",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Cao, Xueyan and Liu, Chenxi and Peng, Mugen",
  abstract  = "In this paper, we examine the potential benefits of
               non-orthogonal multiple access (NOMA) in achieving
               energy-efficient mobile edge computing (MEC) in wireless
               networks. To this end, we consider an uplink communication
               system where the edge users (EUEs) adopt NOMA protocol to
               offload their own tasks to the edge access points in the
               presence of cellular users (CUEs) performing regular uplink
               transmissions. We first characterize the energy consumption of
               our considered system. Then, taking the delay constraints of the
               CUEs and EUEs into consideration, we show how the energy
               consumption of the system can be optimized by judiciously
               determining the task offloading allocation, the subchannel
               allocation, as well as the power allocation. In order to solve
               the non-convex problem, an iterative Stackelberg-game-based
               scheme is proposed, in which the EUEs perform the task and power
               allocation as leaders, while the CUEs perform the subchannel
               allocation as followers. Numerical results show that, compared
               to exiting solutions, our proposed NOMA-based scheme can
               significantly reduce the energy consumption of the system, and
               the performance improvement becomes more profound when the delay
               constraints of the CUEs and EUEs become stringent.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Task analysis;NOMA;Delays;Resource management;Energy
               consumption;Manganese;Uplink;Wireless;EdgeFogCloudIoT",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149056"
}

@INPROCEEDINGS{Antevski2020-gx,
  title     = "A Q-learning strategy for federation of {5G} services",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Antevski, Kiril and Mart{\'\i}n-P{\'e}rez, Jorge and
               Garcia-Saavedra, Andres and Bernardos, Carlos J and Li, Xi and
               Baranda, Jorge and Mangues-Bafalluy, Josep and Martnez, Ricardo
               and Vettori, Luca",
  abstract  = "5G networks aim to provide orchestration of services across
               multiple administrative domains through the concept of
               federation. In this paper, we are exploring the federation
               feature of a platform for 5G transport network of vertical
               services. Then we formulate the decision problem that directly
               impacts the revenue of 5G administrative domains, and we propose
               as solution a Q-learning algorithm. The simulation results show
               near optimum profit maximization and a well-trained Q-learning
               algorithm can outperform the intuitive ``greedy'' approach in a
               realistic scenario.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/A/Antevski et al. 2020 - A Q-learning strategy for federation of 5G services.pdf",
  keywords  = "Computer architecture;Industries;5G mobile
               communication;Heuristic algorithms;Optimization;Machine
               learning;Vehicle
               dynamics;multi-domain;federation;NFV;algorithms;machine-learning;5G6G",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149082"
}

@INPROCEEDINGS{Lee2020-hn,
  title     = "Wireless Link Scheduling for {D2D} Communications with Graph
               Embedding Technique",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Lee, Mengyuan and Yu, Guanding and Li, Geoffrey Ye",
  abstract  = "Link scheduling for device-to-device (D2D) communications is
               usually formulated as an NP-hard non-convex combinatorial
               problem, which is difficult to get the optimal solution.
               Traditional methods are mainly based on mathematical
               optimization techniques with the help of accurate channel state
               information (CSI), which is costly to obtain. In this paper, we
               propose a graph embedding based method to achieve link
               scheduling without CSI for D2D communications. We first
               construct a fully-connected directed graph for the D2D network,
               and then compute a low-dimensional feature vector for each node
               in the graph based on the distances of both communication and
               interference links. Finally, a scheduling strategy can be
               learned based on the graph embedding results by utilizing a
               multi-layer classifier. Extensive simulation demonstrates that
               the proposed method is near-optimal compared with the existing
               state-of-art methods and only needs hundreds of training network
               layouts. It is also competitive in terms of scalability and
               generalizability to more complicated scenarios.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Device-to-device communication;Wireless
               communication;Interference;Training;Scheduling;Layout;Computer
               architecture;Machine learning;device-to-device
               communications;graph embedding;link scheduling;combinatorial
               optimization",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149089"
}

@INPROCEEDINGS{Shi2020-hi,
  title     = "Device Scheduling with Fast Convergence for Wireless Federated
               Learning",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Shi, Wenqi and Zhou, Sheng and Niu, Zhisheng",
  abstract  = "Owing to the increasing need for massive data analysis and model
               training at the network edge, as well as the rising concerns
               about the data privacy, a new distributed training framework
               called federated learning (FL) has emerged. In each iteration of
               FL (called round), the edge devices update local models based on
               their own data and contribute to the global training by
               uploading the model updates via wireless channels. Due to the
               limited spectrum resources, only a portion of the devices can be
               scheduled in each round. While most of the existing work on
               scheduling focuses on the convergence of FL w.r.t. rounds, the
               convergence performance under a total training time budget is
               not yet explored. In this paper, a joint bandwidth allocation
               and scheduling problem is formulated to capture the long-term
               convergence performance of FL, and is solved by being decoupled
               into two sub-problems. For the bandwidth allocation sub-problem,
               the derived optimal solution suggests to allocate more bandwidth
               to the devices with worse channel conditions or weaker
               computation capabilities. For the device scheduling sub-problem,
               by revealing the trade-off between the number of rounds required
               to attain a certain model accuracy and the latency per round, a
               greedy policy is inspired, that continuously selects the device
               that consumes the least time in model updating until achieving a
               good trade-off between the learning efficiency and latency per
               round. The experiments show that the proposed policy outperforms
               other state-of-the-art scheduling policies, with the best
               achievable model accuracy under training time budgets.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/S/Shi et al. 2020 - Device Scheduling with Fast Convergence for Wireless Federated Learning.pdf",
  keywords  = "Data models;Training;Convergence;Computational modeling;Channel
               allocation;Bandwidth;Wireless communication;MLNetworking",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149138"
}

@INPROCEEDINGS{Saeidian2020-ge,
  title     = "Downlink Power Control in Dense {5G} Radio Access Networks
               Through Deep Reinforcement Learning",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Saeidian, Sara and Tayamon, Soma and Ghadimi, Euhanna",
  abstract  = "During the past decades, a myriad of inter/intracell
               interference mitigation techniques has been suggested for
               different wireless technologies. Nevertheless, the concept of
               downlink power control for interference mitigation has yet to be
               explored in 5G radio access networks. In this paper, we propose
               a data-driven approach based on deep reinforcement learning for
               downlink power control in dense 5G networks. The solution builds
               upon the well-known DQN algorithm and its recent extensions,
               aiming to maximize user rates. Using a 5Gcompliant system-level
               simulator, we compare the performance of our proposed method to
               fixed power allocation approaches. Test results show that the
               proposed method is successful at improving data rates at the
               cell-edge while reducing total transmitted power compared to the
               baseline.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Downlink;5G mobile communication;Interference;Power
               control;Resource management;Machine learning;OFDM;Downlink power
               control;interference mitigation;deep reinforcement
               learning;radio resource management.;Wireless",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149157"
}

@INPROCEEDINGS{Bhattacharjee2020-sc,
  title     = "{Time-Sensitive} Networking for {5G} Fronthaul Networks",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Bhattacharjee, Sushmit and Schmidt, Robert and Katsalis, Kostas
               and Chang, Chia-Yu and Bauschert, Thomas and Nikaein, Navid",
  abstract  = "In 5G radio access networks, meeting the performance
               requirements of the fronthaul network is quite challenging.
               Recent standardization and research activities are focusing on
               exploiting the IEEE Time Sensitive Networking (TSN) technology
               for fronthaul networks. In this work we evaluate the performance
               of Ethernet TSN networks based on IEEE 802.1Qbv and IEEE
               802.1Qbu for carrying real fronthaul traffic and benchmark it
               against Ethernet with Strict priority and Round Robin
               scheduling. We demonstrate that both 802.1Qbv and 802.1Qbu can
               be well used to protect high-priority traffic flows even in
               overload conditions.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  keywords  = "Delays;3GPP;5G mobile communication;Synchronization;Jitter;Logic
               gates;5G;fronthaul networks;IEEE TSN;mobile network;5G6G",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149161"
}

@INPROCEEDINGS{Pham2020-rc,
  title     = "Learning Framework for {IoT} Services Chain Implementation in
               Edge Cloud Platform",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Pham, Chuan and Nguyen, Duong Tuan and Tran, Nguyen H and
               Nguyen, Kim Khoa and Cheriet, Mohamed",
  abstract  = "As an emerging solution to latency requirements of Internet of
               Things (IoT) services, edge computing can bring powerful
               processing capacity closer to data sources. However, with the
               limited resources at edge nodes, a major challenge is finding
               optimal resources in distributed edges to reduce the operational
               costs of service deployment. Prior works focus mainly on static
               optimization which may not work efficiently with the
               time-varying workloads and resource constraints. In this paper,
               we, therefore, consider a dynamic allocation framework in the
               edge-cloud network over the long run with uncertainty workloads.
               In such a system, we introduce a JOint Routing and Placement
               problem for IoT services, called JORP, that dynamically assigns
               resources according to workload demand in order to reduce the
               operational costs in long term. Inspired from the well-known
               algorithm, branch-and-bound (BnB), for solving the mixed-integer
               non linear problems (MINLPs) like JORP, we bring the learning
               concept to address the high complexity of BnB when the search
               space is huge. Particularly, we design a deep neural network
               (DNN) and train it under the imitation learning to mimic
               branching behaviors in BnB for searching the optimal solution.
               Finally, simulations show our solution outperforms baselines in
               terms of convergence and operational cost.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Resource management;Routing;Cloud computing;Logic gates;Task
               analysis;Delays;Edge computing;Internet of Things;Edge
               Computing;Cloud;Resource Allocation;Branch-and-Bound;Deep Neural
               Network;NFV",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149170"
}

@INPROCEEDINGS{Zhang2020-rd,
  title     = "{Learning-Based} Computation Offloading for Edge Networks with
               Heterogeneous Resources",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Zhang, Liqiang and Luo, Jingjing and Gao, Lin and Zheng, Fu-Chun",
  abstract  = "Mobile edge computing (MEC) has shown its potential in serving
               computation intensive tasks via offloading. However, the
               heterogeneity of MEC systems and the dynamic nature of wireless
               environment pose a great challenge to the design of offloading
               policies. In this paper, we investigate this computation
               offloading problem, where the heterogeneities of computational
               resource, channel state, task type and input data size are
               considered. We first propose a greedy algorithm, in which each
               arrival task is greedily offloaded to the edge server with
               minimal utility, based on a global information of network
               states. While this greedy algorithm performs well in terms of
               system utility, the overhead incurred to collect the global
               information is large, especially in dense MEC scenarios and
               time-varying channel scenarios. Inspired by this observation, we
               then propose a model-free offloading algorithm based on
               reinforcement learning, which does not rely on such kind of
               information and can make offloading decisions based on learning
               experience. By so doing, the communication overhead can be
               largely reduced. Extensive simulations show that the two
               proposed algorithms have similar performance in terms of system
               utility and can decrease the system utility by up to 50\%
               compared with two widely used algorithms. The robustness of the
               two proposed algorithms is further verified.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Task analysis;Servers;Computational modeling;Heuristic
               algorithms;Wireless communication;Edge computing;Learning
               (artificial intelligence);Mobile edge computing;heterogeneous
               networks;task offloading;reinforcement learning;MLNetworking;NFV",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149171"
}

@INPROCEEDINGS{Sliwa2020-oc,
  title     = "{LIMITS}: Lightweight Machine Learning for {IoT} Systems with
               Resource Limitations",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Sliwa, Benjamin and Piatkowski, Nico and Wietfeld, Christian",
  abstract  = "Exploiting big data knowledge on small devices will pave the way
               for building truly cognitive Internet of Things (IoT) systems.
               Although machine learning has led to great advancements for
               IoT-based data analytics, there remains a huge methodological
               gap for the deployment phase of trained machine learning models.
               For given resource-constrained platforms such as Microcontroller
               Units (MCUs), model choice and parametrization are typically
               performed based on heuristics or analytical models. However,
               these approaches are only able to provide rough estimates of the
               required system resources as they do not consider the interplay
               of hardware, compilerspecific optimizations, and code
               dependencies. In this paper, we present the novel open source
               framework LIghtweight Machine learning for IoT Systems (LIMITS),
               which applies a platform-in-the-loop approach explicitly
               considering the actual compilation toolchain of the target IoT
               platform. LIMITS focuses on high-level tasks such as experiment
               automation, platform-specific code generation, and sweet spot
               determination. The solid foundations of validated low-level
               model implementations are provided by the coupled
               well-established data analysis framework Waikato Environment for
               Knowledge Analysis (WEKA). We apply and validate LIMITS in two
               case studies focusing on cellular data rate prediction and
               radio-based vehicle classification, where we compare different
               learning models and real world IoT platforms with memory
               constraints from 16 kB to 4 MB and demonstrate its potential to
               catalyze the development of machine learning-enabled IoT
               systems.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/S/Sliwa et al. 2020 - LIMITS - Lightweight Machine Learning for IoT Systems with Resource Limitations.pdf",
  keywords  = "Machine learning;Analytical models;Data models;Data
               analysis;Task analysis;Vegetation;Training;EdgeFogCloudIoT",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149180"
}

@INPROCEEDINGS{Wu2020-ru,
  title     = "{Mobility-Aware} Deep Reinforcement Learning with Glimpse
               Mobility Prediction in Edge Computing",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Wu, Chao-Lun and Chiu, Te-Chuan and Wang, Chih-Yu and Pang,
               Ai-Chun",
  abstract  = "Mobile/multi-access edge computing (MEC) is therefore developed
               to support the upcoming AI-aware mobile services, which require
               low latency and intensive computation resources at the edge of
               the network. One of the most challenging issues in MEC is
               service provision with mobility consideration. It has been known
               that the offloading and migration decision need to be jointly
               handled to maximize the utility of networks within the latency
               constraints, which is challenging when users are in mobility. In
               this paper, we propose Mobility-Aware Deep Reinforcement
               Learning (M-DRL) framework for mobile service provision problems
               in the MEC system. M-DRL is composed of two parts: DRL
               specialized in supporting multiple users joint training, and
               glimpse, a seq2seq model customized for mobility prediction to
               predict a sequence of locations just like a ``glimpse'' of
               future. Through integrating the proposed DRL and glimpse
               mobility prediction model, the proposed M-DRL framework is
               optimized to handle the service provision problem in MEC with
               acceptable computation complexity and near-optimal performance.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/W/Wu et al. 2020 - Mobility-Aware Deep Reinforcement Learning with Glimpse Mobility Prediction in Edge Computing.pdf",
  keywords  = "Servers;Predictive models;Computational modeling;Machine
               learning;Markov processes;Prediction algorithms;Task
               analysis;EdgeFogCloudIoT",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149185"
}

@INPROCEEDINGS{Mahmoudi2020-kl,
  title     = "Cost-efficient Distributed optimization In Machine Learning Over
               Wireless Networks",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Mahmoudi, Afsaneh and Ghadikolaei, Hossein S and Fischione,
               Carlo",
  abstract  = "This paper addresses the problem of distributed training of a
               machine learning model over the nodes of a wireless
               communication network. Existing distributed training methods are
               not explicitly designed for these networks, which usually have
               physical limitations on bandwidth, delay, or computation, thus
               hindering or even blocking the training tasks. To address such a
               problem, we consider a general class of algorithms where the
               training is performed by iterative distributed computations
               across the nodes. We assume that the nodes have some background
               traffic and communicate using the slotted-ALOHA protocol. We
               propose an iteration-termination criterion to investigate the
               trade-off between achievable training performance and the
               overall cost of running the algorithms. We show that, given a
               total running budget, the training performance becomes worse as
               either the background communication traffic or the dimension of
               the training problem increases. We conclude that a co-design of
               distributed optimization algorithms and communication protocols
               is essential for the success of machine learning over wireless
               networks and edge computing.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/M/Mahmoudi et al. 2020 - Cost-efficient Distributed optimization In Machine Learning Over Wireless Networks.pdf",
  keywords  = "Optimization;Training;Machine learning;Computational
               modeling;Protocols;Machine learning
               algorithms;Convergence;Distributed optimization;efficient
               algorithm;latency;convergence;machine learning;MLNetworking",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149216"
}

@INPROCEEDINGS{Wong2020-wr,
  title     = "Bricklayer: Resource Composition on the Spot Market",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Wong, Walter and Corneo, Lorenzo and Zavodovski, Aleksandr and
               Zhou, Pengyuan and Mohan, Nitinder and Kangasharju, Jussi",
  abstract  = "AWS offers discounted transient virtual instances as a way to
               sell unused resources in their data-centers, and users can enjoy
               up to 90\% discount as compared to the regular on-demand
               pricing. Despite the economic incentives to purchase these
               transient instances, they do not come with regular availability
               SLAs, meaning that they can be evicted at any moment. Hence, the
               user is responsible for managing the instance availability to
               meet the application requirements. In this paper, we present
               Bricklayer, a software tool that assists users to better use
               transient resources in the cloud, reducing costs for the same
               amount of resources, and increasing the overall instance
               availability. Bricklayer searches for possible combinations of
               smaller and cheaper instances to compose the requested amount of
               resources while deploying them into different spot markets to
               reduce the risk of eviction. We implemented and evaluated
               Bricklayer using 3 months of historical data from AWS and found
               out that it can reduce up 54\% of the regular spot price and up
               to 95\% compared to the standard on-demand pricing.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  keywords  = "Cloud computing;Pricing;Measurement;Transient
               analysis;Servers;Electronic mail;Graphics processing units;Cloud
               computing;spot instances;availability",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149218"
}

@INPROCEEDINGS{Ramadan2020-oi,
  title     = "Traffic Forecasting using Temporal Line Graph Convolutional
               Network: Case Study",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Ramadan, Abdelrahman and Elbery, Ahmed and Zorba, Nizar and
               Hassanein, Hossam S",
  abstract  = "Traffic forecasting is imperative to Intelligent Transportation
               Systems (ITS), and it has always been considered as a
               challenging research topic, due to the complex topological
               structure of the urban road network and the temporal stochastic
               nature of dynamic change. Popular sports events attract vast
               numbers of spectators travelling to the event, which will have a
               substantial effect on ITS, showing peaks on the network that can
               collapse a smart city's ITS. In this paper, we tackle traffic
               forecasting and use the Doha network in Qatar and the FIFA World
               Cup 2022 (FWC 2022) event as a case study. We propose a novel
               technique for embedding road network graphs into a
               Temporal-Graph Convolutional Network. The embedding process
               includes a modification to the graph weights based on graph
               theory and the properties of the line graph. Extensive
               simulations are carried out on a real-world calibrated dataset
               from Doha's road network. Our Temporal Line Graph Convolutional
               Network (TLGCN) proposal shows outstanding performance when
               compared to state-of-the-art techniques, not only for huge
               special events but also for the regular daily traffic.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Roads;Forecasting;Convolution;Logic gates;Predictive
               models;Recurrent neural networks;Vehicle dynamics;Traffic
               Forecasting;Line Graphs;Temporal Line Graph Convolutional
               Network;TLGCN;T-GCN;Spatiotemporal Dependence;ActualTraffic",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149233"
}

@INPROCEEDINGS{Chen2020-kv,
  title     = "{Data-Driven} Optimization for Resource Provision in
               {Non-Cooperative} Edge Computing Market",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Chen, Rui and Li, Liang and Hou, Ronghui and Yang, Tingting and
               Wang, Li and Pan, Miao",
  abstract  = "The advance of edge computing pushes computing functionalities
               to the network edge and brings lucrative opportunities for edge
               operators (EOs) to cater the users with low latency requirement.
               Unlike in cloud computing, edge servers have limited computing
               capacity and require a proper resource planning. To avoid loss
               of potential profit, a promising way is to outsource cloud
               resources from a public cloud with additional cost when the edge
               computing capacity is insufficient to meet the real-time
               demands. Besides, the uncertainty of future demands also affects
               EOs' profits. It's essential to consider the interaction among
               market participants with different risk attitudes. To this end,
               we study multiple risk-averse EOs with one risk-neutral Cloud
               Provider (CP) in an edge computing market, where each EO
               competes to serve the users by determining the optimal resource
               provision strategies given the demand and the outsource price
               charged by the CP, and the CP sets the price based on the best
               responses of the EOs. We model the interaction between EOs and
               CP as a two stage Stackelberg game, and employ a data-driven
               optimization approach to characterize the uncertainty. We
               explore the existence and uniqueness of subgame Nash
               equilibrium, and find the equilibrium based on the Sample
               Average Approximation (SAA) method. Extensive simulations using
               real-world cluster data traces verify the effectiveness of the
               proposed method.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  keywords  = "Cloud computing;Edge
               computing;Uncertainty;Games;Delays;Computational
               modeling;Robustness;Edge computing;Non-cooperating
               game;Data-driven optimization;EdgeFogCloudIoT",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149382"
}

@INPROCEEDINGS{Liu2020-uw,
  title     = "Intelligent Offloading for {Multi-Access} Edge Computing: A New
               {Actor-Critic} Approach",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Liu, Kai-Hsiang and Liao, Wanjiun",
  abstract  = "Multi-access Edge Computing (MEC) is promising to handle
               computation-intensive and latency-sensitive applications for 5G
               and beyond. Users can benefit from task offloading via wireless
               channels to MEC servers deployed at the nearby network edge.
               However, the radio resource is scarce and the computing resource
               in MEC is limited as compared to the remote cloud. Upon making
               an offloading decision, it is also important to efficiently
               allocate radio resource and MEC computing resource to ensure
               better service for the upload tasks. In this paper, we target
               the long-term delay and energy consumption performance in a
               multi-user system, and design an online solution based on Deep
               Reinforcement Learning (DRL) to deal with time-varying user
               requests and wireless channel conditions. To obtain better
               convergence property, we propose a new Actor-Critic model,
               called Discrete And Continuous Actor-Critic (DAC), to jointly
               optimize the continuous actions (i.e., radio resource allocation
               and computing resource allocation) and the discrete action
               (i.e., offloading decisions), and train the model iteratively
               with a weighted loss function. Our simulation results show that
               DAC outperforms existing solutions based on DDPG, DQN, and
               others, in terms of convergence speed, delay, and energy
               performance.",
  pages     = "1--6",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/L/Liu and Liao 2020 - Intelligent Offloading for Multi-Access Edge Computing - A New Actor-Critic Approach.pdf",
  keywords  = "Task analysis;Delays;Resource management;Energy
               consumption;Computational modeling;Servers;Wireless
               communication;Multi-access Edge Computing (MEC);Deep
               Reinforcement Learning (DRL);Actor-Critic (AC);DAC;MLNetworking",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149387"
}

@INPROCEEDINGS{Abdah2020-mt,
  title     = "Handover Prediction Integrated with Service Migration in {5G}
               Systems",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Abdah, Hadeel and Barraca, Jo{\~a}o Paulo and Aguiar, Rui L",
  abstract  = "As the research community inclines toward adopting increasingly
               complex techniques for future networks, and simple methods are
               often ignored, being labeled as trivial. In this paper, we argue
               that simple methods can sometimes outperform more sophisticated
               ones. We demonstrate that by evaluating two prediction
               mechanisms to forecast mobile user's handovers exploiting
               user-network association patterns. We perform a series of
               experiments on real-world data, evaluating the performance
               characteristics of such methods over more sophisticated and
               complex prediction techniques. Furthermore, we discuss how to
               easily bootstrap these mechanisms into the 5G network
               architecture. We suggest the use of these methods associated
               with Multi-access Edge Computing (MEC) scenarios, as a mean to
               identify favorable edge nodes to host the mobile applications,
               to best provide continuous and QoS-aware service for mobile
               users.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/A/Abdah et al. 2020 - Handover Prediction Integrated with Service Migration in 5G Systems.pdf",
  keywords  = "Handover;5G mobile communication;History;Quality of
               service;Machine learning algorithms;Prediction algorithms;5G6G",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149426"
}

@INPROCEEDINGS{Zhang2020-we,
  title     = "Robust Deep Learning for Wireless Network Optimization",
  booktitle = "{ICC} 2020 - 2020 {IEEE} International Conference on
               Communications ({ICC})",
  author    = "Zhang, Shuai and Yin, Bo and Wang, Suyang and Cheng, Yu",
  abstract  = "Wireless optimization involves repeatedly solving difficult
               optimization problems, and data-driven deep learning techniques
               have great promise to alleviate this issue through its pattern
               matching capability: past optimal solutions can be used as the
               training data in a supervised learning paradigm so that the
               neural network can generate an approximate solution using a
               fraction of the computational cost, due to its high representing
               power and parallel implementation. However, making this approach
               practical in networking scenarios requires careful,
               domain-specific consideration, currently lacking in similar
               works. In this paper, we use deep learning in a wireless network
               scheduling and routing to predict if subsets of the network
               links are going to be used, so that the effective problem scale
               is reduced. A real-world concern is the varying data importance:
               training samples are not equally important due to class
               imbalance or different label quality. To compensate for this
               fact, we develop an adaptive sample weighting scheme which
               dynamically weights the batch samples in the training process.
               In addition, we design a novel loss function that uses
               additional network-layer feature information to improve the
               solution quality. We also discuss a post-processing step that
               gives a good threshold value to balance the trade-off between
               prediction quality and problem scale reduction. By numerical
               simulations, we demonstrate that these measures improve both the
               prediction quality and scale reduction when training from data
               of varied importance.",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  keywords  = "Training;Optimization;Machine learning;Wireless networks;Loss
               measurement;Supervised learning;Neural networks;multi-hop
               wireless mesh network;deep learning;network utility
               maximization;Wireless;MLNetworking",
  issn      = "1938-1883",
  doi       = "10.1109/ICC40277.2020.9149445"
}

@INPROCEEDINGS{Sardellitti2018-sk,
  title     = "Optimal Association of Mobile Users to {Multi-Access} Edge
               Computing Resources",
  booktitle = "2018 {IEEE} International Conference on Communications Workshops
               ({ICC} Workshops)",
  author    = "Sardellitti, S and Merluzzi, M and Barbarossa, S",
  abstract  = "Multi-access edge computing (MEC) plays a key role in
               fifth-generation (5G) networks in bringing cloud functionalities
               at the edge of the radio access network, in close proximity to
               mobile users. In this paper we focus on mobile-edge computation
               offloading, a way to transfer heavy demanding, and
               latency-critical applications from mobile handsets to
               close-located MEC servers, in order to reduce latency and/or
               energy consumption. Our goal is to provide an optimal strategy
               to associate mobile users to access points (AP) and MEC hosts,
               while contextually optimizing the allocation of radio and
               computational resources to each user, with the objective of
               minimizing the overall user transmit power under latency
               constraints incorporating both communication and computation
               times. The overall problem is a mixed-binary problem. To
               overcome its inherent computational complexity, we propose two
               alternative strategies: i) a method based on successive convex
               approximation (SCA) techniques, proven to converge to local
               optimal solutions; ii) an approach hinging on matching theory,
               based on formulating the assignment problem as a matching game.",
  pages     = "1--6",
  month     =  may,
  year      =  2018,
  file      = "All Papers/S/Sardellitti et al. 2018 - Optimal Association of Mobile Users to Multi-Access Edge Computing Resources.pdf",
  keywords  = "computational complexity;convex programming;game theory;mobile
               computing;mobile handsets;resource allocation;mobile
               users;multiaccess edge computing resources;radio access
               network;mobile-edge computation offloading;mobile handsets;MEC
               servers;access points;Servers;Optimization;Resource
               management;5G mobile communication;Energy consumption;Radio
               access networks;Linear programming;EdgeFogCloudIoT",
  issn      = "2474-9133",
  doi       = "10.1109/ICCW.2018.8403594"
}

@INPROCEEDINGS{Moser1994-lq,
  title     = "Extended virtual synchrony",
  booktitle = "14th International Conference on Distributed Computing Systems",
  author    = "Moser, L E and Amir, Y and Melliar-Smith, P M and Agarwal, D A",
  abstract  = "We formulate a model of extended virtual synchrony that defines
               a group communication transport service for multicast and
               broadcast communication in a distributed system. The model
               extends the virtual synchrony model of the Isis system to
               support continued operation in all components of a partitioned
               network. The significance of extended virtual synchrony is that,
               during network partitioning and remerging and during process
               failure and recovery, it maintains a consistent relationship
               between the delivery of messages and the delivery of
               configuration changes across all processes in the system and
               provides well-defined self-delivery and failure atomicity
               properties. We describe an algorithm that implements extended
               virtual synchrony and construct a filter that reduces extended
               virtual synchrony to virtual synchrony.>",
  pages     = "56--65",
  month     =  jun,
  year      =  1994,
  keywords  = "Broadcasting;Multicast protocols;Intersymbol
               interference;Multicast algorithms;Partitioning algorithms;Local
               area networks;Hardware;Electronic switching systems;Computer
               science;Bridges",
  doi       = "10.1109/ICDCS.1994.302392"
}

@INPROCEEDINGS{Zhang2017-rr,
  title     = "Joint Optimization of Chain Placement and Request Scheduling for
               Network Function Virtualization",
  booktitle = "2017 {IEEE} 37th International Conference on Distributed
               Computing Systems ({ICDCS})",
  author    = "Zhang, Q and Xiao, Y and Liu, F and Lui, J C S and Guo, J and
               Wang, T",
  abstract  = "Compared with executing Network Functions (NFs) on dedicated
               hardwares, the recent trend of Network Function Virtualization
               (NFV) holds the promise for operators to flexibly deploy
               software-based NFs on commodity servers. However, virtual NFs
               (VNFs) are normally ``chained'' together to provide a specific
               network service. Thus, an efficient scheme is needed to place
               the VNF chains across the network and effectively schedule
               requests to service instances, which can maximize the average
               resource utilization of each node in service and simultaneously
               minimize the average response latency of each request. To this
               end, we formulate first VNF chains placement problem as a
               variant of bin-packing problem, which is NP-hard, and we model
               request scheduling problem based on the key concepts from open
               Jackson network. To jointly optimize the performance of NFV, we
               propose a priority-driven weighted algorithm to improve resource
               utilization and a heuristic algorithm to reduce response
               latency. Through extensive trace-driven simulations, we show
               that our methods can indeed enhance performance in diverse
               scenarios. In particular, we can improve the average resource
               utilization by 33.4\% and can reduce the average total latency
               by 19.9\% as compared with the state-of-the-art methods.",
  pages     = "731--741",
  month     =  jun,
  year      =  2017,
  file      = "All Papers/Z/Zhang et al. 2017 - Joint Optimization of Chain Placement and Request Scheduling for Network Function Virtualization.pdf",
  keywords  = "bin packing;computational complexity;computer
               centres;optimisation;queueing theory;resource
               allocation;scheduling;software defined
               networking;virtualisation;joint optimization;chain
               placement;request scheduling;network function
               virtualization;NFV;commodity servers;virtual NFs;VNFs;resource
               utilization;bin-packing problem;NP-hard;open Jackson
               network;Resource management;Computational
               modeling;Schedules;Heuristic algorithms;Mathematical
               model;Processor scheduling;Packet loss;Network Function
               Virtualization (NFV);Chain Placement;Request Scheduling;Response
               Latency;Open Jackson Network;ToRead;NFV",
  issn      = "1063-6927",
  doi       = "10.1109/ICDCS.2017.232"
}

@INPROCEEDINGS{Zhang2018-in,
  title     = "{Q-Placement}: {Reinforcement-Learning-Based} Service Placement
               in {Software-Defined} Networks",
  booktitle = "2018 {IEEE} 38th International Conference on Distributed
               Computing Systems ({ICDCS})",
  author    = "Zhang, Z and Ma, L and Leung, K K and Tassiulas, L and Tucker, J",
  abstract  = "In software-defined networking (SDN) paradigm, where the control
               and data plane are separated, the scalability of the SDN
               controller in the control plane is critical and can affect the
               overall network performance significantly. To improve controller
               scalability, efforts have been put into enhancing the capability
               of SDN switches in the data plane, to make them more autonomous
               in providing routine services without consulting the controller.
               In this regard, we investigate the service placement problem on
               SDN switches aiming at minimizing the average accumulated
               service costs for end users. To solve this problem, we propose a
               novel reinforcement-learning-based algorithm with guaranteed
               performance and convergence rate, called Q-placement. Comparing
               to traditional optimization techniques, Q-placement exhibits
               many appealing features, such as performance-tuneable
               optimization and off-the-shelf implementation. Extensive
               evaluations show that Q-placement consistently outperforms
               benchmarks and other state-of-the-art algorithms in both
               synthetic and real networks. Moreover, these evaluations reveal
               insights into how the network topological properties (e.g.,
               density), servicing capacities, and controller's roles affect
               the accumulated service costs, which is useful in service
               planning tasks.",
  pages     = "1527--1532",
  month     =  jul,
  year      =  2018,
  file      = "All Papers/Z/Zhang et al. 2018 - Q-Placement - Reinforcement-Learning-Based Service Placement in Software-Defined Networks.pdf",
  keywords  = "learning (artificial intelligence);optimisation;software defined
               networking;telecommunication network topology;optimization
               techniques;Q-placement;convergence
               rate;reinforcement-learning-based algorithm;average accumulated
               service costs;service placement problem;controller
               scalability;network performance;control plane;SDN
               controller;data plane;software-defined networking
               paradigm;reinforcement-learning-based service placement;service
               planning tasks;network topological properties;real
               networks;synthetic networks;performance-tuneable
               optimization;Control systems;Network
               topology;Scalability;Optimization;Electronic mail;Task
               analysis;Decision making;SDN;reinforcement learning;service
               placement;q learning;NFV",
  issn      = "2575-8411",
  doi       = "10.1109/ICDCS.2018.00159"
}

@INPROCEEDINGS{Li2020-hx,
  title     = "A Community Platform for Research on Pricing and Distributed
               Machine Learning",
  booktitle = "2020 {IEEE} 40th International Conference on Distributed
               Computing Systems ({ICDCS})",
  author    = "Li, Xuanzhe and Gomena, Samuel and Ballard, Logan and Li, Juntao
               and Aryafar, Ehsan and Joe-Wong, Carlee",
  abstract  = "Data generated by increasingly pervasive and intelligent devices
               has led to an explosion in the use of machine learning (ML) and
               artificial intelligence, with ever more complex models trained
               to support applications in fields as diverse as healthcare,
               finance, and robotics. In order to train these models in a
               reasonable amount of time, the training is often distributed
               among multiple machines. However, paying for these machines
               (either by constructing a local cloud infrastructure or renting
               machines through an external provider such as Amazon AWS) is
               very costly. We propose to reduce these costs by creating a
               marketplace of computing resources designed to support
               distributed machine learning algorithms. Through our marketplace
               (coined ``DeepMarket''), users can lend their spare computing
               resources (when not needed) or augment their resources with
               available DeepMarket machines to train their ML models. Such a
               marketplace directly provides several benefits for two groups of
               researchers: (i) ML researchers would be able to train their
               models with much reduced cost, and (ii) network economics
               researchers would be able to experiment with different compute
               pricing mechanisms. The focus of this Demo is to introduce the
               audience to DeepMarket and its user interface (named ``PLUTO'').
               In particular, we will bring a few laptops with pre-installed
               PLUTO applications so that users can see how they can create an
               account on DeepMarket servers, lend their resource, borrow
               available resources, submit ML jobs, and retrieve the results.
               Our overall goal is to encourage the conference audience to
               install PLUTO on their own machines and create a user and
               developer community around DeepMarket.",
  pages     = "1223--1226",
  month     =  nov,
  year      =  2020,
  keywords  = "Training;Biological system modeling;Computational
               modeling;Pricing;Machine learning;User interfaces;Pluto;Machine
               Learning;Artificial Intelligence;Network
               Economics;Containerization;Pricing;Distributed Computing",
  issn      = "2575-8411",
  doi       = "10.1109/ICDCS47774.2020.00117"
}

@INPROCEEDINGS{Merizig2020-ei,
  title     = "Machine Learning Approach for Energy Consumption Prediction in
               Datacenters",
  booktitle = "2020 2nd International Conference on Mathematics and Information
               Technology ({ICMIT})",
  author    = "Merizig, Abdelhak and Bendahmane, Toufik and Merzoug, Soltane
               and Kazar, Okba",
  abstract  = "Cloud Computing represents the ideal solution for end-users
               either small medium enterprises or simple clients. This solution
               is given as a for clients to go from classic service concept to
               oriented service concept. Moreover, this paradigm collects a set
               of operations which made them a complex task to the managers.
               Since the coming of the Cloud Computing encourages service
               providers to deploy their services. These enormous services need
               some infrastructure services that are located in datacenters in
               order to execute them. Due to this use, Cloud infrastructure
               owners are concerned by the huge energy consumed during this
               execution. This problematic will affect the use of costs for the
               services providers. To tackle this problem, in this work, we
               present several models presented in machine learning methods in
               order to predict the energy to be consumed for the next use.
               These forecasts could help the infrastructure providers to
               propose a plan and some analytics to eliminate the waste of used
               resources during the execution of services. The implementation
               of this model has been provided in order to evaluate our system.
               The obtained results demonstrate the effectiveness of our
               proposed system.",
  pages     = "142--148",
  month     =  feb,
  year      =  2020,
  keywords  = "Cloud Computing;Energy Consumption;Datacenter Energy
               Prediction;Support Vector Regression;SVR;Artificial Neural
               Networks;Time Series;Machine Learning;Cloud Services",
  doi       = "10.1109/ICMIT47780.2020.9046987"
}

@INPROCEEDINGS{Wu2001-ih,
  title     = "Design and analysis of code distribution systems with active
               networks",
  booktitle = "Proceedings 15th International Conference on Information
               Networking",
  author    = "Wu, Z D and Wang, X G",
  abstract  = "An analytical queueing model is proposed based on our study of
               the code distribution and especially in the integrated or
               in-band code distribution systems with active networks. By using
               this model the system performance and reliability can be
               evaluated so as to provide insights into their behaviour. Since
               there are different proposals for the code distribution reported
               recently, it is significant to provide a technique to find an
               optimal decision in the network design. A procedure to help with
               this decision is presented. Some numerical results of using our
               performance model and decision procedure are also demonstrated.",
  pages     = "313--318",
  month     =  jan,
  year      =  2001,
  file      = "All Papers/W/Wu and Wang 2001 - Design and analysis of code distribution systems with active networks.pdf",
  keywords  = "Queueing analysis;Analytical models;System
               performance;Proposals;IP networks;Web and internet
               services;Computer networks;Information technology;Bonding;Gold",
  doi       = "10.1109/ICOIN.2001.905445"
}

@ARTICLE{Chen_undated-uw,
  title  = "How Does the Workload Look Like in Production Cloud? Analysis and
            Clustering of Workloads on Alibaba Cluster Trace",
  author = "Chen, Wenyan and Ye, Kejiang and Wang, Yang and Xu, Guoyao and Xu,
            Cheng-Zhong",
  file   = "All Papers/C/Chen et al. - How Does the Workload Look Like in Production Cloud - Analysis and Clustering of Workloads on Alibaba Cluster Trace.pdf",
  doi    = "10.1109/ICPADS.2018.00024"
}

@INPROCEEDINGS{Troia2018-fi,
  title     = "Deep {Learning-Based} Traffic Prediction for Network
               Optimization",
  booktitle = "2018 20th International Conference on Transparent Optical
               Networks ({ICTON})",
  author    = "Troia, S and Alvizu, R and Zhou, Y and Maier, G and Pattavina, A",
  abstract  = "In recent years, researchers realized that the analysis of
               traffic datasets can reveal valuable information for the
               management of mobile and metro-core networks. That is getting
               more and more true with the increase in the use of social media
               and Internet applications on mobile devices. In this work, we
               focus on deep learning methods to make prediction of traffic
               matrices that allow us to proactively optimize the resource
               allocations of optical backbone networks. Recurrent Neural
               Networks (RNNs) are designed for sequence prediction problems
               and they achieved great results in the past years in tasks like
               speech recognition, handwriting recognition and prediction of
               time series data. We investigated a particular type of RNN, the
               Gated Recurrent Units (GRU), able to achieve great accuracy (<;
               7.4 of mean absolute error). Then, we used the predictions to
               dynamically and proactively allocate the resources of an optical
               network. Comparing numerical results of static vs. dynamic
               allocation based on predictions, we can estimate a saving of
               66.3\% of the available capacity in the network, managing
               unexpected traffic peaks.",
  pages     = "1--4",
  month     =  jul,
  year      =  2018,
  file      = "All Papers/T/Troia et al. 2018 - Deep Learning-Based Traffic Prediction for Network Optimization.pdf",
  keywords  = "learning (artificial intelligence);mobile
               communication;optimisation;recurrent neural
               nets;telecommunication computing;telecommunication
               traffic;dynamic allocation;traffic prediction;network
               optimization;metro-core networks;social media;mobile
               devices;deep learning methods;traffic matrices;resource
               allocations;optical backbone networks;Recurrent Neural
               Networks;sequence prediction problems;Gated Recurrent Units;mean
               absolute error;optical network;Predictive models;Recurrent
               neural networks;Logic gates;Artificial neural networks;Resource
               management;Optimization;Machine learning;deep learning;machine
               learning;internet traffic prediction;network optimization",
  issn      = "2161-2064",
  doi       = "10.1109/ICTON.2018.8473978"
}

@INPROCEEDINGS{Rago2020-tq,
  title     = "A Softwarized Service Infrastructure for the Dynamic
               Orchestration of {IT} Resources in {5G} Deployments",
  booktitle = "2020 22nd International Conference on Transparent Optical
               Networks ({ICTON})",
  author    = "Rago, Arcangela and Piro, Giuseppe and Boggia, Gennaro and Dini,
               Paolo",
  abstract  = "Thanks to the 5G, telco operators can offer a new set of
               advanced services to mobile users which makes use of
               heterogeneous IT resources deployed at the edge of the network.
               However, their optimal management is not a simple task to
               accomplish because of the extreme variability characterizing
               offered services, traffic profile, user distributions,
               bandwidth, computing, and memory capabilities available for
               nodes hosting IT resources. To provide preliminary answers in
               this direction, this paper presents a high-level description of
               a softwarized service infrastructure, based on the ETSI-NFV
               specifications, able to dynamically orchestrate IT resources.
               Specifically, the number of users attached to the base stations
               and the capabilities of nodes hosting IT resources are
               continuously monitored through Software-Defined Networking
               facilities and reported to a high-level orchestrator. Here, a
               Convolutional Long Short-Term Memory scheme is firstly adopted
               to provide a spatio-temporal prediction of user distributions
               and related traffic demands. Then, an optimization problem is
               executed for configuring location, settings, amount, and usage
               of IT resources, based on the prediction outcomes. The behavior
               of the prediction process is deeply investigated. The
               optimization problem, instead, is described in its preliminary
               formulation, which gives a clear idea of future research
               activities in this direction.",
  pages     = "1--4",
  month     =  jul,
  year      =  2020,
  keywords  = "Optimization;Base stations;5G mobile communication;Machine
               learning;Bandwidth;Resource management;Autonomous
               vehicles;software-defined networking;ETSI-NFV;network
               optimization;deep learning",
  issn      = "2161-2064",
  doi       = "10.1109/ICTON51198.2020.9203757"
}

@INPROCEEDINGS{Venkatapuram2020-ft,
  title     = "Custom Silicon at Facebook: A Datacenter Infrastructure
               Perspective on Video Transcoding and Machine Learning",
  booktitle = "2020 {IEEE} International Electron Devices Meeting ({IEDM})",
  author    = "Venkatapuram, Prahlad and Wang, Zhao and Mallipedi, Chandra",
  abstract  = "This paper describes several important aspects of the
               Application Specific Integrated Circuits (ASIC) development
               efforts happening at Facebook to serve its hyperscale datacenter
               networks. Motivations, key features, benefits and challenges for
               custom silicon development supporting machine learning
               algorithms and video processing are introduced. Power strategies
               and reliability considerations are also addressed.",
  pages     = "9.7.1--9.7.4",
  month     =  dec,
  year      =  2020,
  keywords  = "Application specific integrated circuits;Machine learning
               algorithms;Social networking (online);Transcoding;Machine
               learning;Silicon;Integrated circuit reliability",
  issn      = "2156-017X",
  doi       = "10.1109/IEDM13553.2020.9372038"
}

@ARTICLE{noauthor_2019-vr,
  title    = "{IEEE} Standard for {Floating-Point} Arithmetic",
  abstract = "This standard specifies interchange and arithmetic formats and
              methods for binary and decimal floating-point arithmetic in
              computer programming environments. This standard specifies
              exception conditions and their default handling. An
              implementation of a floating-point system conforming to this
              standard may be realized entirely in software, entirely in
              hardware, or in any combination of software and hardware. For
              operations specified in the normative part of this standard,
              numerical results and exceptions are uniquely determined by the
              values of the input data, sequence of operations, and destination
              formats, all under user control.",
  journal  = "IEEE Std 754-2019 (Revision of IEEE 754-2008)",
  pages    = "1--84",
  month    =  jul,
  year     =  2019,
  keywords = "IEEE Standards;Floating-point
              arithmetic;arithmetic;binary;computer;decimal;exponent;floating-point;format;IEEE
              754;interchange;NaN;number;rounding;significand;subnormal.;GDS",
  doi      = "10.1109/IEEESTD.2019.8766229"
}

@INPROCEEDINGS{Adolf2016-dj,
  title     = "Fathom: reference workloads for modern deep learning methods",
  booktitle = "2016 {IEEE} International Symposium on Workload Characterization
               ({IISWC})",
  author    = "Adolf, Robert and Rama, Saketh and Reagen, Brandon and Wei,
               Gu-Yeon and Brooks, David",
  abstract  = "Deep learning has been popularized by its recent successes on
               challenging artificial intelligence problems. One of the reasons
               for its dominance is also an ongoing challenge: the need for
               immense amounts of computational power. Hardware architects have
               responded by proposing a wide array of promising ideas, but to
               date, the majority of the work has focused on specific
               algorithms in somewhat narrow application domains. While their
               specificity does not diminish these approaches, there is a clear
               need for more flexible solutions. We believe the first step is
               to examine the characteristics of cutting edge models from
               across the deep learning community. Consequently, we have
               assembled Fathom: a collection of eight archetypal deep learning
               workloads for study. Each of these models comes from a seminal
               work in the deep learning community, ranging from the familiar
               deep convolutional neural network of Krizhevsky et al., to the
               more exotic memory networks from Facebook's AI research group.
               Fathom has been released online, and this paper focuses on
               understanding the fundamental performance characteristics of
               each model. We use a set of application-level modeling tools
               built around the TensorFlow deep learning framework in order to
               analyze the behavior of the Fathom workloads. We present a
               breakdown of where time is spent, the similarities between the
               performance profiles of our models, an analysis of behavior in
               inference and training, and the effects of parallelism on
               scaling.",
  pages     = "1--10",
  month     =  sep,
  year      =  2016,
  file      = "All Papers/A/Adolf et al. 2016 - Fathom - reference workloads for modern deep learning methods.pdf",
  keywords  = "Machine learning;Hardware;Computer architecture;Computational
               modeling;Analytical models;Training;Libraries",
  doi       = "10.1109/IISWC.2016.7581275"
}

@INPROCEEDINGS{Breslau1999-oe,
  title     = "Web caching and Zipf-like distributions: evidence and
               implications",
  booktitle = "{IEEE} {INFOCOM} '99. Conference on Computer Communications.
               Proceedings. Eighteenth Annual Joint Conference of the {IEEE}
               Computer and Communications Societies. The Future is Now (Cat.
               {No.99CH36320})",
  author    = "Breslau, L and {Pei Cao} and {Li Fan} and Phillips, G and
               Shenker, S",
  abstract  = "This paper addresses two unresolved issues about Web caching.
               The first issue is whether Web requests from a fixed user
               community are distributed according to Zipf's (1929) law. The
               second issue relates to a number of studies on the
               characteristics of Web proxy traces, which have shown that the
               hit-ratios and temporal locality of the traces exhibit certain
               asymptotic properties that are uniform across the different sets
               of the traces. In particular, the question is whether these
               properties are inherent to Web accesses or whether they are
               simply an artifact of the traces. An answer to these unresolved
               issues will facilitate both Web cache resource planning and
               cache hierarchy design. We show that the answers to the two
               questions are related. We first investigate the page request
               distribution seen by Web proxy caches using traces from a
               variety of sources. We find that the distribution does not
               follow Zipf's law precisely, but instead follows a Zipf-like
               distribution with the exponent varying from trace to trace.
               Furthermore, we find that there is only (i) a weak correlation
               between the access frequency of a Web page and its size and (ii)
               a weak correlation between access frequency and its rate of
               change. We then consider a simple model where the Web accesses
               are independent and the reference probability of the documents
               follows a Zipf-like distribution. We find that the model yields
               asymptotic behaviour that are consistent with the experimental
               observations, suggesting that the various observed properties of
               hit-ratios and temporal locality are indeed inherent to Web
               accesses observed by proxies. Finally, we revisit Web cache
               replacement algorithms and show that the algorithm that is
               suggested by this simple model performs best on real trace data.
               The results indicate that while page requests do indeed reveal
               short-term correlations and other structures, a simple model for
               an independent request stream following a Zipf-like distribution
               is sufficient to capture certain asymptotic properties observed
               at Web proxies.",
  volume    =  1,
  pages     = "126--134 vol.1",
  month     =  mar,
  year      =  1999,
  file      = "All Papers/B/Breslau et al. 1999 - Web caching and Zipf-like distributions - evidence and implications.pdf",
  keywords  = "information resources;cache storage;statistical
               analysis;correlation methods;Web caching;Zipf-like
               distributions;Web requests;Zipf's law;Web proxy
               traces;hit-ratios;temporal locality;asymptotic properties;cache
               resource planning;cache hierarchy design;page request
               distribution;weak correlation;access frequency;Web page
               size;reference probability;asymptotic behaviour;experimental
               observations;cache replacement algorithms;real trace
               data;short-term correlations;Frequency;Web pages;Computer
               science;Web sites;Explosives;Internet;NetworkTraffic",
  issn      = "0743-166X",
  doi       = "10.1109/INFCOM.1999.749260"
}

@INPROCEEDINGS{Jin_Cao2004-il,
  title     = "Stochastic models for generating synthetic {HTTP} source traffic",
  booktitle = "{IEEE} {INFOCOM} 2004",
  author    = "{Jin Cao} and Cleveland, W S and {Yuan Gao} and Jeffay, K and
               Smith, F D and Weigle, M",
  abstract  = "New source-level models for aggregated HTTP traffic and a design
               for their integration with the TCP transport layer are built and
               validated using two large-scale collections of TCP/IP packet
               header traces. An implementation of the models and the design in
               the ns network simulator can be used to generate web traffic in
               network simulations",
  volume    =  3,
  pages     = "1546--1557 vol.3",
  month     =  mar,
  year      =  2004,
  file      = "All Papers/J/Jin Cao et al. 2004 - Stochastic models for generating synthetic HTTP source traffic.pdf",
  keywords  = "hypermedia;IP networks;telecommunication traffic;transport
               protocols;Web sites;stochastic models;synthetic HTTP source
               traffic;source-level models;TCP transport layer;IP packet
               header;ns network simulator;web traffic;Stochastic
               processes;Traffic control;Telecommunication traffic;Peer to peer
               computing;Statistics;Network
               servers;Hardware;Internet;Protocols;Computer
               science;NetworkTraffic",
  issn      = "0743-166X",
  doi       = "10.1109/INFCOM.2004.1354568"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Gursun2011-vh,
  title     = "Describing and forecasting video access patterns",
  booktitle = "2011 Proceedings {IEEE} {INFOCOM}",
  author    = "G{\"u}rsun, G and Crovella, M and Matta, I",
  abstract  = "Computer systems are increasingly driven by workloads that
               reflect large-scale social behavior, such as rapid changes in
               the popularity of media items like videos. Capacity planners and
               system designers must plan for rapid, massive changes in
               workloads when such social behavior is a factor. In this paper
               we make two contributions intended to assist in the design and
               provisioning of such systems.We analyze an extensive dataset
               consisting of the daily access counts of hundreds of thousands
               of YouTube videos. In this dataset, we find that there are two
               types of videos: those that show rapid changes in popularity,
               and those that are consistently popular over long time periods.
               We call these two types rarely-accessed and frequently-accessed
               videos, respectively. We observe that most of the videos in our
               data set clearly fall in one of these two types. In this work,
               we study the frequently-accessed videos by asking two questions:
               first, is there a relatively simple model that can describe its
               daily access patterns? And second, can we use this simple model
               to predict the number of accesses that a video will have in the
               near future, as a tool for capacity planning? To answer these
               questions we develop a framework for characterization and
               forecasting of access patterns. We show that for
               frequently-accessed videos, daily access patterns can be
               extracted via principal component analysis, and used efficiently
               for forecasting.",
  pages     = "16--20",
  month     =  apr,
  year      =  2011,
  file      = "All Papers/G/Gürsun et al. 2011 - Describing and forecasting video access patterns.pdf",
  keywords  = "Internet;multimedia systems;principal component analysis;video
               retrieval;video access pattern forecasting;large-scale social
               behavior;capacity planners;system designers;YouTube
               videos;rarely-accessed videos;frequently-accessed videos;daily
               access patterns;principal component analysis;Forecasting;Time
               series analysis;YouTube;Predictive models;Autoregressive
               processes;Matrix decomposition;Internet;GeneralNetworking",
  issn      = "0743-166X",
  doi       = "10.1109/INFCOM.2011.5934965"
}

@INPROCEEDINGS{Peng2012-fy,
  title     = "{VDN}: Virtual machine image distribution network for cloud data
               centers",
  booktitle = "2012 Proceedings {IEEE} {INFOCOM}",
  author    = "Peng, Chunyi and Kim, Minkyong and Zhang, Zhe and Lei, Hui",
  abstract  = "Cloud computing centers face the key challenge of provisioning
               diverse virtual machine instances in an elastic and scalable
               manner. To address this challenge, we have performed an analysis
               of VM instance traces collected at six production data centers
               during four months. One key finding is that the number of
               instances created from the same VM image is relatively small at
               a given time and thus conventional file-based p2p sharing
               approaches may not be effective. Based on the understanding that
               different VM image files often have many common chunks of data,
               we propose a chunk-level Virtual machine image Distribution
               Network (VDN). Our distribution scheme takes advantage of the
               hierarchical network topology of data centers to reduce the VM
               instance provisioning time and also to minimize the overhead of
               maintaining chunk location information. Evaluation shows that
               VDN achieves as much as 30-80$\times$ speed up for large VM
               images under heavy traffic.",
  pages     = "181--189",
  month     =  mar,
  year      =  2012,
  keywords  = "Servers;Linux;Collaboration;Network topology;Peer to peer
               computing;Virtual machining",
  issn      = "0743-166X",
  doi       = "10.1109/INFCOM.2012.6195556"
}

@INPROCEEDINGS{Waldmann2017-at,
  title     = "Traffic model for {HTTP-based} adaptive streaming",
  booktitle = "2017 {IEEE} Conference on Computer Communications Workshops
               ({INFOCOM} {WKSHPS})",
  author    = "Waldmann, S and Miller, K and Wolisz, A",
  abstract  = "The amount of video traffic on the Internet has seen a
               tremendous increase over the past few years. In 2020, it is
               predicted to account for 85\% of the total Internet consumer
               traffic. Due to this dominant role, streaming traffic has to be
               considered by workload models used to evaluate the performance
               of networking systems. A de facto standard technology for
               Internet-based Video on Demand (VoD) services is HTTP-Based
               Adaptive Streaming (HAS), which is also increasingly used for
               live streaming. Unfortunately, HAS clients produce a very
               specific workload pattern that is not appropriately represented
               by traditional HTTP traffic models. In the present work, we
               propose a stochastic model accurately describing such traffic,
               along with a methodology to generate synthetic traffic using
               functionality provided by commonly available
               numerical/scientific software libraries. We perform a proof of
               concept by fitting the model to a data set collected in a
               residential Wi-Fi environment, and generating synthetic traffic
               matching the characteristics of the traffic in the collected
               data set.",
  pages     = "683--688",
  month     =  may,
  year      =  2017,
  file      = "All Papers/W/Waldmann et al. 2017 - Traffic model for HTTP-based adaptive streaming.pdf",
  keywords  = "hypermedia;Internet;software libraries;telecommunication
               traffic;transport protocols;video on demand;video
               streaming;wireless LAN;Internet consumer traffic;commonly
               available numerical/scientific software libraries;synthetic
               traffic;stochastic model;traditional HTTP traffic
               models;specific workload pattern;live streaming;Demand
               services;facto standard technology;networking systems;workload
               models;video traffic;adaptive streaming;traffic model;Streaming
               media;Adaptation models;Computational modeling;Mathematical
               model;Throughput;Correlation;Solid modeling;Traffic
               model;traffic generator;adaptive
               streaming;MPEG-DASH;NetworkTraffic",
  doi       = "10.1109/INFCOMW.2017.8116459"
}

@INPROCEEDINGS{Fuerst2016-hj,
  title     = "Kraken: Online and elastic resource reservations for
               multi-tenant datacenters",
  booktitle = "{IEEE} {INFOCOM} 2016 - The 35th Annual {IEEE} International
               Conference on Computer Communications",
  author    = "Fuerst, C and Schmid, S and Suresh, L and Costa, P",
  abstract  = "In multi-tenant cloud environments, the absence of strict
               network performance guarantees leads to unpredictable job
               execution times. To address this issue, recently there have been
               several proposals on how to provide guaranteed network
               performance. These proposals, however, rely on computing
               resource reservation schedules a priori. Unfortunately, this is
               not practical in today's cloud environments, where application
               demands are inherently unpredictable, e.g., due to differences
               in the input datasets or phenomena such as failures and
               stragglers. To overcome these limitations, we designed KRAKEN, a
               system that allows tenants to dynamically request and update
               minimum guarantees for both network bandwidth and compute
               resources at runtime. Unlike previous work, Kraken does not
               require prior knowledge about the resource needs of the tenants'
               applications but allows tenants to modify their reservation at
               runtime. Kraken achieves this through an online resource
               reservation scheme which comes with provable optimality
               guarantees. In this paper, we motivate the need for dynamic
               resource reservation schemes, present how this is provided by
               Kraken, and evaluate Kraken via extensive simulations.",
  pages     = "1--9",
  month     =  apr,
  year      =  2016,
  file      = "All Papers/F/Fuerst et al. 2016 - Kraken - Online and elastic resource reservations for multi-tenant datacenters.pdf",
  keywords  = "cloud computing;computer centres;resource
               allocation;scheduling;Kraken system;resource reservation
               schedule;multitenant data center;multitenant cloud
               environment;Bandwidth;Copper;Runtime;Clustering
               algorithms;Switches;Substrates;Virtual machining;Datacentre",
  doi       = "10.1109/INFOCOM.2016.7524466"
}

@INPROCEEDINGS{Xu2018-ul,
  title     = "Experience-driven Networking: A Deep Reinforcement Learning
               based Approach",
  booktitle = "{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer
               Communications",
  author    = "Xu, Z and Tang, J and Meng, J and Zhang, W and Wang, Y and Liu,
               C H and Yang, D",
  abstract  = "Modern communication networks have become very complicated and
               highly dynamic, which makes them hard to model, predict and
               control. In this paper, we develop a novel experience-driven
               approach that can learn to well control a communication network
               from its own experience rather than an accurate mathematical
               model, just as a human learns a new skill (such as driving,
               swimming, etc). Specifically, we, for the first time, propose to
               leverage emerging Deep Reinforcement Learning (DRL) for enabling
               model-free control in communication networks; and present a
               novel and highly effective DRL-based control framework, DRL-TE,
               for a fundamental networking problem: Traffic Engineering (TE).
               The proposed framework maximizes a widely-used utility function
               by jointly learning network environment and its dynamics, and
               making decisions under the guidance of powerful Deep Neural
               Networks (DNNs). We propose two new techniques, TE-aware
               exploration and actor-critic-based prioritized experience
               replay, to optimize the general DRL framework particularly for
               TE. To validate and evaluate the proposed framework, we
               implemented it in ns-3, and tested it comprehensively with both
               representative and randomly generated network topologies.
               Extensive packet-level simulation results show that 1) compared
               to several widely-used baseline methods, DRL-TE significantly
               reduces end-to-end delay and consistently improves the network
               utility, while offering better or comparable throughput; 2)
               DRL-TE is robust to network changes; and 3) DRL-TE consistently
               outperforms a state-of-the-art DRL method (for continuous
               control), Deep Deterministic Policy Gradient (DDPG), which,
               however, does not offer satisfying performance.",
  publisher = "ieeexplore.ieee.org",
  pages     = "1871--1879",
  month     =  apr,
  year      =  2018,
  file      = "All Papers/X/Xu et al. 2018 - Experience-driven Networking - A Deep Reinforcement Learning based Approach.pdf",
  keywords  = "learning (artificial intelligence);mathematical
               analysis;neurocontrollers;telecommunication
               control;telecommunication network topology;telecommunication
               traffic;model-free control;TE-aware exploration;deep
               deterministic policy gradient;communication network
               topologies;mathematical model;deep neural networks;deep
               reinforcement learning based approach;experience-driven
               approach;general DRL-TE framework;traffic
               engineering;DNN;actor-critic-based prioritized experience
               replay;packet-level simulation;DDPG;Mathematical
               model;Communication networks;Queueing analysis;Resource
               management;Predictive models;Delays;Aerospace
               electronics;Experience-driven Networking;Deep Reinforcement
               Learning;Traffic Engineering;MLNetworking",
  doi       = "10.1109/INFOCOM.2018.8485853"
}

@INPROCEEDINGS{Blenk2018-lv,
  title     = "{NeuroViNE}: A Neural Preprocessor for Your Virtual Network
               Embedding Algorithm",
  booktitle = "{IEEE} {INFOCOM} 2018 - {IEEE} Conference on Computer
               Communications",
  author    = "Blenk, A and Kalmbach, P and Zerwas, J and Jarschel, M and
               Schmid, S and Kellerer, W",
  abstract  = "Network virtualization enables increasingly diverse network
               services to cohabit and share a given physical infrastructure
               and its resources, with the possibility to rely on different
               network architectures and protocols optimized towards specific
               requirements. In order to ensure a predictable performance
               despite shared resources, network virtualization requires a
               strict performance isolation and hence, resource reservations.
               Moreover, the creation of virtual networks should be fast and
               efficient. The underlying NP-hard algorithmic problem is known
               as the Virtual Network Embedding (VNE) problem and has been
               studied intensively over the last years. This paper presents
               NeuroViNE, a novel approach to speed up and improve a wide range
               of existing VNE algorithms: NeuroViNE is based on a search space
               reduction mechanism and preprocesses a problem instance by
               extracting relevant subgraphs, i.e., good combinations of
               substrate nodes and links. These subgraphs can then be fed to an
               existing algorithm for faster and more resource-efficient
               embeddings. NeuroViNE relies on a Hopfield network, and its
               performance benefits are investigated in simulations for random
               networks, real substrate networks, and data center networks.",
  pages     = "405--413",
  month     =  apr,
  year      =  2018,
  file      = "All Papers/B/Blenk et al. 2018 - NeuroViNE - A Neural Preprocessor for Your Virtual Network Embedding Algorithm.pdf",
  keywords  = "computational complexity;computer centres;graph theory;Hopfield
               neural nets;optimisation;random processes;resource
               allocation;search problems;virtualisation;NeuroViNE;neural
               preprocessor;Virtual Network Embedding algorithm;network
               virtualization;shared resources;strict performance
               isolation;resource reservations;Virtual Network Embedding
               problem;resource-efficient embeddings;Hopfield network;random
               networks;substrate networks;data center networks;network
               services;network architectures;physical infrastructure;NP-hard
               algorithmic problem;search space reduction mechanism;subgraphs
               extraction;Substrates;Neurons;Bandwidth;Heuristic
               algorithms;Manganese;Conferences;Virtualization;MLNetworking;NFV",
  doi       = "10.1109/INFOCOM.2018.8486263"
}

@INPROCEEDINGS{Wang2019-fs,
  title     = "Distributed Machine Learning with a Serverless Architecture",
  booktitle = "{IEEE} {INFOCOM} 2019 - {IEEE} Conference on Computer
               Communications",
  author    = "Wang, Hao and Niu, Di and Li, Baochun",
  abstract  = "The need to scale up machine learning, in the presence of a
               rapid growth of data both in volume and in variety, has sparked
               broad interests to develop distributed machine learning systems,
               typically based on parameter servers. However, since these
               systems are based on a dedicated cluster of physical or virtual
               machines, they have posed non-trivial cluster management
               overhead to machine learning practitioners and data scientists.
               In addition, there exists an inherent mismatch between the
               dynamically varying resource demands during a model training job
               and the inflexible resource provisioning model of current
               cluster-based systems. In this paper, we propose SIREN, an
               asynchronous distributed machine learning framework based on the
               emerging serverless architecture, with which stateless functions
               can be executed in the cloud without the complexity of building
               and maintaining virtual machine infrastructures. With SIREN, we
               are able to achieve a higher level of parallelism and elasticity
               by using a swarm of stateless functions, each working on a
               different batch of data, while greatly reducing system
               configuration overhead. Furthermore, we propose a scheduler
               based on Deep Reinforcement Learning to dynamically control the
               number and memory size of the stateless functions that should be
               used in each training epoch. The scheduler learns from the
               training process itself, in pursuit for the minimum possible
               training time given a cost. With our real-world prototype
               implementation on AWS Lambda, extensive experimental results
               have shown that SIREN can reduce model training time by up to
               44\%, as compared to traditional machine learning training
               benchmarks on AWS EC2 at the same cost.",
  pages     = "1288--1296",
  month     =  apr,
  year      =  2019,
  keywords  = "Training;Servers;Computer architecture;Machine
               learning;Computational modeling;Cloud computing;Data models",
  issn      = "2641-9874",
  doi       = "10.1109/INFOCOM.2019.8737391"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Blocher2020-yl,
  title     = "Letting off {STEAM}: Distributed Runtime Traffic Scheduling for
               Service Function Chaining",
  booktitle = "{IEEE} {INFOCOM} 2020 - {IEEE} Conference on Computer
               Communications",
  author    = "Bl{\"o}cher, M and Khalili, R and Wang, L and Eugster, P",
  abstract  = "Network function virtualization has introduced a high degree of
               flexibility for orchestrating service functions. The
               provisioning of chains of service functions requires making
               decisions on both (1) placement of service functions and (2)
               scheduling of traffic through them. The placement problem (1)
               can be tackled during the planning phase, by exploiting
               coarse-grained traffic information, and has been studied
               extensively. However, runtime traffic scheduling (2) for
               optimizing system utilization and service quality, as required
               for future edge cloud and mobile carrier scenarios, has not been
               addressed so far.We fill this gap by presenting a queuing-based
               system model to characterize the runtime traffic scheduling
               problem for service function chaining. We propose a
               throughput-optimal scheduling policy, called integer allocation
               maximum pressure policy (IA-MPP). To ensure practicality in
               large distributed settings, we propose multi-site cooperative
               IA-MPP (STEAM), fulfilling runtime requirements while achieving
               near-optimal performance. We examine our policies in various
               settings representing real-world scenarios. STEAM closely
               matches IA-MPP in terms of throughput, and significantly
               outperforms (possible adaptations of) existing static or
               coarse-grained dynamic solutions, requiring 30\%-60\% less
               server capacity for similar service quality. Our STEAM prototype
               shows feasibility running on a standard server.",
  pages     = "824--833",
  month     =  jul,
  year      =  2020,
  file      = "All Papers/B/Blöcher et al. 2020 - Letting off STEAM - Distributed Runtime Traffic Scheduling for Service Function Chaining.pdf",
  keywords  = "cloud computing;integer programming;queueing
               theory;scheduling;telecommunication
               traffic;virtualisation;distributed runtime traffic
               scheduling;service function chaining;network function
               virtualization;service functions;coarse-grained traffic
               information;system utilization;future edge cloud;mobile carrier
               scenarios;runtime traffic scheduling problem;throughput-optimal
               scheduling policy;called integer allocation maximum pressure
               policy;IA-MPP;runtime requirements;similar service
               quality;Servers;Runtime;Dynamic scheduling;Prototypes;Resource
               management;Stochastic processes;Service function
               chaining;runtime traffic scheduling;stochastic processing
               networks;NFV",
  issn      = "2641-9874",
  doi       = "10.1109/INFOCOM41043.2020.9155404"
}

@INPROCEEDINGS{Telenyk2018-zq,
  title     = "Modeling of the Data Center Resource Management Using
               Reinforcement Learning",
  booktitle = "2018 International {Scientific-Practical} Conference Problems of
               Infocommunications. Science and Technology ({PIC} {S} T)",
  author    = "Telenyk, Sergii and Zharikov, Eduard and Rolik, Oleksandr",
  abstract  = "Cloud data centers are most dynamic systems in a modern digital
               world. To deliver the high-performance and fault-tolerant IT
               services to end users effectively it is necessary to develop new
               methods for data center resource management while adapting to
               the emergence of new requirements. In this paper, the authors
               refine and evaluate the previously proposed method for cloud
               data center resource management based on the reinforcement
               learning approach. The proposed method takes into account the
               power consumption and the number of SLA violations in the
               management policy. The power consumption is managed by switching
               physical servers to active or sleep state depending on current
               utilization of three resources: CPU, memory, and network
               bandwidth. The proposed reinforcement learning agent allows to
               determine the optimal policy for managing the physical servers
               without creating an environment model and preliminary
               information about the workload. The evaluation results show that
               the proposed method allows to decrease the SLA violation time,
               to serve more VM schedule requests when the number of VMs is
               changing frequently, and to decrease the utilization of data
               center network due to decreased number of migrations.",
  pages     = "289--296",
  month     =  oct,
  year      =  2018,
  keywords  = "Data centers;Resource management;Cloud computing;Power
               demand;Data models;Reinforcement learning;Heuristic
               algorithms;data center;resource management;learning agent;energy
               efficiency",
  doi       = "10.1109/INFOCOMMST.2018.8632064"
}

@INPROCEEDINGS{Lima2020-nx,
  title     = "An {NFV} {MANO} Architecture with a Resource Allocation
               Mechanism Based on Game Theory",
  booktitle = "{IEEE} {INFOCOM} 2020 - {IEEE} Conference on Computer
               Communications Workshops ({INFOCOM} {WKSHPS})",
  author    = "Lima, D H S and Aquino, A L L and Curado, M",
  abstract  = "This work presents an NFV management and orchestration
               architecture design to exploit resource allocation problems. We
               divide the architecture into six different modules: NFV
               orchestrator, VNF manager, infrastructure manager, VNF discover,
               VNF Monitor, and resource allocation. The last three ones are
               our main contribution. Specifically to resource allocation, we
               propose a game theory algorithm that aims to optimize the
               matching of the users' requests with Infrastructure providers'
               offers. In order to evaluate the proposal, we consider two
               different approaches to compare with our solution in terms of
               the number of resources allocated to users, the number of
               resources allocated by the infrastructure providers, and the
               final price of the transaction. Our solution was able to
               allocate around 30\% more resources and save around 32\% when
               compared with the other solutions in simulated scenarios.",
  pages     = "1009--1014",
  month     =  jul,
  year      =  2020,
  file      = "All Papers/L/Lima et al. 2020 - An NFV MANO Architecture with a Resource Allocation Mechanism Based on Game Theory.pdf",
  keywords  = "game theory;resource allocation;software
               architecture;virtualisation;NFV MANO architecture;resource
               allocation mechanism;orchestration architecture design;NFV
               orchestrator;VNF manager;infrastructure manager;VNF Monitor;game
               theory algorithm;infrastructure providers;VNF discover;Network
               Function Virtualization;Resource management;Computer
               architecture;Proposals;Indium phosphide;III-V semiconductor
               materials;Monitoring;Games;Network Function
               Virtualization;Network Architecture;Service Broker",
  doi       = "10.1109/INFOCOMWKSHPS50562.2020.9162719"
}

@INPROCEEDINGS{Bellavista2019-yy,
  title     = "A Support Infrastructure for Machine Learning at the Edge in
               Smart City Surveillance",
  booktitle = "2019 {IEEE} Symposium on Computers and Communications ({ISCC})",
  author    = "Bellavista, Paolo and Chatzimisios, Periklis and Foschini, Luca
               and Paradisioti, Marianna and Scotece, Domenico",
  abstract  = "Nowadays, the massive usage of mobile and IoT applications
               generate large amounts of data. Due to several reasons,
               including latency and bandwidth, it is not practical to send all
               generated data to the cloud. Recent standardization efforts,
               namely, Fog computing and the Multi-access Edge Computing (MEC),
               provide an extension of Cloud computing storage and network
               resources placed in a geographically distributed manner at the
               edge of the network closer to mobiles and IoT devices. These
               paradigms allow low latency, high bandwidth, and location-based
               awareness. In this paper, we present an infrastructure to
               support distributed Machine Learning (ML) by enabling edge
               devices to collaboratively learn a shared model while keeping
               local knowledge stored at the edge of the network. In addition,
               we claim the possibility of improving the model through the
               cloud that acts as a supervisor of the system that contains the
               global knowledge of the entire system through the integration of
               local edge models. We describe our architectural proposal and
               analyze a case study, namely video streaming processing for face
               recognition, deployed in a collaborative edge network. Finally,
               we report experimental results that show the potential
               advantages of using our approach instead of ML algorithms
               completely expected at the cloud infrastructure.",
  pages     = "1189--1194",
  month     =  jun,
  year      =  2019,
  keywords  = "Face recognition;Training;Edge Computing;Collaborative
               Edge;Machine Learning;Smart City;Video Analytics;Face
               recognition",
  issn      = "2642-7389",
  doi       = "10.1109/ISCC47284.2019.8969779"
}

@INPROCEEDINGS{Sheng_Tao2017-cc,
  title     = "Fairness-aware dynamic rate control and flow scheduling for
               network function virtualization",
  booktitle = "2017 {IEEE/ACM} 25th International Symposium on Quality of
               Service ({IWQoS})",
  author    = "{Sheng Tao} and {Lin Gu} and {Deze Zeng} and {Hai Jin} and {Kan
               Hu}",
  abstract  = "By softwarizing traditional dedicated hardware based functions
               to virtualized network functions (VNFs) that can run on standard
               commodity servers, network function virtualization (NFV)
               technology promises high efficiency, flexibility and
               scalability. To NFV service providers, one primary concern is to
               maximize network throughput and reduce service time. To reach
               this goal, two main challenges should be tackled: 1) how to
               schedule the unpredictable and burst network flows; 2) how to
               fairly allocate resources between various flows with different
               resource requirements. In this paper, we are motivated to
               investigate a throughput maximization problem with joint
               consideration of fairness between multiple flows using a
               discrete time queuing model. By taking advantages of Lyapunov
               optimization techniques, we propose a low-complexity online
               distributed algorithm that can achieve arbitrary optimal utility
               with different fairness levels by tuning the fairness bias. The
               high efficiency of our proposal is validated by both theoretical
               analysis and extensive simulation studies.",
  pages     = "1--6",
  month     =  jun,
  year      =  2017,
  file      = "All Papers/S/Sheng Tao et al. 2017 - Fairness-aware dynamic rate control and flow scheduling for network function virtualization.pdf",
  keywords  = "computer networks;queueing theory;resource
               allocation;telecommunication
               scheduling;virtualisation;fairness-aware dynamic rate
               control;flow scheduling;network function virtualization;NFV
               service provider;resource allocation;throughput maximization
               problem;discrete time queuing model;Lyapunov optimization
               technique;low-complexity online distributed algorithm;Radio
               frequency;Throughput;Heuristic algorithms;Algorithm design and
               analysis;Artificial neural networks;Dynamic
               scheduling;Optimization;NFV",
  doi       = "10.1109/IWQoS.2017.7969123"
}

@ARTICLE{Soltanmohammadi2016-zb,
  title    = "A Survey of Traffic Issues in {Machine-to-Machine} Communications
              Over {LTE}",
  author   = "Soltanmohammadi, E and Ghavami, K and Naraghi-Pour, M",
  abstract = "Machine-to-machine (M2M) communication, also referred to as
              Internet of Things (IoT), is a global network of devices such as
              sensors, actuators, and smart appliances which collect
              information, and can be controlled and managed in real time over
              the Internet. Due to their universal coverage, cellular networks
              and the Internet together offer the most promising foundation for
              the implementation of M2M communication. With the worldwide
              deployment of the fourth generation (4G) of cellular networks,
              the long-term evolution (LTE) and LTE-advanced standards have
              defined several quality-of-service classes to accommodate the M2M
              traffic. However, cellular networks are mainly optimized for
              human-to-human (H2H) communication. The characteristics of M2M
              traffic are different from the human-generated traffic and
              consequently create sever problems in both radio access and the
              core networks (CNs). This survey on M2M communication in
              LTE/LTE-A explores the issues, solutions, and the remaining
              challenges to enable and improve M2M communication over cellular
              networks. We first present an overview of the LTE networks and
              discuss the issues related to M2M applications on LTE. We
              investigate the traffic issues of M2M communications and the
              challenges they impose on both access channel and traffic channel
              of a radio access network and the congestion problems they create
              in the CN. We present a comprehensive review of the solutions for
              these problems which have been proposed in the literature in
              recent years and discuss the advantages and disadvantages of each
              method. The remaining challenges are also discussed in detail.",
  journal  = "IEEE Internet of Things Journal",
  volume   =  3,
  number   =  6,
  pages    = "865--884",
  month    =  dec,
  year     =  2016,
  keywords = "4G mobile communication;cellular radio;Internet;Internet of
              Things;Long Term Evolution;machine-to-machine
              communication;quality of experience;radio access
              networks;telecommunication traffic;traffic
              issues;machine-to-machine communications;M2M
              communication;Internet of Things;IoT;4G cellular network
              deployment;universal coverage;fourth generation cellular network
              deployment;Long-Term Evolution;LTE-advanced
              standards;quality-of-service class;M2M traffic
              characteristics;human-to-human communication;H2H
              communication;human-generated traffic;core
              networks;LTE-LTE-A;access channel;traffic channel;radio access
              network;congestion problems;Long Term
              Evolution;Uplink;Downlink;Radio access networks;Internet of
              things;Telecommunication traffic;Machine-to-machine
              communications;Access channel;Internet of Things (IoT);long-term
              evolution (LTE);machine-to-machine (M2M) communication;traffic
              channel;EdgeFogCloudIoT",
  issn     = "2327-4662",
  doi      = "10.1109/JIOT.2016.2533541"
}

@ARTICLE{Linguaglossa2019-cj,
  title    = "Survey of Performance Acceleration Techniques for Network
              Function Virtualization",
  author   = "Linguaglossa, Leonardo and Lange, Stanislav and Pontarelli,
              Salvatore and R{\'e}tv{\'a}ri, G{\'a}bor and Rossi, Dario and
              Zinner, Thomas and Bifulco, Roberto and Jarschel, Michael and
              Bianchi, Giuseppe",
  abstract = "The ongoing network softwarization trend holds the promise to
              revolutionize network infrastructures by making them more
              flexible, reconfigurable, portable, and more adaptive than ever.
              Still, the migration from hard-coded/hard-wired network functions
              toward their software-programmable counterparts comes along with
              the need for tailored optimizations and acceleration techniques
              so as to avoid or at least mitigate the throughput/latency
              performance degradation with respect to fixed function network
              elements. The contribution of this paper is twofold. First, we
              provide a comprehensive overview of the host-based network
              function virtualization (NFV) ecosystem, covering a broad range
              of techniques, from low-level hardware acceleration and
              bump-in-the-wire offloading approaches to high-level software
              acceleration solutions, including the virtualization technique
              itself. Second, we derive guidelines regarding the design,
              development, and operation of NFV-based deployments that meet the
              flexibility and scalability requirements of modern communication
              networks.",
  journal  = "Proc. IEEE",
  volume   =  107,
  number   =  4,
  pages    = "746--764",
  month    =  apr,
  year     =  2019,
  file     = "All Papers/L/Linguaglossa et al. 2019 - Survey of Performance Acceleration Techniques for Network Function Virtualization.pdf",
  keywords = "Acceleration;Performance evaluation;Ecosystems;Network function
              virtualization;Virtualization;Virtualization;Communication
              networks;Fast packet processing;network function virtualization
              (NFV);offloading;performance acceleration;virtualization",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2019.2896848"
}

@ARTICLE{Park2019-ni,
  title    = "Wireless Network Intelligence at the Edge",
  author   = "Park, Jihong and Samarakoon, Sumudu and Bennis, Mehdi and Debbah,
              M{\'e}rouane",
  abstract = "Fueled by the availability of more data and computing power,
              recent breakthroughs in cloud-based machine learning (ML) have
              transformed every aspect of our lives from face recognition and
              medical diagnosis to natural language processing. However,
              classical ML exerts severe demands in terms of energy, memory,
              and computing resources, limiting their adoption for
              resource-constrained edge devices. The new breed of intelligent
              devices and high-stake applications (drones, augmented/virtual
              reality, autonomous systems, and so on) requires a novel paradigm
              change calling for distributed, low-latency and reliable ML at
              the wireless network edge (referred to as edge ML). In edge ML,
              training data are unevenly distributed over a large number of
              edge nodes, which have access to a tiny fraction of the data.
              Moreover, training and inference are carried out collectively
              over wireless links, where edge devices communicate and exchange
              their learned models (not their private data). In a first of its
              kind, this article explores the key building blocks of edge ML,
              different neural network architectural splits and their inherent
              tradeoffs, as well as theoretical and technical enablers stemming
              from a wide range of mathematical disciplines. Finally, several
              case studies pertaining to various high-stake applications are
              presented to demonstrate the effectiveness of edge ML in
              unlocking the full potential of 5G and beyond.",
  journal  = "Proc. IEEE",
  volume   =  107,
  number   =  11,
  pages    = "2204--2239",
  month    =  nov,
  year     =  2019,
  file     = "All Papers/P/Park et al. 2019 - Wireless Network Intelligence at the Edge.pdf",
  keywords = "Cloud computing;Artificial neural networks;5G mobile
              communication;Data models;Wireless networks;Training data;6G
              mobile communication;Machine learning;Scalability;6G;beyond
              5G;distributed machine learning (ML);latency;on-device machine
              learningML;reliability;scalability;ultrareliable and low-latency
              communication (URLLC);Wireless;EdgeFogCloudIoT",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2019.2941458"
}

@ARTICLE{Zhang2020-bz,
  title    = "Mobile Edge Intelligence and Computing for the Internet of
              Vehicles",
  author   = "Zhang, J and Letaief, K B",
  abstract = "The Internet of Vehicles (IoV) is an emerging paradigm that is
              driven by recent advancements in vehicular communications and
              networking. Meanwhile, the capability and intelligence of
              vehicles are being rapidly enhanced, and this will have the
              potential of supporting a plethora of new exciting applications
              that will integrate fully autonomous vehicles, the Internet of
              Things (IoT), and the environment. These trends will bring about
              an era of intelligent IoV, which will heavily depend on
              communications, computing, and data analytics technologies. To
              store and process the massive amount of data generated by
              intelligent IoV, onboard processing and cloud computing will not
              be sufficient due to resource/power constraints and communication
              overhead/latency, respectively. By deploying storage and
              computing resources at the wireless network edge, e.g., radio
              access points, the edge information system (EIS), including edge
              caching, edge computing, and edge AI, will play a key role in the
              future intelligent IoV. EIS will provide not only low-latency
              content delivery and computation services but also localized data
              acquisition, aggregation, and processing. This article surveys
              the latest development in EIS for intelligent IoV. Key design
              issues, methodologies, and hardware platforms are introduced. In
              particular, typical use cases for intelligent vehicles are
              illustrated, including edge-assisted perception, mapping, and
              localization. In addition, various open-research problems are
              identified.",
  journal  = "Proc. IEEE",
  volume   =  108,
  number   =  2,
  pages    = "246--261",
  month    =  feb,
  year     =  2020,
  file     = "All Papers/Z/Zhang and Letaief 2020 - Mobile Edge Intelligence and Computing for the Internet of Vehicles.pdf",
  keywords = "data acquisition;data analysis;Internet of Things;mobile
              computing;mobile radio;traffic engineering computing;data
              analytics technologies;onboard processing;cloud
              computing;wireless network edge;edge information system;EIS;edge
              caching;edge computing;edge AI;intelligent IoV;computation
              services;localized data acquisition;intelligent
              vehicles;edge-assisted perception;mobile edge
              intelligence;vehicular communications;autonomous
              vehicles;Internet of Vehicles;resource-power
              constraints;communication overhead-latency;radio access
              points;Cloud computing;Intelligent vehicles;Sensors;Task
              analysis;Artificial intelligence;Wireless
              communication;Vehicle-to-everything;Internet of Things;Vehicular
              ad hoc networks;Autonomous driving;edge AI;Internet of Vehicles
              (IoV);mobile edge computing (MEC);vehicular
              communications;wireless caching;ATOS",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2019.2947490"
}

@ARTICLE{Zhuang2020-dc,
  title    = "{SDN/NFV-Empowered} Future {IoV} With Enhanced Communication,
              Computing, and Caching",
  author   = "Zhuang, W and Ye, Q and Lyu, F and Cheng, N and Ren, J",
  abstract = "Internet-of-Vehicles (IoV) connects vehicles, sensors,
              pedestrians, mobile devices, and the Internet with advanced
              communication and networking technologies, which can enhance road
              safety, improve road traffic management, and support immerse user
              experience. However, the increasing number of vehicles and other
              IoV devices, high vehicle mobility, and diverse service
              requirements render the operation and management of IoV
              intractable. Software-defined networking (SDN) and network
              function virtualization (NFV) technologies offer potential
              solutions to achieve flexible and automated network management,
              global network optimization, and efficient network resource
              orchestration with cost-effectiveness and are envisioned as a key
              enabler to future IoV. In this article, we provide an overview of
              SDN/NFV-enabled IoV, in which SDN/NFV technologies are leveraged
              to enhance the performance of IoV and enable diverse IoV
              scenarios and applications. In particular, the IoV and SDN/NFV
              technologies are first introduced. Then, the state-of-the-art
              research works are surveyed comprehensively, which is categorized
              into topics according to the role that the SDN/NFV technologies
              play in IoV, i.e., enhancing the performance of data
              communication, computing, and caching, respectively. Some open
              research issues are discussed for future directions.",
  journal  = "Proc. IEEE",
  volume   =  108,
  number   =  2,
  pages    = "274--291",
  month    =  feb,
  year     =  2020,
  keywords = "cache storage;computer network management;Internet;Internet of
              Things;mobile radio;road safety;road traffic;software defined
              networking;virtualisation;caching;data communication;network
              function virtualization technologies;software-defined
              networking;pedestrians;sensors;Internet;SDN-NFV-empowered future
              IoV;immerse user experience;efficient network resource
              orchestration;global network optimization;automated network
              management;flexible network management;diverse service
              requirements;high vehicle mobility;IoV devices;road traffic
              management;road safety;networking technologies;advanced
              communication;mobile devices;Internet-of-Vehicles;enhanced
              communication;Servers;5G mobile communication;Resource
              management;Task analysis;Vehicular ad hoc
              networks;Vehicle-to-everything;Edge caching;fifth generation
              (5G);Internet of Vehicles (IoV);mobile edge computing
              (MEC);quality of service;resource slicing;software-defined
              networking (SDN)/network function virtualization (NFV);vehicle
              mobility",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2019.2951169"
}

@ARTICLE{Liang2020-xb,
  title    = "{Deep-Learning-Based} Wireless Resource Allocation With
              Application to Vehicular Networks",
  author   = "Liang, L and Ye, H and Yu, G and Li, G Y",
  abstract = "It has been a long-held belief that judicious resource allocation
              is critical to mitigating interference, improving network
              efficiency, and ultimately optimizing wireless communication
              performance. The traditional wisdom is to explicitly formulate
              resource allocation as an optimization problem and then exploit
              mathematical programming to solve the problem to a certain level
              of optimality. Nonetheless, as wireless networks become
              increasingly diverse and complex, for example, in the
              high-mobility vehicular networks, the current design
              methodologies face significant challenges and thus call for
              rethinking of the traditional design philosophy. Meanwhile, deep
              learning, with many success stories in various disciplines,
              represents a promising alternative due to its remarkable power to
              leverage data for problem solving. In this article, we discuss
              the key motivations and roadblocks of using deep learning for
              wireless resource allocation with application to vehicular
              networks. We review major recent studies that mobilize the
              deep-learning philosophy in wireless resource allocation and
              achieve impressive results. We first discuss
              deep-learning-assisted optimization for resource allocation. We
              then highlight the deep reinforcement learning approach to
              address resource allocation problems that are difficult to handle
              in the traditional optimization framework. We also identify some
              research directions that deserve further investigation.",
  journal  = "Proc. IEEE",
  volume   =  108,
  number   =  2,
  pages    = "341--356",
  month    =  feb,
  year     =  2020,
  file     = "All Papers/L/Liang et al. 2020 - Deep-Learning-Based Wireless Resource Allocation With Application to Vehicular Networks.pdf",
  keywords = "interference suppression;learning (artificial
              intelligence);mathematical programming;mobility management
              (mobile radio);radio networks;resource
              allocation;telecommunication computing;high-mobility vehicular
              networks;traditional design philosophy;deep learning;problem
              solving;wireless resource allocation;deep-learning-assisted
              optimization;deep reinforcement learning approach;traditional
              optimization framework;network efficiency;wireless communication
              performance;optimization problem;wireless networks;judicious
              resource allocation problem;interference mitigation;mathematical
              programming;Resource management;Wireless
              communication;Optimization;Deep learning;Mathematical
              model;Computational
              modeling;Interference;Vehicle-to-everything;Vehicular ad hoc
              networks;Deep learning;reinforcement learning (RL);resource
              allocation;vehicular networks;wireless communications",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2019.2957798"
}

@ARTICLE{Minerva2020-rv,
  title    = "Digital Twin in the {IoT} Context: A Survey on Technical
              Features, Scenarios, and Architectural Models",
  author   = "Minerva, R and Lee, G M and Crespi, N",
  abstract = "Digital twin (DT) is an emerging concept that is gaining
              attention in various industries. It refers to the ability to
              clone a physical object (PO) into a software counterpart. The
              softwarized object, termed logical object, reflects all the
              important properties and characteristics of the original object
              within a specific application context. To fully determine the
              expected properties of the DT, this article surveys the
              state-of-the-art starting from the original definition within the
              manufacturing industry. It takes into account related proposals
              emerging in other fields, namely augmented and virtual reality
              (e.g., avatars), multiagent systems, and virtualization. This
              survey thereby allows for the identification of an extensive set
              of DT features that point to the ``softwarization'' of POs. To
              properly consolidate a shared DT definition, a set of
              foundational properties is identified and proposed as a common
              ground outlining the essential characteristics (must-haves) of a
              DT. Once the DT definition has been consolidated, its technical
              and business value is discussed in terms of applicability and
              opportunities. Four application scenarios illustrate how the DT
              concept can be used and how some industries are applying it. The
              scenarios also lead to a generic DT architectural model. This
              analysis is then complemented by the identification of software
              architecture models and guidelines in order to present a general
              functional framework for the DT. This article, eventually,
              analyses a set of possible evolution paths for the DT considering
              its possible usage as a major enabler for the softwarization
              process.",
  journal  = "Proc. IEEE",
  volume   =  108,
  number   =  10,
  pages    = "1785--1824",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/M/Minerva et al. 2020 - Digital Twin in the IoT Context - A Survey on Technical Features, Scenarios, and Architectural Models.pdf",
  keywords = "augmented reality;Internet of Things;software
              architecture;softwarized object;shared DT definition;generic DT
              architectural model;software architecture;IoT context;software
              counterpart;logical object;digital twin;augmented reality;virtual
              reality;Internet of Things;Cloning;Digital twin;Software
              engineering;Solid modeling;Software architecture;Manufacturing
              processing;Artificial intelligence (AI);business models;cyber
              physical systems (CPSs);digital twin (DT);Internet of Things
              (IoT);machine learning (ML);multiagent systems;network function
              virtualization;sensors;servitization;smart city;software
              architecture;softwarization;virtual and augmented
              reality;EdgeFogCloudIoT",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2020.2998530"
}

@ARTICLE{Zhuang2021-du,
  title    = "A Comprehensive Survey on Transfer Learning",
  author   = "Zhuang, F and Qi, Z and Duan, K and Xi, D and Zhu, Y and Zhu, H
              and Xiong, H and He, Q",
  abstract = "Transfer learning aims at improving the performance of target
              learners on target domains by transferring the knowledge
              contained in different but related source domains. In this way,
              the dependence on a large number of target-domain data can be
              reduced for constructing target learners. Due to the wide
              application prospects, transfer learning has become a popular and
              promising area in machine learning. Although there are already
              some valuable and impressive surveys on transfer learning, these
              surveys introduce approaches in a relatively isolated way and
              lack the recent advances in transfer learning. Due to the rapid
              expansion of the transfer learning area, it is both necessary and
              challenging to comprehensively review the relevant studies. This
              survey attempts to connect and systematize the existing transfer
              learning research studies, as well as to summarize and interpret
              the mechanisms and the strategies of transfer learning in a
              comprehensive way, which may help readers have a better
              understanding of the current research status and ideas. Unlike
              previous surveys, this survey article reviews more than 40
              representative transfer learning approaches, especially
              homogeneous transfer learning approaches, from the perspectives
              of data and model. The applications of transfer learning are also
              briefly introduced. In order to show the performance of different
              transfer learning models, over 20 representative transfer
              learning models are used for experiments. The models are
              performed on three different data sets, that is, Amazon Reviews,
              Reuters-21578, and Office-31, and the experimental results
              demonstrate the importance of selecting appropriate transfer
              learning models for different applications in practice.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  1,
  pages    = "43--76",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/Z/Zhuang et al. 2021 - A Comprehensive Survey on Transfer Learning.pdf",
  keywords = "Task analysis;Semisupervised learning;Data models;Covariance
              matrices;Machine learning;Adaptation models;Domain
              adaptation;interpretation;machine learning;transfer learning;ML",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2020.3004555"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Jakovetic2020-ov,
  title    = "{Primal--Dual} Methods for {Large-Scale} and Distributed Convex
              Optimization and Data Analytics",
  author   = "Jakoveti{\'c}, D and Bajovi{\'c}, D and Xavier, J and Moura, J M
              F",
  abstract = "The augmented Lagrangian method (ALM) is a classical optimization
              tool that solves a given ``difficult'' (constrained) problem via
              finding solutions of a sequence of ``easier'' (often
              unconstrained) subproblems with respect to the original (primal)
              variable, wherein constraints satisfaction is controlled via the
              so-called dual variables. ALM is highly flexible with respect to
              how primal subproblems can be solved, giving rise to a plethora
              of different primal-dual methods. The powerful ALM mechanism has
              recently proved to be very successful in various large-scale and
              distributed applications. In addition, several significant
              advances have appeared, primarily on precise complexity results
              with respect to computational and communication costs in the
              presence of inexact updates and design and analysis of novel
              optimal methods for distributed consensus optimization. We
              provide a tutorial-style introduction to ALM and its variants for
              solving convex optimization problems in large-scale and
              distributed settings. We describe control-theoretic tools for the
              algorithms' analysis and design, survey recent results, and
              provide novel insights into the context of two emerging
              applications: federated learning and distributed energy trading.",
  journal  = "Proc. IEEE",
  volume   =  108,
  number   =  11,
  pages    = "1923--1938",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/J/Jakovetić et al. 2020 - Primal–Dual Methods for Large-Scale and Distributed Convex Optimization and Data Analytics.pdf",
  keywords = "convex programming;data analysis;augmented Lagrangian
              method;classical optimization tool;constraint satisfaction;dual
              variables;primal subproblems;distributed
              applications;communication costs;distributed consensus
              optimization;distributed convex optimization
              problems;control-theoretic tools;distributed energy trading;ALM
              mechanism;primal-dual methods;computational costs;federated
              learning;data analytics;Optimization;Convergence;Convex
              functions;Computer architecture;Complexity theory;Control
              theory;Linear programming;Augmented Lagrangian;consensus
              optimization;distributed energy trading;distributed
              optimization;federated learning;iteration complexity;primal--dual
              methods;ML",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2020.3007395"
}

@ARTICLE{Nokleby2020-tq,
  title    = "{Scaling-Up} Distributed Processing of Data Streams for Machine
              Learning",
  author   = "Nokleby, M and Raja, H and Bajwa, W U",
  abstract = "Emerging applications of machine learning in numerous
              areas-including online social networks, remote sensing,
              Internet-of-Things (IoT) systems, smart grids, and more-involve
              continuous gathering of and learning from streams of data
              samples. Real-time incorporation of streaming data into the
              learned machine learning models is essential for improved
              inference in these applications. Furthermore, these applications
              often involve data that are either inherently gathered at
              geographically distributed entities due to physical reasons, for
              example, IoT systems and smart grids, or that are intentionally
              distributed across multiple computing machines for memory,
              storage, computational, and/or privacy reasons. Training of
              machine learning models in this distributed, streaming setting
              requires solving stochastic optimization (SO) problems in a
              collaborative manner over communication links between the
              physical entities. When the streaming data rate is high compared
              with the processing capabilities of individual computing entities
              and/or the rate of the communications links, this poses a
              challenging question: How can one best leverage the incoming data
              for distributed training of machine learning models under
              constraints on computing capabilities and/or communications rate?
              A large body of research in distributed online optimization has
              emerged in recent decades to tackle this and related problems.
              This article reviews recently developed methods that focus on
              large-scale distributed SO in the compute- and bandwidth-limited
              regimes, with an emphasis on convergence analysis that explicitly
              accounts for the mismatch between computation, communication, and
              streaming rates and provides sufficient conditions for
              order-optimum convergence. In particular, it focuses on methods
              that solve: 1) distributed stochastic convex problems and 2)
              distributed principal component analysis, which is a nonconvex
              problem with the geometric structure that permits global
              convergence. For such methods, this article discusses recent
              advances in terms of distributed algorithmic designs when faced
              with high-rate streaming data. Furthermore, it reviews
              theoretical guarantees underlying these methods that show that
              there exist regimes in which systems can learn from distributed
              processing of streaming data at order-optimal rates-nearly as
              fast as if all the data were processed at a single superpowerful
              machine.",
  journal  = "Proc. IEEE",
  volume   =  108,
  number   =  11,
  pages    = "1984--2012",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/N/Nokleby et al. 2020 - Scaling-Up Distributed Processing of Data Streams for Machine Learning.pdf",
  keywords = "concave programming;convex programming;data handling;distributed
              algorithms;learning (artificial intelligence);principal component
              analysis;stochastic processes;stochastic programming;nonconvex
              problem;distributed principal component analysis;online social
              networks;distributed algorithmic designs;stochastic convex
              problems;distributed training;stochastic optimization;IoT
              systems;geographically distributed entities;machine
              learning;Internet of Things;data stream processing;distributed
              processing;Machine learning;Training data;Distributed
              databases;Computational modeling;Data
              models;Optimization;Stochastic processes;Convex
              optimization;distributed training;empirical risk minimization
              (ERM);federated learning;machine learning;minibatching;principal
              component analysis (PCA);stochastic gradient descent
              (SGD);stochastic optimization (SO);streaming data;ML",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2020.3021381"
}

@ARTICLE{Mokhtari2020-nt,
  title    = "Stochastic {Quasi-Newton} Methods",
  author   = "Mokhtari, A and Ribeiro, A",
  abstract = "Large-scale data science trains models for data sets containing
              massive numbers of samples. Training is often formulated as the
              solution of empirical risk minimization problems that are
              optimization programs whose complexity scales with the number of
              elements in the data set. Stochastic optimization methods
              overcome this challenge, but they come with their own set of
              limitations. This article discusses recent developments to
              accelerate the convergence of stochastic optimization through the
              exploitation of second-order information. This is achieved with
              stochastic variants of quasi-Newton methods that approximate the
              curvature of the objective function using stochastic gradient
              information. The reasons for why this leads to faster convergence
              are discussed along with the introduction of an incremental
              method that exploits memory to achieve a superlinear convergence
              rate. This is the best-known convergence rate for a stochastic
              optimization method. Stochastic quasi-Newton methods are applied
              to several problems, including prediction of the click-through
              rate of an advertisement displayed in response to a specific
              search engine query by a specific visitor. Experimental
              evaluations showcase reductions in overall computation time
              relative to stochastic gradient descent algorithms.",
  journal  = "Proc. IEEE",
  volume   =  108,
  number   =  11,
  pages    = "1906--1922",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/M/Mokhtari and Ribeiro 2020 - Stochastic Quasi-Newton Methods.pdf",
  keywords = "approximation theory;convergence;convergence of numerical
              methods;data analysis;function approximation;gradient
              methods;learning (artificial intelligence);Newton
              method;optimisation;stochastic programming;search engine
              query;data science;advertisement click-through rate
              prediction;objective function curvature
              approximation;second-order information;stochastic gradient
              descent algorithms;superlinear convergence rate;stochastic
              gradient information;stochastic variants;stochastic
              optimization;empirical risk minimization problems;large-scale
              data science;stochastic quasiNewton
              methods;Convergence;Stochastic processes;Optimization;Risk
              management;Probability distribution;Computational
              efficiency;Approximation algorithms;Optimization
              algorithms;quasi-Newton methods;stochastic optimization;ML",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2020.3023660"
}

@ARTICLE{Gower2020-nk,
  title    = "{Variance-Reduced} Methods for Machine Learning",
  author   = "Gower, R M and Schmidt, M and Bach, F and Richt{\'a}rik, P",
  abstract = "Stochastic optimization lies at the heart of machine learning,
              and its cornerstone is stochastic gradient descent (SGD), a
              method introduced over 60 years ago. The last eight years have
              seen an exciting new development: variance reduction for
              stochastic optimization methods. These variance-reduced (VR)
              methods excel in settings where more than one pass through the
              training data is allowed, achieving a faster convergence than SGD
              in theory and practice. These speedups underline the surge of
              interest in VR methods and the fast-growing body of work on this
              topic. This review covers the key principles and main
              developments behind VR methods for optimization with finite data
              sets and is aimed at nonexpert readers. We focus mainly on the
              convex setting and leave pointers to readers interested in
              extensions for minimizing nonconvex functions.",
  journal  = "Proc. IEEE",
  volume   =  108,
  number   =  11,
  pages    = "1968--1983",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/G/Gower et al. 2020 - Variance-Reduced Methods for Machine Learning.pdf",
  keywords = "gradient methods;learning (artificial
              intelligence);optimisation;stochastic processes;finite data
              sets;VR methods;stochastic optimization;variance
              reduction;SGD;stochastic gradient descent;machine
              learning;variance-reduced methods;convex setting;Machine
              learning;Optimization;Data models;Computational
              modeling;Logistics;Stochastic processes;Training data;Machine
              learning;optimization;variance reduction;ML",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2020.3028013"
}

@ARTICLE{Baumann2021-al,
  title    = "Wireless Control for Smart Manufacturing: Recent Approaches and
              Open Challenges",
  author   = "Baumann, Dominik and Mager, Fabian and Wetzker, Ulf and Thiele,
              Lothar and Zimmerling, Marco and Trimpe, Sebastian",
  abstract = "Smart manufacturing aims to overcome the limitations of today's
              rigid assembly lines by making the material flow and
              manufacturing process more flexible, versatile, and scalable. The
              main economic drivers are higher resource and cost efficiency as
              the manufacturers can more quickly adapt to changing market needs
              and also increase the lifespan of their production sites. The
              ability to close feedback loops fast and reliably over long
              distances among mobile robots, remote sensors, and human
              operators is a key enabler for smart manufacturing. Thus, this
              article provides a perspective on control and coordination over
              wireless networks. Based on an analysis of real-world use cases,
              we identify the main technical challenges that need to be solved
              to close the large gap between the current state of the art in
              industry and the vision of smart manufacturing. We discuss to
              what extent existing control-over-wireless solutions in the
              literature address those challenges, including our own approach
              toward a tight integration of control and wireless communication.
              In addition to a theoretical analysis of closed-loop stability,
              practical experiments on a cyber--physical testbed demonstrate
              that our approach supports relevant smart manufacturing
              scenarios. This article concludes with a discussion of open
              challenges and future research directions.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  4,
  pages    = "441--467",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/B/Baumann et al. 2021 - Wireless Control for Smart Manufacturing - Recent Approaches and Open Challenges.pdf",
  keywords = "Wireless communication;Drones;Smart manufacturing;Wireless sensor
              networks;Task analysis;Networked control systems;Cyber-physical
              systems;Mesh networks;Fourth Industrial Revolution;Cyber-physical
              systems (CPSs);manufacturing industries;networked control
              systems;wireless mesh networks;Wireless",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2020.3032633"
}

@ARTICLE{Chen2021-vb,
  title    = "Wireless Networked Multirobot Systems in Smart Factories",
  author   = "Chen, Kwang-Cheng and Lin, Shih-Chun and Hsiao, Jen-Hao and Liu,
              Chun-Hung and Molisch, Andreas F and Fettweis, Gerhard P",
  abstract = "Smart manufacturing based on artificial intelligence and
              information communication technology will become the main
              contributor to the digital economy of the upcoming decades. In
              order to execute flexible production, smart manufacturing must
              holistically integrate wireless networking, computing, and
              automatic control technologies. This article discusses the
              challenges of this complex system engineering from a wireless
              networking perspective. Starting from enabling flexible
              reconfiguration of a smart factory, we discuss existing wireless
              technology and the trends of wireless networking evolution to
              facilitate multirobot smart factories. Furthermore, the special
              sequential decision-making of a multirobot manufacturing system
              is examined. Social learning can be used to extend the resilience
              of precision operation in a multirobot system by taking network
              topology into consideration, which also introduces a new vision
              for the cybersecurity of smart factories. A summary of highlights
              of technological opportunities for holistic facilitation of
              wireless networked multirobot smart factories rounds off this
              article.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  4,
  pages    = "468--494",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/C/Chen et al. 2021 - Wireless Networked Multirobot Systems in Smart Factories.pdf",
  keywords = "Smart manufacturing;5G mobile communication;Robots;Wireless
              communication;Wireless sensor networks;Production
              facilities;Robot sensing systems;Fourth Industrial Revolution;6G
              mobile communication;Internet of Things;Low latency
              communication;5G;6G;artificial intelligence (AI);cyber--physical
              system (CPS);cybersecurity;Industry 4.0;Internet of
              Things;machine learning;multiagent system;multirobot systems
              (MRSs);smart factory;smart manufacturing;ultrareliable and
              low-latency communication (uRLLC);wireless
              communications;wireless networks",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2020.3033753"
}

@ARTICLE{She2021-zb,
  title    = "A Tutorial on Ultrareliable and {Low-Latency} Communications in
              6G: Integrating Domain Knowledge Into Deep Learning",
  author   = "She, C and Sun, C and Gu, Z and Li, Y and Yang, C and Poor, H V
              and Vucetic, B",
  abstract = "As one of the key communication scenarios in the fifth-generation
              and also the sixth-generation (6G) mobile communication networks,
              ultrareliable and low-latency communications (URLLCs) will be
              central for the development of various emerging mission-critical
              applications. State-of-the-art mobile communication systems do
              not fulfill the end-to-end delay and overall reliability
              requirements of URLLCs. In particular, a holistic framework that
              takes into account latency, reliability, availability,
              scalability, and decision-making under uncertainty is lacking.
              Driven by recent breakthroughs in deep neural networks, deep
              learning algorithms have been considered as promising ways of
              developing enabling technologies for URLLCs in future 6G
              networks. This tutorial illustrates how domain knowledge (models,
              analytical tools, and optimization frameworks) of communications
              and networking can be integrated into different kinds of deep
              learning algorithms for URLLCs. We first provide some background
              of URLLCs and review promising network architectures and deep
              learning frameworks for 6G. To better illustrate how to improve
              learning algorithms with domain knowledge, we revisit model-based
              analytical tools and cross-layer optimization frameworks for
              URLLCs. Following this, we examine the potential of applying
              supervised/unsupervised deep learning and deep reinforcement
              learning in URLLCs and summarize related open problems. Finally,
              we provide simulation and experimental results to validate the
              effectiveness of different learning algorithms and discuss future
              directions.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  3,
  pages    = "204--246",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/S/She et al. 2021 - A Tutorial on Ultrareliable and Low-Latency Communications in 6G - Integrating Domain Knowledge Into Deep Learning.pdf",
  keywords = "Deep learning;6G mobile communication;Knowledge engineering;Ultra
              reliable low latency communication;Tools;Network
              architecture;Optimization;Cross-layer optimization;deep
              reinforcement learning (DRL);sixth generation (6G);supervised
              deep learning;ultrareliable and low-latency communications
              (URLLCs);unsupervised deep learning;ML;5G6G",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2021.3053601"
}

@ARTICLE{Park2021-ai,
  title    = "{Communication-Efficient} and Distributed Learning Over Wireless
              Networks: Principles and Applications",
  author   = "Park, Jihong and Samarakoon, Sumudu and Elgabli, Anis and Kim,
              Joongheon and Bennis, Mehdi and Kim, Seong-Lyun and Debbah,
              M{\'e}rouane",
  abstract = "Machine learning (ML) is a promising enabler for the
              fifth-generation (5G) communication systems and beyond. By
              imbuing intelligence into the network edge, edge nodes can
              proactively carry out decision-making and, thereby, react to
              local environmental changes and disturbances while experiencing
              zero communication latency. To achieve this goal, it is essential
              to cater for high ML inference accuracy at scale under the
              time-varying channel and network dynamics, by continuously
              exchanging fresh data and ML model updates in a distributed way.
              Taming this new kind of data traffic boils down to improving the
              communication efficiency of distributed learning by optimizing
              communication payload types, transmission techniques, and
              scheduling, as well as ML architectures, algorithms, and data
              processing methods. To this end, this article aims to provide a
              holistic overview of relevant communication and ML principles
              and, thereby, present communication-efficient and distributed
              learning frameworks with selected use cases.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  5,
  pages    = "796--819",
  month    =  may,
  year     =  2021,
  file     = "All Papers/P/Park et al. 2021 - Communication-Efficient and Distributed Learning Over Wireless Networks - Principles and Applications.pdf;All Papers/P/Park et al. 2021 - Communication-Efficient and Distributed Learning Over Wireless Networks - Principles and Applications.pdf",
  keywords = "Data models;Training;Distributed databases;Wireless sensor
              networks;Network topology;5G mobile
              communication;Servers;6G;beyond 5G;beyond federated learning
              (FL);communication efficiency;distributed machine
              learning;Wireless;ML",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2021.3055679"
}

@ARTICLE{Samek2021-gf,
  title    = "Explaining Deep Neural Networks and Beyond: A Review of Methods
              and Applications",
  author   = "Samek, W and Montavon, G and Lapuschkin, S and Anders, C J and
              M{\"u}ller, K-R",
  abstract = "With the broader and highly successful usage of machine learning
              (ML) in industry and the sciences, there has been a growing
              demand for explainable artificial intelligence (XAI).
              Interpretability and explanation methods for gaining a better
              understanding of the problem-solving abilities and strategies of
              nonlinear ML, in particular, deep neural networks, are,
              therefore, receiving increased attention. In this work, we aim
              to: 1) provide a timely overview of this active emerging field,
              with a focus on ``post hoc'' explanations, and explain its
              theoretical foundations; 2) put interpretability algorithms to a
              test both from a theory and comparative evaluation perspective
              using extensive simulations; 3) outline best practice aspects,
              i.e., how to best include interpretation methods into the
              standard usage of ML; and 4) demonstrate successful usage of XAI
              in a representative selection of application scenarios. Finally,
              we discuss challenges and possible future directions of this
              exciting foundational field of ML.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  3,
  pages    = "247--278",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/S/Samek et al. 2021 - Explaining Deep Neural Networks and Beyond - A Review of Methods and Applications.pdf",
  keywords = "Industries;Systematics;Neural
              networks;Tools;Kernel;Stress;Unsupervised learning;Black-box
              models;deep learning;explainable artificial intelligence
              (XAI);Interpretability;model transparency;neural networks;ML",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2021.3060483"
}

@ARTICLE{Tataria2021-dp,
  title    = "{6G} Wireless Systems: Vision, Requirements, Challenges,
              Insights, and Opportunities",
  author   = "Tataria, Harsh and Shafi, Mansoor and Molisch, Andreas F and
              Dohler, Mischa and Sj{\"o}land, Henrik and Tufvesson, Fredrik",
  abstract = "Mobile communications have been undergoing a generational change
              every ten years or so. However, the time difference between the
              so-called ``G's'' is also decreasing. While fifth-generation (5G)
              systems are becoming a commercial reality, there is already
              significant interest in systems beyond 5G, which we refer to as
              the sixth generation (6G) of wireless systems. In contrast to the
              already published papers on the topic, we take a top-down
              approach to 6G. More precisely, we present a holistic discussion
              of 6G systems beginning with lifestyle and societal changes
              driving the need for next-generation networks. This is followed
              by a discussion into the technical requirements needed to enable
              6G applications, based on which we dissect key challenges and
              possibilities for practically realizable system solutions across
              all layers of the Open Systems Interconnection stack (i.e., from
              applications to the physical layer). Since many of the 6G
              applications will need access to an order-of-magnitude more
              spectrum, utilization of frequencies between 100 GHz and 1 THz
              becomes of paramount importance. As such, the 6G ecosystem will
              feature a diverse range of frequency bands, ranging from below 6
              GHz up to 1 THz. We comprehensively characterize the limitations
              that must be overcome to realize working systems in these bands
              and provide a unique perspective on the physical and higher layer
              challenges relating to the design of next-generation core
              networks, new modulation and coding methods, novel
              multiple-access techniques, antenna arrays, wave propagation,
              radio frequency transceiver design, and real-time signal
              processing. We rigorously discuss the fundamental changes
              required in the core networks of the future, such as the redesign
              or significant reduction of the transport architecture that
              serves as a major source of latency for time-sensitive
              applications. This is in sharp contrast to the present
              hierarchical network architectures that are not suitable to
              realize many of the anticipated 6G services. While evaluating the
              strengths and weaknesses of key candidate 6G technologies, we
              differentiate what may be practically achievable over the next
              decade, relative to what is possible in theory. Keeping this in
              mind, we present concrete research challenges for each of the
              discussed system aspects, providing inspiration for what follows.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  7,
  pages    = "1166--1199",
  month    =  jul,
  year     =  2021,
  file     = "All Papers/T/Tataria et al. 2021 - 6G Wireless Systems - Vision, Requirements, Challenges, Insights, and Opportunities.pdf",
  keywords = "Array signal processing;Next generation
              working;Transceivers;Signal processing;6G mobile
              communication;Massive MIMO;Radio frequency;Physical
              layer;Beamforming;next-generation core network;physical layer
              (PHY);radio frequency (RF) transceivers;signal
              processing;sixth-generation (6G);terahertz (THz);ultramassive
              multiple-input multiple-output
              (MIMO);waveforms;Mobile\_Wireless;ServicesDescription",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2021.3061701"
}

@ARTICLE{Hmamouche2021-ft,
  title    = "New Trends in Stochastic Geometry for Wireless Networks: A
              Tutorial and Survey",
  author   = "Hmamouche, Yassine and Benjillali, Mustapha and Saoudi, Samir and
              Yanikomeroglu, Halim and Renzo, Marco Di",
  abstract = "Next-generation wireless networks are expected to be highly
              heterogeneous, multilayered, with embedded intelligence at both
              the core and edge of the network. In such a context, system-level
              performance evaluation will be very important to formulate
              relevant insights into tradeoffs that govern such a complex
              system. Over the past decade, SG has emerged as a powerful
              analytical tool to evaluate the system-level performance of
              wireless networks and capture their tendency toward
              heterogeneity. However, with the imminent onset of this crucial
              new decade, where global commercialization of fifth generation
              (5G) is expected to emerge and essential research questions
              related to beyond 5G (B5G) are intended to be identified, we are
              wondering about the role that a powerful tool, such as SG, should
              play. In this article, we first aim to track and summarize the
              novel SG models and techniques developed during the last decade
              in the evaluation of wireless networks. Next, we will outline how
              SG has been used to capture the properties of emerging RANs for
              5G/B5G and quantify the benefits of key enabling technologies.
              Finally, we will discuss new horizons that will breathe new life
              into the use of SG in the foreseeable future, for instance, using
              SG to evaluate performance metrics in the visionary paradigm of
              molecular communications. Also, we will review how SG is
              envisioned to cooperate with machine learning that is seen as a
              crucial component in the race toward ubiquitous wireless
              intelligence. Another important insight is Grothendieck's
              toposes, which is considered as a powerful mathematical concept
              that can help to solve long-standing problems formulated in SG.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  7,
  pages    = "1200--1252",
  month    =  jul,
  year     =  2021,
  file     = "All Papers/H/Hmamouche et al. 2021 - New Trends in Stochastic Geometry for Wireless Networks - A Tutorial and Survey.pdf",
  keywords = "5G mobile communication;Wireless networks;Geometry;Wireless
              communication;Signal to noise ratio;Tutorials;Stochastic
              processes;6G mobile communication;Interference;Noise
              measurement;Fifth-generation (5G) and beyond 5G (B5G)
              networks;point process (PP)
              theory;signal-to-interference-plus-noise ratio (SINR);stochastic
              geometry (SG);Wireless",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2021.3061778"
}

@ARTICLE{Haut2021-ab,
  title    = "Distributed Deep Learning for Remote Sensing Data Interpretation",
  author   = "Haut, Juan M and Paoletti, Mercedes E and Moreno-{\'A}lvarez,
              Sergio and Plaza, Javier and Rico-Gallego, Juan-Antonio and
              Plaza, Antonio",
  abstract = "As a newly emerging technology, deep learning (DL) is a very
              promising field in big data applications. Remote sensing often
              involves huge data volumes obtained daily by numerous in-orbit
              satellites. This makes it a perfect target area for data-driven
              applications. Nowadays, technological advances in terms of
              software and hardware have a noticeable impact on Earth
              observation applications, more specifically in remote sensing
              techniques and procedures, allowing for the acquisition of data
              sets with greater quality at higher acquisition ratios. This
              results in the collection of huge amounts of remotely sensed
              data, characterized by their large spatial resolution (in terms
              of the number of pixels per scene), and very high spectral
              dimensionality, with hundreds or even thousands of spectral
              bands. As a result, remote sensing instruments on spaceborne and
              airborne platforms are now generating data cubes with extremely
              high dimensionality, imposing several restrictions in terms of
              both processing runtimes and storage capacity. In this article,
              we provide a comprehensive review of the state of the art in DL
              for remote sensing data interpretation, analyzing the strengths
              and weaknesses of the most widely used techniques in the
              literature, as well as an exhaustive description of their
              parallel and distributed implementations (with a particular focus
              on those conducted using cloud computing systems). We also
              provide quantitative results, offering an assessment of a DL
              technique in a specific case study (source code available:
              https://github.com/mhaut/cloud-dnn-HSI). This article concludes
              with some remarks and hints about future challenges in the
              application of DL techniques to distributed remote sensing data
              interpretation problems. We emphasize the role of the cloud in
              providing a powerful architecture that is now able to manage vast
              amounts of remotely sensed data due to its implementation
              simplicity, low cost, and high efficiency compared to other
              parallel and distributed architectures, such as grid computing or
              dedicated clusters.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  8,
  pages    = "1320--1349",
  month    =  aug,
  year     =  2021,
  keywords = "Remote sensing;Cloud computing;Distributed databases;Distributed
              computing;Data processing;Big Data;Computer architecture;Deep
              learning;Big data;cloud computing;deep learning (DL);parallel and
              distributed architectures;remote sensing;MLNetworking",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2021.3063258"
}

@ARTICLE{He2021-ks,
  title    = "A Survey of {Millimeter-Wave} Communication: {Physical-Layer}
              Technology Specifications and Enabling Transmission Technologies",
  author   = "He, Shiwen and Zhang, Yan and Wang, Jiaheng and Zhang, Jian and
              Ren, Ju and Zhang, Yaoxue and Zhuang, Weihua and Shen, Xuemin",
  abstract = "Millimeter-wave (mmWave) frequency bands, which offer abundant
              underutilized spectral resources, have been explored and
              exploited in the past several years to meet the requirements of
              emerging wireless services highlighted by high data rates,
              ultrareliability, and ultralow delivery latency. Yet, the unique
              characteristics of mmWave, e.g., continuous wide bandwidth, large
              path, and penetration losses, along with hardware constraints,
              call for innovative technologies for mmWave communication.
              Recently, an extensive amount of work on mmWave communication has
              been carried out by researchers and practitioners from both
              academia and industry, and various technologies have been
              developed for mmWave communication systems to fulfill the full
              potential of mmWave frequency bands. In this article, we present
              a comprehensive survey of the standardization of mmWave
              communication, the latest progress and outcomes of the research
              on mmWave communication technologies, and the emerging
              applications of mmWave communication. In particular, we provide a
              timely and in-depth summary of the state-of-the-art technology
              specifications of mmWave communication with an emphasis on the
              physical (PHY) layer. Then, we elaborate on a number of
              well-established or promising antenna architectures in mmWave
              communication systems and investigate the enabling PHY layer
              transmission technologies. Finally, we show some existing and
              emerging applications of mmWave communication and discuss the
              potential open research issues.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  10,
  pages    = "1666--1705",
  month    =  oct,
  year     =  2021,
  keywords = "Radio frequency;Costs;Baseband;Massive
              MIMO;Throughput;Hardware;Loss measurement;Millimeter wave
              communication;Antenna architecture;hardware
              impairment;millimeter-wave (mmWave) communication;technology
              specification;transceiver design;Wireless",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2021.3107494"
}

@ARTICLE{Xu2021-qw,
  title    = "Edge Intelligence: Empowering Intelligence to the Edge of Network",
  author   = "Xu, Dianlei and Li, Tong and Li, Yong and Su, Xiang and Tarkoma,
              Sasu and Jiang, Tao and Crowcroft, Jon and Hui, Pan",
  abstract = "Edge intelligence refers to a set of connected systems and
              devices for data collection, caching, processing, and analysis
              proximity to where data are captured based on artificial
              intelligence. Edge intelligence aims at enhancing data processing
              and protects the privacy and security of the data and users.
              Although recently emerged, spanning the period from 2011 to now,
              this field of research has shown explosive growth over the past
              five years. In this article, we present a thorough and
              comprehensive survey of the literature surrounding edge
              intelligence. We first identify four fundamental components of
              edge intelligence, i.e., edge caching, edge training, edge
              inference, and edge offloading based on theoretical and practical
              results pertaining to proposed and deployed systems. We then aim
              for a systematic classification of the state of the solutions by
              examining research results and observations for each of the four
              components and present a taxonomy that includes practical
              problems, adopted techniques, and application goals. For each
              category, we elaborate, compare, and analyze the literature from
              the perspectives of adopted techniques, objectives, performance,
              advantages and drawbacks, and so on. This article provides a
              comprehensive survey of edge intelligence and its application
              areas. In addition, we summarize the development of the emerging
              research fields and the current state of the art and discuss the
              important open issues and possible theoretical and technical
              directions.",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  11,
  pages    = "1778--1837",
  month    =  nov,
  year     =  2021,
  keywords = "Training data;Data privacy;Systematics;Edge computing;Data
              collection;Market research;Artificial intelligence;Inference
              algorithms;Artificial intelligence (AI);edge caching;edge
              computing;inference;model
              training;offloading;MLNetworking;EdgeFogCloudIoT",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2021.3119950"
}

@ARTICLE{Chiaraviglio2021-ph,
  title    = "Do Dense {5G} Networks Increase Exposure to Electromagnetic
              Fields? [Point of View]",
  author   = "Chiaraviglio, Luca and Turco, Sara and Bianchi, Giuseppe and
              Blefari-Melazzi, Nicola",
  abstract = "The debate over whether electromagnetic fields (EMFs) exposure
              poses a danger to human health is recurring and goes back
              centuries to when our society first began to rely on electricity
              [1].",
  journal  = "Proc. IEEE",
  volume   =  109,
  number   =  12,
  pages    = "1880--1887",
  month    =  dec,
  year     =  2021,
  file     = "All Papers/C/Chiaraviglio et al. 2021 - Do Dense 5G Networks Increase Exposure to Electromagnetic Fields - [Point of View].pdf",
  keywords = "Base stations;5G mobile communication;Wireless
              communication;Regulation;Electromagnetic fields;Antennas;Array
              signal processing;Electric variables measurement;Electric
              fields;Radiation protection;Radiation safety;Mobile\_Wireless",
  issn     = "1558-2256",
  doi      = "10.1109/JPROC.2021.3125528"
}

@ARTICLE{Chen2018-bx,
  title    = "Task Offloading for Mobile Edge Computing in Software Defined
              {Ultra-Dense} Network",
  author   = "Chen, M and Hao, Y",
  abstract = "With the development of recent innovative applications (e.g.,
              augment reality, self-driving, and various cognitive
              applications), more and more computation-intensive and
              data-intensive tasks are delay-sensitive. Mobile edge computing
              in ultra-dense network is expected as an effective solution for
              meeting the low latency demand. However, the distributed
              computing resource in edge cloud and energy dynamics in the
              battery of mobile device makes it challenging to offload tasks
              for users. In this paper, leveraging the idea of software defined
              network, we investigate the task offloading problem in
              ultra-dense network aiming to minimize the delay while saving the
              battery life of user's equipment. Specifically, we formulate the
              task offloading problem as a mixed integer non-linear program
              which is NP-hard. In order to solve it, we transform this
              optimization problem into two sub-problems, i.e., task placement
              sub-problem and resource allocation sub-problem. Based on the
              solution of the two sub-problems, we propose an efficient
              offloading scheme. Simulation results prove that the proposed
              scheme can reduce 20\% of the task duration with 30\% energy
              saving, compared with random and uniform task offloading schemes.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  36,
  number   =  3,
  pages    = "587--597",
  month    =  mar,
  year     =  2018,
  file     = "All Papers/C/Chen and Hao 2018 - Task Offloading for Mobile Edge Computing in Software Defined Ultra-Dense Network.pdf",
  keywords = "augmented reality;cloud computing;integer programming;linear
              programming;mobile computing;nonlinear programming;resource
              allocation;cognitive applications;data-intensive tasks;mobile
              edge computing;low latency demand;distributed computing
              resource;edge cloud;energy dynamics;mobile device;offload
              tasks;software defined network;task offloading problem;mixed
              integer nonlinear program;optimization problem;task placement
              sub-problem;resource allocation sub-problem;efficient offloading
              scheme;task duration;random task offloading schemes;uniform task
              offloading schemes;software defined ultra-dense network;augment
              reality;computation-intensive tasks;innovative applications;Task
              analysis;Cloud computing;Edge computing;Mobile handsets;Computer
              architecture;Resource management;Microprocessors;Software defined
              networking;mobile edge computing;task offloading;resource
              allocation;EdgeFogCloudIoT",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2018.2815360"
}

@ARTICLE{Wang2019-xj,
  title    = "Adaptive Federated Learning in Resource Constrained Edge
              Computing Systems",
  author   = "Wang, Shiqiang and Tuor, Tiffany and Salonidis, Theodoros and
              Leung, Kin K and Makaya, Christian and He, Ting and Chan, Kevin",
  abstract = "Emerging technologies and applications including Internet of
              Things, social networking, and crowd-sourcing generate large
              amounts of data at the network edge. Machine learning models are
              often built from the collected data, to enable the detection,
              classification, and prediction of future events. Due to
              bandwidth, storage, and privacy concerns, it is often impractical
              to send all the data to a centralized location. In this paper, we
              consider the problem of learning model parameters from data
              distributed across multiple edge nodes, without sending raw data
              to a centralized place. Our focus is on a generic class of
              machine learning models that are trained using
              gradient-descent-based approaches. We analyze the convergence
              bound of distributed gradient descent from a theoretical point of
              view, based on which we propose a control algorithm that
              determines the best tradeoff between local update and global
              parameter aggregation to minimize the loss function under a given
              resource budget. The performance of the proposed algorithm is
              evaluated via extensive experiments with real datasets, both on a
              networked prototype system and in a larger-scale simulated
              environment. The experimentation results show that our proposed
              approach performs near to the optimum with various machine
              learning models and different data distributions.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  37,
  number   =  6,
  pages    = "1205--1221",
  month    =  jun,
  year     =  2019,
  file     = "All Papers/W/Wang et al. 2019 - Adaptive Federated Learning in Resource Constrained Edge Computing Systems.pdf",
  keywords = "Machine learning;Data models;Convergence;Distributed
              databases;Machine learning algorithms;Training;Peer-to-peer
              computing;Distributed machine learning;federated learning;mobile
              edge computing;wireless networking;ATOS;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2019.2904348"
}

@ARTICLE{Zhang2019-ey,
  title    = "Deep Transfer Learning for Intelligent Cellular Traffic
              Prediction Based on {Cross-Domain} Big Data",
  author   = "Zhang, C and Zhang, H and Qiao, J and Yuan, D and Zhang, M",
  abstract = "Machine (deep) learning-enabled accurate traffic modeling and
              prediction is an indispensable part for future big data-driven
              intelligent cellular networks, since it can help autonomic
              network control and management as well as service provisioning.
              Along this line, this paper proposes a novel deep learning
              architecture, namely Spatial-Temporal Cross-domain neural Network
              (STCNet), to effectively capture the complex patterns hidden in
              cellular data. By adopting a convolutional long short-term memory
              network as its subcomponent, STCNet has a strong ability in
              modeling spatial-temporal dependencies. Besides, three kinds of
              cross-domain datasets are actively collected and modeled by
              STCNet to capture the external factors that affect traffic
              generation. As diversity and similarity coexist among cellular
              traffic from different city functional zones, a clustering
              algorithm is put forward to segment city areas into different
              groups, and consequently, a successive inter-cluster transfer
              learning strategy is designed to enhance knowledge reuse. In
              addition, the knowledge transferring among different kinds of
              cellular traffic is also explored with the proposed STCNet model.
              The effectiveness of STCNet is validated through real-world
              cellular traffic datasets using three kinds of evaluation
              metrics. The experimental results demonstrate that STCNet
              outperforms the state-of-the-art algorithms. In particular, the
              transfer learning based on STCNet brings about 4\% 13\% extra
              performance improvements.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  37,
  number   =  6,
  pages    = "1389--1401",
  month    =  jun,
  year     =  2019,
  file     = "All Papers/Z/Zhang et al. 2019 - Deep Transfer Learning for Intelligent Cellular Traffic Prediction Based on Cross-Domain Big Data.pdf",
  keywords = "Big Data;cellular radio;learning (artificial intelligence);neural
              nets;pattern clustering;telecommunication
              computing;telecommunication traffic;intelligent cellular traffic
              prediction;autonomic network control;deep learning
              architecture;real-world cellular traffic datasets;convolutional
              long short-term memory network;knowledge transfer;inter-cluster
              transfer learning strategy;city functional zones;big data-driven
              intelligent cellular networks;spatial-temporal cross-domain
              neural network;STCNet;Predictive models;Correlation;Deep
              learning;Urban areas;Big Data;Wireless
              communication;Spatiotemporal phenomena;Cellular traffic
              prediction;big data;deep learning;intelligent traffic
              management;Code;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2019.2904363"
}

@ARTICLE{Gu2019-gs,
  title    = "{Fairness-Aware} Dynamic Rate Control and Flow Scheduling for
              Network Utility Maximization in Network Service Chain",
  author   = "Gu, L and Zeng, D and Tao, S and Guo, S and Jin, H and Zomaya, A
              Y and Zhuang, W",
  abstract = "Network function virtualization (NFV) decouples the traditional
              network functions from specific or proprietary hardware, such
              that virtualized network functions (VNFs) can run in software
              form. By exploring NFV, a consecutive set of VNFs can constitute
              a service function chain (SFC) to provide the network service.
              From the perspective of network service providers, how to
              maximize the network utility is always one of the major concerns.
              To this end, there are two main issues need to be considered at
              runtime: 1) how to handle the unpredictable network traffic
              burst? and 2) how to fairly allocate resources among various
              flows to satisfy different traffic demands? In this paper, we
              investigate a fairness-aware flow scheduling problem for network
              utility maximization, with joint consideration of resource
              allocation and rate control. Based on a discrete-time queuing
              model, we propose a low-complexity online-distributed algorithm
              using the Lyapunov optimization framework, which can achieve
              arbitrary optimal utility with different fairness levels by
              tuning the fairness bias parameter. We theoretically analyze the
              optimality of the algorithm and evaluate its efficiency by both
              simulation and testbed-based experiments.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  37,
  number   =  5,
  pages    = "1059--1071",
  month    =  may,
  year     =  2019,
  file     = "All Papers/G/Gu et al. 2019 - Fairness-Aware Dynamic Rate Control and Flow Scheduling for Network Utility Maximization in Network Service Chain.pdf",
  keywords = "computer networks;distributed algorithms;optimisation;queueing
              theory;resource allocation;scheduling;telecommunication
              traffic;virtualisation;fairness-aware dynamic rate
              control;network utility maximization;network service
              chain;network function virtualization;NFV;virtualized network
              functions;VNFs;service function chain;network service
              providers;fairly allocate resources;fairness-aware flow
              scheduling problem;fairness bias parameter;optimal
              utility;network traffic burst;online-distributed
              algorithm;Lyapunov optimization;Servers;Throughput;Resource
              management;Quality of service;Dynamic scheduling;Heuristic
              algorithms;Semantics;NFV;flow scheduling;rate
              control;fairness;network utility maximization;NFV",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2019.2906746"
}

@ARTICLE{Ortiz2019-xw,
  title    = "{SCAROS}: A Scalable and Robust {Self-Backhauling} Solution for
              Highly Dynamic {Millimeter-Wave} Networks",
  author   = "Ortiz, Andrea and Asadi, Arash and Sim, Gek Hong and Steinmetzer,
              Daniel and Hollick, Matthias",
  abstract = "Millimeter-wave (mmWave) backhauling is key to ultra-dense
              deployments in beyond-5G networks because providing every base
              station with a dedicated fiber-optic backhaul link to the core
              network is technically too complicated and economically too
              costly. Self-backhauling allows the operators to provide fiber
              connectivity only to a small subset of base stations (Fiber-BSs),
              whereas the rest of the base stations reach the core network via
              a (multi-hop) wireless link towards the Fiber-BS. Although a very
              attractive architecture, self-backhauling is proven to be an
              NP-hard route selection and resource allocation problem. The
              existing self-backhauling solutions lack practicality because:
              (i) they require solving a fairly complex combinatorial problem
              every time there is a change in the network (e.g., channel
              fluctuations), or (ii) they ignore the impact of network dynamics
              which are inherent to mobile networks. In this article, we
              propose SCAROS which is a semi-distributed learning algorithm
              that aims at minimizing the end-to-end latency as well as
              enhancing the robustness against network dynamics including load
              imbalance, channel variations, and link failures. We benchmark
              SCAROS against state-of-the-art approaches under a real-world
              deployment scenario in Manhattan and using realistic beam
              patterns obtained from off-the-shelf mmWave devices. The
              evaluation demonstrates that SCAROS achieves the lowest latency,
              at least 1.8$\times$ higher throughput, and the highest
              flexibility against variability or link failures in the system.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  37,
  number   =  12,
  pages    = "2685--2698",
  month    =  dec,
  year     =  2019,
  keywords = "Base stations;Optical fiber
              networks;Interference;Throughput;Heuristic
              algorithms;Robustness;Millimeter wave communication;machine
              learning;self-backhauling;route selection and resource allocation",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2019.2947925"
}

@ARTICLE{Pei2020-js,
  title    = "Optimal {VNF} Placement via Deep Reinforcement Learning in
              {SDN/NFV-Enabled} Networks",
  author   = "Pei, J and Hong, P and Pan, M and Liu, J and Zhou, J",
  abstract = "The emerging paradigm - Software-Defined Networking (SDN) and
              Network Function Virtualization (NFV) - makes it feasible and
              scalable to run Virtual Network Functions (VNFs) in
              commercial-off-the-shelf devices, which provides a variety of
              network services with reduced cost. Benefitting from centralized
              network management, lots of information about network devices,
              traffic and resources can be collected in SDN/NFV-enabled
              networks. Using powerful machine learning tools, algorithms can
              be designed in a customized way according to the collected
              information to efficiently optimize network performance. In this
              paper, we study the VNF placement problem in SDN/NFV-enabled
              networks, which is naturally formulated as a Binary Integer
              Programming (BIP) problem. Using deep reinforcement learning, we
              propose a Double Deep Q Network-based VNF Placement Algorithm
              (DDQN-VNFPA). Specifically, DDQN determines the optimal solution
              from a prohibitively large solution space and DDQN-VNFPA then
              places/releases VNF Instances (VNFIs) following a threshold-based
              policy. We evaluate DDQN-VNFPA with trace-driven simulations on a
              real-world network topology. Evaluation results show that
              DDQN-VNFPA can get improved network performance in terms of the
              reject number and reject ratio of Service Function Chain Requests
              (SFCRs), throughput, end-to-end delay, VNFI running time and load
              balancing compared with the algorithms in existing literatures.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  2,
  pages    = "263--278",
  month    =  feb,
  year     =  2020,
  file     = "All Papers/P/Pei et al. 2020 - Optimal VNF Placement via Deep Reinforcement Learning in SDN - NFV-Enabled Networks.pdf",
  keywords = "integer programming;learning (artificial intelligence);resource
              allocation;software defined networking;telecommunication
              computing;telecommunication network management;telecommunication
              network topology;telecommunication traffic;virtualisation;optimal
              VNF placement;deep reinforcement learning;network function
              virtualization;network services;centralized network
              management;VNF placement problem;double deep Q
              network;DDQN-VNFPA;real-world network topology;NFV-enabled
              networks;SDN-enabled networks;virtual network functions;binary
              integer programming problem;BIP problem;threshold-based
              policy;trace-driven simulations;service function chain
              requests;end-to-end delay;VNFI running time;load
              balancing;Reinforcement
              learning;Middleboxes;Training;Delays;Neural
              networks;Optimization;Forecasting;Software-defined
              networking;network function virtualization;VNF placement;deep
              reinforcement learning;NFV;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2019.2959181"
}

@ARTICLE{Gu2020-fi,
  title     = "Intelligent {VNF} Orchestration and Flow Scheduling via
               {Model-Assisted} Deep Reinforcement Learning",
  author    = "Gu, L and Zeng, D and Li, W and Guo, S and Zomaya, A Y and Jin,
               H",
  abstract  = "Hosting virtualized network functions (VNF) has been regarded as
               an effective way to realize network function virtualization
               (NFV). Considering the cost diversity in cloud computing, from
               the perspective of service providers, it is significant to
               orchestrate the VNFs and schedule the traffic flows for network
               utility maximization (NUM) as it implies maximal revenue.
               However, traditional heuristic solutions based on optimization
               models usually follow some assumptions, limiting their
               applicability. Recent studies have shown that deep reinforcement
               learning (DRL) is a promising way to tackle such limitations.
               However, DRL agent training also suffers from slow convergence
               problem, especially with complex control problems. We notice
               that optimization models actually can be applied to accelerate
               the DRL training. Therefore, we are motivated to design a
               model-assisted DRL framework for VNF orchestration in this
               paper. Other than letting the agent blindly explore actions, the
               heuristic solutions are used to guide the training process.
               Based on such principle, the DRL framework is also redesigned
               accordingly. Experiment results validate the high efficiency of
               our model-assisted DRL framework as it not only converges
               23$\times$ faster than traditional DRL algorithm, but also with
               higher performance at the same time.",
  journal   = "IEEE J. Sel. Areas Commun.",
  publisher = "ieeexplore.ieee.org",
  volume    =  38,
  number    =  2,
  pages     = "279--291",
  month     =  feb,
  year      =  2020,
  file      = "All Papers/G/Gu et al. 2020 - Intelligent VNF Orchestration and Flow Scheduling via Model-Assisted Deep Reinforcement Learning.pdf",
  keywords  = "cloud computing;intelligent networks;learning (artificial
               intelligence);optimisation;telecommunication
               scheduling;telecommunication traffic;virtualisation;intelligent
               VNF orchestration;model-assisted deep reinforcement
               learning;virtualized network functions;network function
               virtualization;cost diversity;cloud computing;service
               providers;network utility maximization;maximal
               revenue;optimization models;DRL agent training;slow convergence
               problem;complex control problems;model-assisted DRL
               framework;traffic flow scheduling;Servers;Delays;Training;Cloud
               computing;Optimization;Reinforcement learning;Quality of
               service;VNF orchestration;flow scheduling;deep reinforcement
               learning;network utility maximization",
  issn      = "1558-0008",
  doi       = "10.1109/JSAC.2019.2959182"
}

@ARTICLE{Yan2020-cc,
  title    = "Automatic Virtual Network Embedding: A Deep Reinforcement
              Learning Approach With Graph Convolutional Networks",
  author   = "Yan, Zhongxia and Ge, Jingguo and Wu, Yulei and Li, Liangxiong
              and Li, Tong",
  abstract = "Virtual network embedding arranges virtual network services onto
              substrate network components. The performance of embedding
              algorithms determines the effectiveness and efficiency of a
              virtualized network, making it a critical part of the network
              virtualization technology. To achieve better performance, the
              algorithm needs to automatically detect the network status which
              is complicated and changes in a time-varying manner, and to
              dynamically provide solutions that can best fit the current
              network status. However, most existing algorithms fail to provide
              automatic embedding solutions in an acceptable running time. In
              this paper, we combine deep reinforcement learning with a novel
              neural network structure based on graph convolutional networks,
              and propose a new and efficient algorithm for automatic virtual
              network embedding. In addition, a parallel reinforcement learning
              framework is used in training along with a newly-designed
              multi-objective reward function, which has proven beneficial to
              the proposed algorithm for automatic embedding of virtual
              networks. Extensive simulation results under different scenarios
              show that our algorithm achieves best performance on most metrics
              compared with the existing state-of-the-art solutions, with upto
              39.6\% and 70.6\% improvement on acceptance ratio and average
              revenue, respectively. Moreover, the results also demonstrate
              that the proposed solution possesses good robustness.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  6,
  pages    = "1040--1057",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/Y/Yan et al. 2020 - Automatic Virtual Network Embedding - A Deep Reinforcement Learning Approach With Graph Convolutional Networks.pdf",
  keywords = "Substrates;Heuristic algorithms;Feature
              extraction;Training;Virtualization;Machine learning;Neural
              networks;Network virtualization;virtual network
              embedding;reinforcement learning;graph convolutional
              network;ML;MLNetworking;NFV",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.2986662"
}

@ARTICLE{Alleg2020-fj,
  title    = "Joint Diversity and Redundancy for Resilient Service Chain
              Provisioning",
  author   = "Alleg, A and Ahmed, T and Mosbah, M and Boutaba, R",
  abstract = "Achieving network resiliency in terms of availability,
              reliability and fault tolerance is a central concern for network
              designers and operators to achieve business continuity and
              increase productivity. It is particularly challenging in
              increasingly virtualized network environments where network
              services are exposed to both hardware (e.g., bare-metal servers,
              switches, links, etc.) and software (VNF instances) failures.
              This increased risk of failures can severely deteriorate the
              quality of the deployed services and even lead to complete
              service outages. In this context, deploying services in
              operational networks often exacerbates the availability problem
              and requires considering availability of hardware and software
              components both individually and collectively. A key challenge in
              this perspective is the additional resources needed to achieve
              partial or full recovery after failures. In this paper, we
              propose a joint selective diversity and tailored redundancy
              mechanism to provision resilient services in an NFV framework.
              Diversity splits a single VNF into a pool of ``N'' active
              instances called replicas while redundancy provides ``P'' standby
              ready-to-use instances called backups. Based on an enhanced N+P
              model, we propose a placement solution of Service Function Chains
              (SFC) modeled as a Mixed Integer Linear Program (MILP). The
              proposed solution is designed to meet a target SFC availability
              level and, at the same time, to reduce the inherent cost due to
              diversity (overhead) and redundancy (backup resources). We
              evaluate the efficiency of the proposed solution through
              numerically and experimentally. Results demonstrate that our
              solution, not only, improves service resiliency by avoiding
              complete service outages but can also overcome network resource
              fragmentation.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  7,
  pages    = "1490--1504",
  month    =  jul,
  year     =  2020,
  keywords = "business continuity;computer network management;computer network
              reliability;integer programming;linear programming;software fault
              tolerance;virtualisation;joint diversity;resilient Service chain
              provisioning;network resiliency;reliability;fault
              tolerance;central concern;network designers;business
              continuity;virtualized network environments;network
              services;bare-metal servers;software failures;VNF
              instances;deployed services;complete service outages;operational
              networks;availability problem;software components;joint selective
              diversity;tailored redundancy mechanism;provision resilient
              services;single VNF;active instances;ready-to-use
              instances;Service Function Chains;target SFC availability
              level;service resiliency;network resource
              fragmentation;Redundancy;Resilience;Hardware;Software;Middleboxes;Network
              function virtualization;Network function virtualization;network
              resilience;redundancy;network reliability;placement and
              chaining;network diversity",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.2986867"
}

@ARTICLE{Muqaddas2020-jg,
  title    = "Optimal State Replication in Stateful Data Planes",
  author   = "Muqaddas, A S and Sviridov, G and Giaccone, P and Bianco, A",
  abstract = "In SDN stateful data planes, switches can execute algorithms to
              process traffic based on local states. This approach permits to
              offload decisions from the controller to the switches, thus
              reducing the latency when reacting to network events. We consider
              distributed network applications that process traffic at each
              switch based on local replicas of network-wide states.
              Replicating a state across multiple switches poses many
              challenges, because the number of state replicas and their
              placement affects both the data traffic distribution and the
              amount of synchronization traffic among the replicas. In this
              paper, we formulate the optimal placement problem for replicated
              states, taking into account the data traffic routing, to ensure
              that traffic flows are properly managed by network applications,
              and the synchronization traffic between replicas, to ensure state
              coherence. Due to the high complexity required to find the
              optimal solution, we also propose an approximated algorithm to
              scale to large network instances. We numerically show that this
              algorithm, despite its simplicity, well approximates the optimal
              solution. We also show the beneficial effects of state
              replication with respect to the single-replica scenario, so far
              considered in the literature. Finally, we provide an asymptotic
              analysis to find the optimal number of replicas.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  7,
  pages    = "1388--1400",
  month    =  jul,
  year     =  2020,
  file     = "All Papers/M/Muqaddas et al. 2020 - Optimal State Replication in Stateful Data Planes.pdf",
  keywords = "computer network management;optimisation;software defined
              networking;telecommunication control;telecommunication network
              routing;telecommunication switching;telecommunication
              traffic;optimal state replication;SDN stateful data planes;local
              states;network applications;local replicas;network-wide
              states;multiple switches;state replicas;data traffic
              distribution;optimal number;single-replica scenario;approximated
              algorithm;state coherence;traffic flows;data traffic
              routing;replicated states;optimal placement
              problem;synchronization
              traffic;Routing;Synchronization;Protocols;Complexity
              theory;Network topology;Delays;Control systems;Software defined
              networking (SDN);stateful data planes;state replication",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.2986939"
}

@ARTICLE{Mondal2020-cg,
  title    = "{FlowMan}: {QoS-Aware} Dynamic Data Flow Management in
              {Software-Defined} Networks",
  author   = "Mondal, A and Misra, S",
  abstract = "In this paper, we study the problem of data flow management in
              the presence of heterogeneous flows - elephant and mice flows -
              in software-defined networks (SDNs). Most of the researchers
              considered the homogeneous flows in SDN in the existing
              literature. The optimal data flow management in the presence of
              heterogeneous flows is NP-hard. Hence, we propose a game
              theory-based heterogeneous data flow management scheme, named
              FlowMan. In FlowMan, initially, we use a generalized Nash
              bargaining game to obtain a sub-optimal problem, which is
              NP-complete in nature. By solving it, we get the Pareto optimal
              solution for data-rate associated with each switch. Thereafter,
              we use a heuristic method to decide the flow-association with the
              switches, distributedly, which, in turn, helps to get a Pareto
              optimal solution. Extensive simulation results depict that
              FlowMan is capable of ensuring quality-of-service (QoS) for data
              flow management in the presence of heterogeneous flows. In
              particular, FlowMan is capable of reducing network delay by
              77.8-98.7\%, while ensuring 24.6-47.8\% increase in network
              throughput, compared to the existing schemes such as FlowStat and
              CURE. Additionally, FlowMan ensures that per-flow delay is
              reduced by 27.7\% with balanced load distribution among the SDN
              switches.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  7,
  pages    = "1366--1373",
  month    =  jul,
  year     =  2020,
  keywords = "computer network management;game theory;Pareto
              optimisation;quality of service;software defined
              networking;telecommunication switching;QoS-aware dynamic data
              flow management;software-defined networks;heterogeneous
              flows;homogeneous flows;game theory-based heterogeneous data flow
              management scheme;Nash bargaining game;Pareto optimal
              solution;FlowMan;quality-of-service;QoS;SDN
              switches;NP-hard;Switches;Delays;Throughput;Quality of
              service;Mice;Artificial neural networks;Load
              balancing;software-defined networks;Nash bargaining
              game;IoT;heterogeneous flow;FutureInternet",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.2999682"
}

@ARTICLE{Sonkoly2020-ad,
  title    = "{5G} Applications From Vision to Reality: {Multi-Operator}
              Orchestration",
  author   = "Sonkoly, B and Szab{\'o}, R and N{\'e}meth, B and Czentye, J and
              Haja, D and Szalay, M and D{\'o}ka, J and Ger{\H o}, B P and
              Jocha, D and Toka, L",
  abstract = "Envisioned 5G applications and services, such as Tactile
              Internet, Industry 4.0 use-cases, remote control of drone swarms,
              pose serious challenges to the underlying networks and cloud
              platforms. On the one hand, evolved cloud infrastructures provide
              the IT basis for future applications. On the other hand,
              networking is in the middle of a momentous revolution and
              important changes are mainly driven by Network Function
              Virtualization (NFV) and Software Defined Networking (SDN). A
              diverse set of cloud and network resources, controlled by
              different technologies and owned by cooperating or competing
              providers, should be coordinated and orchestrated in a novel way
              in order to enable future applications and fulfill application
              level requirements. In this paper, we propose a novel cross
              domain orchestration system which provides wholesale XaaS
              (Anything as a Service) services over multiple administrative and
              technology domains. Our goal is threefold. First, we design a
              novel orchestration system exploiting a powerful information
              model and propose a versatile embedding algorithm with advanced
              capabilities as a key enabler. The main features of the
              architecture include i) efficient and multi-purpose service
              embedding algorithms which can be implemented based on graph
              models, ii) inherent multidomain support, iii) programmable
              aggregation of different resources, iv) information hiding
              together with flexible delegation of certain requirements
              enabling multi-operator use-cases, and v) support for legacy
              technologies. Second, we present our proof-of-concept prototype
              implementing the proposed system. Third, we establish a dedicated
              test environment spanning across multiple European sites
              encompassing sandbox environments from both operators and the
              academia in order to evaluate the operation of the system.
              Dedicated experiments confirm the feasibility and good
              scalability of the whole framework.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  7,
  pages    = "1401--1416",
  month    =  jul,
  year     =  2020,
  file     = "All Papers/S/Sonkoly et al. 2020 - 5G Applications From Vision to Reality - Multi-Operator Orchestration.pdf",
  keywords = "5G mobile communication;cloud computing;software defined
              networking;virtualisation;Network Function
              Virtualization;Software Defined Networking;network
              resources;cooperating competing providers;application level
              requirements;novel cross domain orchestration system;multiple
              administrative technology domains;novel orchestration
              system;information model;versatile embedding
              algorithm;multipurpose service;multioperator use-cases;legacy
              technologies;5G applications;multioperator orchestration;Tactile
              Internet;remote control;drone swarms;cloud platforms;evolved
              cloud infrastructures;5G mobile communication;Industries;Cloud
              computing;Robots;Production facilities;Network slicing;Network
              function virtualization;NFV;SDN;resource orchestration;5G;5G6G",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.2999684"
}

@ARTICLE{Matoussi2020-xl,
  title    = "{5G} {RAN}: Functional Split Orchestration Optimization",
  author   = "Matoussi, S and Fajjari, I and Costanzo, S and Aitsaadi, N and
              Langar, R",
  abstract = "5G RAN aims to evolve new technologies spanning the Cloud
              infrastructure, virtualization techniques and Software Defined
              Network capabilities. Advanced solutions are introduced to split
              the functions of the Radio Access Network (RAN) between
              centralized and distributed locations. Such paradigms improve RAN
              flexibility and reduce the infrastructure deployment cost without
              impacting the user quality of service. We propose a novel
              functional split orchestration scheme that aims at minimizing the
              RAN deployment cost, while considering the requirements of its
              processing network functions and the capabilities of the Cloud
              infrastructure. With a fine grained approach on user basis, we
              show that the proposed solution optimizes both processing and
              bandwidth resource usage, while minimizing the overall energy
              consumption compared to i) cell-centric, ii) distributed and iii)
              centralized Cloud-RAN approaches. Moreover, we evaluate the
              effectiveness of our proposal in a 5G experimental prototype,
              based on Open Air Interface (OAI). We show that our solution
              achieves good performance in terms of total deployment cost and
              resolution time.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  7,
  pages    = "1448--1463",
  month    =  jul,
  year     =  2020,
  keywords = "4G mobile communication;5G mobile communication;cloud
              computing;optimisation;quality of service;radio access
              networks;software defined networking;virtualisation;functional
              split orchestration optimization;Cloud
              infrastructure;virtualization techniques;Software Defined Network
              capabilities;Radio Access Network;centralized distributed
              locations;RAN flexibility;infrastructure deployment cost;novel
              functional split orchestration scheme;RAN deployment
              cost;processing network functions;user basis;bandwidth resource
              usage;centralized Cloud-RAN approaches;5G experimental
              prototype;total deployment cost;resolution time;Computer
              architecture;Computational modeling;Cloud computing;Bandwidth;5G
              mobile communication;Resource
              management;Optimization;C-RAN;NFV;functional
              split;heuristic;PSO;optimization;OAI;5G6G",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.2999685"
}

@ARTICLE{Zhang2020-ox,
  title    = "{CFR-RL}: Traffic Engineering With Reinforcement Learning in
              {SDN}",
  author   = "Zhang, J and Ye, M and Guo, Z and Yen, C-Y and Chao, H J",
  abstract = "Traditional Traffic Engineering (TE) solutions can achieve the
              optimal or near-optimal performance by rerouting as many flows as
              possible. However, they do not usually consider the negative
              impact, such as packet out of order, when frequently rerouting
              flows in the network. To mitigate the impact of network
              disturbance, one promising TE solution is forwarding the majority
              of traffic flows using Equal-Cost Multi-Path (ECMP) and
              selectively rerouting a few critical flows using Software-Defined
              Networking (SDN) to balance link utilization of the network.
              However, critical flow rerouting is not trivial because the
              solution space for critical flow selection is enormous. Moreover,
              it is impossible to design a heuristic algorithm for this problem
              based on fixed and simple rules, since rule-based heuristics are
              unable to adapt to the changes of the traffic matrix and network
              dynamics. In this paper, we propose CFR-RL (Critical Flow
              Rerouting-Reinforcement Learning), a Reinforcement Learning-based
              scheme that learns a policy to select critical flows for each
              given traffic matrix automatically. CFR-RL then reroutes these
              selected critical flows to balance link utilization of the
              network by formulating and solving a simple Linear Programming
              (LP) problem. Extensive evaluations show that CFR-RL achieves
              near-optimal performance by rerouting only 10\%-21.3\% of total
              traffic.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  10,
  pages    = "2249--2259",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/Z/Zhang et al. 2020 - CFR-RL - Traffic Engineering With Reinforcement Learning in SDN.pdf",
  keywords = "learning (artificial intelligence);linear programming;software
              defined networking;telecommunication network
              routing;telecommunication traffic;SDN;Traditional Traffic
              Engineering;network disturbance;TE solution;Equal-Cost
              MultiPath;Software-Defined Networking;link utilization;solution
              space;critical flow selection;fixed rules;rule-based
              heuristics;Critical Flow Rerouting-Reinforcement
              Learning;Reinforcement Learning-based scheme;given traffic
              matrix;selected critical flows;CFR-RL;linear programming
              problem;Routing;Heuristic algorithms;Linear programming;Control
              systems;Optimization;Reinforcement learning;Quality of
              service;Reinforcement learning;software-defined
              networking;traffic engineering;load balancing;network disturbance
              mitigation;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3000371"
}

@ARTICLE{Gao2020-ez,
  title    = "Personalized {QoE} Improvement for Networking Video Service",
  author   = "Gao, Y and Wei, X and Zhou, L",
  abstract = "Personalized networking video service, as an inevitable trend
              recently, has become the core part for users' quality of
              experience (QoE) improvement. Unfortunately, existing schemes of
              QoE improvement are far from personalized since most of them only
              focus on network-level or user-level optimization. How to realize
              personalized QoE improvement for networking video service has
              been widely considered as a fundamental technical challenge. To
              get over this dilemma, this work proposes a personalized QoE
              improvement scheme by fully taking advantage of the time-varying
              influences on users' QoE, including user-awareness,
              device-awareness and contextawareness. The highlights of this
              work lie in that, the proposed scheme realizes the
              personalization comprehensively considering all these three
              dimensions, meanwhile, it is so robust that can be applied to the
              application scenario where the observable users' data is not
              sufficient. Specifically, we firstly design a comprehensive data
              collection strategy and accurately classify these collected data.
              Then, an efficient deep learning (DL)-based model for
              personalized characteristics extraction is proposed to precisely
              characterize personalization with temporal, spatial and periodic
              correlations. Subsequently, to resolve the data sparsity issue, a
              federated learning (FL)-based architecture with
              privacy-protection is designed by securely exchanging encrypted
              parameters with other users. Importantly, we design an
              optimization scheme based on comprehensive MOS formula for
              personalized QoE improvement. Experimental results demonstrate
              that the proposed scheme has a significantly better performance
              on the personalized QoE improvement.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  10,
  pages    = "2311--2323",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/G/Gao et al. 2020 - Personalized QoE Improvement for Networking Video Service.pdf",
  keywords = "learning (artificial intelligence);neural nets;quality of
              experience;video servers;user-level optimization;personalized QoE
              improvement scheme;personalized characteristics
              extraction;personalized networking video service;quality of
              experience;network-level
              optimization;user-awareness;device-awareness;context
              awareness;deep learning-based model;data sparsity issue;federated
              learning-based architecture;privacy-protection;Quality of
              experience;Data models;Optimization;Data collection;Data
              privacy;Sports;Deep learning;Personalized QoE;networking
              video;deep learning;federated learning;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3000395"
}

@ARTICLE{Qian2020-qp,
  title    = "Reinforcement {Learning-Based} Optimal Computing and Caching in
              Mobile Edge Network",
  author   = "Qian, Y and Wang, R and Wu, J and Tan, B and Ren, H",
  abstract = "Joint pushing and caching are commonly considered an effective
              way to adapt to tidal effects in networks. However, the problem
              of how to precisely predict users' future requests and push or
              cache the proper content remains to be solved. In this paper, we
              investigate a joint pushing and caching policy in a general
              mobile edge computing (MEC) network with multiuser and multicast
              data. We formulate the joint pushing and caching problem as an
              infinite-horizon average-cost Markov decision process (MDP). Our
              aim is not only to maximize bandwidth utilization but also to
              decrease the total quantity of data transmitted. Then, a joint
              pushing and caching policy based on hierarchical reinforcement
              learning (HRL) is proposed, which considers both long-term file
              popularity and short-term temporal correlations of user requests
              to fully utilize bandwidth. To address the curse of
              dimensionality, we apply a divide-and-conquer strategy to
              decompose the joint base station and user cache optimization
              problem into two subproblems: the user cache optimization
              subproblem and the base station cache optimization subproblem. We
              apply value function approximation Q-learning and a deep
              Q-network (DQN) to solve these two subproblems. Furthermore, we
              provide some insights into the design of deep reinforcement
              learning in network caching. The simulation results show that the
              proposed policy can learn content popularity very well and
              predict users' future demands precisely. Our approach outperforms
              existing schemes on various parameters including the base station
              cache size, the number of users and the total number of files in
              multiple scenarios.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  10,
  pages    = "2343--2355",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/Q/Qian et al. 2020 - Reinforcement Learning-Based Optimal Computing and Caching in Mobile Edge Network.pdf",
  keywords = "cache storage;function approximation;learning (artificial
              intelligence);Markov processes;mobile
              computing;optimisation;telecommunication computing;hierarchical
              reinforcement learning;long-term file popularity;short-term
              temporal correlations;user requests;user cache optimization
              subproblem;base station cache optimization subproblem;deep
              Q-network;deep reinforcement learning;network caching;base
              station cache size;mobile edge computing network;optimal
              computing;infinite-horizon average-cost;Markov decision
              process;optimal caching;multiuser data;multicast
              data;pushing-caching policy;divide-and-conquer strategy;value
              function approximation;Base stations;Optimization;Reinforcement
              learning;Bandwidth;Correlation;Markov processes;Throughput;Joint
              pushing and caching;deep reinforcement learning;mobile edge
              network",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3000396"
}

@ARTICLE{Rusek2020-wb,
  title    = "{RouteNet}: Leveraging Graph Neural Networks for Network Modeling
              and Optimization in {SDN}",
  author   = "Rusek, K and Su{\'a}rez-Varela, J and Almasan, P and Barlet-Ros,
              P and Cabellos-Aparicio, A",
  abstract = "Network modeling is a key enabler to achieve efficient network
              operation in future self-driving Software-Defined Networks.
              However, we still lack functional network models able to produce
              accurate predictions of Key Performance Indicators (KPI) such as
              delay, jitter or loss at limited cost. In this paper we propose
              RouteNet, a novel network model based on Graph Neural Network
              (GNN) that is able to understand the complex relationship between
              topology, routing, and input traffic to produce accurate
              estimates of the per-source/destination per-packet delay
              distribution and loss. RouteNet leverages the ability of GNNs to
              learn and model graph-structured information and as a result, our
              model is able to generalize over arbitrary topologies, routing
              schemes and traffic intensity. In our evaluation, we show that
              RouteNet is able to predict accurately the delay distribution
              (mean delay and jitter) and loss even in topologies, routing and
              traffic unseen in the training (worst case MRE = 15.4\%). Also,
              we present several use cases where we leverage the KPI
              predictions of our GNN model to achieve efficient routing
              optimization and network planning.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  10,
  pages    = "2260--2270",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/R/Rusek et al. 2020 - RouteNet - Leveraging Graph Neural Networks for Network Modeling and Optimization in SDN.pdf",
  keywords = "computer network performance evaluation;delays;graph
              theory;jitter;learning (artificial intelligence);neural
              nets;optimisation;software defined networking;telecommunication
              computing;telecommunication network planning;telecommunication
              network routing;telecommunication network
              topology;telecommunication traffic;RouteNet;graph neural
              network;network modeling;efficient network operation;self-driving
              software-defined networks;functional network models;network
              model;GNN model;network planning;network topology;key performance
              indicators;network traffic;delay distribution;routing
              optimization;KPI predictions;Optimization;Delays;Routing;Network
              topology;Computational modeling;Topology;Predictive models;Graph
              Neural Networks;network modeling;network
              optimization;Software-Defined Networks;SDN;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3000405"
}

@ARTICLE{He2020-lm,
  title    = "A {Meta-Learning} Scheme for Adaptive {Short-Term} Network
              Traffic Prediction",
  author   = "He, Q and Moayyedi, A and D{\'a}n, G and Koudouridis, G P and
              Tengkvist, P",
  abstract = "Network traffic prediction is a fundamental prerequisite for
              dynamic resource provisioning in wireline and wireless networks,
              but is known to be challenging due to non-stationarity and due to
              its burstiness and self-similar nature. The prediction of network
              traffic at the user level is particularly challenging, because
              the traffic characteristics emerge from a complex interaction of
              user level and application protocol behavior. In this work we
              address the problem of predicting the network traffic at the user
              level over a short horizon, motivated by its applications in
              cellular scheduling. Motivated by recent works on robust
              adversarial learning, we treat the prediction problem for
              non-stationary traffic in an adversarial context, and propose a
              meta-learning scheme that consists of a set of predictors, each
              optimized to predict a particular kind of traffic, and of a
              master policy that is trained for choosing the best fit predictor
              dynamically based on recent prediction performance, using deep
              reinforcement learning. We evaluate the proposed meta-learning
              scheme on a variety of traffic traces consisting of video and
              non-video traffic. Our results show that it consistently
              outperforms state-of-the-art predictors, and can adapt to before
              unseen traffic without the need for retraining the individual
              predictors.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  10,
  pages    = "2271--2283",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/H/He et al. 2020 - A Meta-Learning Scheme for Adaptive Short-Term Network Traffic Prediction.pdf",
  keywords = "cellular radio;learning (artificial intelligence);mobile
              computing;protocols;telecommunication
              scheduling;telecommunication traffic;user level;application
              protocol behavior;short horizon;robust adversarial
              learning;prediction problem;nonstationary traffic;meta-learning
              scheme;prediction performance;deep reinforcement learning;traffic
              traces;nonvideo traffic;unseen traffic;adaptive short-term
              network traffic prediction;dynamic resource provisioning;wireless
              networks;nonstationarity;traffic
              characteristics;wireline;cellular scheduling;Predictive
              models;Time series analysis;Streaming media;Mathematical
              model;Wireless networks;Aggregates;Downlink;Meta learning;deep
              reinforcement learning;network traffic prediction;wireless
              networks;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3000408"
}

@ARTICLE{Kirilin2020-oy,
  title    = "{RL-Cache}: {Learning-Based} Cache Admission for Content Delivery",
  author   = "Kirilin, V and Sundarrajan, A and Gorinsky, S and Sitaraman, R K",
  abstract = "Content delivery networks (CDNs) distribute much of the Internet
              content by caching and serving the objects requested by users. A
              major goal of a CDN is to maximize the hit rates of its caches,
              thereby enabling faster content downloads to the users. Content
              caching involves two components: an admission algorithm to decide
              whether to cache an object and an eviction algorithm to determine
              which object to evict from the cache when it is full. In this
              paper, we focus on cache admission and propose a novel algorithm
              called RL-Cache that uses model-free reinforcement learning (RL)
              to decide whether or not to admit a requested object into the
              CDN's cache. Unlike prior approaches that use a small set of
              criteria for decision making, RL-Cache weights a large set of
              features that include the object size, recency, and frequency of
              access. We develop a publicly available implementation of
              RL-Cache and perform an evaluation using production traces for
              the image, video, and web traffic classes from Akamai's CDN. The
              evaluation shows that RL-Cache improves the hit rate in
              comparison with the state of the art and imposes only a modest
              resource overhead on the CDN servers. Further, RL-Cache is robust
              enough that it can be trained in one location and executed on
              request traces of the same or different traffic classes in other
              locations of the same geographic region. The paper also reports
              extensive analyses of the RL-Cache sensitivity to its features
              and hyperparameter values. The analyses validate the made design
              choices and reveal interesting insights into the RL-Cache
              behavior.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  10,
  pages    = "2372--2385",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/K/Kirilin et al. 2020 - RL-Cache - Learning-Based Cache Admission for Content Delivery.pdf",
  keywords = "cache storage;content management;decision
              making;Internet;learning (artificial intelligence);content
              delivery networks;content caching;CDN cache;RL-Cache
              weights;learning-based cache admission;RL-cache behavior;RL-cache
              sensitivity;reinforcement learning;CDN servers;Internet
              content;decision making;Servers;Production;Sensitivity;Neural
              networks;Stochastic processes;Optimization;Machine learning
              algorithms;Content delivery network;caching;cache admission;hit
              rate;object feature;neural network;direct policy search;Monte
              Carlo sampling;stochastic optimization;traffic
              class;image;video;web;production trace",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3000415"
}

@ARTICLE{Chemouil2020-so,
  title    = "Guest Editorial Special Issue on Advances in Artificial
              Intelligence and Machine Learning for Networking",
  author   = "Chemouil, P and Hui, P and Kellerer, W and Limam, N and Stadler,
              R and Wen, Y",
  abstract = "https://www.youtube.com/watch?v=SQmgSOi5oos",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  10,
  pages    = "2229--2233",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/C/Chemouil et al. 2020 - Guest Editorial Special Issue on Advances in Artificial Intelligence and Machine Learning for Networking.pdf",
  keywords = "Special issues and sections;Artificial intelligence;Machine
              learning;Reinforcement learning;Streaming media;Quality of
              experience;Privacy;ML",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3003065"
}

@ARTICLE{Hui2020-oc,
  title    = "Reservation Service: Trusted Relay Selection for Edge Computing
              Services in Vehicular Networks",
  author   = "Hui, Y and Su, Z and Luan, T H and Li, C",
  abstract = "Driven by the ever-increasing demands of vehicular services, edge
              computing has become a promising paradigm to facilitate edge
              services in vehicular networks by using edge computing devices
              (ECDs). To enhance the service experience, we develop a
              reservation service framework, where the reservation service
              request of a vehicle needs to be relayed to one of the ECDs which
              is ahead of its driving direction. However, due to the various
              behaviors of vehicles, not all the vehicles are trustworthy and
              willing to join in the service request relay process. Therefore,
              how to exploit the cooperation between ECDs and vehicles to relay
              the service request by considering the dynamic traffic status and
              the behaviors of vehicles becomes a challenge. As an effort to
              address this problem, we propose a trusted relay selection scheme
              for edge services to facilitate the proposed reservation service
              framework. Specifically, we first design the request relay
              mechanism based on the dynamic traffic status to guarantee the
              efficiency of the relay process. Then, the reputation management
              mechanism is presented to constrain the behaviors of vehicles,
              where a vehicle with high reputation value can enjoy the price
              discount for computing service. Based on the designed request
              relay and reputation management mechanisms, a reputation-based
              auction approach is then proposed to select relay vehicles (RVs)
              to reduce the cost of the relay service. Simulation results show
              that the proposed reservation service framework can manage
              vehicles efficiently and lead to the lowest cost for the relay
              services compared with the conventional schemes.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  12,
  pages    = "2734--2746",
  month    =  dec,
  year     =  2020,
  keywords = "Relays;Edge computing;Vehicle dynamics;Intelligent
              vehicles;Simulation;Task analysis;Cloud computing;Vehicular ad
              hoc networks;Vehicular networks;reservation service;relay
              scheme;edge computing;reputation-based auction
              game;EdgeFogCloudIoT",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3005468"
}

@ARTICLE{Tang2020-gi,
  title    = "Deep Reinforcement Learning for Dynamic {Uplink/Downlink}
              Resource Allocation in High Mobility {5G} {HetNet}",
  author   = "Tang, F and Zhou, Y and Kato, N",
  abstract = "Recently, the 5G is widely deployed for supporting communications
              of high mobility nodes including train, vehicular and unmanned
              aerial vehicles (UAVs) largely emerged as the main components for
              constructing the wireless heterogeneous network (HetNet). To
              further improve the radio utilization, the Time Division Duplex
              (TDD) is considered to be the potential full-duplex communication
              technology in the high mobility 5G network. However, the high
              mobility of users leads to the high dynamic network traffic and
              unpredicted link state change. A new method to predict the
              dynamic traffic and channel condition and schedule the TDD
              configuration in real-time is essential for the high mobility
              environment. In this paper, we investigate the channel model in
              the high mobility and heterogeneous network and proposed a novel
              deep reinforcement learning based intelligent TDD configuration
              algorithm to dynamically allocate radio resources in an online
              manner. In the proposal, the deep neural network is employed to
              extract the features of the complex network information, and the
              dynamic Q-value iteration based reinforcement learning with
              experience replay memory mechanism is proposed to adaptively
              change TDD Up/Down-link ratio by evaluated rewards. The
              simulation results show that the proposal achieves significant
              network performance improvement in terms of both network
              throughput and packet loss rate, comparing with conventional TDD
              resource allocation algorithms.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  12,
  pages    = "2773--2782",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/T/Tang et al. 2020 - Deep Reinforcement Learning for Dynamic Uplink - Downlink Resource Allocation in High Mobility 5G HetNet.pdf",
  keywords = "5G mobile communication;Resource management;Heuristic
              algorithms;Vehicle dynamics;Interference;Dynamic
              scheduling;Device-to-device communication;Vehicular ad hoc
              networks;5G;high mobility;resource allocation;unmanned aerial
              vehicle (UAV);time division duplex (TDD);reinforcement learning
              (RL);Q-learning;deep learning;deep belief network;heterogeneous
              network (HetNet);EdgeFogCloudIoT",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3005495"
}

@ARTICLE{He2020-mm,
  title    = "Guest Editorial {5G} Wireless Communications With High Mobility",
  author   = "He, R and Bai, F and Mao, G and H{\"a}rri, J and Ky{\"o}sti, P",
  abstract = "The fifth generation (5G) wireless communication networks are
              expected to support communications with high mobility, e.g., with
              a speed up to 500 km/h. Hence 5G communications will have
              numerous applications in high mobility scenarios, such as high
              speed railways (HSRs), vehicular ad hoc networks, and unmanned
              aerial vehicles (UAVs) communications [1]--[3]. The 5G systems
              will provide advanced communication platforms enabling reliable
              transmission for the Wireless Train Backbone (WLTB) or Wireless
              Train Control \& Management System (WTCMS) [4], [5]. They will
              also enable new services or enhancements for vehicular
              communications in Intelligent Transportation System (ITS)
              [6]--[9]. The coordination and swarming control for UAVs will
              also benefit from 5G capabilities, as UAV-based 5G infrastructure
              modeling and improvement have begun receiving attention [10]. In
              general, high mobility communication is not only about how large
              is the maximum speed, it is more about the challenges caused by
              mobility. In high mobility scenarios, a wireless channel is
              rapidly time varying, Doppler shifts and spreads can be much
              larger than those in cellular communications, and if modeled
              statistically, the channel will be non-wide-sense stationary
              (non-WSS) over a short time period. In addition, network topology
              can change quickly, and switching among base stations (BSs)
              and/or peer nodes can be more frequent, not forgetting 5G
              challenges in cross-border mobility [11].",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  12,
  pages    = "2717--2722",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/H/He et al. 2020 - Guest Editorial 5G Wireless Communications With High Mobility.pdf",
  keywords = "Special issues and sections;5G mobile communication;Unmanned
              aerial vehicles;High-speed rail transportation;Vehicular ad hoc
              networks;Done;5G6G",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3005498"
}

@ARTICLE{Di_Renzo2020-gw,
  title    = "Smart Radio Environments Empowered by Reconfigurable Intelligent
              Surfaces: How It Works, State of Research, and The Road Ahead",
  author   = "Di Renzo, M and Zappone, A and Debbah, M and Alouini, M-S and
              Yuen, C and de Rosny, J and Tretyakov, S",
  abstract = "Reconfigurable intelligent surfaces (RISs) are an emerging
              transmission technology for application to wireless
              communications. RISs can be realized in different ways, which
              include (i) large arrays of inexpensive antennas that are usually
              spaced half of the wavelength apart; and (ii) metamaterial-based
              planar or conformal large surfaces whose scattering elements have
              sizes and inter-distances much smaller than the wavelength.
              Compared with other transmission technologies, e.g., phased
              arrays, multi-antenna transmitters, and relays, RISs require the
              largest number of scattering elements, but each of them needs to
              be backed by the fewest and least costly components. Also, no
              power amplifiers are usually needed. For these reasons, RISs
              constitute a promising software-defined architecture that can be
              realized at reduced cost, size, weight, and power (C-SWaP
              design), and are regarded as an enabling technology for realizing
              the emerging concept of smart radio environments (SREs). In this
              paper, we (i) introduce the emerging research field of
              RIS-empowered SREs; (ii) overview the most suitable applications
              of RISs in wireless networks; (iii) present an
              electromagnetic-based communication-theoretic framework for
              analyzing and optimizing metamaterial-based RISs; (iv) provide a
              comprehensive overview of the current state of research; and (v)
              discuss the most important research issues to tackle. Owing to
              the interdisciplinary essence of RIS-empowered SREs, finally, we
              put forth the need of reconciling and reuniting C. E. Shannon's
              mathematical theory of communication with G. Green's and J. C.
              Maxwell's mathematical theories of electromagnetism for
              appropriately modeling, analyzing, optimizing, and deploying
              future wireless networks empowered by RISs.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  38,
  number   =  11,
  pages    = "2450--2525",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/D/Di Renzo et al. 2020 - Smart Radio Environments Empowered by Reconfigura ... ligent Surfaces - How It Works, State of Research, and The Road Ahead.pdf",
  keywords = "conformal antennas;electromagnetic wave scattering;metamaterial
              antennas;planar antenna arrays;software radio;reconfigurable
              intelligent surfaces;wireless communications;multiantenna
              transmitters;smart radio environments;RIS-empowered
              SRE;metamaterial-based planar;software-defined
              architecture;electromagnetic-based communication-theoretic
              framework;metamaterial-based RIS;wireless networks;cost size
              weight and power design;C-SWaP design;Surface waves;Wireless
              networks;Wireless sensor networks;Communication system
              security;Prototypes;Surface treatment;5G;6G;reconfigurable
              intelligent surfaces;smart radio environments;mathematical theory
              of communication;mathematical theory of electromagnetism;Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3007211"
}

@ARTICLE{Qian2021-ga,
  title    = "{Multi-Operator} Spectrum Sharing for Massive {IoT} Coexisting in
              {5G/B5G} Wireless Networks",
  author   = "Qian, B and Zhou, H and Ma, T and Yu, K and Yu, Q and Shen, X",
  abstract = "With a massive number of Internet-of-Things (IoT) devices
              connecting with the Internet via 5G or beyond 5G (B5G) wireless
              networks, how to support massive access for coexisting cellular
              users and IoT devices with quality-of-service (QoS) guarantees
              over limited radio spectrum is one of the main challenges. In
              this paper, we investigate the multi-operator dynamic spectrum
              sharing problem to support the coexistence of rate guaranteed
              cellular users and massive IoT devices. For the spectrum sharing
              among mobile network operators (MNOs), we introduce a wireless
              spectrum provider (WSP) to make spectrum trading with MNOs
              through the Stackelberg pricing game. This framework is inspired
              by the active radio access network (RAN) sharing architecture of
              3GPP, which is regarded as a promising solution for MNOs to
              improve the resource utilization and reduce deployment and
              operation cost. For the coexistence of cellular users and IoT
              devices under each MNO, we propose the coexisting access rules to
              ensure their QoS and the priority of cellular users. In
              particular, we prove the uniqueness of the Stackelberg
              equilibrium (SE) solution, which can maximize the payoffs of MNOs
              and WSP simultaneously. Moreover, we propose an iterative
              algorithm for the Stackelberg pricing game, which is proved to
              achieve the unique SE solution. Extensive numerical simulations
              demonstrate that, the payoffs of WSP and MNOs are maximized and
              the SE solution can be reached. Meanwhile, the proposed
              multi-operator dynamic spectrum sharing algorithm can support
              more than almost 40\% IoT devices compared with the existing
              no-sharing method, and the gap is less than about 10\% compared
              with the exhaustive method.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  3,
  pages    = "881--895",
  month    =  mar,
  year     =  2021,
  keywords = "Bandwidth;Pricing;Games;Wireless networks;3GPP;5G mobile
              communication;Quality of service;5G/B5G;IoT devices;cellular
              users;spectrum sharing;Stackelberg pricing game;massive access",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3018803"
}

@ARTICLE{Liu2021-nk,
  title    = "Resource Allocation for {Energy-Efficient} {MEC} in
              {NOMA-Enabled} Massive {IoT} Networks",
  author   = "Liu, Binghong and Liu, Chenxi and Peng, Mugen",
  abstract = "Integrating mobile edge computing (MEC) into the Internet of
              Things (IoT) enables the IoT devices of limited computation
              capabilities and energy to offload their computation-intensive
              and delay-sensitive tasks to the network edge, thereby providing
              high quality of service to the devices. In this article, we apply
              non-orthogonal multiple access (NOMA) technique to enable massive
              connectivity and investigate how it can be exploited to achieve
              energy-efficient MEC in IoT networks. In order to maximize the
              energy efficiency for offloading, while simultaneously satisfying
              the maximum tolerable delay constraints of IoT devices, a joint
              radio and computation resource allocation problem is formulated,
              which takes both intra- and inter-cell interference into
              consideration. To tackle this intractable mixed integer
              non-convex problem, we first decouple it into separated radio and
              computation resource allocation problems. Then, the radio
              resource allocation problem is further decomposed into a
              subchannel allocation problem and a power allocation problem,
              which can be solved by matching and sequential convex programming
              algorithms, respectively. Based on the obtained radio resource
              allocation solution, the computation resource allocation problem
              can be solved by utilizing the Knapsack method. Numerical results
              validate our analysis and show that our proposed scheme can
              significantly improve the energy efficiency of NOMA-enabled MEC
              in IoT networks compared to the existing baselines.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  4,
  pages    = "1015--1027",
  month    =  apr,
  year     =  2021,
  keywords = "Resource management;NOMA;Delays;Interference;Task
              analysis;Internet of Things;Silicon carbide;Massive Internet of
              Things (IoT);multi-cell networks;mobile edge computing
              (MEC);non-orthogonal multiple access (NOMA);resource
              allocation;convex optimization;EdgeFogCloudIoT;Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3018809"
}

@ARTICLE{Al-Eryani2021-hy,
  title    = "Multiple Access in {Cell-Free} Networks: Outage Performance,
              Dynamic Clustering, and Deep Reinforcement {Learning-Based}
              Design",
  author   = "Al-Eryani, Yasser and Akrout, Mohamed and Hossain, Ekram",
  abstract = "In future cell-free (or cell-less) wireless networks, a large
              number of devices in a geographical area will be served
              simultaneously in non-orthogonal multiple access scenarios by a
              large number of distributed access points (APs), which coordinate
              with a centralized processing pool. For such a centralized
              cell-free network with static predefined beamforming design, we
              first derive a closed-form expression of uplink outage
              probability for a user/device. To reduce the complexity of joint
              processing of received signals in presence of a large number of
              devices and APs, we propose a novel dynamic cell-free network
              architecture. In this architecture, the distributed APs are
              clustered (i.e. partitioned) among a set of subgroups with each
              subgroup acting as a virtual AP in a distributed antenna system
              (DAS). The conventional static cell-free network is a special
              case of this dynamic cell-free network when the cluster size is
              one. For this dynamic cell-free network, we propose a successive
              interference cancellation (SIC)-enabled signal detection method
              and an inter-user-interference (IUI)-aware receive diversity
              combining scheme. We then formulate the general problem of
              clustering the APs and designing the beamforming vectors with an
              objective such as maximizing the sum rate or maximizing the
              minimum rate. To this end, we propose a hybrid deep reinforcement
              learning (DRL) model, namely, a deep deterministic policy
              gradient (DDPG)-deep double Q-network (DDQN) model to solve the
              optimization problem for online implementation with low
              complexity. The DRL model for sum-rate optimization significantly
              outperforms that for maximizing the minimum rate in terms of
              average per-user rate performance. Also, in our system setting,
              the proposed DDPG-DDQN scheme is found to achieve around 78\% of
              the rate achievable through an exhaustive search-based design.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  4,
  pages    = "1028--1042",
  month    =  apr,
  year     =  2021,
  keywords = "Array signal processing;Interference;Computer
              architecture;Network architecture;NOMA;Uplink;Signal
              detection;Cell-free network;receive diversity;successive
              interference cancellation (SIC);outage
              probability;clustering;deep reinforcement learning
              (DRL);deterministic policy gradient (DDPG);double Q-network
              (DQN);Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3018825"
}

@ARTICLE{Chen2021-ww,
  title    = "Massive Access for {5G} and Beyond",
  author   = "Chen, X and Ng, D W K and Yu, W and Larsson, E G and Al-Dhahir, N
              and Schober, R",
  abstract = "Massive access, also known as massive connectivity or massive
              machine-type communication (mMTC), is one of the main use cases
              of the fifth-generation (5G) and beyond 5G (B5G) wireless
              networks. A typical application of massive access is the cellular
              Internet of Things (IoT). Different from conventional human-type
              communication, massive access aims at realizing efficient and
              reliable communications for a massive number of IoT devices.
              Hence, the main characteristics of massive access include low
              power, massive connectivity, and broad coverage, which require
              new concepts, theories, and paradigms for the design of
              next-generation cellular networks. This paper presents a
              comprehensive survey of massive access design for B5G wireless
              networks. Specifically, we provide a detailed review of massive
              access from the perspectives of theory, protocols, techniques,
              coverage, energy, and security. Furthermore, several future
              research directions and challenges are identified.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  3,
  pages    = "615--637",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/C/Chen et al. 2021 - Massive Access for 5G and Beyond.pdf",
  keywords = "5G mobile communication;Communication system security;Wireless
              networks;Internet of Things;Reliability;Cellular
              networks;B5G;massive access;cellular IoT;low power;massive
              connectivity;broad coverage;Wireless;Mobile\_Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3019724"
}

@ARTICLE{Lu2021-va,
  title    = "Deep {Multi-Task} Learning for Cooperative {NOMA}: System Design
              and Principles",
  author   = "Lu, Y and Cheng, P and Chen, Z and Mow, W H and Li, Y and
              Vucetic, B",
  abstract = "Envisioned as a promising component of the future wireless
              Internet-of-Things (IoT) networks, the non-orthogonal multiple
              access (NOMA) technique can support massive connectivity with a
              significantly increased spectral efficiency. Cooperative NOMA is
              able to further improve the communication reliability of users
              under poor channel conditions. However, the conventional system
              design suffers from several inherent limitations and is not
              optimized from the bit error rate (BER) perspective. In this
              article, we develop a novel deep cooperative NOMA scheme, drawing
              upon the recent advances in deep learning (DL). We develop a
              novel hybrid-cascaded deep neural network (DNN) architecture such
              that the entire system can be optimized in a holistic manner. On
              this basis, we construct multiple loss functions to quantify the
              BER performance and propose a novel multi-task oriented two-stage
              training method to solve the end-to-end training problem in a
              self-supervised manner. The learning mechanism of each DNN module
              is then analyzed based on information theory, offering insights
              into the explainable DNN architecture and its corresponding
              training method. We also adapt the proposed scheme to handle the
              power allocation (PA) mismatch between training and inference and
              incorporate it with channel coding to combat signal
              deterioration. Simulation results verify its advantages over
              orthogonal multiple access (OMA) and the conventional cooperative
              NOMA scheme in various scenarios.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  1,
  pages    = "61--78",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/L/Lu et al. 2021 - Deep Multi-Task Learning for Cooperative NOMA - System Design and Principles.pdf",
  keywords = "NOMA;Training;Bit error rate;Simulation;Silicon carbide;Deep
              learning;Australia;Cooperative non-orthogonal multiple
              access;explainable deep learning;multi-task learning;neural
              network;self-supervised learning;MLNetworking;Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3036943"
}

@ARTICLE{Ding2021-sc,
  title    = "Optimal Contract Design for Efficient Federated Learning With
              {Multi-Dimensional} Private Information",
  author   = "Ding, N and Fang, Z and Huang, J",
  abstract = "As an emerging machine learning technique, federated learning has
              received significant attention recently due to its promising
              performance in mitigating privacy risks and costs. While most of
              the existing work of federated learning focused on designing
              learning algorithm to improve training performance, the incentive
              issue for encouraging users' participation is still
              under-explored. This paper presents an analytical study on the
              server's optimal incentive mechanism design, in the presence of
              users' multi-dimensional private information (e.g., training cost
              and communication delay). Specifically, we consider a
              multi-dimensional contract-theoretic approach, with a key
              contribution of summarizing users' multi-dimensional private
              information into a one-dimensional criterion that allows a
              complete order of users. We further perform the analysis in three
              information scenarios to reveal the impact of information
              asymmetry levels on server's optimal strategy and minimum cost.
              We show that weakly incomplete information does not increase the
              server's cost (comparing with the complete information scenario)
              when training data is IID, but it in general does when data is
              non-IID. Furthermore, the optimal mechanism design under strongly
              incomplete information is much more challenging, and it is not
              always optimal for the server to incentivize the group of users
              with the lowest training cost and delay to participate.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  1,
  pages    = "186--200",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/D/Ding et al. 2021 - Optimal Contract Design for Efficient Federated Learning With Multi-Dimensional Private Information.pdf",
  keywords = "Servers;Training;Collaborative work;Data
              models;Contracts;Delays;Privacy;Federated learning;incentive
              mechanism;multi-dimensional contract;information
              asymmetry;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3036944"
}

@ARTICLE{Nguyen2021-qf,
  title    = "{Fast-Convergent} Federated Learning",
  author   = "Nguyen, H T and Sehwag, V and Hosseinalipour, S and Brinton, C G
              and Chiang, M and Vincent Poor, H",
  abstract = "Federated learning has emerged recently as a promising solution
              for distributing machine learning tasks through modern networks
              of mobile devices. Recent studies have obtained lower bounds on
              the expected decrease in model loss that is achieved through each
              round of federated learning. However, convergence generally
              requires a large number of communication rounds, which induces
              delay in model training and is costly in terms of network
              resources. In this paper, we propose a fast-convergent federated
              learning algorithm, called $\mathsf FOLB$ , which performs
              intelligent sampling of devices in each round of model training
              to optimize the expected convergence speed. We first
              theoretically characterize a lower bound on improvement that can
              be obtained in each round if devices are selected according to
              the expected improvement their local models will provide to the
              current global model. Then, we show that $\mathsf FOLB$ obtains
              this bound through uniform sampling by weighting device updates
              according to their gradient information. $\mathsf FOLB$ is able
              to handle both communication and computation heterogeneity of
              devices by adapting the aggregations according to estimates of
              device's capabilities of contributing to the updates. We evaluate
              $\mathsf FOLB$ in comparison with existing federated learning
              algorithms and experimentally show its improvement in trained
              model accuracy, convergence speed, and/or model stability across
              various machine learning tasks and datasets.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  1,
  pages    = "201--218",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/N/Nguyen et al. 2021 - Fast-Convergent Federated Learning.pdf",
  keywords = "Collaborative work;Convergence;Data models;Computational
              modeling;Servers;Performance evaluation;Optimization;Federated
              learning;distributed optimization;fast convergence
              rate;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3036952"
}

@ARTICLE{Abbasloo2021-xb,
  title    = "Wanna Make Your {TCP} Scheme Great for Cellular Networks? Let
              Machines Do It for You!",
  author   = "Abbasloo, S and Yen, C-Y and Chao, H J",
  abstract = "Can we instead of designing yet another new TCP algorithm, design
              a TCP plug-in that can enable machines to automatically boost the
              performance of the existing/future TCP designs in cellular
              networks? We answer this question by introducing DeepCC. DeepCC
              leverages advanced deep reinforcement learning (DRL) techniques
              to let machines automatically learn how to steer
              throughput-oriented TCP algorithms toward achieving applications'
              desired delays in a highly dynamic network such as the cellular
              network. We used DeepCC plug-in to boost the performance of
              various old and new TCP schemes including TCP Cubic, Google's
              BBR, TCP Westwood, and TCP Illinois in cellular networks. Through
              both extensive trace-based evaluations and real-world
              experiments, we show that not only DeepCC can significantly
              improve the performance of TCP schemes, but also after
              accompanied by DeepCC, these schemes can outperform
              state-of-the-art TCP protocols including new clean-slate machine
              learning-based designs and the ones designed solely for cellular
              networks.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  1,
  pages    = "265--279",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/A/Abbasloo et al. 2021 - Wanna Make Your TCP Scheme Great for Cellular Networks - Let Machines Do It for You!.pdf",
  keywords = "Cellular networks;Delays;Throughput;Reinforcement learning;Task
              analysis;Real-time systems;Kernel;TCP;bufferbloat;congestion
              control;cellular network;deep reinforcement learning;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3036958"
}

@ARTICLE{Prakash2021-lk,
  title    = "Coded Computing for {Low-Latency} Federated Learning Over
              Wireless Edge Networks",
  author   = "Prakash, S and Dhakal, S and Akdeniz, M R and Yona, Y and Talwar,
              S and Avestimehr, S and Himayat, N",
  abstract = "Federated learning enables training a global model from data
              located at the client nodes, without data sharing and moving
              client data to a centralized server. Performance of federated
              learning in a multi-access edge computing (MEC) network suffers
              from slow convergence due to heterogeneity and stochastic
              fluctuations in compute power and communication link qualities
              across clients. We propose a novel coded computing framework,
              CodedFedL, that injects structured coding redundancy into
              federated learning for mitigating stragglers and speeding up the
              training procedure. CodedFedL enables coded computing for
              non-linear federated learning by efficiently exploiting
              distributed kernel embedding via random Fourier features that
              transforms the training task into computationally favourable
              distributed linear regression. Furthermore, clients generate
              local parity datasets by coding over their local datasets, while
              the server combines them to obtain the global parity dataset.
              Gradient from the global parity dataset compensates for
              straggling gradients during training, and thereby speeds up
              convergence. For minimizing the epoch deadline time at the MEC
              server, we provide a tractable approach for finding the amount of
              coding redundancy and the number of local data points that a
              client processes during training, by exploiting the statistical
              properties of compute as well as communication delays. We also
              characterize the leakage in data privacy when clients share their
              local parity datasets with the server. Additionally, we analyze
              the convergence rate and iteration complexity of CodedFedL under
              simplifying assumptions, by treating CodedFedL as a stochastic
              gradient descent algorithm. Finally, for demonstrating gains that
              CodedFedL can achieve in practice, we conduct numerical
              experiments using practical network parameters and benchmark
              datasets, in which CodedFedL speeds up the overall training time
              by up to $15\times $ in comparison to the benchmark schemes.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  1,
  pages    = "233--250",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/P/Prakash et al. 2021 - Coded Computing for Low-Latency Federated Learning Over Wireless Edge Networks.pdf",
  keywords = "Servers;Training;Collaborative work;Encoding;Distributed
              databases;Convergence;Redundancy;Distributed computing;machine
              learning;edge computing;wireless
              communication;GeneralNetworking;Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3036961"
}

@ARTICLE{Ren2021-vi,
  title    = "Accelerating {DNN} Training in Wireless Federated Edge Learning
              Systems",
  author   = "Ren, Jinke and Yu, Guanding and Ding, Guangyao",
  abstract = "Training task in classical machine learning models, such as deep
              neural networks, is generally implemented at a remote cloud
              center for centralized learning, which is typically
              time-consuming and resource-hungry. It also incurs serious
              privacy issue and long communication latency since a large amount
              of data are transmitted to the centralized node. To overcome
              these shortcomings, we consider a newly-emerged framework, namely
              federated edge learning, to aggregate local learning updates at
              the network edge in lieu of users' raw data. Aiming at
              accelerating the training process, we first define a novel
              performance evaluation criterion, called learning efficiency. We
              then formulate a training acceleration optimization problem in
              the CPU scenario, where each user device is equipped with CPU.
              The closed-form expressions for joint batchsize selection and
              communication resource allocation are developed and some
              insightful results are highlighted. Further, we extend our
              learning framework to the GPU scenario. The optimal solution in
              this scenario is manifested to have the similar structure as that
              of the CPU scenario, recommending that our proposed algorithm is
              applicable in more general systems. Finally, extensive
              experiments validate the theoretical analysis and demonstrate
              that the proposed algorithm can reduce the training time and
              improve the learning accuracy simultaneously.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  1,
  pages    = "219--232",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/R/Ren et al. 2021 - Accelerating DNN Training in Wireless Federated Edge Learning Systems.pdf",
  keywords = "Training;Computational modeling;Servers;Acceleration;Resource
              management;Task analysis;Graphics processing units;Federated edge
              learning;learning efficiency;training acceleration;batchsize
              selection;resource allocation;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3036971"
}

@ARTICLE{Song2021-xu,
  title    = "Wireless Distributed Edge Learning: How Many Edge Devices Do We
              Need?",
  author   = "Song, Jaeyoung and Kountouris, Marios",
  abstract = "We consider distributed machine learning at the wireless edge,
              where a parameter server builds a global model with the help of
              multiple wireless edge devices that perform computations on local
              dataset partitions. Edge devices transmit the result of their
              computations (updates of current global model) to the server
              using a fixed rate and orthogonal multiple access over an error
              prone wireless channel. In case of a transmission error, the
              undelivered packet is retransmitted until successfully decoded at
              the receiver. Leveraging on the fundamental tradeoff between
              computation and communication in distributed systems, our aim is
              to derive how many edge devices are needed to minimize the
              average completion time while guaranteeing convergence. We
              provide upper and lower bounds for the average completion and we
              find a necessary condition for adding edge devices in two
              asymptotic regimes, namely the large dataset and the high
              accuracy regime. Conducted experiments on real datasets and
              numerical results confirm our analysis and substantiate our claim
              that the number of edge devices should be carefully selected for
              timely distributed edge learning.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  7,
  pages    = "2120--2134",
  month    =  jul,
  year     =  2021,
  file     = "All Papers/S/Song and Kountouris 2021 - Wireless Distributed Edge Learning - How Many Edge Devices Do We Need.pdf",
  keywords = "Computational modeling;Wireless communication;Servers;Performance
              evaluation;Learning systems;Machine learning;Data
              models;Distributed machine learning;mobile edge computing;timely
              computing;computation-communication tradeoff;distributed
              optimization;ATOS;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3041379"
}

@ARTICLE{Troia2021-ki,
  title    = "On Deep Reinforcement Learning for Traffic Engineering in
              {SD-WAN}",
  author   = "Troia, Sebastian and Sapienza, Federico and Var{\'e}, Leonardo
              and Maier, Guido",
  abstract = "The demand for reliable and efficient Wide Area Networks (WANs)
              from business customers is continuously increasing. Companies and
              enterprises use WANs to exchange critical data between
              headquarters, far-off business branches and cloud data centers.
              Many WANs solutions have been proposed over the years, such as:
              leased lines, Frame Relay, Multi-Protocol Label Switching (MPLS),
              Virtual Private Networks (VPN). Each solution positions
              differently in the trade-off between reliability, Quality of
              Service (QoS) and cost. Today, the emerging technology for WAN is
              Software-Defined Wide Area Networking (SD-WAN) that introduces
              the Software-Defined Networking (SDN) paradigm into the
              enterprise-network market. SD-WAN can support differentiated
              services over public WAN by dynamically reconfiguring in
              real-time network devices at the edge of the network according to
              network measurements and service requirements. On the one hand,
              SD-WAN reduces the high costs of guaranteed QoS WAN solutions (as
              MPLS), without giving away reliability in practical scenarios. On
              the other, it brings numerous technical challenges, such as the
              implementation of Traffic Engineering (TE) methods. TE is
              critically important for enterprises not only to efficiently
              orchestrate network traffic among the edge devices, but also to
              keep their services always available. In this work, we develop
              different kind of TE algorithms with the aim of improving the
              performance of an SD-WAN based network in terms of service
              availability. We first evaluate the performance of baseline TE
              algorithms. Then, we implement different deep Reinforcement
              Learning (deep-RL) algorithms to overcome the limitations of the
              baseline approaches. Specifically, we implement three kinds of
              deep-RL algorithms, which are: policy gradient, TD- $\lambda$ and
              deep Q-learning. Results show that a deep-RL algorithm with a
              well-designed reward function is capable of increasing the
              overall network availability and guaranteeing network protection
              and restoration in SD-WAN.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  7,
  pages    = "2198--2212",
  month    =  jul,
  year     =  2021,
  file     = "All Papers/T/Troia et al. 2021 - On Deep Reinforcement Learning for Traffic Engineering in SD-WAN.pdf",
  keywords = "Wide area networks;Quality of service;Multiprotocol label
              switching;Virtual private networks;Cloud
              computing;Routing;Switches;Software-Defined Networking
              (SDN);Software-Defined Wide Area Network (SD-WAN);deep
              reinforcement learning;Enterprise
              Networking;FutureInternet;Mobile\_Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3041385"
}

@ARTICLE{Zheng2021-tc,
  title    = "Design and Analysis of Uplink and Downlink Communications for
              Federated Learning",
  author   = "Zheng, Sihui and Shen, Cong and Chen, Xiang",
  abstract = "Communication has been known to be one of the primary bottlenecks
              of federated learning (FL), and yet existing studies have not
              addressed the efficient communication design, particularly in
              wireless FL where both uplink and downlink communications have to
              be considered. In this paper, we focus on the design and analysis
              of physical layer quantization and transmission methods for
              wireless FL. We answer the question of what and how to
              communicate between clients and the parameter server and evaluate
              the impact of the various quantization and transmission options
              of the updated model on the learning performance. We provide new
              convergence analysis of the well-known FED AVG under non-i.i.d.
              dataset distributions, partial clients participation, and
              finite-precision quantization in uplink and downlink
              communications. These analyses reveal that, in order to achieve
              an O(1/T) convergence rate with quantization, transmitting the
              weight requires increasing the quantization level at a
              logarithmic rate, while transmitting the weight differential can
              keep a constant quantization level. Comprehensive numerical
              evaluation on various real-world datasets reveals that the
              benefit of a FL-tailored uplink and downlink communication design
              is enormous - a carefully designed quantization and transmission
              achieves more than 98\% of the floating-point baseline accuracy
              with fewer than 10\% of the baseline bandwidth, for majority of
              the experiments on both i.i.d. and non-i.i.d. datasets. In
              particular, 1-bit quantization (3.1\% of the floating-point
              baseline bandwidth) achieves 99.8\% of the floating-point
              baseline accuracy at almost the same convergence rate on MNIST,
              representing the best known bandwidth-accuracy tradeoff to the
              best of the authors' knowledge.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  7,
  pages    = "2150--2167",
  month    =  jul,
  year     =  2021,
  file     = "All Papers/Z/Zheng et al. 2021 - Design and Analysis of Uplink and Downlink Communications for Federated Learning.pdf",
  keywords = "Quantization (signal);Downlink;Uplink;Convergence;Wireless
              communication;Computational modeling;Collaborative work;Wireless
              federated learning;convergence analysis;communication
              design;ATOS;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3041388"
}

@ARTICLE{Phan2021-lp,
  title    = "{DeepMatch}: {Fine-Grained} Traffic Flow Measurement in {SDN}
              With Deep Dueling Neural Networks",
  author   = "Phan, Trung V and Nguyen, Tri Gia and Bauschert, Thomas",
  abstract = "In this paper, we propose a novel flow rule matching framework,
              DeepMatch, in Software-Defined Networking (SDN) to provide a
              fine-grained traffic flow measurement capability. Specifically,
              the flow rule matching control at a particular SDN switch is
              examined to maximize the traffic flow granularity degree while
              proactively protecting the flow-table in the switch from being
              overflowed. This control process is supervised by a control
              module referred to as DeepMatch instance. Regarding this
              instance, an optimization problem is formulated based on a Markov
              decision process (MDP) and a Partially Observable Markov decision
              process (POMDP), respectively. We develop a deep dueling neural
              network based flow rule matching control algorithm to solve the
              optimization problem, thereby quickly attaining a significant
              traffic flow granularity level and eliminating the switch
              flow-table overflow problem. Furthermore, we propose an
              experience data sharing (EDS) mechanism that enables a new
              instance to learn faster about the flow rule matching control.
              The results of our performance evaluation show that, by applying
              the DeepMatch framework in a highly dynamic traffic scenario, the
              traffic flow granularity degree at the access and the core
              switches increases by 24.0\% and 31.63\%, respectively, compared
              to the FlowStat method. DeepMatch is also highly outperforming
              the ReWiFlow, SDN-Mon, and Exact-Match approaches. In addition,
              by employing the EDS mechanism, a new instance can reduce its
              learning time up to 46.42\% for supervising an access switch and
              up to 37.50\% for supervising a core switch.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  7,
  pages    = "2056--2075",
  month    =  jul,
  year     =  2021,
  keywords = "Switches;Control systems;Process control;Monitoring;Markov
              processes;Optimization;Neural networks;Traffic flow
              measurement;Markov decision process;partially observable Markov
              decision process;deep reinforcement learning;software-defined
              networking;MLAspects",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2020.3041406"
}

@ARTICLE{Saurav2021-hz,
  title    = "Game of Ages in a Distributed Network",
  author   = "Saurav, Kumar and Vaze, Rahul",
  abstract = "We consider a distributed IoT network, where each node wants to
              minimize its own age of information and there is a cost to make
              any transmission. A collision model is considered, where any
              transmission is successful from a node to a common monitor if no
              other node transmits in the same slot. Nodes cannot coordinate
              their transmission, and can learn about the network only via
              binary collision information. Under this distributed competition
              model, the objective of this paper is to find a distributed
              transmission strategy for each node that converges to an
              equilibrium that only depends on the past observations seen by
              each node and does not require network information, e.g., the
              number of other nodes, or their strategies. A simple update
              strategy is shown to converge to an equilibrium for any number of
              nodes that are unknown to the update strategy. The equilibrium
              achieved is in fact a Nash equilibrium for a suitable utility
              function, that captures all the right tradeoffs for each node.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  5,
  pages    = "1240--1249",
  month    =  may,
  year     =  2021,
  file     = "All Papers/S/Saurav and Vaze 2021 - Game of Ages in a Distributed Network.pdf",
  keywords = "Monitoring;Games;Information age;Throughput;Probabilistic
              logic;Nash equilibrium;History;Age of information;distributed
              equilibrium;game theory;Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3065053"
}

@ARTICLE{Yates2021-ko,
  title    = "Age of Information: An Introduction and Survey",
  author   = "Yates, Roy D and Sun, Yin and Brown, D Richard and Kaul, Sanjit K
              and Modiano, Eytan and Ulukus, Sennur",
  abstract = "We summarize recent contributions in the broad area of age of
              information (AoI). In particular, we describe the current state
              of the art in the design and optimization of low-latency
              cyberphysical systems and applications in which sources send
              time-stamped status updates to interested recipients. These
              applications desire status updates at the recipients to be as
              timely as possible; however, this is typically constrained by
              limited system resources. We describe AoI timeliness metrics and
              present general methods of AoI evaluation analysis that are
              applicable to a wide variety of sources and systems. Starting
              from elementary single-server queues, we apply these AoI methods
              to a range of increasingly complex systems, including energy
              harvesting sensors transmitting over noisy channels, parallel
              server systems, queueing networks, and various single-hop and
              multi-hop wireless networks. We also explore how update age is
              related to MMSE methods of sampling, estimation and control of
              stochastic processes. The paper concludes with a review of
              efforts to employ age optimization in cyberphysical applications.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  5,
  pages    = "1183--1210",
  month    =  may,
  year     =  2021,
  file     = "All Papers/Y/Yates et al. 2021 - Age of Information - An Introduction and Survey.pdf",
  keywords = "Monitoring;Measurement;Queueing analysis;Sun;Cyber-physical
              systems;Wireless networks;Surgery;Age of information
              (AoI);queueing systems;communication networks;timely source
              coding;information freshness;selective encoding;wireless
              communication;time measurement;packet delay;age-delay
              tradeoff;age-energy tradeoff;non-linear age penalty;information
              update system;Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3065072"
}

@ARTICLE{Lemic2021-ba,
  title    = "Survey on Terahertz Nanocommunication and Networking: A
              {Top-Down} Perspective",
  author   = "Lemic, Filip and Abadal, Sergi and Tavernier, Wouter and
              Stroobant, Pieter and Colle, Didier and Alarc{\'o}n, Eduard and
              Marquez-Barja, Johann and Famaey, Jeroen",
  abstract = "Recent developments in nanotechnology herald nanometer-sized
              devices expected to bring light to a number of groundbreaking
              applications. Communication with and among nanodevices will be
              needed for unlocking the full potential of such applications. As
              the traditional communication approaches cannot be directly
              applied in nanocommunication, several alternative paradigms have
              emerged. Among them, electromagnetic nanocommunication in the
              terahertz (THz) frequency band is particularly promising, mainly
              due to the breakthrough of novel materials such as graphene. For
              this reason, numerous research efforts are nowadays targeting THz
              band nanocommunication and consequently nanonetworking. As it is
              expected that these trends will continue in the future, we see it
              beneficial to summarize the current status in these research
              domains. In this survey, we therefore aim to provide an overview
              of the current THz nanocommunication and nanonetworking research.
              Specifically, we discuss the applications envisioned to be
              supported by nanonetworks operating in the THz band, together
              with the requirements such applications pose on the underlying
              nanonetworks. Subsequently, we provide an overview of the current
              contributions on the different layers of the protocol stack, as
              well as the available channel models and experimentation tools.
              Finally, we identify a number of open research challenges and
              outline several future research directions.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  6,
  pages    = "1506--1543",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/L/Lemic et al. 2021 - Survey on Terahertz Nanocommunication and Networking - A Top-Down Perspective.pdf",
  keywords = "Nanocommunication (telecommunication);Nanoscale
              devices;Electromagnetics;Protocols;Graphene;Tools;Channel
              models;Nanotechnology;electromagnetic;terahertz;nanocommunication;nanonetworking;protocols;channel
              models;experimentation tools;Wireless;Mobile\_Wireless",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3071837"
}

@ARTICLE{Gu2021-yj,
  title    = "{Knowledge-Assisted} Deep Reinforcement Learning in {5G}
              Scheduler Design: From Theoretical Framework to Implementation",
  author   = "Gu, Zhouyou and She, Changyang and Hardjawana, Wibowo and Lumb,
              Simon and McKechnie, David and Essery, Todd and Vucetic, Branka",
  abstract = "In this paper, we develop a knowledge-assisted deep reinforcement
              learning (DRL) algorithm to design wireless schedulers in the
              fifth-generation (5G) cellular networks with time-sensitive
              traffic. Since the scheduling policy is a deterministic mapping
              from channel and queue states to scheduling actions, it can be
              optimized by using deep deterministic policy gradient (DDPG). We
              show that a straightforward implementation of DDPG converges
              slowly, has a poor quality-of-service (QoS) performance, and
              cannot be implemented in real-world 5G systems, which are
              non-stationary in general. To address these issues, we propose a
              theoretical DRL framework, where theoretical models from wireless
              communications are used to formulate a Markov decision process in
              DRL. To reduce the convergence time and improve the QoS of each
              user, we design a knowledge-assisted DDPG (K-DDPG) that exploits
              expert knowledge of the scheduler design problem, such as the
              knowledge of the QoS, the target scheduling policy, and the
              importance of each training sample, determined by the
              approximation error of the value function and the number of
              packet losses. Furthermore, we develop an architecture for online
              training and inference, where K-DDPG initializes the scheduler
              off-line and then fine-tunes the scheduler online to handle the
              mismatch between off-line simulations and non-stationary
              real-world systems. Simulation results show that our approach
              reduces the convergence time of DDPG significantly and achieves
              better QoS than existing schedulers (reducing 30\% 50\% packet
              losses). Experimental results show that with off-line
              initialization, our approach achieves better initial QoS than
              random initialization and the online fine-tuning converges in few
              minutes.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  7,
  pages    = "2014--2028",
  month    =  jul,
  year     =  2021,
  file     = "All Papers/G/Gu et al. 2021 - Knowledge-Assisted Deep Reinforcement Learning in 5G Scheduler Design - From Theoretical Framework to Implementation.pdf",
  keywords = "5G mobile communication;Wireless communication;Quality of
              service;Training;Computer architecture;Reinforcement
              learning;Delays;Deep reinforcement learning;wireless scheduler
              design;time-sensitive traffic;online
              implementation;ML4Net;Mobile\_Wireless;MLAspects",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3078498"
}

@ARTICLE{Huang2021-vp,
  title    = "Scalable Orchestration of Service Function Chains in
              {NFV-Enabled} Networks: A Federated Reinforcement Learning
              Approach",
  author   = "Huang, Haojun and Zeng, Cheng and Zhao, Yangmin and Min, Geyong
              and Zhu, Yingying and Miao, Wang and Hu, Jia",
  abstract = "Network function virtualization (NFV) is critical to the
              scalability and flexibility of various network services in the
              form of service function chains (SFCs), which refer to a set of
              Virtual Network Functions (VNFs) chained in a specific order.
              However, the NFV performance is hard to fulfill the
              ever-increasing requirements of network services mainly due to
              the static orchestrations of SFCs. To tackle this issue, a novel
              Scalable SFC Orchestration (SSCO) scheme is proposed in this
              paper for NFV-enabled networks via federated reinforcement
              learning. SSCO has three remarkable characteristics
              distinguishing from the previous work: (1) A
              federated-learning-based framework is designed to train a global
              learning model, with time-variant local model explorations, for
              scalable SFC orchestration, while avoiding data sharing among
              stakeholders; (2) SSCO allows for parameter update among local
              clients and the cloud server just at the first and last epochs of
              each episode to ensure that distributed clients can make model
              optimization at a low communication cost; (3) SSCO introduces an
              efficient deep reinforcement learning (DRL) approach, with the
              local learning knowledge of available resources and instantiation
              cost, to map VNFs into networks flexibly. Furthermore, a
              loss-weight-based mechanism is proposed to generate and exploit
              reference samples in replay buffers for future training, avoiding
              the strong relevance of samples. Simulation results obtained from
              different working scenarios demonstrate that SSCO can
              significantly reduce placement errors and improve resource
              utilization ratio to place time-variant VNFs compared with the
              state-of-the-art mechanisms. Furthermore, the results show that
              the proposed approach can achieve desirable scalability.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  8,
  pages    = "2558--2571",
  month    =  aug,
  year     =  2021,
  keywords = "Training;Servers;Reinforcement learning;Micromechanical
              devices;Data models;Optimization;Bandwidth;Network function
              virtualization;service function chains;federated learning;deep
              reinforcement learning;resource allocation;NFV",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3087227"
}

@ARTICLE{Sangdeh2021-gj,
  title    = "{DeepMux}: {Deep-Learning-Based} Channel Sounding and Resource
              Allocation for {IEEE} 802.11ax",
  author   = "Sangdeh, Pedram Kheirkhah and Zeng, Huacheng",
  abstract = "MU-MIMO and OFDMA are two key techniques in IEEE 802.11ax
              standard. Although these two techniques have been intensively
              studied in cellular networks, their joint optimization in Wi-Fi
              networks has been rarely explored as OFDMA was introduced to
              Wi-Fi networks for the first time in 802.11ax. The marriage of
              these two techniques in Wi-Fi networks creates both opportunities
              and challenges in the practical design of MAC-layer protocols and
              algorithms to optimize airtime overhead, spectral efficiency, and
              computational complexity. In this paper, we present DeepMux, a
              deep-learning-based MU-MIMO-OFDMA transmission scheme for
              802.11ax networks. DeepMux mainly comprises two components:
              deep-learning-based channel sounding (DLCS) and
              deep-learning-based resource allocation (DLRA), both of which
              reside in access points (APs) and impose no
              computational/communication burden on Wi-Fi clients. DLCS reduces
              the airtime overhead of 802.11 protocols by leveraging the deep
              neural networks (DNNs). It uses uplink channels to train the DNNs
              for downlink channels, making the training process easy to
              implement. DLRA employs a DNN to solve the mixed-integer resource
              allocation problem, enabling an AP to obtain a near-optimal
              solution in polynomial time. We have built a wireless testbed to
              examine the performance of DeepMux in real-world environments.
              Our experimental results show that DeepMux reduces the sounding
              overhead by 62.0\% 90.5\% and increases the network throughput by
              26.3\% 43.6\%.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  8,
  pages    = "2333--2346",
  month    =  aug,
  year     =  2021,
  keywords = "Wireless communication;Training;Protocols;IEEE 802.11ax
              Standard;Downlink;Throughput;Resource management;IEEE
              802.11ax;machine learning;deep neural network;Wi-Fi;multi-user
              MIMO;OFDMA;channel sounding;resource
              allocation;Mobile\_Wireless;wifi",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3087246"
}

@ARTICLE{Li2021-yl,
  title    = "{TCP-NeuRoc}: Neural Adaptive {TCP} Congestion Control With
              Online Changepoint Detection",
  author   = "Li, Wenzhong and Gao, Shaohua and Li, Xiang and Xu, Yeting and
              Lu, Sanglu",
  abstract = "Congestion control is a fundamental mechanism for TCP protocol,
              which has been extensively studied in the past three decades.
              However, our experimental evaluations show that the state-of-art
              congestion control algorithms such as Cubic and BBR are far from
              optimal: they have unresolved issues such as insufficient usage
              of available bandwidth, inadaptable to dynamic bandwidth
              variants, and compromising on one or more performance dimensions.
              To address these challenges, we propose a novel congestion
              control mechanism called NeuRoc that coordinately uses online
              changepoint detection and deep reinforcement learning (DRL)
              technique to generate the optimal congestion control policy,
              which allows TCP operating at Kleinrock 's optimal operation
              point to achieve fully bandwidth usage and low latency. To
              address the practical issues of deploying the deep learning based
              congestion control mechanism, we propose a cold-started training
              and deployment framework to reduce the cost of bootstrap. We
              implement NeuRoc on an emulation platform which connects to the
              Linux network protocol stack through virtual network interfaces.
              Extensive experiments show that NeuRoc achieves the best
              throughput-latency tradeoff compared with the state-of-the-arts
              in a variety of scenarios.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  8,
  pages    = "2461--2475",
  month    =  aug,
  year     =  2021,
  keywords = "Bandwidth;Heuristic algorithms;Throughput;Protocols;Reinforcement
              learning;Neural networks;Delays;Congestion control;deep
              reinforcement learning;online changepoint
              detection;CongestionControl",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3087247"
}

@ARTICLE{Tung2021-qm,
  title    = "Effective Communications: A Joint Learning and Communication
              Framework for {Multi-Agent} Reinforcement Learning Over Noisy
              Channels",
  author   = "Tung, Tze-Yang and Kobus, Szymon and Roig, Joan Pujol and
              G{\"u}nd{\"u}z, Deniz",
  abstract = "We propose a novel formulation of the ``effectiveness problem''
              in communications, put forth by Shannon and Weaver in their
              seminal work ``The Mathematical Theory of Communication'', by
              considering multiple agents communicating over a noisy channel in
              order to achieve better coordination and cooperation in a
              multi-agent reinforcement learning (MARL) framework.
              Specifically, we consider a multi-agent partially observable
              Markov decision process (MA-POMDP), in which the agents, in
              addition to interacting with the environment, can also
              communicate with each other over a noisy communication channel.
              The noisy communication channel is considered explicitly as part
              of the dynamics of the environment, and the message each agent
              sends is part of the action that the agent can take. As a result,
              the agents learn not only to collaborate with each other but also
              to communicate ``effectively'' over a noisy channel. This
              framework generalizes both the traditional communication problem,
              where the main goal is to convey a message reliably over a noisy
              channel, and the ``learning to communicate'' framework that has
              received recent attention in the MARL literature, where the
              underlying communication channels are assumed to be error-free.
              We show via examples that the joint policy learned using the
              proposed framework is superior to that where the communication is
              considered separately from the underlying MA-POMDP. This is a
              very powerful framework, which has many real world applications,
              from autonomous vehicle planning to drone swarm control, and
              opens up the rich toolbox of deep reinforcement learning for the
              design of multi-user communication systems.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  8,
  pages    = "2590--2603",
  month    =  aug,
  year     =  2021,
  file     = "All Papers/T/Tung et al. 2021 - Effective Communications - A Joint Learning and Commu ... Framework for Multi-Agent Reinforcement Learning Over Noisy Channels.pdf",
  keywords = "Noise measurement;Protocols;Channel
              coding;Semantics;Reinforcement learning;Modulation;Wireless
              communication;Learning to communicate;reinforcement learning
              (RL);multi-agent systems;joint source-channel coding;error
              correction coding;Wireless;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3087248"
}

@ARTICLE{Akbari2021-cy,
  title    = "Age of Information Aware {VNF} Scheduling in Industrial {IoT}
              Using Deep Reinforcement Learning",
  author   = "Akbari, Mohammad and Abedi, Mohammad Reza and Joda, Roghayeh and
              Pourghasemian, Mohsen and Mokari, Nader and Erol-Kantarci, Melike",
  abstract = "In delay-sensitive industrial Internet of Things (IIoT)
              applications, the age of information (AoI) is employed to
              characterize the freshness of information. Meanwhile, the
              emerging network function virtualization provides flexibility and
              agility for service providers to deliver a given network service
              using a sequence of virtual network functions (VNFs). However,
              suitable VNF placement and scheduling in these schemes is NP-hard
              and finding a globally optimal solution by traditional approaches
              is complex. Recently, deep reinforcement learning (DRL) has
              appeared as a viable way to solve such problems. In this paper,
              we first utilize single agent low-complex compound action
              actor-critic RL to cover both discrete and continuous actions and
              jointly minimize VNF cost and AoI in terms of network resources
              under end-to-end Quality of Service constraints. To surmount the
              single-agent capacity limitation for learning, we then extend our
              solution to a multi-agent DRL scheme in which agents collaborate
              with each other. Simulation results demonstrate that single-agent
              schemes significantly outperform the greedy algorithm in terms of
              average network cost and AoI. Moreover, multi-agent solution
              decreases the average cost by dividing the tasks between the
              agents. However, it needs more iterations to be learned due to
              the requirement on the agents' collaboration.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  39,
  number   =  8,
  pages    = "2487--2500",
  month    =  aug,
  year     =  2021,
  file     = "All Papers/A/Akbari et al. 2021 - Age of Information Aware VNF Scheduling in Industrial IoT Using Deep Reinforcement Learning.pdf",
  keywords = "Industrial Internet of Things;Delays;Measurement;Information
              age;Reinforcement learning;Quality of service;Resource
              management;Industrial Internet of Things;network function
              virtualization;age of information;deep reinforcement
              learning;compound actions;multi-agent;NFV",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3087264"
}

@ARTICLE{Letaief2022-kl,
  title    = "Edge Artificial Intelligence for 6G: Vision, Enabling
              Technologies, and Applications",
  author   = "Letaief, Khaled B and Shi, Yuanming and Lu, Jianmin and Lu,
              Jianhua",
  abstract = "The thriving of artificial intelligence (AI) applications is
              driving the further evolution of wireless networks. It has been
              envisioned that 6G will be transformative and will revolutionize
              the evolution of wireless from ``connected things'' to
              ``connected intelligence''. However, state-of-the-art deep
              learning and big data analytics based AI systems require
              tremendous computation and communication resources, causing
              significant latency, energy consumption, network congestion, and
              privacy leakage in both of the training and inference processes.
              By embedding model training and inference capabilities into the
              network edge, edge AI stands out as a disruptive technology for
              6G to seamlessly integrate sensing, communication, computation,
              and intelligence, thereby improving the efficiency,
              effectiveness, privacy, and security of 6G networks. In this
              paper, we shall provide our vision for scalable and trustworthy
              edge AI systems with integrated design of wireless communication
              strategies and decentralized machine learning models. New design
              principles of wireless networks, service-driven resource
              allocation optimization methods, as well as a holistic end-to-end
              system architecture to support edge AI will be described.
              Standardization, software and hardware platforms, and application
              scenarios are also discussed to facilitate the industrialization
              and commercialization of edge AI systems.",
  journal  = "IEEE J. Sel. Areas Commun.",
  volume   =  40,
  number   =  1,
  pages    = "5--36",
  month    =  jan,
  year     =  2022,
  file     = "All Papers/L/Letaief et al. 2022 - Edge Artificial Intelligence for 6G - Vision, Enabling Technologies, and Applications.pdf",
  keywords = "Artificial intelligence;6G mobile communication;Task
              analysis;Sensors;Communication system
              security;Training;Standards;6G;edge AI;edge training;edge
              inference;federated learning;over-the-air
              computation;task-oriented communication;service-driven resource
              allocation;large-scale optimization;end-to-end
              architecture;ATOS;MLNetworking",
  issn     = "1558-0008",
  doi      = "10.1109/JSAC.2021.3126076"
}

@ARTICLE{Ding2015-xb,
  title    = "Cooperative {Non-Orthogonal} Multiple Access in {5G} Systems",
  author   = "Ding, Z and Peng, M and Poor, H V",
  abstract = "Non-orthogonal multiple access (NOMA) has received considerable
              recent attention as a promising candidate for 5G systems. A key
              feature of NOMA is that users with better channel conditions have
              prior information about the messages of other users. This prior
              knowledge is fully exploited in this letter, where a cooperative
              NOMA scheme is proposed. The outage probability and diversity
              order achieved by this cooperative NOMA scheme are analyzed, and
              an approach based on user pairing is also proposed to reduce
              system complexity.",
  journal  = "IEEE Commun. Lett.",
  volume   =  19,
  number   =  8,
  pages    = "1462--1465",
  month    =  aug,
  year     =  2015,
  file     = "All Papers/D/Ding et al. 2015 - Cooperative Non-Orthogonal Multiple Access in 5G Systems.pdf",
  keywords = "5G mobile communication;cooperative communication;diversity
              reception;multi-access systems;probability;telecommunication
              network reliability;wireless channels;cooperative nonorthogonal
              multiple access system;5G system;cooperative NOMA scheme;outage
              probability;diversity order;system complexity reduction;Signal to
              noise ratio;Interference;Diversity reception;Reliability;Base
              stations;Resource management;Complexity theory;Non-orthogonal
              multiple access (NOMA);cooperative multiple access;5G
              communications;Non-orthogonal multiple access (NOMA);cooperative
              multiple access;5G communications;Mobile\_Wireless",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2015.2441064"
}

@ARTICLE{Pham2018-ta,
  title    = "Virtual Network Function Scheduling: A Matching Game Approach",
  author   = "Pham, C and Tran, N H and Hong, C S",
  abstract = "Network function virtualization is a promising technique for
              telecom providers to efficiently manage network services at low
              cost. However, existing works mainly focus on resource allocation
              and thus leave behind an important issue: the virtual network
              function (VNF) scheduling. Current approaches, e.g., round-robin
              scheduling or heuristic algorithms, still expose some unsolved
              issues, such as high computational cost and inability to perform
              online scheduling. In this letter, we propose a matching-based
              algorithm to solve the NP-hard VNF scheduling problem. This
              approach can guarantee a stable scheduling, in which all network
              services are satisfied with the assignment. Finally, the
              effectiveness of our method is verified through numerical
              evaluation, showing that our approach can increase the number of
              completed VNFs by 36.8\% compared with the current round-robin
              method.",
  journal  = "IEEE Commun. Lett.",
  volume   =  22,
  number   =  1,
  pages    = "69--72",
  month    =  jan,
  year     =  2018,
  file     = "All Papers/P/Pham et al. 2018 - Virtual Network Function Scheduling - A Matching Game Approach.pdf",
  keywords = "game theory;resource allocation;telecommunication
              scheduling;virtualisation;virtual network function
              scheduling;matching game approach;network function
              virtualization;telecom providers;network services;resource
              allocation;round-robin scheduling;heuristic algorithms;online
              scheduling;NP-hard VNF scheduling problem;stable
              scheduling;current round-robin method;NP;Processor
              scheduling;Resource management;Schedules;Games;Virtualization;Job
              shop scheduling;Network function virtualization;service chain",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2017.2747509"
}

@ARTICLE{Ahmed2020-vi,
  title    = "Periodic Traffic Scheduling for {IEEE} 802.11ah Networks",
  author   = "Ahmed, Nurzaman and Hussain, Md Iftekhar",
  abstract = "In this letter, we propose a novel channel access mechanism for
              IEEE 802.11ah to reduce delay and energy consumption in
              large-scale networks. The proposed protocol predicts the service
              interval of a monitoring application and schedules the subsequent
              frames before their arrivals without requiring any further
              contention. By doing so, the proposed protocol significantly
              reduces access delay and unnecessary wake-ups. In saturated
              network conditions, the proposed protocol shows throughput
              improvement up to 25\%, and reduces average packet latency and
              energy consumption up to 55.5\% and 48.4\% respectively as
              compared to the traditional scheme.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  7,
  pages    = "1510--1513",
  month    =  jul,
  year     =  2020,
  file     = "All Papers/A/Ahmed and Hussain 2020 - Periodic Traffic Scheduling for IEEE 802.11ah Networks.pdf",
  keywords = "Schedules;Delays;Monitoring;Media Access Protocol;Resource
              management;Energy consumption;Internet of Things (IoT);IEEE
              802.11ah;Restricted Access Window (RAW);Periodic Traffic
              Scheduling;Wireless",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.2981087"
}

@ARTICLE{Mukherjee2020-fz,
  title    = "Revenue Maximization in {Delay-Aware} Computation Offloading
              Among Service Providers With Fog Federation",
  author   = "Mukherjee, M and Kumar, V and Lloret, J and Zhang, Q",
  abstract = "In this letter, we study the computational offloading scheme for
              the delay-aware tasks of the end-users in the fog computing
              network. We consider a fog federation of different service
              providers where an individual fog node allocates its computing
              resources to the end-user in its proximity, while a fog manager
              coordinates the load balancing among the fog nodes over the
              entire network. At first, an individual fog node aims to maximize
              its revenue by selling the computational resources to the
              end-user in a distributed manner without any global knowledge of
              the network. To further maximize the overall revenue considering
              all fog nodes in the fog federation, the fog manager utilizes the
              remaining computing resources of the underloaded fog nodes. The
              extensive simulation results show the revenue improvement
              leveraging fog federation over entire network while maintaining
              the same and even better delay-performance for the end-users.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  8,
  pages    = "1799--1803",
  month    =  aug,
  year     =  2020,
  keywords = "distributed processing;financial management;resource
              allocation;revenue maximization;delay-aware computation
              offloading;fog federation;fog computing network;computational
              resources;underloaded fog nodes;service providers;load
              balancing;Task analysis;Delays;Computational
              modeling;Computational efficiency;Pricing;Frequency
              modulation;Collaboration;Fog computing;fog
              federation;pricing;computational offloading;delay-sensitive tasks",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.2992781"
}

@ARTICLE{Cai2020-ky,
  title    = "Feedback Strategies for Online Fountain Codes With Limited
              Feedback",
  author   = "Cai, P and Zhang, Y and Wu, Y and Chang, X and Pan, C",
  abstract = "The performance of conventional online fountain codes is feedback
              dependent. In this letter, an analysis of online fountain codes
              is conducted to assess their performance under limited feedback
              conditions. To obtain a feedback point selection scheme with
              approximately optimal overhead performance when the number of
              feedbacks is fixed, two heuristic table-look-up strategies are
              proposed. The simulation results confirm the effectiveness of the
              proposed strategies and verify that the online fountain codes are
              capable of achieving a nearly comparable full-feedback
              performance with only partial feedback.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  9,
  pages    = "1870--1874",
  month    =  sep,
  year     =  2020,
  keywords = "decoding;feedback;graph theory;optimisation;random codes;random
              processes;feedback strategies;conventional online fountain
              codes;limited feedback conditions;feedback point selection
              scheme;approximately optimal overhead performance;comparable
              full-feedback performance;partial feedback;Decoding;Frequency
              modulation;Receivers;Encoding;Transmitters;Probability;Upper
              bound;Online fountain codes;feedback;overhead analysis",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.2997683"
}

@ARTICLE{Wang2020-sr,
  title    = "Joint Optimization of Transmission Bandwidth Allocation and Data
              Compression for {Mobile-Edge} Computing Systems",
  author   = "Wang, J-B and Zhang, J and Ding, C and Zhang, H and Lin, M and
              Wang, J",
  abstract = "This letter investigates a multiuser mobile-edge computing (MEC)
              system with data compression technique to reduce the redundancy
              of sensed data, save the energy consumption and reduce the
              latency for wireless transmission. In this regard, the problem of
              minimizing the total energy consumption by jointly optimizing the
              transmission bandwidth allocation and data compression ratio is
              firstly considered under the constraints of latency and limited
              computation resource. Moreover, Lagragian and iterative-based
              algorithms are proposed to solve the non-convex optimization
              problem. Finally, simulation results are shown to verify the
              efficiency of the proposed algorithms and demonstrate that the
              application of data compression technique into MEC system can
              effectively reduce the energy consumption and latency.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  10,
  pages    = "2245--2249",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/W/Wang et al. 2020 - Joint Optimization of Transmission Bandwidth Allocation and Data Compression for Mobile-Edge Computing Systems.pdf",
  keywords = "bandwidth allocation;concave programming;convex programming;data
              compression;iterative methods;mobile computing;Lagragian
              algorithms;iterative-based algorithms;MEC system;nonconvex
              optimization problem;limited computation resource;data
              compression ratio;total energy consumption;wireless
              transmission;sensed data;data compression technique;mobile-edge
              computing system;transmission bandwidth allocation;joint
              optimization;Energy consumption;Data compression;Channel
              allocation;Optimization;Convex functions;Servers;Wireless
              communication;Mobile-edge computing;data compression;energy
              efficient;bandwidth allocation;Done;EdgeFogCloudIoT",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.2998474"
}

@ARTICLE{Liu2020-qd,
  title    = "A Novel Approach for Service Function Chain Dynamic Orchestration
              in Edge Clouds",
  author   = "Liu, Y and Lu, H and Li, X and Zhao, D and Wu, W and Lu, G",
  abstract = "Network function virtualization (NFV) and mobile edge computing
              (MEC) are two of the promising technologies that are expected to
              play a critical role in mobile edge cloud networks, thus
              satisfying ambitious quality of experience (QoE) requirements of
              the Internet of things (IoT) applications. In MEC-NFV system, a
              service function chain (SFC) consists of an ordered set of
              virtual network functions (VNFs) that are connected based on the
              business logic of service providers. However, the inefficiency of
              the SFC orchestration process is one major problem due to the
              dynamic nature of mobile edge cloud networks and abundance of IoT
              terminals. In this letter, a quantum machine learning (QML)-based
              scheme is proposed as solution that can handle complex and
              dynamic SFC orchestration in mobile edge cloud networks.
              Simulation results show that our proposal significantly provides
              more than 8-fold reduction of run time compared to the Viterbi
              algorithm, and the end-to-end delay is only about 1.1 times of
              the exact solution.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  10,
  pages    = "2231--2235",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/L/Liu et al. 2020 - A Novel Approach for Service Function Chain Dynamic Orchestration in Edge Clouds.pdf",
  keywords = "cloud computing;Internet of Things;learning (artificial
              intelligence);mobile computing;quality of
              experience;virtualisation;end-to-end delay;Viterbi algorithm;IoT
              terminals;dynamic SFC orchestration;complex SFC orchestration;SFC
              orchestration process;virtual network functions;MEC-NFV
              system;mobile edge cloud networks;mobile edge computing;service
              function chain dynamic orchestration;Cloud computing;Hidden
              Markov models;Internet of Things;Delays;Quality of
              experience;Heuristic algorithms;Network function virtualization
              (NFV);mobile edge computing (MEC);service function chain
              (SFC);quality of experience (QoE);quantum machine learning
              (QML);EdgeFogCloudIoT;NFV",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3000588"
}

@ARTICLE{Li2020-ve,
  title    = "The {LSTM-Based} Advantage {Actor-Critic} Learning for Resource
              Management in Network Slicing With User Mobility",
  author   = "Li, R and Wang, C and Zhao, Z and Guo, R and Zhang, H",
  abstract = "Network slicing aims to efficiently provision diversified
              services with distinct requirements over the same physical
              infrastructure. Therein, in order to efficiently allocate
              resources across slices, demand-aware inter-slice resource
              management is of significant importance. In this letter, we
              consider a scenario that contains several slices in a radio
              access network with base stations that share the same physical
              resources (e.g., bandwidth or slots). We primarily leverage
              advantage actor-critic (A2C), one typical deep reinforcement
              learning (DRL) algorithm, to solve this problem by considering
              the varying service demands as the environment state and the
              allocated resources as the environment action. However, given
              that the user mobility toughens the difficulty to perceive the
              environment, we further incorporate the long short-term memory
              (LSTM) into A2C, and put forward an LSTM-A2C algorithm to track
              the user mobility and improve the system utility. We verify the
              performance of the proposed LSTM-A2C through extensive
              simulations.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  9,
  pages    = "2005--2009",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/L/Li et al. 2020 - The LSTM-Based Advantage Actor-Critic Learning for Resource Management in Network Slicing With User Mobility.pdf",
  keywords = "learning (artificial intelligence);mobility management (mobile
              radio);neural nets;radio access networks;resource
              allocation;telecommunication computing;user mobility;LSTM-A2C
              algorithm;LSTM-based advantage actor-critic learning;demand-aware
              inter-slice resource management;radio access network;deep
              reinforcement learning algorithm;network slicing resource
              management;demand-aware interslice resource management;long
              short-term memory;Resource management;Bandwidth;Feature
              extraction;Network slicing;Heuristic algorithms;Base
              stations;Reinforcement learning;network slicing;deep
              reinforcement learning;long short-term memory (LSTM);advantage
              actor critic (A2C);user mobility;MLNetworking",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3001227"
}

@ARTICLE{Psomas2020-vw,
  title    = "Wireless Powered Mobile Edge Computing: Offloading Or Local
              Computation?",
  author   = "Psomas, C and Krikidis, I",
  abstract = "Mobile-edge computing (MEC) and wireless power transfer are
              technologies that can assist in the implementation of next
              generation wireless networks, which will deploy a large number of
              computational and energy limited devices. In this letter, we
              consider a point-to-point MEC system, where the device harvests
              energy from the access point's (AP's) transmitted signal to power
              the offloading and/or the local computation of a task. By taking
              into account the non-linearities of energy harvesting, we provide
              analytical expressions for the probability of successful
              computation and for the average number of successfully computed
              bits. Our results show that a hybrid scheme of partial offloading
              and local computation is not always efficient. In particular, the
              decision to offload and/or compute locally, depends on the
              system's parameters such as the distance to the AP and the number
              of bits that need to be computed.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  11,
  pages    = "2642--2646",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/P/Psomas and Krikidis 2020 - Wireless Powered Mobile Edge Computing - Offloading Or Local Computation.pdf",
  keywords = "Task analysis;Wireless communication;Fading
              channels;Servers;Energy harvesting;Downlink;Uplink;Mobile edge
              computing;wireless power transfer;non-linear energy
              harvesting;EdgeFogCloudIoT",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3012102"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ding2020-kz,
  title    = "Unveiling the Importance of {SIC} in {NOMA} {Systems---Part}
              {II}: New Results and Future Directions",
  author   = "Ding, Z and Schober, R and Poor, H V",
  abstract = "In most existing works on non-orthogonal multiple access (NOMA),
              the decoding order of successive interference cancellation (SIC)
              is prefixed and based on either the users' channel conditions or
              their quality of service (QoS) requirements. A recent work on
              NOMA assisted semi-grant-free transmission showed that the use of
              a more sophisticated hybrid SIC scheme can yield significant
              performance improvements. This letter illustrates how the concept
              of hybrid SIC can be generalized and applied to different NOMA
              applications. We first use NOMA assisted mobile edge computing
              (MEC) as an example to illustrate the benefits of hybrid SIC,
              where new results for offloading energy minimization are
              presented. Then, future directions for generalizing hybrid SIC
              with adaptive decoding order selection as well as its promising
              applications are discussed.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  11,
  pages    = "2378--2382",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/D/Ding et al. 2020 - Unveiling the Importance of SIC in NOMA Systems—Part II - New Results and Future Directions.pdf",
  keywords = "Silicon carbide;NOMA;Task analysis;Quality of
              service;Decoding;Minimization;Energy consumption;Non-orthogonal
              multiple access (NOMA);hybrid successive interference
              cancellation (SIC);mobile edge computing (MEC);Mobile\_Wireless",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3012601"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ding2020-ya,
  title    = "Unveiling the Importance of {SIC} in {NOMA} {Systems---Part} 1:
              State of the Art and Recent Findings",
  author   = "Ding, Z and Schober, R and Poor, H V",
  abstract = "The key idea of non-orthogonal multiple access (NOMA) is to serve
              multiple users simultaneously at the same time and frequency,
              which can result in excessive multiple-access interference. As a
              crucial component of NOMA systems, successive interference
              cancelation (SIC) is key to combating this multiple-access
              interference, and is the focus of this letter, where an overview
              of SIC decoding order selection schemes is provided. In
              particular, selecting the SIC decoding order based on the users'
              channel state information (CSI) and the users' quality of service
              (QoS), respectively, is discussed. The limitations of these two
              approaches are illustrated, and then a recently proposed scheme,
              termed hybrid SIC, which dynamically adapts the SIC decoding
              order is presented and shown to achieve a surprising performance
              improvement that cannot be realized by the conventional SIC
              decoding order selection schemes individually.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  11,
  pages    = "2373--2377",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/D/Ding et al. 2020 - Unveiling the Importance of SIC in NOMA Systems—Part 1 - State of the Art and Recent Findings.pdf",
  keywords = "NOMA;Silicon carbide;Decoding;Quality of
              service;Interference;Performance gain;Delays;Non-orthogonal
              multiple access (NOMA);hybrid successive interference
              cancellation (SIC);outage probability error
              floors;Mobile\_Wireless",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3012604"
}

@ARTICLE{Lin2020-ds,
  title    = "Analytical Modeling of {NOMA-Based} Mobile Edge Computing Systems
              With Randomly Located Users",
  author   = "Lin, L and Zhou, W and Zhao, Z",
  abstract = "Non-orthogonal multiple access (NOMA) has been treated as a
              promising technology to improve the computation offloading
              performance of mobile edge computing (MEC). A complete
              understanding of the impact of applying NOMA on MEC is essential
              for network operators to deploy MEC systems. In this letter, we
              develop a mathematical framework to analyze the impact of NOMA on
              offloading decisions in MEC, which captures the interaction among
              the offloading decisions of multiple co-channel users. Using
              stochastic geometry and order statistics, we derive the
              expression for the computation offloading probability of the $n$
              -th ranked user in terms of serving distances in a NOMA-based MEC
              system. Numerical simulations are performed to verify the
              accuracy of our analysis and demonstrate the performance gain of
              applying NOMA on MEC.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  12,
  pages    = "2965--2968",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/L/Lin et al. 2020 - Analytical Modeling of NOMA-Based Mobile Edge Computing Systems With Randomly Located Users.pdf",
  keywords = "Task analysis;NOMA;Interference;Servers;Edge computing;Energy
              consumption;Computational modeling;NOMA;MEC;offloading
              decision;stochastic geometry;computation offloading
              probability;EdgeFogCloudIoT;Wireless",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3013002"
}

@ARTICLE{Bracciale2020-mu,
  title    = "Lyapunov {Drift-Plus-Penalty} Optimization for Queues With Finite
              Capacity",
  author   = "Bracciale, L and Loreti, P",
  abstract = "Lyapunov optimization is a powerful control technique that allows
              the stabilisation of real or virtual queues while optimizing a
              performance objective. The method has become popular due to the
              fact that it applies a greedy optimization that does not rely on
              any statistical knowledge of the underlying process. Moreover,
              the technique includes a parameter $V$ to control the stability
              vs utility trade-off, offering a theoretical bound on the
              performance. However, in its basic form, the optimization ensures
              that queues are only asymptotically stable, and there is no
              guarantee that queues will stay below a given threshold at all
              times. This can affect the applicability of this technique to
              many applications with queues with limited capacity or,
              equivalently, systems with hard delay constraints. In this letter
              we analyse the conditions under which it is possible to set a
              queue maximum capacity constraint, and we provide a technique to
              set an extra bound on the parameter $V$ to enforce such
              constraint.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  11,
  pages    = "2555--2558",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/B/Bracciale and Loreti 2020 - Lyapunov Drift-Plus-Penalty Optimization for Queues With Finite Capacity.pdf",
  keywords = "Optimization;Queueing analysis;Numerical models;Process
              control;Stability criteria;Stochastic
              processes;Lyapunov;drift-plus-penalty;queuing systems;network
              optimization;Mobile\_Wireless",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3013125"
}

@ARTICLE{Qin2020-fz,
  title    = "Performance Evaluation of {UAV-Enabled} Cellular Networks With
              {Battery-Limited} Drones",
  author   = "Qin, Y and Kishk, M A and Alouini, M-S",
  abstract = "Unmanned aerial vehicles (UAVs) can be used as flying base
              stations (BSs) to offload Macro-BSs in hotspots. However, due to
              the limited battery on-board, UAVs can typically stay in
              operation for less than 1.5 hours. Afterward, the UAV has to fly
              back to a dedicated charging station that recharges/replaces the
              UAV's battery. In this letter, we study the performance of a
              UAV-enabled cellular network while capturing the influence of the
              spatial distribution of the charging stations. In particular, we
              use tools from stochastic geometry to derive the coverage
              probability of a UAV-enabled cellular network as a function of
              the battery size, the density of the charging stations, and the
              time required for recharging/replacing the battery.",
  journal  = "IEEE Commun. Lett.",
  volume   =  24,
  number   =  12,
  pages    = "2664--2668",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/Q/Qin et al. 2020 - Performance Evaluation of UAV-Enabled Cellular Networks With Battery-Limited Drones.pdf",
  keywords = "Charging stations;Batteries;Cellular networks;Probability;Power
              demand;Geometry;Rotors;Stochastic geometry;Poisson point
              process;drone;availability probability;coverage
              probability;Wireless",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3013286"
}

@ARTICLE{Yu2021-rq,
  title    = "Low Reliable and Low Latency Communications for Mission Critical
              Distributed Industrial Internet of Things",
  author   = "Yu, D and Li, W and Xu, H and Zhang, L",
  abstract = "Achieving ubiquitous ultra-reliable low latency consensus in
              centralized wireless communication systems can be costly and hard
              to scale up. The consensus mechanism, which has been widely
              utilized in distributed systems, can provide fault tolerance to
              the critical consensus, even though the individual communication
              link reliability is relatively low. In this article, a widely
              used consensus mechanism, Raft, is introduced to the Industrial
              Internet of Things (IIoT) to achieve ultra-reliable and low
              latency consensus, where the consensus reliability performance in
              terms of nodes number and link transmission reliability is
              investigated. We propose a new concept, Reliability Gain, to show
              the linear relationship between consensus reliability and
              communication link transmission reliability. We also find that
              the time latency of consensus is contradictory to consensus
              reliability. These conclusions can provide guides to deploy Raft
              protocol in distributed IIoT systems.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  1,
  pages    = "313--317",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/Y/Yu et al. 2021 - Low Reliable and Low Latency Communications for Mission Critical Distributed Industrial Internet of Things.pdf",
  keywords = "Wireless communication;Reliability
              theory;Protocols;Actuators;Synchronization;Distributed industrial
              Internet of Things;consensus
              mechanism;raft;reliability;latency;fault
              tolerance;URLLC;Distributed Systems",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3021367"
}

@ARTICLE{Sun2021-gm,
  title    = "Combining Deep Reinforcement Learning With Graph Neural Networks
              for Optimal {VNF} Placement",
  author   = "Sun, P and Lan, J and Li, J and Guo, Z and Hu, Y",
  abstract = "Network Function Virtualization (NFV) technology utilizes
              software to implement network function as virtual instances,
              which reduces the cost on various middlebox hardware. A Virtual
              Network Function (VNF) instance requires multiple resource types
              in the network (e.g., CPU, memory). Therefore, an efficient VNF
              placement policy should consider both the resource utilization
              problem and the Quality of Service (QoS) of flows, which is
              proved NP-hard. Recent studies employ Deep Reinforcement Learning
              (DRL) to solve the VNF placement problem, but existing DRL-based
              solutions cannot generalize well to different topologies. In this
              letter, we propose to combine the advantage of DRL and Graph
              Neural Network (GNN) to design our VNF placement scheme DeepOpt.
              Simulation results show that DeepOpt outperforms the
              state-of-the-art VNF placement schemes and shows a much better
              generalization ability in different network topologies.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  1,
  pages    = "176--180",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/S/Sun et al. 2021 - Combining Deep Reinforcement Learning With Graph Neural Networks for Optimal VNF Placement.pdf",
  keywords = "Neural networks;Delays;Network topology;Quality of
              service;Training;Machine learning;Software;Deep reinforcement
              learning;network function virtualization;graph neural
              networks;software-defined networking;MLNetworking;NFV",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3025298"
}

@ARTICLE{Nikbakht2021-wi,
  title    = "Unsupervised Learning for Parametric Optimization",
  author   = "Nikbakht, Rasoul and Jonsson, Anders and Lozano, Angel",
  abstract = "This letter proposes the unsupervised training of a feedforward
              neural network to solve parametric optimization problems
              involving large numbers of parameters. Such unsupervised
              training, which consists in repeatedly sampling parameter values
              and performing stochastic gradient descent, foregoes the taxing
              precomputation of labeled training data that supervised learning
              necessitates. As an example of application, we put this technique
              to use on a rather general constrained quadratic program.
              Follow-up letters subsequently apply it to more specialized
              wireless communication problems, some of them nonconvex in
              nature. In all cases, the performance of the proposed procedure
              is very satisfactory and, in terms of computational cost, its
              scalability with the problem dimensionality is superior to that
              of convex solvers.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  3,
  pages    = "678--681",
  month    =  mar,
  year     =  2021,
  keywords = "Optimization;Artificial neural networks;Training;Linear
              programming;Wireless communication;Databases;Machine
              learning;neural networks;unsupervised learning;parametric
              optimization;convex optimization;quadratic program",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3027981"
}

@ARTICLE{Nikbakht2021-dy,
  title    = "Unsupervised Learning for {C-RAN} Power Control and Power
              Allocation",
  author   = "Nikbakht, Rasoul and Jonsson, Anders and Lozano, Angel",
  abstract = "This letter applies a feedforward neural network trained in an
              unsupervised fashion to the problem of optimizing the transmit
              powers in centralized radio access networks operating on a
              cell-free basis. Both uplink and downlink are considered. Various
              objectives are entertained, some leading to convex formulations
              and some that do not. In all cases, the performance of the
              proposed procedure is very satisfactory and, in terms of
              computational cost, the scalability is manifestly superior to
              that of convex solvers. Moreover, the optimization relies on
              directly measurable channel gains, with no need for user location
              information.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  3,
  pages    = "687--691",
  month    =  mar,
  year     =  2021,
  keywords = "Uplink;Downlink;Signal to noise ratio;Power
              control;Interference;Optimization;Resource management;Neural
              networks;unsupervised learning;cell-free networks;ultradense
              networks;power control;power allocation;C-RAN",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3027991"
}

@ARTICLE{Nikbakht2021-lh,
  title    = "Unsupervised Learning for Cellular Power Control",
  author   = "Nikbakht, Rasoul and Jonsson, Anders and Lozano, Angel",
  abstract = "This letter applies a feedforward neural network trained in an
              unsupervised fashion to the problem of optimizing the transmit
              powers in cellular wireless systems. Both uplink and downlink are
              considered, with either centralized or distributed power control.
              Various objectives are entertained, all of them such that the
              problem can be cast in convex form. The performance of the
              proposed procedure is very satisfactory and, in terms of
              computational cost, the scalability with the system
              dimensionality is markedly superior to that of convex solvers.
              Moreover, the optimization relies on directly measurable channel
              gains, with no need for user location information.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  3,
  pages    = "682--686",
  month    =  mar,
  year     =  2021,
  keywords = "Signal to noise ratio;Artificial neural
              networks;Interference;Power
              control;Uplink;Downlink;Optimization;Machine learning;neural
              networks;unsupervised learning;power control;cellular systems",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3027994"
}

@ARTICLE{Bellalta2021-bg,
  title    = "On the {Low-Latency} Region of {Best-Effort} Links for
              {Delay-Sensitive} Streaming Traffic",
  author   = "Bellalta, Boris",
  abstract = "This letter analyzes the Low-latency Region (LLR) of a
              best-effort link (i.e., no traffic differentiation, and first
              come first serve scheduling) carrying both delay-sensitive (DS)
              streaming and non-delay-sensitive (NDS) background traffic.
              Moreover, inside the LLR, we show it exists a proportional fair
              arrival rate allocation for both the DS and NDS traffic streams.
              This optimal operating point results from maximizing a simple
              throughput-delay trade-off that considers the NDS traffic load,
              and the mean delay of the DS packets. To show how the presented
              trade-off could be used to allocate NDS traffic in a realistic
              scenario, we use Google Stadia traffic traces to generate the DS
              flow. Results from this use-case confirm that the
              throughput-delay trade-off also works regardless the distribution
              of the packet arrival and packet service times.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  3,
  pages    = "970--974",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/B/Bellalta 2021 - On the Low-Latency Region of Best-Effort Links for Delay-Sensitive Streaming Traffic.pdf",
  keywords = "Delays;Resource management;Transmitters;Wireless
              fidelity;Throughput;Scheduling;Network
              interfaces;Low-latency;delay-sensitive traffic;proportional fair
              rate allocation;M/G/1;FutureInternet",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3039926"
}

@ARTICLE{Samarakoon2021-ef,
  title    = "Predictive {Ultra-Reliable} Communication: A Survival Analysis
              Perspective",
  author   = "Samarakoon, Sumudu and Bennis, Mehdi and Saad, Walid and Debbah,
              M{\'e}rouane",
  abstract = "Ultra-reliable communication (URC) is a key enabler for
              supporting immersive and mission-critical 5G applications.
              Meeting the strict reliability requirements of these applications
              is challenging due to the absence of accurate statistical models
              tailored to URC systems. In this letter, the wireless
              connectivity over dynamic channels is characterized via
              statistical learning methods. In particular, model-based and
              data-driven learning approaches are proposed to estimate the
              non-blocking connectivity statistics over a set of training
              samples with no knowledge on the dynamic channel statistics.
              Using principles of survival analysis, the reliability of
              wireless connectivity is measured in terms of the probability of
              channel blocking events. Moreover, the maximum transmission
              duration for a given reliable non-blocking connectivity is
              predicted in conjunction with the confidence of the inferred
              transmission duration. Results show that the accuracy of
              detecting channel blocking events is higher using the model-based
              method for low to moderate reliability targets requiring low
              sample complexity. In contrast, the data-driven method yields a
              higher detection accuracy for higher reliability targets at the
              cost of $100\times $ sample complexity.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  4,
  pages    = "1221--1225",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/S/Samarakoon et al. 2021 - Predictive Ultra-Reliable Communication - A Survival Analysis Perspective.pdf",
  keywords = "Reliability;Numerical models;Data models;Analytical
              models;Wireless communication;Probabilistic logic;Training
              data;URC;channel blocking;survival analysis;statistical
              learning;5G",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3047446"
}

@ARTICLE{Huang2021-sk,
  title    = "{Meta-Learning} Based Dynamic Computation Task Offloading for
              Mobile Edge Computing Networks",
  author   = "Huang, Liang and Zhang, Luxin and Yang, Shicheng and Qian, Li
              Ping and Wu, Yuan",
  abstract = "Deep learning-based algorithms provide a promising solution to
              efficiently generate offloading decisions in mobile edge
              computing (MEC) networks. However, considering dynamic MEC
              devices or offloading tasks, most of them require large-scale
              training data and long training time to retrain the deep neural
              networks (DNNs). In this letter, we propose a MEta-Learning-based
              computation Offloading (MELO) algorithm for dynamic computation
              tasks in MEC networks. Specifically, it learns from historical
              MEC task scenarios and adapts to a new MEC task scenario with a
              few training samples. Numerical results show that the proposed
              algorithm can adapt to a new MEC task scenario and achieve 99\%
              accuracy via 1-step fine-tuning using only 10 training samples.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  5,
  pages    = "1568--1572",
  month    =  may,
  year     =  2021,
  keywords = "Task analysis;Training;Servers;Delays;Heuristic
              algorithms;Computational modeling;Wireless
              communication;Mobile-edge computing;meta-learning;deep
              learning;computation offloading;MLNetworking",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2020.3048075"
}

@ARTICLE{Alvarez-Horcajo2021-wx,
  title    = "A Hybrid {SDN} Switch Based on Standard {P4} Code",
  author   = "Alvarez-Horcajo, Joaquin and Mart{\'\i}nez-Yelmo, Isa{\'\i}as and
              Lopez-Pajares, Diego and Carral, Juan A and Savi, Marco",
  abstract = "This letter presents an enhanced hybrid Software-Defined
              Networking (SDN) layer-2 switch whose behavior is specified by
              the Programming Protocol-independent Packet Processors (P4)
              language. Its SDN capabilities are enabled by using P4Runtime as
              control plane protocol to specify the forwarding rules used by
              its programmable data plane. Additionally, the device is also
              able to exploit P4 registers for an autonomous self-definition of
              its forwarding capabilities, with the goal of avoiding an
              overload of the SDN control plane. Its performance is better than
              other P4 proposals based on non-standard externs and similar to
              other platform-dependent implementations.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  5,
  pages    = "1482--1485",
  month    =  may,
  year     =  2021,
  file     = "All Papers/A/Alvarez-Horcajo et al. 2021 - A Hybrid SDN Switch Based on Standard P4 Code.pdf",
  keywords = "Registers;Protocols;Switches;Arrays;Standards;Media Access
              Protocol;Process control;P4;ARP-path protocol;P4
              registers;forwarding tables;autonomous path
              selection;FutureInternet",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2021.3049570"
}

@ARTICLE{Haenggi2021-at,
  title    = "Meta {Distributions---Part} 1: Definition and Examples",
  author   = "Haenggi, Martin",
  abstract = "Meta distributions (MDs) have emerged as a powerful tool in the
              analysis of wireless networks. Compared to standard
              distributions, they enable a clean separation of the different
              sources of randomness, resulting in sharper, more refined
              results. In particular, they capture the disparity of the
              performances of individual links or users. In this first part of
              a two-letter series, we start from first principles and give the
              formal definition of MDs and present several simple yet
              illustrative examples. Part 2 explores the properties of the MD
              in more depth and offers multiple interpretations and
              applications.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  7,
  pages    = "2089--2093",
  month    =  jul,
  year     =  2021,
  keywords = "Random variables;Reliability;Rayleigh channels;Probability
              density function;Power control;Wireless networks;Uplink;Meta
              distributions;wireless networks;stochastic geometry;point
              processes;signal fraction;interference;Wireless",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2021.3069662"
}

@ARTICLE{Haenggi2021-ai,
  title    = "Meta {Distributions---Part} 2: Properties and Interpretations",
  author   = "Haenggi, Martin",
  abstract = "In the companion letter (Part 1), we have defined and exemplified
              meta distributions (MDs) as a natural extension of the concepts
              of the mean and distribution of a random variable. Here we
              provide an in-depth discussion of the properties and
              interpretations of MDs. It includes original results on the
              calculation of MDs in the monotone case and two applications to
              simple Poisson wireless networks models.",
  journal  = "IEEE Commun. Lett.",
  volume   =  25,
  number   =  7,
  pages    = "2094--2098",
  month    =  jul,
  year     =  2021,
  keywords = "Reliability;Rayleigh channels;Interference;Wireless
              networks;Upper bound;Transmitters;Receivers;Meta
              distributions;wireless networks;stochastic geometry;point
              processes;interference;Wireless",
  issn     = "1558-2558",
  doi      = "10.1109/LCOMM.2021.3069681"
}

@ARTICLE{Chen2020-hc,
  title    = "{Cross-Layer} Resource Allocation in {Wireless-Enabled} {NFV}",
  author   = "Chen, J and Liu, H and Jia, H",
  abstract = "In this letter, we propose a cross layer resource optimization
              model and solution to wireless enabled SFC network. Particularly,
              we aim to minimize end-to-end downlink latency including both
              wireless and wired delay by optimizing VNF placement, routing
              path and wireless resources in terms of resource block for each
              SFC. We formulate the problem into a non-linear non-convex
              integer optimization problem and propose a heuristic method HCLRA
              to solve the problem. The simulation results show that the
              latency performance of the proposed heuristic algorithm HCLRA is
              close to optimal and better than a suboptimal genetic algorithm.",
  journal  = "IEEE Wireless Communications Letters",
  volume   =  9,
  number   =  6,
  pages    = "879--883",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/C/Chen et al. 2020 - Cross-Layer Resource Allocation in Wireless-Enabled NFV.pdf",
  keywords = "genetic algorithms;resource allocation;telecommunication
              computing;telecommunication network routing;telecommunication
              traffic;virtualisation;wireless channels;wireless enabled SFC
              network;VNF placement;wireless resources;nonlinear nonconvex
              integer optimization problem;cross-layer resource
              allocation;wireless-enabled NFV;cross layer resource optimization
              model;end-to-end downlink latency;wireless delay;wired
              delay;routing path;HCLRA heuristic method;suboptimal genetic
              algorithm;Delays;Optimization;Antennas;Resource management;Base
              stations;Wireless networks;NFV;wireless;SFC;cross layer;resource
              allocation;NFV",
  issn     = "2162-2345",
  doi      = "10.1109/LWC.2020.2974198"
}

@ARTICLE{Fan2020-tj,
  title    = "Intelligent Resource Scheduling Based on Locality Principle in
              Data Center Networks",
  author   = "Fan, W and He, J and Han, Z and Li, P and Wang, R",
  abstract = "Cloud computing is developing rapidly and playing an increasingly
              important role in many fields. Resource management of data center
              networks (DCNs) is critical to the performance of cloud
              computing. Due to the large-scale, distributed, and
              cross-regional characteristics of DCNs, there is insufficient
              resource utilization. In this article, we discuss the latest
              developments in the resource management and optimization of DCNs.
              We also designed an intelligent resource search algorithm to
              efficiently search excellent resources in massive computing
              resources. We propose a novel intelligent resource scheduling
              based on the locality principle algorithm (RSLP). The proposed
              strategy can search computing resources dynamically and
              efficiently, and allocate large resources to large tasks
              efficiently. Furthermore, we outline critical demands and
              guidance for researchers and designers to achieve the convergence
              of artificial intelligence and data centers.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  10,
  pages    = "94--100",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/F/Fan et al. 2020 - Intelligent Resource Scheduling Based on Locality Principle in Data Center Networks.pdf",
  keywords = "artificial intelligence;cloud computing;computer
              centres;optimisation;resource allocation;scheduling;search
              problems;intelligent resource scheduling;data center
              networks;cloud computing;resource management;cross-regional
              characteristics;resource utilization;intelligent resource search
              algorithm;locality principle algorithm;artificial
              intelligence;computing resources;resource
              allocation;optimization;Data centers;Artificial
              intelligence;Cloud computing;Resource
              management;Scheduling;Containers;Processor
              scheduling;ML;EdgeFogCloudIoT",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.1900324"
}

@ARTICLE{Giordani2020-aj,
  title    = "Toward {6G} Networks: Use Cases and Technologies",
  author   = "Giordani, M and Polese, M and Mezzavilla, M and Rangan, S and
              Zorzi, M",
  abstract = "Reliable data connectivity is vital for the ever increasingly
              intelligent, automated, and ubiquitous digital world. Mobile
              networks are the data highways and, in a fully connected,
              intelligent digital world, will need to connect everything,
              including people to vehicles, sensors, data, cloud resources, and
              even robotic agents. Fifth generation (5G) wireless networks,
              which are currently being deployed, offer significant advances
              beyond LTE, but may be unable to meet the full connectivity
              demands of the future digital society. Therefore, this article
              discusses technologies that will evolve wireless networks toward
              a sixth generation (6G) and which we consider as enablers for
              several potential 6G use cases. We provide a fullstack,
              system-level perspective on 6G scenarios and requirements, and
              select 6G technologies that can satisfy them either by improving
              the 5G design or by introducing completely new communication
              paradigms.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  3,
  pages    = "55--61",
  month    =  mar,
  year     =  2020,
  file     = "All Papers/G/Giordani et al. 2020 - Toward 6G Networks - Use Cases and Technologies.pdf",
  keywords = "5G mobile communication;6G mobile communication;Long Term
              Evolution;LTE;6G networks;sixth generation;5G;fifth generation
              wireless networks;intelligent digital world;data highways;mobile
              networks;data connectivity;6G mobile communication;5G mobile
              communication;Reliability;Wireless networks;Internet of
              Things;Intelligent sensors;Done;idea;5G6G;ServicesDescription",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.1900411"
}

@ARTICLE{Khan2020-gy,
  title    = "Federated Learning for Edge Networks: Resource Optimization and
              Incentive Mechanism",
  author   = "Khan, L U and Pandey, S R and Tran, N H and Saad, W and Han, Z
              and Nguyen, M N H and Hong, C S",
  abstract = "Recent years have witnessed a rapid proliferation of smart
              Internet of Things (IoT) devices. IoT devices with intelligence
              require the use of effective machine learning paradigms.
              Federated learning can be a promising solution for enabling
              IoT-based smart applications. In this article, we present the
              primary design aspects for enabling federated learning at the
              network edge. We model the incentive- based interaction between a
              global server and participating devices for federated learning
              via a Stackelberg game to motivate the participation of the
              devices in the federated learning process. We present several
              open research challenges with their possible solutions. Finally,
              we provide an outlook on future research.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  10,
  pages    = "88--93",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/K/Khan et al. 2020 - Federated Learning for Edge Networks - Resource Optimization and Incentive Mechanism.pdf",
  keywords = "game theory;incentive schemes;Internet of Things;learning
              (artificial intelligence);optimisation;Stackelberg game;resource
              optimization;network edge;IoT-based smart applications;machine
              learning;Internet of Things;edge networks;federated
              learning;global server;incentive mechanism;Computational
              modeling;Collaborative work;Optimization;Data
              models;Servers;Internet of
              Things;Training;Done;ReferenceChasing;ATOS;EdgeFogCloudIoT;ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.1900649"
}

@ARTICLE{Spinnewyn2020-md,
  title    = "Service Orchestration in {NFV-Based} Traditional and Emerging
              Cloud Environments: State of the Art and Research Challenges",
  author   = "Spinnewyn, B and Latre, S and Botero, J F",
  abstract = "Neutral host SCPs represent a key element of the 5G vision of
              ultra-dense mobile networks. However, current business models
              mostly focus on multi-year agreements for large venues, such as
              stadiums and hotel chains. These business agreements are
              regulated through binding SLAs, which tend to be too cumbersome
              and costly for smaller-scale SCPs. As a result, the neutral host
              model does not scale up to its full potential. In this article,
              we propose a framework to enable the participation of small- to
              medium-sized players in the cellular market as providers offering
              network resources to MNOs. To this purpose, we review the current
              and emerging spectrum and technology opportunities that SCPs can
              use for neutral host deployments. We also propose the use of
              blockchain-enabled smart contracts as a simple and cost-efficient
              alternative to traditional SLAs for small-scale SCPs. To
              demonstrate this, we describe a proof of concept implementation
              of an Ethereum-based smart contract platform for best effort
              service between an SCP and an MNO. Our simulations on potential
              smart contract-based deployments in city center Dublin show that
              the received signal strength in the considered area will increase
              by an average of 10 percent.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  8,
  pages    = "76--81",
  month    =  aug,
  year     =  2020,
  file     = "All Papers/S/Spinnewyn et al. 2020 - Service Orchestration in NFV-Based Traditional and Emerging Cloud Environments - State of the Art and Research Challenges.pdf",
  keywords = "5G mobile communication;cellular
              radio;contracts;cryptography;telecommunication network
              management;telecommunication services;scalable user-deployed
              ultra-dense networks;ultra-dense mobile networks;business
              models;multiyear agreements;hotel chains;business
              agreements;SLA;neutral host model;cellular market;network
              resources;neutral host deployments;blockchain-enabled smart
              contracts;Ethereum;neutral host SCP;smart contract-based
              deployments;MNO;5G mobile communication;Dublin;Smart
              contracts;Blockchain;Gallium arsenide;5G mobile
              communication;Microcell networks;Urban areas;NFV\_SDN",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.1900759"
}

@ARTICLE{Du2020-xo,
  title    = "The Internet of Things as a Deep Neural Network",
  author   = "Du, R and Magnusson, S and Fischione, C",
  abstract = "An important task in the Internet of Things (IoT) is field
              monitoring, where multiple IoT nodes take measurements and
              communicate them to the base station or the cloud for processing,
              inference, and analysis. When the measurements are
              high-dimensional (e.g., videos or time-series data), IoT networks
              with limited bandwidth and low-power devices may not be able to
              support such frequent transmissions with high data rates. To
              ensure communication efficiency, this article proposes to model
              the measurement compression at IoT nodes and the inference at the
              base station or cloud as a deep neural network (DNN). We propose
              a new framework where the data to be transmitted from nodes are
              the intermediate outputs of a layer of the DNN. We show how to
              learn the model parameters of the DNN and study the trade-off
              between the communication rate and the inference accuracy. The
              experimental results show that we can save approximately 96
              percent transmissions with only a degradation of 2.5 percent in
              inference accuracy, which shows the potentiality to enable many
              new IoT data analysis applications that generate a large amount
              of measurements.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  9,
  pages    = "20--25",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/D/Du et al. 2020 - The Internet of Things as a Deep Neural Network.pdf",
  keywords = "cloud computing;data analysis;Internet of Things;neural nets;IoT
              data analysis;deep neural network;communication
              efficiency;Internet of Things;cloud computing;edge computing;Data
              models;Base stations;Internet of Things;Pollution
              measurement;Distributed databases;Computational
              modeling;Bandwidth;MLNetworking;EdgeFogCloudIoT",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000015"
}

@ARTICLE{Baenke2020-vq,
  title    = "{Millimeter-Wave} Downlink Coverage Extension Strategies",
  author   = "Baenke, J and Chaudhuri, K R and Deshpande, A and Halder, A and
              Irizarry, M and Saxena, N and Sharma, S and Yang, R",
  abstract = "Traditionally, the uplink and downlink in a cellular network are
              associated with the same base station and the same frequency band
              since it promotes simpler network design, maintenance, and
              operation because of reduced complexity. However, when deploying
              millimeter-wave (mmWave) spectrum, network coverage has
              limitations hinging on the nature of the mmWave propagation
              characteristics relative to the traditional sub 6 GHz frequency
              bands. In this article, we present a method for an extension of
              the mmWave downlink coverage by demonstrating the benefits of
              decoupling the reverse and forward link frequencies and by
              leveraging macro-type radio deployment heights to maximize the
              line-of-sight coverage. The accompanying link budget analysis and
              coverage simulations quantify the benefit of the coverage
              extension methodology to be as much as 6.8 dB while extending
              possible coverage range distance to greater than 2 km.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  9,
  pages    = "74--78",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/B/Baenke et al. 2020 - Millimeter-Wave Downlink Coverage Extension Strategies.pdf",
  keywords = "cellular radio;millimetre wave communication;millimetre wave
              propagation;telecommunication network planning;coverage range
              distance;coverage extension methodology;coverage simulations;link
              budget analysis;line-of-sight coverage;macro-type radio
              deployment heights;forward link frequencies;mmWave downlink
              coverage;mmWave propagation characteristics;network
              coverage;frequency band;base station;cellular
              network;millimeter-wave downlink coverage extension
              strategies;frequency 6.0 GHz;Propagation losses;Throughput;5G
              mobile communication;Uplink;Downlink;Cellular networks;Monte
              Carlo methods;Mobile\_Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000046"
}

@ARTICLE{Jiang2021-ee,
  title    = "Traffic Prediction and Random Access Control Optimization:
              Learning and {Non-Learning-Based} Approaches",
  author   = "Jiang, Nan and Deng, Yansha and Nallanathan, Arumugam",
  abstract = "Random access channel (RACH) procedures in modern wireless
              communications are generally based on multi-channel
              slotted-ALOHA, which can be optimized by flexibly organizing
              devices' transmission and retransmission. However, due to the
              lack of information about the traffic generation statistics and
              the occurrence of random collision, optimizing RACH in an exact
              manner is generally challenging. In this article, we first
              summarize the general structure of optimization for different
              RACH schemes, and then review existing RACH optimization methods
              based on machine learning (ML) and non-ML techniques. We
              demonstrate that the ML-based methods can better optimize RACH
              schemes compared to non-ML-based methods due to their capability
              in solving high-complexity long-term optimization problems. To
              further improve the RACH performance, we introduce a decoupled
              learning strategy (DLS) for access control optimization, which
              individually executes two sub-tasks: traffic prediction and
              access control configuration. In detail, the traffic prediction
              relies on an online supervised learning method adopting recurrent
              neural networks that can accurately capture traffic statistics
              over consecutive frames, while the access control configuration
              uses either a non-ML-based controller or a cooperatively trained
              deep reinforcement learning (DRL)-based controller selected
              depending on the complexity of different random access schemes.
              Numerical results show that the DLS optimizer considerably
              outperforms conventional DRL optimizers in terms of higher
              training efficiency and better access performance.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  3,
  pages    = "16--22",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/J/Jiang et al. 2021 - Traffic Prediction and Random Access Control Optimization - Learning and Non-Learning-Based Approaches.pdf",
  keywords = "Access control;Wireless communication;Training data;Recurrent
              neural networks;Supervised learning;Optimization
              methods;Reinforcement learning;Internet of Things;MLNetworking",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000099"
}

@ARTICLE{Linnartz2020-zy,
  title    = "Wireless Communication over an {LED} Channel",
  author   = "Linnartz, J-P M G and Deng, X and Alexeev, A and Mardanikorani, S",
  abstract = "Today, we see a sharp rise in the interest in communicating
              wirelessly over an optical channel, for instance, building
              networks that are free from adjacent cell interference. LEDs are
              already widely used in illumination, and LED LiFi systems that
              carry up to gigabits per second are commercially available.
              Progress in wireless communication relies, to a significant
              extent, on a solid understanding of the channel properties.
              However, the LED channel distinctly differs from the RF channel.
              In this overview, we explore the peculiarities of the LED channel
              and discuss the consequences for the design of the communication
              system.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  12,
  pages    = "77--82",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/L/Linnartz et al. 2020 - Wireless Communication over an LED Channel.pdf",
  keywords = "Wireless communication;Standardization;Light emitting
              diodes;Light fidelity;Channel models;Research and
              development;Mobile\_Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000138"
}

@ARTICLE{Xu2020-xk,
  title    = "Scalable Learning Paradigms for {Data-Driven} Wireless
              Communication",
  author   = "Xu, Y and Yin, F and Xu, W and Lee, C-H and Lin, J and Cui, S",
  abstract = "The marriage of wireless big data and machine learning techniques
              revolutionizes wireless systems by introducing data-driven
              philosophy. However, the ever exploding data volume and model
              complexity will limit centralized solutions to learn and respond
              within a reasonable time. Therefore, scalability becomes a
              critical issue to be solved. In this article, we aim to provide a
              systematic discussion of the building blocks of scalable
              data-driven wireless networks. On one hand, we discuss the
              forward-looking architecture and computing framework of scalable
              data-driven systems from a global perspective. On the other hand,
              we discuss relevant learning algorithms and model training
              strategies performed at each individual node from a local
              perspective. We also highlight several promising research
              directions in the context of scalable data-driven wireless
              communications to inspire future research.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  10,
  pages    = "81--87",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/X/Xu et al. 2020 - Scalable Learning Paradigms for Data-Driven Wireless Communication.pdf",
  keywords = "Big Data;cellular radio;learning (artificial intelligence);mobile
              computing;mobile radio;scalable data-driven systems;model
              complexity;wireless systems;machine learning techniques;wireless
              Big Data;data-driven wireless communication;scalable learning
              paradigms;Computer architecture;Wireless
              communication;Computational modeling;Convex functions;Servers;Big
              Data;Machine learning;ML;Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000143"
}

@ARTICLE{Chen2020-tb,
  title    = "Key Enabling Technologies for the {Post-5G} Era: Fully Adaptive,
              {All-Spectra} Coordinated Radio Access Network with Function
              Decoupling",
  author   = "Chen, Y-W and Zhang, R and Hsu, C-W and Chang, G-K",
  abstract = "With the rapidly growing bandwidth demand for wireless
              applications, new system technologies related to post-5G are
              emerging. In this article, an all-spectra fully adaptive and
              coordinated radio access network (RAN) is reported and discussed.
              By employing a fiber-wireless integration and networking
              architecture, all data-carrying channels could be aggregated in
              the same fiber access infrastructure. This enables a coordinated
              RAN with function decoupling, in which lower RF, 5G New Radio
              (NR), sub-THz, and even lightwave are employed; also, different
              types of services are delivered depending on their physical layer
              properties. Promising scenarios are discussed, such as integrated
              access of wireless NR-free space optical (FSO) backhauling and
              indoor systems via visible light communication (VLC) and
              efficient NR beamforming aided by VLC positioning system. The
              former use case can enhance the network throughput and
              reliability. This is because both FSO and NR can support high
              channel capacity due to their abundant bandwidth. Meanwhile, with
              the advancement of novel DSP techniques, the stability of the
              NR-FSO link under diverse weather turbulences or suffering from
              burst mode interference can be enhanced. The latter scenario
              provides an alternative solution for high-speed data link and a
              simplified beam management via the VLC-aided positioning system.
              VLC can concurrently provide ubiquitous indoor illumination, data
              transmission, and positioning. With the help of artificial
              intelligence algorithms, a VLC-based precision positioning system
              can provide a location accuracy of less than 1 cm, and it is able
              to meet the narrow beam size of the NR beamformer in a 3D model.
              Therefore, it is foreseeable that an all-spectra function
              decoupled RAN can serve as a unified network platform to support
              all wireless applications while optimizing system throughput,
              channel condition, network coverage, and software/ hardware
              complexity for post-5G mobile data networks.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  9,
  pages    = "60--66",
  month    =  sep,
  year     =  2020,
  keywords = "cellular radio;channel capacity;free-space optical
              communication;radio access networks;post-5G mobile data
              networks;network coverage;unified network platform;all-spectra
              function;NR beamformer;VLC-based precision positioning
              system;high-speed data;NR-FSO link;high channel capacity;network
              throughput;VLC positioning system;wireless NR-free
              space;integrated access;physical layer properties;5G New
              Radio;coordinated RAN;fiber access infrastructure;data-carrying
              channels;networking architecture;fiber-wireless
              integration;wireless applications;bandwidth demand;all-spectra
              coordinated radio access network;Wireless communication;5G mobile
              communication;Meteorology;Wireless sensor
              networks;Interference;Radio access networks;Visible light
              communication;5G6G",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000186"
}

@ARTICLE{Savazzi2021-ov,
  title    = "Opportunities of Federated Learning in Connected, Cooperative,
              and Automated Industrial Systems",
  author   = "Savazzi, Stefano and Nicoli, Monica and Bennis, Mehdi and
              Kianoush, Sanaz and Barbieri, Luca",
  abstract = "Next-generation autonomous and networked industrial systems
              (i.e., robots, vehicles, drones) have driven advances in
              ultra-reliable low-laten-cy communications (URLLC) and computing.
              These networked multi-agent systems require fast,
              communication-efficient, and distributed machine learning (ML) to
              provide mission-crit-ical control functionalities. Distributed ML
              techniques, including federated learning (FL), represent a
              mushrooming multidisciplinary research area weaving together
              sensing, communication, and learning. FL enables continual model
              training in distributed wireless systems: rather than fusing raw
              data samples at a centralized server, FL leverages a cooperative
              fusion approach where networked agents, connected via URLLC, act
              as distributed learners that periodically exchange their locally
              trained model parameters. This article explores emerging
              opportunities of FL for the next-generation networked industrial
              systems. Open problems are discussed, focusing on cooperative
              driving in connected automated vehicles and collaborative
              robotics in smart manufacturing.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  2,
  pages    = "16--21",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/S/Savazzi et al. 2021 - Opportunities of Federated Learning in Connected, Cooperative, and Automated Industrial Systems.pdf",
  keywords = "Wireless sensor networks;Service robots;Ultra reliable low
              latency communication;Robot sensing systems;Collaborative
              work;Data models;Next generation networking",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000200"
}

@ARTICLE{Matthaiou2021-yz,
  title    = "The Road to 6G: Ten Physical Layer Challenges for Communications
              Engineers",
  author   = "Matthaiou, M and Yurduseven, O and Ngo, H Q and Morales-Jimenez,
              D and Cotton, S L and Fusco, V F",
  abstract = "While the deployment of 5G cellular systems will continue well
              into the next decade, much interest is already being generated
              toward technologies that will underlie its successor, 6G.
              Undeniably, 5G will have a transformative impact on the way we
              live and communicate, but it is still far away from supporting
              the Internet of Everything, where upward of 1 million devices/km3
              (both terrestrial and aerial) will require ubiquitous, reliable,
              low-latency connectivity. This article looks at some of the
              fundamental problems that pertain to key physical layer enablers
              for 6G. This includes highlighting challenges related to
              intelligent reflecting surfaces, cell-free massive MIMO, and THz
              communications. Our analysis covers theoretical modeling
              challenges, hardware implementation issues, and scalability,
              among others. The article concludes by delineating the critical
              role of signal processing in the new era of wireless
              communications.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  1,
  pages    = "64--69",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/M/Matthaiou et al. 2021 - The Road to 6G - Ten Physical Layer Challenges for Communications Engineers.pdf",
  keywords = "6G mobile communication;Wireless communication;5G mobile
              communication;Scalability;Signal processing;Physical
              layer;Surface treatment;5G6G;ServicesDescription",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000208"
}

@ARTICLE{Han2020-gk,
  title    = "{Artificial-Intelligence-Enabled} Air Interface for 6G:
              Solutions, Challenges, and Standardization Impacts",
  author   = "Han, S and Xie, T and I, C-L and Chai, L and Liu, Z and Yuan, Y
              and Cui, C",
  abstract = "As 3GPP has completed Release 16 specifications and worldwide 5G
              commercialization is speeding up, global interest in 6G is
              starting to grow. An interesting and important question is: will
              the rapid progress in artificial intelligence (AI) eventually
              alleviate the tremendous efforts required for future
              standardization of 6G and beyond? In this article, the potential
              impacts of AI on the air interface design and standardization are
              investigated. The AI-enabled network architecture is first
              discussed. The higher layer, physical layer, and cross-layer
              design empowered by AI capability are further presented. Based on
              these designs, the future 6G and beyond are expected to enter
              into an AI era. For potential new use cases and more challenging
              requirements, the network is capable of automatic updating the
              air interface protocols, which may substantially reduce the
              standardization efforts and costs of wireless communication
              networks.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  10,
  pages    = "73--79",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/H/Han et al. 2020 - Artificial-Intelligence-Enabled Air Interface for 6G - Solutions, Challenges, and Standardization Impacts.pdf",
  keywords = "3G mobile communication;5G mobile communication;6G mobile
              communication;artificial intelligence;protocols;telecommunication
              computing;artificial-intelligence-enabled air
              interface;3GPP;artificial intelligence;air interface
              design;AI-enabled network architecture;AI era;air interface
              protocols;5G commercialization;6G and beyond;wireless
              communication network;Artificial intelligence;6G mobile
              communication;Protocols;5G mobile communication;Communication
              channels;Wireless communication;ML;Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000218"
}

@ARTICLE{Polese2020-iw,
  title    = "Toward {End-to-End}, {Full-Stack} {6G} Terahertz Networks",
  author   = "Polese, M and Jornet, J M and Melodia, T and Zorzi, M",
  abstract = "Recent evolutions in semiconductors have brought the terahertz
              band into the spotlight as an enabler for terabit-per-second
              communications in 6G networks. Most of the research so far,
              however, has focused on understanding the physics of terahertz
              devices, circuitry, and propagation, and on studying physical
              layer solutions. However, integrating this technology in complex
              mobile networks requires proper design of the full communication
              stack, to address link- and system-level challenges related to
              network setup, management, coordination, energy efficiency, and
              end-to-end connectivity. This article provides an overview of the
              issues that need to be overcome to introduce the terahertz
              spectrum in mobile networks, from a MAC, network, and transport
              layer perspective, with consideration on the performance of
              end-to-end data flows on terahertz connections.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  11,
  pages    = "48--54",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/P/Polese et al. 2020 - Toward End-to-End, Full-Stack 6G Terahertz Networks.pdf",
  keywords = "Bandwidth;Base stations;3GPP;Signal to noise ratio;Physical
              layer;6G mobile
              communication;Done;Mobile\_Wireless;Wireless;ServicesDescription",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000224"
}

@ARTICLE{Liu2020-yy,
  title    = "{5G} Deployment: Standalone vs. {Non-Standalone} from the
              Operator Perspective",
  author   = "Liu, G and Huang, Y and Chen, Z and Liu, L and Wang, Q and Li, N",
  abstract = "The fifth generation (5G) mobile network is standardized and
              developed to explore the mobile market beyond 2020. In response
              to the diverse strategies of 5G deployment, five alternative
              network architectures have been proposed to 3GPP by different
              mobile operators. To fulfill the urgent deployment requirement
              from some operators, an early drop of 5G, termed as
              non-standalone (NSA) new radio (NR), was completed at the end of
              2017. After that, the standardization of a new 5G system,
              including th standalone (SA) new radio access network, was
              finished in June 2018. This article analyzes and compares the SA
              NR and NSA NR deployment modes in terms of coverage, network
              capability, interworking between 4G and 5G, complexity and cost
              of network deployment, and the latest industry progress. NSA NR
              performs better in interworking performance in the initial phase,
              while SA NR performs better in network capabilities, device
              performance, simple network deployment, and cost efficiency. 5G
              SA NR is recommended for operators who have the ambition to
              explore new opportunities in the vertical and enterprise markets.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  11,
  pages    = "83--89",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/L/Liu et al. 2020 - 5G Deployment - Standalone vs. Non-Standalone from the Operator Perspective.pdf",
  keywords = "5G mobile communication;Long Term Evolution;3GPP;Computer
              architecture;Network architecture;Base stations;Performance
              evaluation;Done;Boring;5G6G",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000230"
}

@ARTICLE{Restuccia2020-ow,
  title    = "Deep Learning at the Physical Layer: System Challenges and
              Applications to {5G} and Beyond",
  author   = "Restuccia, F and Melodia, T",
  abstract = "The unprecedented requirements of IoT have made fine-grained
              optimization of spectrum resources an urgent necessity. Thus,
              designing techniques able to extract knowledge from the spectrum
              in real time and select the optimal spectrum access strategy
              accordingly has become more important than ever. Moreover, 5G
              networks will require complex management schemes to deal with
              problems such as adaptive beam management and rate selection.
              Although deep learning (DL) has been successful in modeling
              complex phenomena, commercially available wireless devices are
              still very far from actually adopting learning-based techniques
              to optimize their spectrum usage. In this article, we first
              discuss the need for real-time DL at the physical layer, and then
              summarize the current state of the art and existing limitations.
              We conclude the article by discussing an agenda of research
              challenges and how DL can be applied to address crucial problems
              in 5G and beyond networks.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  10,
  pages    = "58--64",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/R/Restuccia and Melodia 2020 - Deep Learning at the Physical Layer - System Challenges and Applications to 5G and Beyond.pdf",
  keywords = "5G mobile communication;Internet of Things;learning (artificial
              intelligence);neural nets;optimisation;radio spectrum
              management;deep learning;wireless devices;spectrum usage;physical
              layer;fine-grained optimization;spectrum resources;optimal
              spectrum access strategy;adaptive beam management;rate
              selection;IoT;5G network;Real-time systems;Wireless
              communication;Hardware;Neural networks;Feature
              extraction;Modulation;Deep learning;ML;Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000243"
}

@ARTICLE{Herlich2021-dt,
  title    = "Measuring and Monitoring Reliability of Wireless Networks",
  author   = "Herlich, M and Maier, C",
  abstract = "In the future, connected vehicles and automated factories will
              increasingly use wireless communication. Because the cost of
              communication failures in these applications is high, the
              wireless networks must be reliable and continuously monitored. To
              address this need, we propose a general approach to determine the
              reliability of wireless networks and describe implementations of
              it. Our approach focuses on checking necessary assumptions
              instead of assuming they are fulfilled. We successfully tested
              our methods in an operating industrial environment. Our tests
              show that both the initial measurement of the reliability of the
              wireless network and continued monitoring are a helpful tool for
              factory operators. In the future, methods such as those described
              in this article should be used to ensure the reliable operation
              of wireless networks in critical scenarios.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  1,
  pages    = "76--81",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/H/Herlich and Maier 2021 - Measuring and Monitoring Reliability of Wireless Networks.pdf",
  keywords = "Connected vehicles;Wireless networks;Production
              facilities;Reliability;Monitoring;Testing;Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000250"
}

@ARTICLE{Zhang2020-ok,
  title    = "Beyond 100 Gb/s Optoelectronic Terahertz Communications: Key
              Technologies and Directions",
  author   = "Zhang, L and Pang, X and Jia, S and Wang, S and Yu, X",
  abstract = "The terahertz band (0.1 THz-10 THz) with massive spectrum
              resources is recognized as a promising candidate for future
              rate-greedy applications, such as 6G communications.
              Optoelectronic terahertz communications are beneficial for
              realizing 100 Gb/s data rate and beyond, which have greatly
              promoted the progress of the 6G research. In this article, we
              give technical insight into the key technologies of
              optoelectronic terahertz communications with high data rates in
              the physical layer, including approaches of broadband devices,
              baseband signal processing technologies, and design of advanced
              transmission system architectures. A multicarrier signal
              processing routine with high noise tolerance is proposed and
              experimentally verified in a 500 Gb/s net rate terahertz
              communication system. Finally, we discuss the future directions
              of optoelectronic terahertz technologies toward the target of
              terabit- scale communications.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  11,
  pages    = "34--40",
  month    =  nov,
  year     =  2020,
  keywords = "Optical fibers;Optical mixing;Optical amplifiers;Wireless
              communication;Integrated optics;Mobile\_Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000254"
}

@ARTICLE{Nguyen2021-lc,
  title    = "Digital Twin for {5G} and Beyond",
  author   = "Nguyen, Huan X and Trestian, Ramona and To, Duc and Tatipamula,
              Mallik",
  abstract = "Although many countries have started the initial phase of rolling
              out 5G, it is still in its infancy with researchers from both
              academia and industry facing the challenges of developing it to
              its full potential. With the support of artificial intelligence,
              development of digital transformation through the notion of a
              digital twin has been taking off in many industries such as smart
              manufacturing, oil and gas, construction, bio-engineering, and
              automotive. However, digital twins remain relatively new for
              5G/6G networks, despite the obvious potential in helping develop
              and deploy the complex 5G environment. This article looks into
              these topics and discusses how digital twin could be a powerful
              tool to fulfill the potential of 5G networks and beyond.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  2,
  pages    = "10--15",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/N/Nguyen et al. 2021 - Digital Twin for 5G and Beyond.pdf",
  keywords = "5G mobile communication;Digital twin;6G mobile
              communication;Research and development;Smart manufacturing",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000343"
}

@ARTICLE{Shao2020-gi,
  title    = "{Communication-Computation} Trade-off in {Resource-Constrained}
              Edge Inference",
  author   = "Shao, J and Zhang, J",
  abstract = "The recent breakthrough in artificial intelligence (AI),
              especially deep neural networks (DNNs), has affected every branch
              of science and technology. Particularly, edge AI has been
              envisioned as a major application scenario to provide DNN-based
              services at edge devices. This article presents effective methods
              for edge inference at resource-constrained devices. It focuses on
              device-edge co-inference, assisted by an edge computing server,
              and investigates a critical trade-off among the computational
              cost of the on-device model and the communication overhead of
              forwarding the intermediate feature to the edge server. A general
              three-step framework is proposed for the effective inference:
              model split point selection to determine the on-device model,
              communication-aware model compression to reduce the on-device
              computation and the resulting communication overhead
              simultaneously, and task-oriented encoding of the intermediate
              feature to further reduce the communication overhead. Experiments
              demonstrate that our proposed framework achieves a better
              tradeoff and significantly reduces the inference latency than
              baseline methods.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  12,
  pages    = "20--26",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/S/Shao and Zhang 2020 - Communication-Computation Trade-off in Resource-Constrained Edge Inference.pdf",
  keywords = "Computational modeling;Neural
              networks;Encoding;Servers;Artificial intelligence;Resource
              management;Edge computing;ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000373"
}

@ARTICLE{Sun2020-ld,
  title    = "Edge Learning with Timeliness Constraints: Challenges and
              Solutions",
  author   = "Sun, Y and Shi, W and Huang, X and Zhou, S and Niu, Z",
  abstract = "Future machine learning (ML) powered applications, such as
              autonomous driving and augmented reality, involve training and
              inference tasks with timeliness requirements and are
              communication- and computation-intensive, which demands the edge
              learning framework. The real-time requirements drive us to go
              beyond accuracy for ML. In this article, we introduce the concept
              of timely edge learning, aiming to achieve accurate training and
              inference while minimizing the communication and computation
              delay. We discuss key challenges and propose corresponding
              solutions from data, model, and resource management perspectives
              to meet the timeliness requirements. In particular, for edge
              training, we argue that the total training delay rather than
              rounds should be considered, and propose data or model
              compression, and joint device scheduling and resource management
              schemes for both centralized training and federated learning
              systems. For edge inference, we explore the dependency between
              accuracy and delay for communication and computation, and propose
              dynamic data compression and flexible pruning schemes. Two case
              studies show that the timeliness performance, including the
              training accuracy under a given delay budget and the completion
              ratio of inference tasks within deadline, are highly improved
              with the proposed solutions.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  12,
  pages    = "27--33",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/S/Sun et al. 2020 - Edge Learning with Timeliness Constraints - Challenges and Solutions.pdf",
  keywords = "Training data;Processor scheduling;Data models;Real-time
              systems;Edge computing;Resource management;Task analysis;ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000382"
}

@ARTICLE{Xiao2020-eq,
  title    = "Toward {Self-Learning} Edge Intelligence in {6G}",
  author   = "Xiao, Y and Shi, G and Li, Y and Saad, W and Poor, H V",
  abstract = "Edge intelligence, also called edge-native artificial
              intelligence (AI), is an emerging technological framework
              focusing on seamless integration of AI, communication networks,
              and mobile edge computing. It has been considered to be one of
              the key missing components in the existing 5G network and is
              widely recognized to be one of the most sought after functions
              for tomorrow's wireless 6G cellular systems. In this article, we
              identify the key requirements and challenges of edge-native AI in
              6G. A self-learning architecture based on self-supervised
              generative adversarial nets is introduced to demonstrate the
              potential performance improvement that can be achieved by
              automatic data learning and synthesizing at the edge of the
              network. We evaluate the performance of our proposed
              self-learning architecture in a university campus shuttle system
              connected via a 5G network. Our result shows that the proposed
              architecture has the potential to identify and classify unknown
              services that emerge in edge computing networks. Future trends
              and key research problems for self-learning-enabled 6G edge
              intelligence are also discussed.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  12,
  pages    = "34--40",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/X/Xiao et al. 2020 - Toward Self-Learning Edge Intelligence in 6G.pdf",
  keywords = "6G mobile communication;5G mobile communication;Wireless
              networks;Computer architecture;Learning systems;Artificial
              intelligence;Edge computing;ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000388"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gunduz2020-da,
  title    = "Communicate to Learn at the Edge",
  author   = "G{\"u}nd{\"u}z, D and Kurka, D B and Jankowski, M and Amiri, M M
              and Ozfatura, E and Sreekumar, S",
  abstract = "Bringing the success of modern machine learning (ML) techniques
              to mobile devices can enable many new services and businesses,
              but also poses significant technical and research challenges. Two
              factors that are critical for the success of ML algorithms are
              massive amounts of data and processing power, both of which are
              plentiful, but highly distributed at the network edge. Moreover,
              edge devices are connected through bandwidth- and power-limited
              wireless links that suffer from noise, time variations, and
              interference. Information and coding theory have laid the
              foundations of reliable and efficient communications in the
              presence of channel imperfections, whose application in modern
              wireless networks has been a tremendous success. However, there
              is a clear disconnect between the current coding and
              communication schemes, and the ML algorithms deployed at the
              network edge. In this article, we challenge the current approach
              that treats these problems separately, and argue for a joint
              communication and learning paradigm for both the training and
              inference stages of edge learning.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  12,
  pages    = "14--19",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/G/Gündüz et al. 2020 - Communicate to Learn at the Edge.pdf",
  keywords = "Training data;Machine learning algorithms;Wireless
              networks;Machine learning;Interference;Reliability theory;Mobile
              handsets;ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000394"
}

@ARTICLE{Chen2020-wj,
  title    = "Wireless Communications for Collaborative Federated Learning",
  author   = "Chen, M and Poor, H V and Saad, W and Cui, S",
  abstract = "To facilitate the deployment of machine learning in resource and
              privacy-constrained systems such as the Internet of Things,
              federated learning (FL) has been proposed as a means for enabling
              edge devices to train a shared learning model while promoting
              privacy. However, Google's seminal FL algorithm requires all
              devices to be directly connected with a central controller, which
              limits its applications. In contrast, this article introduces a
              novel FL framework, called collaborative FL (CFL), which enables
              edge devices to implement FL with less reliance on a central
              controller. The fundamentals of this framework are developed and
              a number of communication techniques are proposed so as to
              improve CFL performance. An overview of centralized learning,
              Google's FL, and CFL is presented. For each type of learning, the
              basic architecture as well as its advantages, drawbacks, and
              operating conditions are introduced. Then four CFL performance
              metrics are presented, and a suite of communication techniques
              ranging from network formation, device scheduling, mobility
              management, to coding are introduced to optimize the performance
              of CFL. For each technique, future research opportunities are
              discussed. In a nutshell, this article showcases how CFL can be
              effectively implemented at the edge of large-scale wireless
              systems.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  12,
  pages    = "48--54",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/C/Chen et al. 2020 - Wireless Communications for Collaborative Federated Learning.pdf",
  keywords = "Wireless communication;Performance
              evaluation;Privacy;Collaborative work;Reliability;Object
              recognition;ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000397"
}

@ARTICLE{Qiao2020-ha,
  title    = "Adaptable Switch: A Heterogeneous Switch Architecture for
              {Network-Centric} Computing",
  author   = "Qiao, S and Hu, C and Brebner, G and Zou, J and Guan, X",
  abstract = "Network-centric computing offloads and disaggregates computing
              and data processing from CPU to network in order to support
              growing throughput, big data volume, and information complexity
              in data centers. An emerging paradigm is employing SmartNIC for
              network-centric computing, which introduces user-specific
              processing at a host's network interface. In this article, we
              take this initiative further to tackle current proprietary
              processing and computing issues in the network core: the
              switches. We propose a new hardware architecture called adaptable
              switch. Based on testing of its prototype empowering three use
              cases, we demonstrate that both high throughput and processing
              flexibility can be achieved simultaneously on an adaptable
              switch.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  12,
  pages    = "64--69",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/Q/Qiao et al. 2020 - Adaptable Switch - A Heterogeneous Switch Architecture for Network-Centric Computing.pdf",
  keywords = "Switching systems;Prototypes;Computer
              architecture;Switches;Throughput;Hardware;Heterogenous
              networks;SDN",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000399"
}

@ARTICLE{Santos2020-xe,
  title    = "Breaking Down Network Slicing: Hierarchical Orchestration of
              {End-to-End} Networks",
  author   = "Santos, J F and Liu, W and Jiao, X and Neto, N V and Pollin, S
              and Marquez-Barja, J M and Moerman, I and DaSilva, L A",
  abstract = "Network slicing is one of the key enabling techniques for 5G,
              allowing network providers to support services with diverging
              requirements on top of their physical infrastructure. In this
              article, we address the limited support and oversimplified
              resource allocation on different network segments of existing
              end-to-end (E2E) orchestration solutions. We propose a
              hierarchical orchestration scheme for E2E networks, breaking down
              the E2E resource management and network slicing problems per
              network segment. We introduce a higher-level orchestrator, the
              hyperstrator, to coordinate the distributed orchestrators and
              deploy network slices (NSs) across multiple network segments. We
              developed a prototype implementation of the hyperstrator and
              validated our hierarchical orchestration concept with two
              proofof- concept experiments, showing the NS deployment and the
              impact of the resource allocation per network segment on the
              performance of NSs. The results show that the distributed nature
              of our orchestration architecture introduces negligible overhead
              for provisioning NSs in our particular setting, and confirm the
              need of a hyperstrator for coordinating network segments and
              ensuring consistent QoS for NSs.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  10,
  pages    = "16--22",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/S/Santos et al. 2020 - Breaking Down Network Slicing - Hierarchical Orchestration of End-to-End Networks.pdf",
  keywords = "5G mobile communication;quality of service;resource
              allocation;telecommunication network management;telecommunication
              network planning;network slicing;end-to-end networks;network
              providers;resource allocation;end-to-end orchestration
              solutions;hierarchical orchestration scheme;E2E resource
              management;network segment;higher-level orchestrator;distributed
              orchestrators;5G mobile communication;NS deployment;QoS;Resource
              management;Network slicing;Quality of service;5G mobile
              communication;Cloud computing;Servers;Wireless networks;Done;5G6G",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000406"
}

@ARTICLE{Hosseinalipour2020-sb,
  title    = "From Federated to Fog Learning: Distributed Machine Learning over
              Heterogeneous Wireless Networks",
  author   = "Hosseinalipour, S and Brinton, C G and Aggarwal, V and Dai, H and
              Chiang, M",
  abstract = "Machine learning (ML) tasks are becoming ubiquitous in today's
              network applications. Federated learning has emerged recently as
              a technique for training ML models at the network edge by
              leveraging processing capabilities across the nodes that collect
              the data. There are several challenges with employing
              conventional federated learning in contemporary networks, due to
              the significant heterogeneity in compute and communication
              capabilities that exist across devices. To address this, we
              advocate a new learning paradigm called fog learning, which will
              intelligently distribute ML model training across the continuum
              of nodes from edge devices to cloud servers. Fog learning
              enhances federated learning along three major dimensions:
              network, heterogeneity, and proximity. It considers a multi-layer
              hybrid learning framework consisting of heterogeneous devices
              with various proximities. It accounts for the topology structures
              of the local networks among the heterogeneous nodes at each
              network layer, orchestrating them for collaborative/cooperative
              learning through device-to-device communications. This migrates
              from star network topologies used for parameter transfers in
              federated learning to more distributed topologies at scale. We
              discuss several open research directions toward realizing fog
              learning.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  12,
  pages    = "41--47",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/H/Hosseinalipour et al. 2020 - From Federated to Fog Learning - Distributed Machine Learning over Heterogeneous Wireless Networks.pdf",
  keywords = "Training data;Network topology;Computational modeling;Wireless
              networks;Collaborative work;Edge computing;Device-to-device
              communication;ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000410"
}

@ARTICLE{Baek2021-ii,
  title    = "{3GPP} New Radio Release 16: Evolution of {5G} for Industrial
              Internet of Things",
  author   = "Baek, S and Kim, D and Tesanovic, M and Agiwal, A",
  abstract = "As a key component of the fourth industrial revolution, Industry
              4.0, robotics and autonomous systems (RAS) require enhanced
              ultra-reliable and low-latency communications, which may not be
              guaranteed under the first (Release 15) 5G communications
              standard called New Radio (NR), defined by the 3rd Generation
              Partnership Project (3GPP). Recently, 3GPP Release 16 NR
              standardization has been completed, and a large subset of new
              features targets evolution of existing NR standards for various
              vertical applications. It includes the Industrial Internet of
              Things (IIoT) Work Item, which aims to define enabling
              technologies to fulfill the stringent and time-sensitive
              requirements of RAS applications. In this article, we outline
              newly introduced key technical features of IIoT in NR and how
              they enable new RAS applications.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  1,
  pages    = "41--47",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/B/Baek et al. 2021 - 3GPP New Radio Release 16 - Evolution of 5G for Industrial Internet of Things.pdf",
  keywords = "Performance evaluation;Quality of service;New
              Radio;3GPP;Uplink;Industrial Internet of Things;5G6G",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000526"
}

@ARTICLE{Rodriguez2021-kx,
  title    = "{5G} Swarm Production: Advanced Industrial Manufacturing Concepts
              Enabled by Wireless Automation",
  author   = "Rodriguez, I and Mogensen, R S and Schj{\o}rring, A and
              Razzaghpour, M and Maldonado, R and Berardinelli, G and Adeogun,
              R and Christensen, P H and Mogensen, P and Madsen, O and
              M{\o}ller, C and Pocovi, G and Kolding, T and Rosa, C and
              J{\o}rgensen, B and Barbera, S",
  abstract = "This article presents an overview of current Industry 4.0 applied
              research topics, addressed from both the industrial production
              and wireless communication points of view. A roadmap toward
              achieving the more advanced industrial manufacturing visions and
              concepts, such as ``swarm production'' (nonlinear and fully
              decentralized production) is defined, highlighting relevant
              industrial use cases, their associated communication
              requirements, as well as the integrated technological wireless
              solutions applicable to each of them. Further, the article
              introduces the Aalborg University 5G Smart Production Lab, an
              industrial lab test environment specifically designed to
              prototype and demonstrate different Industrial IoT use cases
              enabled by the integration of robotics, edge-cloud platforms, and
              autonomous systems operated over wireless technologies such as
              4G, 5G, and Wi-Fi. Wireless performance results from various
              operational trials are also presented for two use cases: wireless
              control of industrial production and wireless control of
              autonomous mobile robots.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  1,
  pages    = "48--54",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/R/Rodriguez et al. 2021 - 5G Swarm Production - Advanced Industrial Manufacturing Concepts Enabled by Wireless Automation.pdf",
  keywords = "Wireless communication;5G mobile communication;Service
              robots;Production;Manufacturing;Mobile robots;Wireless
              fidelity;5G6G",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000560"
}

@ARTICLE{Sankhe2021-mu,
  title    = "{ReLy}: Machine Learning for {Ultra-Reliable}, {Low-Latency}
              Messaging in Industrial Robots",
  author   = "Sankhe, Kunal and Jaisinghani, Dheryta and Chowdhury, Kaushik",
  abstract = "Robotic factory floors are transforming the manufacturing sector
              by delivering an unprecedented boost to productivity. However,
              such a paradigm raises questions on safety and coordination,
              especially when in the presence of unexpected events.
              Time-critical communication messages for such industrial robots
              mandate the requirement of ultra-reliable low-latency
              communication (URLLC). Classical WiFi-connected industrial robots
              often suffer from the traditional dense network problems
              prevalent in production WiFi networks, where transmission of an
              emergency notification packet is ``best effort,'' devoid of time
              guarantees. In this work, we propose a machine-learning-based
              framework called ReLy that intelligently embeds the time-critical
              messages in the preamble of outgoing frames at the transmitter.
              These messages are inferred from the channel state information
              variations at the receiver. As ReLy is implemented entirely at
              the physical layer, the transmitter is able to deliver
              information within 5 ms latency and ultra-high reliability of 99
              percent. We experimentally demonstrate the feasibility of
              achieving URLLC with moving robots in a busy workshop with a
              number of other peer robots, equipment, desks, and robotic arms,
              as expected in a typical factory setting.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  4,
  pages    = "75--81",
  month    =  apr,
  year     =  2021,
  keywords = "Service robots;Transmitters;Robot kinematics;Receivers;Ultra
              reliable low latency communication;Production facilities;Time
              factors;Machine learning;MLNetworking;Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000598"
}

@ARTICLE{Garcia-Rodriguez2021-zg,
  title    = "{IEEE} 802.11be: {Wi-Fi} 7 Strikes Back",
  author   = "Garcia-Rodriguez, Adrian and L{\'o}pez-P{\'e}rez, David and
              Galati-Giordano, Lorenzo and Geraci, Giovanni",
  abstract = "As hordes of data-hungry devices challenge its current
              capabilities, Wi-Fi strikes back with 802.11be, alias Wi-Fi 7.
              This brand new amendment promises a (r)evolution of unlicensed
              wireless connectivity as we know it. To appreciate its foreseen
              impact, we start by overviewing the latest Wi-Fi-related news,
              with emphasis on the recently launched Wi-Fi 6E certification
              program. We then provide an updated digest of 802.11be essential
              features, vouching for multi-AP coordination as a must-have for
              critical and latency-sensitive applications. We finally get down
              to the nitty-gritty of one of its most enticing implementations
              -- coordinated beamforming -- for which our standard-compliant
              simulations confirm near-tenfold reductions in worst case delays.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  4,
  pages    = "102--108",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/G/Garcia-Rodriguez et al. 2021 - IEEE 802.11be - Wi-Fi 7 Strikes Back.pdf",
  keywords = "Wireless communication;Wireless fidelity;Telecommunication
              traffic;Array signal processing;Big Data;Data
              processing;Mobile\_Wireless;wifi",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000711"
}

@ARTICLE{Shen2021-zk,
  title    = "Resource Rationing for Wireless Federated Learning: Concept,
              Benefits, and Challenges",
  author   = "Shen, Cong and Xu, Jie and Zheng, Sihui and Chen, Xiang",
  abstract = "We advocate a new resource allocation framework, which we call
              resource rationing, for wireless federated learning (FL). Unlike
              existing resource allocation methods for FL, resource rationing
              focuses on balancing resources across learning rounds so that
              their collective impact on FL performance is explicitly captured.
              This new framework can be integrated seamlessly with existing
              resource allocation schemes to optimize the convergence of FL. In
              particular, a novel ``lat-er-is-better'' principle is at the
              front and center of resource rationing and is validated
              empirically in several instances of wireless FL. We also point
              out technical challenges and research opportunities that are
              worth pursuing. Resource rationing highlights the benefits of
              treating the emerging FL as a new class of service that has its
              own characteristics, and designing communication algorithms for
              this particular service.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  5,
  pages    = "82--87",
  month    =  may,
  year     =  2021,
  file     = "All Papers/S/Shen et al. 2021 - Resource Rationing for Wireless Federated Learning - Concept, Benefits, and Challenges.pdf",
  keywords = "Wireless communication;Stochastic processes;Collaborative
              work;Resource management;Data science;Convergence;Artificial
              intelligence;Net4ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000744"
}

@ARTICLE{Kosek2021-ts,
  title    = "Beyond {QUIC} v1: A First Look at Recent Transport Layer {IETF}
              Standardization Efforts",
  author   = "Kosek, Mike and Shreedhar, Tanya and Bajpai, Vaibhav",
  abstract = "The transport layer is ossified. With most of the research and
              deployment efforts in the past decade focusing on the
              Transmission Control Protocol (TCP) and its extensions, the QUIC
              standardization by the Internet Engineering Task Force (IETF) is
              to be finalized in early 2021. In addition to addressing the most
              urgent issues of TCP, QUIC ensures its future extendibility and
              is destined to drastically change the transport protocol
              landscape. In this work, we present a first look at emerging
              protocols and their IETF standardization efforts beyond QUIC v1.
              While multiple proposed extensions improve on QUIC itself,
              Multiplexed Application Substrate over QUIC Encryption (MASQUE)
              as well as WebTransport present different approaches to address
              long-standing problems, and their interplay expands on QUIC's
              take to address transport layer ossification challenges.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  4,
  pages    = "24--29",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/K/Kosek et al. 2021 - Beyond QUIC v1 - A First Look at Recent Transport Layer IETF Standardization Efforts.pdf",
  keywords = "Transport
              protocols;Multiplexing;TCPIP;Standardization;Internet;Virtual
              private networks;ComputerNetworks",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000877"
}

@ARTICLE{De_Coninck2021-nj,
  title    = "Multiflow {QUIC}: A Generic Multipath Transport Protocol",
  author   = "De Coninck, Quentin and Bonaventure, Olivier",
  abstract = "Transport protocols and their multipath variants used to assume
              that network paths are symmetric and bidirectional. Actually,
              network asymmetries (ADSL, satellite) are frequently observed,
              and some network paths can flow packets in only one direction
              (e.g., due to a firewall). This article proposes to consider the
              more generic notion of unidirectional flows. Based on them, we
              design and implement Multiflow QUIC, a variant of QUIC that is
              aware of network asymmetries to spread data over multiple network
              paths. Our evaluation shows that this more generic approach is
              beneficial in asymmetric cases while being equivalent to
              multipath approaches in symmetric ones.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  5,
  pages    = "108--113",
  month    =  may,
  year     =  2021,
  keywords = "Transport protocols;Satellites;Firewalls
              (computing);Downlink;Scheduling;Reliability;CongestionControl",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000892"
}

@ARTICLE{Parizotto2021-hb,
  title    = "Consistent Composition and Modular Data Plane Programming",
  author   = "Parizotto, Ricardo and Castanheira, Lucas and Bonetti, Fernanda
              and Santos, Anderson and Schaeffer-Filho, Alberto",
  abstract = "Emerging programmable data planes enable us to modify switch
              behavior using software abstractions. However, developing the
              data plane software is challenging and typically made in a
              monolithic manner. We argue that the data plane should be
              developed modularly and employ additional abstractions to compose
              data plane programs and steer packets between them. This article
              presents Programming In-Network Modular Extensions (PRIME), a
              mechanism to compose data plane program modules and define how to
              steer traffic through these modules. Additionally, the system
              employs techniques to ensure that updating the steering
              configuration is consistent according to end-to-end forwarding
              guarantees. We deployed use cases with existing P4 programs on
              BMv2, and the results show that PRIME can compose programs with
              small overheads in terms of latency, the number of forwarding
              tables, and parser states.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  6,
  pages    = "60--65",
  month    =  jun,
  year     =  2021,
  keywords = "Switches;Programming;Software engineering;ProgrammableNetworks",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000904"
}

@ARTICLE{Pajo2021-ps,
  title    = "{ANNs} Going Beyond Time Series Forecasting: An Urban Network
              Perspective",
  author   = "Pajo, Jane Frances and Kousiouris, George and Kyriazis,
              Dimosthenis and Bruschi, Roberto and Davoli, Franco",
  abstract = "5G is expected to bring forth disruptive indus-trial-societal
              transformation by enabling a broad catalog of (radically new,
              highly heterogeneous) applications and services. This scenario
              has called for zero-touch network and service management (ZSM).
              With the recent advancements in artificial intelligence, key ZSM
              capabilities such as the runtime prediction of user demands can
              be facilitated by data-driven and machine learning methods. In
              this respect, the article proposes a runtime prediction approach
              that transforms time series forecasting into a simpler
              multivariate regression problem with artificial neural networks
              (ANNs), structurally optimized with a genetic algorithm (GA)
              metaheuristic. Leveraging on a novel set of input features that
              capture seasonality and calendar effects, the proposed approach
              removes the prediction accuracy's dependence on the temporal
              succession of input data and the forecast horizon. Evaluation
              results based on real telecommunications data show that the
              GA-optimized ANN regressor has better prediction performance. It
              achieved average improvements of 59 percent and 86 percent
              compared to 1-day and 1-hour ahead forecasts obtained with
              state-of-the-art multi-seasonal time series and long short-term
              memory forecasting models, respectively. Furthermore, despite its
              longer training times compared to the baseline models, the
              proposed ANN regressor relaxes the monitoring requirements in 5G
              dynamic management systems by allowing less frequent retraining
              offline.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  5,
  pages    = "88--94",
  month    =  may,
  year     =  2021,
  keywords = "Training;Runtime;5G mobile communication;Time series
              analysis;Urban areas;Artificial neural networks;Predictive
              models;Artificial intelligence;Data science;NetworkTraffic",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2000990"
}

@ARTICLE{Nooruzzaman2021-fu,
  title    = "Hyperscale Data Center Networks with Transparent {HyperX}
              Architecture",
  author   = "Nooruzzaman, Md and Fernando, Xavier",
  abstract = "Due to rapidly growing cloud-based applications and services,
              data center networks handle huge traffic and they started
              receiving significant attention recently. In this article, we
              first review various trends in data center network architectures.
              Then we propose a novel hyperscale architecture named Transparent
              HyperX (TH) and compare it to existing real-world architectures.
              The TH concept is inspired by the conventional HyperX topology,
              but designed with a number of additional all-optical diagonal
              links that maintain pod-to-pod path cost below two hops and
              ensure more transparent connectivity between pods throughout the
              entire data center. This reduction in path cost not only results
              in lower latency, but also saves on the number of switches and
              transponders, which eventually reduces the power consumption.
              This architecture also offers high scalability and supports over
              1 million servers using 100 radix switches.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  6,
  pages    = "120--125",
  month    =  jun,
  year     =  2021,
  keywords = "Data centers;Energy consumption;Power
              demand;Scalability;Telecommunication traffic;Network
              architecture;Market research;DataCenter",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2001070"
}

@ARTICLE{Pan2021-rk,
  title    = "Reconfigurable Intelligent Surfaces for {6G} Systems: Principles,
              Applications, and Research Directions",
  author   = "Pan, Cunhua and Ren, Hong and Wang, Kezhi and Kolb, Jonas
              Florentin and Elkashlan, Maged and Chen, Ming and Di Renzo, Marco
              and Hao, Yang and Wang, Jiangzhou and Swindlehurst, A Lee and
              You, Xiaohu and Hanzo, Lajos",
  abstract = "Reconfigurable intelligent surfaces (RISs) or intelligent
              reflecting surfaces (IRSs) are regarded as one of the most
              promising and revolutionizing techniques for enhancing the
              spectrum and/ or energy efficiency of wireless systems. These
              devices are capable of reconfiguring the wireless propagation
              environment by carefully tuning the phase shifts of a large
              number of low-cost passive reflecting elements. In this article,
              we aim to answer four fundmental questions: 1) Why do we need
              RISs? 2) What is an RIS? 3) What are RIS's applications? 4) What
              are the relevant challenges and future research directions? In
              response, eight promising research directions are pointed out.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  6,
  pages    = "14--20",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/P/Pan et al. 2021 - Reconfigurable Intelligent Surfaces for 6G Systems - Principles, Applications, and Research Directions.pdf",
  keywords = "Wireless communication;6G mobile
              communication;NOMA;Reconfigurable intelligent
              surfaces;Hardware;Energy efficiency;5G mobile
              communication;Mobile\_Wireless;ServicesDescription",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2001076"
}

@ARTICLE{Hoydis2021-gx,
  title    = "Toward a {6G} {AI-Native} Air Interface",
  author   = "Hoydis, Jakob and Aoudia, Fay{\c c}al Ait and Valcarce, Alvaro
              and Viswanathan, Harish",
  abstract = "Each generation of cellular communication systems is marked by a
              defining disruptive technology of its time, such as OFDM for 4G
              or Massive MIMO for 5G. Since AI is the defining technology of
              our time, it is natural to ask what role it could play for 6G.
              While it is clear that 6G must cater to the needs of large
              distributed learning systems, it is less certain if AI will play
              a defining role in the design of 6G itself. The goal of this
              article is to paint a vision of a new air interface that is
              partially designed by AI to enable optimized communication
              schemes for any hardware, radio environment, and application.",
  journal  = "IEEE Commun. Mag.",
  volume   =  59,
  number   =  5,
  pages    = "76--81",
  month    =  may,
  year     =  2021,
  file     = "All Papers/H/Hoydis et al. 2021 - Toward a 6G AI-Native Air Interface.pdf",
  keywords = "6G mobile communication;Communication systems;Distance
              learning;OFDM;Communication channels;Massive MIMO;Hardware;Data
              science;Artificial intelligence;Mobile\_Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.001.2001187"
}

@ARTICLE{Larsson2014-ac,
  title    = "Massive {MIMO} for next generation wireless systems",
  author   = "Larsson, E G and Edfors, O and Tufvesson, F and Marzetta, T L",
  abstract = "Multi-user MIMO offers big advantages over conventional
              point-to-point MIMO: it works with cheap single-antenna
              terminals, a rich scattering environment is not required, and
              resource allocation is simplified because every active terminal
              utilizes all of the time-frequency bins. However, multi-user
              MIMO, as originally envisioned, with roughly equal numbers of
              service antennas and terminals and frequency-division duplex
              operation, is not a scalable technology. Massive MIMO (also known
              as large-scale antenna systems, very large MIMO, hyper MIMO,
              full-dimension MIMO, and ARGOS) makes a clean break with current
              practice through the use of a large excess of service antennas
              over active terminals and time-division duplex operation. Extra
              antennas help by focusing energy into ever smaller regions of
              space to bring huge improvements in throughput and radiated
              energy efficiency. Other benefits of massive MIMO include
              extensive use of inexpensive low-power components, reduced
              latency, simplification of the MAC layer, and robustness against
              intentional jamming. The anticipated throughput depends on the
              propagation environment providing asymptotically orthogonal
              channels to the terminals, but so far experiments have not
              disclosed any limitations in this regard. While massive MIMO
              renders many traditional research problems irrelevant, it
              uncovers entirely new problems that urgently need attention: the
              challenge of making many low-cost low-precision components that
              work effectively together, acquisition and synchronization for
              newly joined terminals, the exploitation of extra degrees of
              freedom provided by the excess of service antennas, reducing
              internal power consumption to achieve total energy efficiency
              reductions, and finding new deployment scenarios. This article
              presents an overview of the massive MIMO concept and contemporary
              research on the topic.",
  journal  = "IEEE Commun. Mag.",
  volume   =  52,
  number   =  2,
  pages    = "186--195",
  month    =  feb,
  year     =  2014,
  file     = "All Papers/L/Larsson et al. 2014 - Massive MIMO for next generation wireless systems.pdf",
  keywords = "access protocols;antenna arrays;frequency division
              multiplexing;MIMO communication;next generation wireless
              systems;massive MIMO;multiuser MIMO;single-antenna
              terminals;resource allocation;time-frequency
              bins;frequency-division duplex operation;large-scale antenna
              systems;very large MIMO;hyper MIMO;full-dimension
              MIMO;ARGOS;service antennas;time-division duplex operation;MAC
              layer;intentional jamming;MIMO;Base stations;Antenna
              arrays;Uplink;Fading;Downlink;Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.2014.6736761"
}

@ARTICLE{Zeydan2016-iz,
  title    = "Big data caching for networking: moving from cloud to edge",
  author   = "Zeydan, E and Bastug, E and Bennis, M and Kader, M A and
              Karatepe, I A and Er, A S and Debbah, M",
  abstract = "In order to cope with the relentless data tsunami in 5G wireless
              networks, current approaches such as acquiring new spectrum,
              deploying more BSs, and increasing nodes in mobile packet core
              networks are becoming ineffective in terms of scalability, cost
              and flexibility. In this regard, context- aware 5G networks with
              edge/cloud computing and exploitation of big data analytics can
              yield significant gains for mobile operators. In this article,
              proactive content caching in 5G wireless networks is investigated
              in which a big-data-enabled architecture is proposed. In this
              practical architecture, a vast amount of data is harnessed for
              content popularity estimation, and strategic contents are cached
              at BSs to achieve higher user satisfaction and backhaul
              offloading. To validate the proposed solution, we consider a
              real-world case study where several hours worth of mobile data
              traffic is collected from a major telecom operator in Turkey, and
              big-data-enabled analysis is carried out, leveraging tools from
              machine learning. Based on the available information and storage
              capacity, numerical studies show that several gains are achieved
              in terms of both user satisfaction and backhaul offloading. For
              example, in the case of 16 BSs with 30 percent of content ratings
              and 13 GB storage size (78 percent of total library size),
              proactive caching yields 100 percent user satisfaction and
              offloads 98 percent of the backhaul.",
  journal  = "IEEE Commun. Mag.",
  volume   =  54,
  number   =  9,
  pages    = "36--42",
  month    =  sep,
  year     =  2016,
  file     = "All Papers/Z/Zeydan et al. 2016 - Big data caching for networking - moving from cloud to edge.pdf",
  keywords = "5G mobile communication;Big Data;cloud computing;learning
              (artificial intelligence);telecommunication
              computing;telecommunication traffic;Big data caching;cloud to
              edge;5G wireless networks;data tsunami;mobile packet core
              networks;context- aware 5G networks;edge-cloud computing;Big data
              analytics;backhaul offloading;mobile data traffic;Turkey;machine
              learning;Big data;Mobile computing;Computer architecture;5G
              mobile communication;Wireless networks;5G6G",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.2016.7565185"
}

@ARTICLE{Letaief2019-vc,
  title    = "The Roadmap to 6G: {AI} Empowered Wireless Networks",
  author   = "Letaief, K B and Chen, W and Shi, Y and Zhang, J and Zhang, Y A",
  abstract = "The recent upsurge of diversified mobile applications, especially
              those supported by AI, is spurring heated discussions on the
              future evolution of wireless communications. While 5G is being
              deployed around the world, efforts from industry and academia
              have started to look beyond 5G and conceptualize 6G. We envision
              6G to undergo an unprecedented transformation that will make it
              substantially different from the previous generations of wireless
              cellular systems. In particular, 6G will go beyond mobile
              Internet and will be required to support ubiquitous AI services
              from the core to the end devices of the network. Meanwhile, AI
              will play a critical role in designing and optimizing 6G
              architectures, protocols, and operations. In this article, we
              discuss potential technologies for 6G to enable mobile AI
              applications, as well as AI-enabled methodologies for 6G network
              design and optimization. Key trends in the evolution to 6G will
              also be discussed.",
  journal  = "IEEE Commun. Mag.",
  volume   =  57,
  number   =  8,
  pages    = "84--90",
  month    =  aug,
  year     =  2019,
  file     = "All Papers/L/Letaief et al. 2019 - The Roadmap to 6G - AI Empowered Wireless Networks.pdf",
  keywords = "artificial intelligence;cellular radio;Internet;mobile
              computing;mobile Internet;ubiquitous AI services;mobile AI
              applications;AI-enabled methodologies;wireless
              communications;wireless cellular systems;mobile applications;AI
              empowered wireless networks;6G architecture
              optimization;protocols;6G network design;6G mobile
              communication;5G mobile communication;Artificial
              intelligence;Hardware;Computer architecture;Wireless
              communication;Protocols;Done;Low quality;5G6G;Wireless;ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.2019.1900271"
}

@ARTICLE{Atov2020-vx,
  title    = "Data Science and Artificial Intelligence for Communications",
  author   = "Atov, I and Chen, K-C and Kamal, A and Louta, M",
  abstract = "The articles in this special section focus on data science and
              artificial intelligence for communication applications. The
              popularity of this Series continues to grow, attracting a lot of
              attention from both researchers and practitioners who are working
              to address various challenges in the network field through the
              advances of artificial intelligence (AI), machine learning (ML),
              and big data analysis. This trend toward learning -based,
              data-driven approaches has been mainly motivated by two things:
              the amount of available data retrieved from devices and network
              equipment, and the need to tune large numbers of networ k
              operational parameters in order to meet the frequently changing
              needs of the services.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  10,
  pages    = "56--57",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/A/Atov et al. 2020 - Data Science and Artificial Intelligence for Communications.pdf",
  keywords = "Special issues and sections;Data science;Artificial
              intelligence;5G mobile communication;Radio frequency;Deel
              learning;Machine learning;ML",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.2020.9247523"
}

@ARTICLE{Alexiou2020-mh,
  title    = "{THz} Communications: A Catalyst for the Wireless Future",
  author   = "Alexiou, A and Andreev, S and Fodor, G and Nagatsuma, T",
  abstract = "The articles in this special section focus on THz communications.",
  journal  = "IEEE Commun. Mag.",
  volume   =  58,
  number   =  11,
  pages    = "12--13",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/A/Alexiou et al. 2020 - THz Communications - A Catalyst for the Wireless Future.pdf",
  keywords = "Special issues and sections;Optical fibers;Computer
              architecture;Microprocessors;IEEE 802.15 Standard;High-speed
              optical techniques;Wireless communication;Mobile\_Wireless",
  issn     = "0163-6804, 1558-1896",
  doi      = "10.1109/MCOM.2020.9269507"
}

@ARTICLE{Vakali2003-cu,
  title    = "Content delivery networks: status and trends",
  author   = "Vakali, A and Pallis, G",
  abstract = "CDNs improve network performance and offer fast and reliable
              applications and services by distributing content to cache
              servers located close to users. The Web's growth has transformed
              communications and business services such that speed, accuracy,
              and availability of network-delivered content has become
              absolutely critical - both on their own terms and in terms of
              measuring Web performance. Proxy servers partially address the
              need for rapid content delivery by providing multiple clients
              with a shared cache location. In this context, if a requested
              object exists in a cache (and the cached version has not
              expired), clients get a cached copy, which typically reduces
              delivery time. CDNs act as trusted overlay networks that offer
              high-performance delivery of common Web objects, static data, and
              rich multimedia content by distributing content load among
              servers that are close to the clients. CDN benefits include
              reduced origin server load, reduced latency for end users, and
              increased throughput. CDNs can also improve Web scalability and
              disperse flash-crowd events. Here we offer an overview of the CDN
              architecture and popular CDN service providers.",
  journal  = "IEEE Internet Comput.",
  volume   =  7,
  number   =  6,
  pages    = "68--74",
  month    =  nov,
  year     =  2003,
  keywords = "Network servers;Delay;File
              servers;Collaboration;Availability;Bandwidth;Telecommunication
              traffic;Terrorism;Throughput;Scalability",
  issn     = "1089-7801, 1941-0131",
  doi      = "10.1109/MIC.2003.1250586"
}

@ARTICLE{Wu2020-bf,
  title    = "State of the Art and Research Challenges in the Security
              Technologies of Network Function Virtualization",
  author   = "Wu, X and Hou, K and Leng, X and Li, X and Yu, Y and Wu, B and
              Chen, Y",
  abstract = "In recent years, network function virtualization (NFV) has drawn
              considerable attention due to its potential for service agility
              and low total cost of ownership. As the core of an NFV
              implementation, security issues in the management and control
              platform must be comprehensively addressed-from architectural
              concept to deployment. In this article, we first analyze the
              state of the security architecture based on ETSI-NFV, and then
              propose useful security practices for an NFV-based management and
              control ecosystem. To encourage future research, we also identify
              the ongoing research challenges and open security issues relevant
              to the NFV.",
  journal  = "IEEE Internet Comput.",
  volume   =  24,
  number   =  1,
  pages    = "25--35",
  month    =  jan,
  year     =  2020,
  file     = "All Papers/W/Wu et al. 2020 - State of the Art and Research Challenges in the Security Technologies of Network Function Virtualization.pdf",
  keywords = "computer network security;virtualisation;network function
              virtualization;security technologies;ETSI-NFV;Computer
              security;Computer architecture;Monitoring;Network function
              virtualization;Hardware;Ecosystems;Network function
              virtualization;Service chain;Microservice;Virtualized network
              function;5G6G",
  issn     = "1089-7801, 1941-0131",
  doi      = "10.1109/MIC.2019.2956712"
}

@ARTICLE{Qiao2020-ed,
  title    = "{6G} Vision: An {AI-Driven} Decentralized Network and Service
              Architecture",
  author   = "Qiao, X and Huang, Y and Dustdar, S and Chen, J",
  abstract = "Recently, following the rapid commercial deployment of 5G
              networks, next-generation mobile communication technology (6G)
              has been attracting increasing attention from global researchers
              and engineers. 6G is envisioned as a distributed, decentralized,
              and intelligent innovative network. However, existing application
              provisioning is still based on a centralized service
              architecture, ubiquitous edge computing, and decentralized AI
              technologies have not been fully exploited. In this article, we
              analyze the problems faced by existing centralized service
              provisioning architecture, and propose design principles for a
              decentralized network and service architecture for a future 6G
              network. Finally, we discuss several open research problems to
              inspire readers to address these.",
  journal  = "IEEE Internet Comput.",
  volume   =  24,
  number   =  4,
  pages    = "33--40",
  month    =  jul,
  year     =  2020,
  file     = "All Papers/Q/Qiao et al. 2020 - 6G Vision - An AI-Driven Decentralized Network and Service Architecture.pdf",
  keywords = "5G mobile communication;6G mobile communication;artificial
              intelligence;next generation networks;quality of
              service;telecommunication services;rapid commercial
              deployment;next-generation mobile communication technology;global
              researchers;distributed network;decentralized,
              network;intelligent innovative network;existing application
              provisioning;centralized service architecture;ubiquitous edge
              computing;AI technologies;centralized service provisioning
              architecture;decentralized network;future 6G network;open
              research problems;6G mobile communication;Computer
              architecture;5G mobile communication;Artificial
              intelligence;Servers;Cloud computing;Computational
              modeling;5G6G;ServicesDescription",
  issn     = "1089-7801, 1941-0131",
  doi      = "10.1109/MIC.2020.2987738"
}

@ARTICLE{Sheoran2021-wh,
  title    = "{AI-Driven} Provisioning in the {5G} Core",
  author   = "Sheoran, Amit and Fahmy, Sonia and Cao, Lianjie and Sharma,
              Puneet",
  abstract = "Network slicing enables communication service providers to
              partition physical infrastructure into logically independent
              networks. Network slices must be provisioned to meet the
              service-level objectives (SLOs) of disparate offerings, such as
              enhanced mobile broadband, ultrareliable low-latency
              communications, and massive machine-type communications. Network
              orchestrators must customize service placement and scaling to
              achieve the SLO of each network slice. In this article, we
              discuss the challenges encountered by network orchestrators in
              allocating resources to disparate 5G network slices, and propose
              the use of artificial intelligence to make core placement and
              scaling decisions that meet the requirements of network slices
              deployed on shared infrastructure. We explore how artificial
              intelligence-driven scaling algorithms, coupled with
              functionality-aware placement, can enable providers to design
              closed-loop solutions to meet the disparate SLOs of future
              network slices.",
  journal  = "IEEE Internet Comput.",
  volume   =  25,
  number   =  2,
  pages    = "18--25",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/S/Sheoran et al. 2021 - AI-Driven Provisioning in the 5G Core.pdf",
  keywords = "5G mobile communication;Resource management;Noise
              measurement;Ultra reliable low latency communication;Network
              slicing;Communication systems;Data centers;Artificial
              intelligence;Low latency communication;Enhanced mobile
              broadband;Telecommunications;5G;AI;Network Functions
              Virtualization;5G6G",
  issn     = "1089-7801, 1941-0131",
  doi      = "10.1109/MIC.2021.3056230"
}

@ARTICLE{Balasubramanian2021-fg,
  title    = "{RIC}: A {RAN} Intelligent Controller Platform for {AI-Enabled}
              Cellular Networks",
  author   = "Balasubramanian, Bharath and Daniels, E Scott and Hiltunen, Matti
              and Jana, Rittwik and Joshi, Kaustubh and Sivaraj, Rajarajan and
              Tran, Tuyen X and Wang, Chengwei",
  abstract = "With the emergence of 5G, network densification, and richer and
              more demanding applications, the radio access network (RAN)---a
              key component of the cellular network infrastructure---will
              become increasingly complex. To tackle this complexity, it is
              critical for the RAN to be able to automate the process of
              deploying, optimizing, and operating while leveraging novel
              data-driven technologies to ultimately improve the end-user
              quality of experience. In this article, we disaggregate the
              traditional monolithic control plane (CP) RAN architecture and
              introduce a RAN Intelligent Controller (RIC) platform decoupling
              the control and data planes of the RAN driving an intelligent and
              continuously evolving radio network by fostering network openness
              and empowering network intelligence with AI-enabled applications.
              We provide functional and software architectures of the RIC and
              discuss its design challenges. We elaborate how the RIC can
              enable near-real-time network optimization in 5G for the
              dual-connectivity use case using machine learning control loops.
              Finally, we provide preliminary results to evaluate the
              performance of our open-source RIC platform.",
  journal  = "IEEE Internet Comput.",
  volume   =  25,
  number   =  2,
  pages    = "7--17",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/B/Balasubramanian et al. 2021 - RIC - A RAN Intelligent Controller Platform for AI-Enabled Cellular Networks.pdf",
  keywords = "5G mobile communication;Computer architecture;Cloud
              computing;Process control;Cellular networks;Resource
              management;Real-time systems;Scientific computing;Radio Access
              Network;Software Defined Network;RAN Disaggregation;Network
              Intelligence;5G6G",
  issn     = "1089-7801, 1941-0131",
  doi      = "10.1109/MIC.2021.3062487"
}

@ARTICLE{Kongetira2005-mk,
  title    = "Niagara: a 32-way multithreaded Sparc processor",
  author   = "Kongetira, P and Aingaran, K and Olukotun, K",
  abstract = "The Niagara processor implements a thread-rich architecture
              designed to provide a high-performance solution for commercial
              server applications. This is an entirely new implementation of
              the Sparc V9 architectural specification, which exploits large
              amounts of on-chip parallelism to provide high throughput. The
              hardware supports 32 threads with a memory subsystem consisting
              of an on-board crossbar, level-2 cache, and memory controllers
              for a highly integrated design that exploits the thread-level
              parallelism inherent to server applications, while targeting low
              levels of power consumption.",
  journal  = "IEEE Micro",
  volume   =  25,
  number   =  2,
  pages    = "21--29",
  month    =  mar,
  year     =  2005,
  keywords = "Power supplies;Costs;Application
              software;Bandwidth;Clocks;Frequency;Hardware;Foot;Enterprise
              resource planning;Microprocessors and microcomputers;Shared
              memory;Multithreaded processors;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/MM.2005.35"
}

@ARTICLE{Andrews2006-is,
  title    = "Xbox 360 System Architecture",
  author   = "Andrews, J and Baker, N",
  abstract = "This article covers the Xbox 360's high-level technical
              requirements, a short system overview, and details of the CPU and
              the GPU. The Xbox 360 contains an aggressive hardware
              architecture and implementation targeted at game console
              workloads. The core silicon implements the product designers'
              goal of providing game developers a hardware platform to
              implement their next-generation game ambitions. The core chips
              include the standard conceptual blocks of CPU, graphics
              processing unit (GPU), memory, and I/O. Each of these components
              and their interconnections are customized to provide a
              user-friendly game console product. The authors describe their
              architectural trade-offs and summarize the system's software
              programming support",
  journal  = "IEEE Micro",
  volume   =  26,
  number   =  2,
  pages    = "25--37",
  month    =  mar,
  year     =  2006,
  keywords = "Hardware;Computer architecture;Silicon;Bandwidth;Streaming
              media;Decoding;Software tools;Design
              optimization;Pipelines;Scalability;Xbox 360;Microsoft;game
              console systems;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/MM.2006.45"
}

@ARTICLE{Lindholm2008-gu,
  title    = "{NVIDIA} Tesla: A Unified Graphics and Computing Architecture",
  author   = "Lindholm, Erik and Nickolls, John and Oberman, Stuart and
              Montrym, John",
  abstract = "To enable flexible, programmable graphics and high-performance
              computing, NVIDIA has developed the Tesla scalable unified
              graphics and parallel computing architecture. Its scalable
              parallel array of processors is massively multithreaded and
              programmable in C or via graphics APIs.",
  journal  = "IEEE Micro",
  volume   =  28,
  number   =  2,
  pages    = "39--55",
  month    =  mar,
  year     =  2008,
  keywords = "Graphics;Computer architecture;Parallel
              processing;Pipelines;Concurrent computing;Load
              management;Multicore processing;Parallel programming;Portable
              computers;Workstations;Hot Chips 19;GPU;parallel
              processor;SIMT;SIMD;unified graphics and parallel computing
              architecture;graphics processing unit;cooperative thread
              array;Tesla;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/MM.2008.31"
}

@ARTICLE{Nickolls2010-wj,
  title    = "The {GPU} Computing Era",
  author   = "Nickolls, John and Dally, William J",
  abstract = "GPU computing is at a tipping point, becoming more widely used in
              demanding consumer applications and high-performance computing.
              This article describes the rapid evolution of GPU
              architectures-from graphics processors to massively parallel
              many-core multiprocessors, recent developments in GPU computing
              architectures, and how the enthusiastic adoption of CPU+GPU
              coprocessing is accelerating parallel applications.",
  journal  = "IEEE Micro",
  volume   =  30,
  number   =  2,
  pages    = "56--69",
  month    =  mar,
  year     =  2010,
  keywords = "Graphics processing unit;Parallel processing;Concurrent
              computing;Computer applications;Pipelines;Layout;Computer
              architecture;Acceleration;Pervasive computing;GPU
              computing;CUDA;scalable parallel computing;heterogeneous CPU+;GPU
              coprocessing;Tesla GPU architecture;Fermi GPU
              architecture;NVIDIA.;GDS",
  issn     = "1937-4143",
  doi      = "10.1109/MM.2010.41"
}

@ARTICLE{Kwak2020-hf,
  title    = "Collaboration of Network Operators and Cloud Providers in
              {Software-Controlled} Networks",
  author   = "Kwak, J and Le, L B and Iosifidis, G and Lee, K and Kim, D I",
  abstract = "The next generation of networks will need to support massive
              requests for services that have very stringent performance
              requirements, and their delivery involves significant computing
              and storage resources. Prominent examples are the mobile
              augmented and virtual reality services, processing of large-scale
              IoT data, and mobile data analytics. In order to address these
              arising challenges, mobile network operators need innovative
              solutions, and one such is the close collaboration with the cloud
              service providers. In this article, we propose a novel system
              architecture that integrates the infrastructure of Mobile Network
              Operator (MNO) and Cloud Service Provider (CSP), leveraging
              recent developments in network softwarization that allows the
              unified control of the network, computing, and storage resources.
              We envision a multi-tier architecture where the CSP and MNO
              exchange real-time information about their resource availability
              and the users' needs, and jointly devise the servicing policy. We
              present a blueprint of the architecture, several application
              examples, and a numerical case study that focuses on distributed
              computing.",
  journal  = "IEEE Netw.",
  volume   =  34,
  number   =  5,
  pages    = "98--105",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/K/Kwak et al. 2020 - Collaboration of Network Operators and Cloud Providers in Software-Controlled Networks.pdf",
  keywords = "cloud computing;data analysis;Internet of Things;mobile
              computing;virtual reality;large-scale IoT data;mobile data
              analytics;mobile network operators;cloud service
              providers;network softwarization;storage resources;servicing
              policy;software-controlled networks;massive requests;performance
              requirements;mobile augmented reality services;virtual reality
              services;CSP;MNO;user needs;Collaboration;Computer
              architecture;Cloud computing;Data
              analysis;Servers;Delays;Integrated circuits;EdgeFogCloudIoT",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.001.1800329"
}

@ARTICLE{Saad2020-pb,
  title    = "A Vision of {6G} Wireless Systems: Applications, Trends,
              Technologies, and Open Research Problems",
  author   = "Saad, Walid and Bennis, Mehdi and Chen, Mingzhe",
  abstract = "The ongoing deployment of 5G cellular systems is continuously
              exposing the inherent limitations of this system, compared to its
              original premise as an enabler for Internet of Everything
              applications. These 5G drawbacks are spurring worldwide
              activities focused on defining the next-generation 6G wireless
              system that can truly integrate far-reaching applications ranging
              from autonomous systems to extended reality. Despite recent 6G
              initiatives (one example is the 6Genesis project in Finland), the
              fundamental architectural and performance components of 6G remain
              largely undefined. In this article, we present a holistic,
              forward-looking vision that defines the tenets of a 6G system. We
              opine that 6G will not be a mere exploration of more spectrum at
              high-frequency bands, but it will rather be a convergence of
              upcoming technological trends driven by exciting, underlying
              services. In this regard, we first identify the primary drivers
              of 6G systems, in terms of applications and accompanying
              technological trends. Then, we propose a new set of service
              classes and expose their target 6G performance requirements. We
              then identify the enabling technologies for the introduced 6G
              services and outline a comprehensive research agenda that
              leverages those technologies. We conclude by providing concrete
              recommendations for the roadmap toward 6G. Ultimately, the intent
              of this article is to serve as a basis for stimulating more
              out-of-the-box research around 6G.",
  journal  = "IEEE Netw.",
  volume   =  34,
  number   =  3,
  pages    = "134--142",
  month    =  may,
  year     =  2020,
  file     = "All Papers/S/Saad et al. 2020 - A Vision of 6G Wireless Systems - Applications, Trends, Technologies, and Open Research Problems.pdf",
  keywords = "6G mobile communication;5G mobile communication;Market
              research;Wireless communication;Sensors;Wireless sensor
              networks;5G6G;Mobile\_Wireless;ServicesDescription",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.001.1900287"
}

@ARTICLE{Zhou2020-ga,
  title    = "When Vehicular Fog Computing Meets Autonomous Driving:
              Computational Resource Management and Task Offloading",
  author   = "Zhou, Z and Liao, H and Wang, X and Mumtaz, S and Rodriguez, J",
  abstract = "Autonomous driving has the potential to make transportation
              systems safer, greener and more efficient. To realize autonomous
              driving, a great deal of in-car cutting-edge applications such as
              augmented reality, dynamic path planning and cognitive driving
              systems are required, which need significant computational
              resources and near realtime response. To cope with this new
              paradigm, vehicular fog computing (VFC) has emerged recently,
              which migrates the computing from congested base stations (or
              cloud servers) to nearby vehicles with under-utilized
              computational resources. in VFC, the designs of server
              recruitment and task offloading strategies under information
              asymmetry and uncertainty pose new technical challenges. in this
              article, we propose a two-stage VFC framework to address these
              challenges. The framework consists of a contract theory based
              vehicular computational resource management mechanism, and a
              matching-learning based task offloading mechanism. Simulation
              results demonstrate that the proposed framework can boost the
              performance of VFC in terms of resource utilization efficiency
              and task offloading delay.",
  journal  = "IEEE Netw.",
  volume   =  34,
  number   =  6,
  pages    = "70--76",
  month    =  nov,
  year     =  2020,
  keywords = "Servers;Task analysis;Contracts;Autonomous vehicles;Resource
              management;Recruitment;Edge computing;Cloud",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.001.1900527"
}

@ARTICLE{Bega2020-cj,
  title    = "{AI-Based} Autonomous Control, Management, and Orchestration in
              5G: From Standards to Algorithms",
  author   = "Bega, D and Gramaglia, M and Perez, R and Fiore, M and Banchs, A
              and Costa-Perez, X",
  abstract = "While the application of artificial intelligence (Ai) to 5G
              networks has raised strong interest, standard solutions to bring
              Ai into 5G systems are still in their infancy and have a long way
              to go before they can be used to build an operational system. in
              this article, we contribute to bridging the gap between standards
              and a working solution by defining a framework that brings
              together the relevant standard specifications and complements
              them with additional building blocks. We populate this framework
              with concrete Ai-based algorithms that serve different purposes
              toward developing a fully operational system. We evaluate the
              performance resulting from applying our framework to control,
              management, and orchestration functions, showing the benefits
              that Ai can bring to 5G systems.",
  journal  = "IEEE Netw.",
  volume   =  34,
  number   =  6,
  pages    = "14--20",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/B/Bega et al. 2020 - AI-Based Autonomous Control, Management, and Orchestration in 5G - From Standards to Algorithms.pdf",
  keywords = "Artificial intelligence;Forecasting;Engines;Prediction
              algorithms;History;Data analysis;3GPP;5G6G;MLNetworking",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.001.2000047"
}

@ARTICLE{Gurugopinath2020-ln,
  title    = "{Non-Orthogonal} Multiple Access with Wireless Caching for
              {5G-Enabled} Vehicular Networks",
  author   = "Gurugopinath, S and Al-Hammadi, Y and Sofotasios, P C and
              Muhaidat, S and Dobre, O A",
  abstract = "The proliferation of connected vehicles along with the high
              demand for rich multimedia services constitute key challenges for
              the emerging 5G-enabled vehicular networks. These challenges
              include, but are not limited to, high spectral efficiency and low
              latency requirements. Recently, the integration of cache-enabled
              networks with NOMA has been shown to reduce the content delivery
              time and traffic congestion in wireless networks. Accordingly, in
              this article, we envisage cache-aided NOMA as a technology
              facilitator for 5G-enabled vehicular networks. In particular, we
              present a cache-aided NOMA architecture, which can address some
              of the aforementioned challenges in these networks. We
              demonstrate that the spectral efficiency gain of the proposed
              architecture, which depends largely on the cached contents,
              significantly outperforms that of conventional vehicular
              networks. Finally, we provide deep insights into the challenges,
              opportunities, and future research trends that will enable the
              practical realization of cache-aided NOMA in 5G-enabled vehicular
              networks.",
  journal  = "IEEE Netw.",
  volume   =  34,
  number   =  5,
  pages    = "127--133",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/G/Gurugopinath et al. 2020 - Non-Orthogonal Multiple Access with Wireless Caching for 5G-Enabled Vehicular Networks.pdf",
  keywords = "5G mobile communication;cache storage;multi-access
              systems;multimedia communication;radio spectrum
              management;telecommunication congestion control;telecommunication
              traffic;vehicular ad hoc networks;cache-aided NOMA
              architecture;nonorthogonal multiple access;wireless
              caching;5G-enabled vehicular networks;spectral
              efficiency;cache-enabled networks;wireless networks;traffic
              congestion;NOMA;Interference;Encoding;Silicon
              carbide;Decoding;Receivers;Wireless communication;Wireless",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.1900564"
}

@ARTICLE{Zeng2020-rm,
  title    = "Cooperative {NOMA}: State of the Art, Key Techniques, and Open
              Challenges",
  author   = "Zeng, M and Hao, W and Dobre, O A and Ding, Z",
  abstract = "In this article, we investigate the integration of NOMA into
              cooperative relaying. We first introduce several fundamental
              cooperative NOMA network structures, and then classify them into
              two categories: user relaying and dedicated relaying. After that,
              we discuss the combination of cooperative NOMA with various
              advanced transmission technologies, such as full-duplex,
              cognitive radio, energy harvesting and multiple-input
              multiple-output. Following this, we focus on system evaluation
              and resource allocation of cooperative NOMA. Finally, major
              challenges and open issues are highlighted.",
  journal  = "IEEE Netw.",
  volume   =  34,
  number   =  5,
  pages    = "205--211",
  month    =  sep,
  year     =  2020,
  keywords = "cooperative communication;multi-access systems;relay networks
              (telecommunication);resource allocation;user relaying;dedicated
              relaying;cooperative NOMA;cooperative relaying;resource
              allocation;Relays;NOMA;Energy
              harvesting;Decoding;Protocols;Interference cancellation;Silicon
              carbide;Wireless",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.1900601"
}

@ARTICLE{She2020-vm,
  title    = "Deep Learning for {Ultra-Reliable} and {Low-Latency}
              Communications in {6G} Networks",
  author   = "She, C and Dong, R and Gu, Z and Hou, Z and Li, Y and Hardjawana,
              W and Yang, C and Song, L and Vucetic, B",
  abstract = "In future 6th generation networks, URLLC will lay the foundation
              for emerging mission-critical applications that have stringent
              requirements on end-to-end delay and reliability. Existing works
              on URLLC are mainly based on theoretical models and assumptions.
              The model-based solutions provide useful insights, but cannot be
              directly implemented in practice. In this article, we first
              summarize how to apply data-driven supervised deep learning and
              deep reinforcement learning in URLLC, and discuss some open
              problems of these methods. To address these open problems, we
              develop a multi-level architecture that enables device
              intelligence, edge intelligence, and cloud intelligence for
              URLLC. The basic idea is to merge theoretical models and
              realworld data in analyzing the latency and reliability and
              training deep neural networks (DNNs). Deep transfer learning is
              adopted in the architecture to fine-tune the pre-trained DNNs in
              non-stationary networks. Further considering that the computing
              capacity at each user and each mobile edge computing server is
              limited, federated learning is applied to improve the learning
              efficiency. Finally, we provide some experimental and simulation
              results and discuss some future directions.",
  journal  = "IEEE Netw.",
  volume   =  34,
  number   =  5,
  pages    = "219--225",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/S/She et al. 2020 - Deep Learning for Ultra-Reliable and Low-Latency Communications in 6G Networks.pdf",
  keywords = "6G mobile communication;learning (artificial intelligence);neural
              nets;telecommunication computing;low-latency communications;6g
              networks;6th generation networks;URLLC;mission-critical
              applications;reliability;model-based solutions;data-driven
              supervised deep learning;deep reinforcement learning;multilevel
              architecture;device intelligence;edge intelligence;cloud
              intelligence;deep transfer learning;nonstationary networks;mobile
              edge computing server;federated learning;learning
              efficiency;pretrained DNN;Ultra reliable low latency
              communication;Deep learning;Computer architecture;6G mobile
              communication;Quality of
              service;Delays;Training;5G6G;MLNetworking",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.1900630"
}

@ARTICLE{Zhang2021-qu,
  title    = "{LNTP}: An {End-to-End} Online Prediction Model for Network
              Traffic",
  author   = "Zhang, Lianming and Zhang, Huan and Tang, Qian and Dong, Pingping
              and Zhao, Zhen and Wei, Yehua and Mei, Jing and Xue, Kaiping",
  abstract = "As network data keeps getting bigger, deep learning is coming to
              play a key role in network design and management. Meanwhile,
              accurate network traffic prediction is of critical importance for
              network management that is implemented to improve the quality of
              service (QoS) for users. However, the performance of existing
              network traffic prediction methods is still poor due to three
              challenges: complicated characteristics of network traffic,
              dynamics of traffic patterns caused by different network
              applications, and a complex set of variations like burstiness. In
              this article, we propose a long short-term memory (LSTM) based
              network traffic prediction (LNTP) model, which aims to forecast
              network traffic timely and accurately. The model can be divided
              into two parts, namely, wavelet transform and LSTM. The working
              process of LNTP falls into three stages, i.e., data acquisition,
              model training, and online learning and prediction. In addition,
              to avoid the negative incentives to models caused by the
              burstiness and adapt to the changing trend of the network
              traffic, a weight optimization algorithm of the neural network
              named sliding window gradient descent (SWGD), is also proposed.
              Extensive experiments based on two real-world network traffic
              datasets demonstrate that our model outperforms the
              state-of-the-art network traffic prediction models by more than
              29 percent.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  1,
  pages    = "226--233",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/Z/Zhang et al. 2021 - LNTP - An End-to-End Online Prediction Model for Network Traffic.pdf",
  keywords = "Predictive models;Telecommunication traffic;Adaptation
              models;Data models;Transforms;Training;Prediction
              algorithms;NetworkTraffic",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.1900647"
}

@ARTICLE{Duan2020-ep,
  title    = "Convergence of Networking and {Cloud/Edge} Computing: Status,
              Challenges, and Opportunities",
  author   = "Duan, Q and Wang, S and Ansari, N",
  abstract = "The wide applications of virtualization and service-oriented
              principles in various emerging networking technologies introduce
              a trend of network cloudification that enables network systems to
              be realized based on cloud technologies and allows network
              services to be provisioned following the cloud service model.
              Network cloudification together with the critical role of
              networking in the latest cloud/edge computing technologies leads
              to the convergence of networking and cloud/edge computing, which
              calls for a holistic vision across the fields of networking and
              computing that may shape relevant technology developments. in
              this article, we attempt to sketch a big picture to reflect the
              current status of on-going research toward network-cloud/edge
              convergence. We first describe the notion of such convergence and
              present an architectural framework for converged
              network-cloud/edge systems. Then, we survey the state of the art
              of enabling technologies for network-cloud/edge convergence by
              reviewing recent progress in relevant standardization and
              technology developments in representative research projects. We
              also discuss challenges that must be fully addressed for
              realizing the convergence of networking and cloud/ edge computing
              and identify some opportunities for future research in this
              exciting interdisciplinary field.",
  journal  = "IEEE Netw.",
  volume   =  34,
  number   =  6,
  pages    = "148--155",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/D/Duan et al. 2020 - Convergence of Networking and Cloud - Edge Computing - Status, Challenges, and Opportunities.pdf",
  keywords = "Cloud computing;Convergence;Computer
              architecture;Virtualization;5G mobile communication;Computational
              modeling;Edge computing;EdgeFogCloudIoT;Cloud",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000089"
}

@ARTICLE{Yang2020-jm,
  title    = "{Artificial-Intelligence-Enabled} Intelligent {6G} Networks",
  author   = "Yang, H and Alphones, A and Xiong, Z and Niyato, D and Zhao, J
              and Wu, K",
  abstract = "With the rapid development of smart terminals and
              infrastructures, as well as diversified applications (e.g.,
              virtual and augmented reality, remote surgery and holographic
              projection) with colorful requirements, current networks (e.g.,
              4G and upcoming 5G networks) may not be able to completely meet
              quickly rising traffic demands. Accordingly, efforts from both
              industry and academia have already been put to the research on 6G
              networks. Recently, artificial intelligence (Ai) has been
              utilized as a new paradigm for the design and optimization of 6G
              networks with a high level of intelligence. Therefore, this
              article proposes an Ai-enabled intelligent architecture for 6G
              networks to realize knowledge discovery, smart resource
              management, automatic network adjustment and intelligent service
              provisioning, where the architecture is divided into four layers:
              intelligent sensing layer, data mining and analytics layer,
              intelligent control layer and smart application layer. We then
              review and discuss the applications of Ai techniques for 6G
              networks and elaborate how to employ the Ai techniques to
              efficiently and effectively optimize the network performance,
              including Ai-empowered mobile edge computing, intelligent
              mobility and handover management, and smart spectrum management.
              We highlight important future research directions and potential
              solutions for Ai-enabled intelligent 6G networks, including
              computation efficiency, algorithms robustness, hardware
              development and energy management.",
  journal  = "IEEE Netw.",
  volume   =  34,
  number   =  6,
  pages    = "272--280",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/Y/Yang et al. 2020 - Artificial-Intelligence-Enabled Intelligent 6G Networks.pdf",
  keywords = "6G mobile communication;Sensors;Computer architecture;Data
              mining;Deep learning;Support vector
              machines;EdgeFogCloudIoT;5G6G;MLNetworking",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000195"
}

@ARTICLE{Yu2021-qg,
  title    = "Toward {Resource-Efficient} Federated Learning in Mobile Edge
              Computing",
  author   = "Yu, Rong and Li, Peichun",
  abstract = "Federated learning is a newly emerged distributed deep learning
              paradigm, where the clients separately train their local neural
              network models with private data and then jointly aggregate a
              global model at the central server. Mobile edge computing is
              aimed at deploying mobile applications at the edge of wireless
              networks. Federated learning in mobile edge computing is a
              prospective distributed framework to deploy deep learning
              algorithms in many application scenarios. The bottleneck of
              federated learning in mobile edge computing is the intensive
              resources of mobile clients in computation, bandwidth, energy,
              and data. This article first illustrates the typical use cases of
              federated learning in mobile edge computing, and then
              investigates the state-of-the-art resource optimization
              approaches in federated learning. The resource-efficient
              techniques for federated learning are broadly divided into two
              classes: the black-box and white-box approaches. For black-box
              approaches, the techniques of training tricks, client selection,
              data compensation, and hierarchical aggregation are reviewed. For
              white-box approaches, the techniques of model compression,
              knowledge distillation, feature fusion, and asynchronous update
              are discussed. After that, a neural-structure-aware resource
              management approach with module-based federated learning is
              proposed, where mobile clients are assigned with different
              subnetworks of the global model according to the status of their
              local resources. Experiments demonstrate the superiority of our
              approach in elastic and efficient resource utilization.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  1,
  pages    = "148--155",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/Y/Yu and Li 2021 - Toward Resource-Efficient Federated Learning in Mobile Edge Computing.pdf",
  keywords = "Deep learning;Computational modeling;Wireless
              networks;Collaborative work;Data models;Resource management;Edge
              computing;ML;EdgeFogCloudIoT",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000295"
}

@ARTICLE{Xu2021-hz,
  title    = "{WIA-NR}: {Ultra-Reliable} {Low-Latency} Communication for
              Industrial Wireless Control Networks over Unlicensed Bands",
  author   = "Xu, Chi and Zeng, Peng and Yu, Haibin and Jin, Xi and Xia,
              Changqing",
  abstract = "With the deep integration of information, communication and
              operation technologies, industrial wireless control networks
              (IWCNs) are playing key roles to drive industrial revolution.
              This article proposes a 5G-based new radio (NR) for IWCNs to
              realize ultra-reliable low-latency communication over unlicensed
              bands, namely WIA-NR. We first define a hierarchical star network
              topology, which employs a hybrid centralized and distributed
              system management architecture. Then, a three-layer protocol
              stack is defined by integrating some fundamental functions of 5G
              and proposing new functions to enhance reliability and
              timeliness. The key technologies include dynamic multi-channel
              listen before talk, adaptive channel hopping, over-the-air time
              synchronization, industrial data priority scheduling, aggregation
              and disaggregation. Finally, system-level simulations are
              performed to evaluate the performance of WIA-NR. The results show
              that WIA-NR can achieve less than 1 ms latency under 99.999
              percent reliability constraint, which can well satisfy critical
              industrial control. Meanwhile, WIA-NR can harmoniously coexist
              with WiFi over unlicensed bands.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  1,
  pages    = "258--265",
  month    =  jan,
  year     =  2021,
  keywords = "5G mobile communication;Protocols;Reliability;Logic gates;Base
              stations;Wireless communication;Network topology;Wireless",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000308"
}

@ARTICLE{Jiang2021-xl,
  title    = "Channel Modeling and Characteristics for {6G} Wireless
              Communications",
  author   = "Jiang, Hao and Mukherjee, Mithun and Zhou, Jie and Lloret, Jaime",
  abstract = "Channel models are vital for theoretical analysis, performance
              evaluation, and system deployment of the communication systems
              between the transmitter and receivers. For sixth-generation (6G)
              wireless networks, channel modeling and characteristics analysis
              should combine different technologies and disciplines, such as
              high-mobil-ity, multiple mobilities, the uncertainty of motion
              trajectory, and the non-stationary nature of time/frequency/space
              domains. In this article, we begin with an overview of the
              salient characteristics in the modeling of 6G wireless channels.
              Then, we discuss the advancement of channel modeling and
              characteristics analysis for next-generation communication
              systems. Finally, we outline the research challenges of channel
              models and characteristics in 6G wireless communications.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  1,
  pages    = "296--303",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/J/Jiang et al. 2021 - Channel Modeling and Characteristics for 6G Wireless Communications.pdf",
  keywords = "6G mobile communication;Analytical
              models;Uncertainty;Transmitters;Wireless
              networks;Trajectory;Channel
              models;Mobile\_Wireless;Wireless;Channels;ServicesDescription",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000348"
}

@ARTICLE{Manias2021-ou,
  title    = "The Need for Advanced Intelligence in {NFV} Management and
              Orchestration",
  author   = "Manias, Dimitrios Michael and Shami, Abdallah",
  abstract = "With the constant demand for connectivity at an all-time high,
              Network Service Providers (NSPs) are required to optimize their
              networks to cope with rising capital and operational expenditures
              required to meet the growing connectivity demand. A solution to
              this challenge was presented through Network Function
              Virtualization (NFV). As network complexity increases and
              futuristic networks take shape, NSPs are required to incorporate
              an increasing amount of operational efficiency into their
              NFV-enabled networks. One such technique is Machine Learning
              (ML), which has been applied to various entities in NFV-enabled
              networks, most notably in the NFV Orchestrator. While traditional
              ML provides tremendous operational efficiencies, including
              realtime and high-volume data processing, challenges such as
              privacy, security, scalability, transferability, and concept
              drift hinder its widespread implementation. Through the adoption
              of Advanced Intelligence techniques such as Reinforcement
              Learning and Federated Learning, NSPs can leverage the benefits
              of traditional ML while simultaneously addressing the major
              challenges traditionally associated with it. This work presents
              the benefits of adopting these advanced techniques, provides a
              list of potential use cases and research topics, and proposes a
              bottom-up micro-functionality approach to applying these methods
              of Advanced Intelligence to NFV Management and Orchestration.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  1,
  pages    = "365--371",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/M/Manias and Shami 2021 - The Need for Advanced Intelligence in NFV Management and Orchestration.pdf",
  keywords = "Data models;Complexity theory;Security;Adaptation models;Data
              privacy;Learning (artificial intelligence);Computational
              modeling;MLNetworking;NFV",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000373"
}

@ARTICLE{Boudi2021-za,
  title    = "{AI-Based} Resource Management in Beyond {5G} Cloud Native
              Environment",
  author   = "Boudi, Abderrahmane and Bagaa, Miloud and P{\"o}yh{\"o}nen,
              Petteri and Taleb, Tarik and Flinck, Hannu",
  abstract = "5G system and beyond targets a large number of emerging
              applications and services that will create extra overhead on
              network traffic. These industrial verticals have aggressive,
              contentious, and conflicting requirements that make the network
              have an arduous mission for achieving the desired objectives. It
              is expected to get requirements with close to zero time latency,
              high data rate, and network reliability. Fortunately, a ray of
              hope comes shining the way of telecom providers with the new
              progress and achievements in machine learning, cloud computing,
              micro-services, and the ETSI ZSM era. For this reason there is a
              colossal impetus from industry and academia toward applying these
              techniques by creating a new concept called CCN environment that
              can cohabit and adapt according to the network and resource
              state, and perceived KPIs. In this article, we pursue the
              aforementioned concept by providing a unified hierarchical
              closed-loop network and service management framework that can
              meet the desired objectives. We propose a cloud-na-tive simulator
              that accurately mimics cloud-native environments, and enables us
              to quickly evaluate new frameworks and ideas. The simulation
              results demonstrate the efficiency of our simulator for parroting
              the real testbeds in various metrics.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  2,
  pages    = "128--135",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/B/Boudi et al. 2021 - AI-Based Resource Management in Beyond 5G Cloud Native Environment.pdf",
  keywords = "Artificial intelligence;5G mobile communication;Cloud
              computing;Telecommunication network
              reliability;Resilience;Monitoring;Resource
              management;5G6G;MLNetworking",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000392"
}

@ARTICLE{Mai2021-su,
  title    = "{In-Network} Intelligence Control: Toward a {Self-Driving}
              Networking Architecture",
  author   = "Mai, Tianle and Garg, Sahil and Yao, Haipeng and Nie, Jiangtian
              and Kaddoum, Georges and Xiong, Zehui",
  abstract = "The past few years have witnessed the compelling applications of
              the Internet of Things (IoT) in our daily life. Meanwhile, with
              the explosion of IoT devices and various applications, the
              expectations for the performance, reliability, and security of
              networks are greater than ever. The current end-host-based or
              centralized control framework incurs too much communication and
              computation overhead, therefore exhibiting tardiness and
              clumsiness in responding to network dynamics. Recently, with the
              advancement of programmable network hardware, it is possible to
              implement network functions inside the network. However, current
              in-network schemes are largely dependent on the manual process,
              which presents poor scalability and robustness. Therefore, in
              this article, we present a new intelligent network control
              architecture, in-network intelligence control. We design
              intelligent in-network devices that can automatically adapt to
              network dynamics by leveraging powerful machine learning adaptive
              abilities. In addition, to enhance the collaboration among
              distributed in-network devices, a centralized management plane is
              introduced to ease the training process of distributed switches.
              To demonstrate the technical feasibility and performance
              advantage of our architecture, we present three use cases:
              in-network load balance, in-network congestion control, and
              in-network DDoS detection.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  2,
  pages    = "53--59",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/M/Mai et al. 2021 - In-Network Intelligence Control - Toward a Self-Driving Networking Architecture.pdf",
  keywords = "Training;Performance evaluation;Scalability;Computer
              architecture;Internet of Things;Security;Computer crime;5G mobile
              communication;MLNetworking",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000412"
}

@ARTICLE{Tak2021-lm,
  title    = "Federated Edge Learning: Design Issues and Challenges",
  author   = "Tak, Afaf and Cherkaoui, Soumaya",
  abstract = "Federated Learning (FL) is a distributed machine learning
              technique, where each device contributes to the learning model by
              independently computing the gradient based on its local training
              data. It has recently become a hot research topic, as it promises
              several benefits related to data privacy and scalability.
              However, implementing FL at the network edge is challenging due
              to system and data heterogeneity and resource constraints. In
              this article, we examine the existing challenges and trade-offs
              in Federated Edge Learning (FEEL). The design of FEEL algorithms
              for resources-efficient learning raises several challenges. These
              challenges are essentially related to the multidisciplinary
              nature of the problem. As the data is the key component of the
              learning, this article advocates a new set of considerations for
              data characteristics in wireless scheduling algorithms in FEEL.
              Hence, we propose a general framework for data-aware scheduling
              as a guideline for future research directions. We also discuss
              the main axes and requirements for data evaluation and some
              exploitable techniques and metrics.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  2,
  pages    = "252--258",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/T/Tak and Cherkaoui 2021 - Federated Edge Learning - Design Issues and Challenges.pdf",
  keywords = "Training data;Computational modeling;Data
              models;Bandwidth;Optimization;Convergence;Distributed
              databases;MLNetworking",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000478"
}

@ARTICLE{Shen2021-gg,
  title    = "Data Management for Future Wireless Networks: Architecture,
              Privacy Preservation, and Regulation",
  author   = "Shen, Xuemin Sherman and Huang, Cheng and Liu, Dongxiao and Xue,
              Liang and Zhuang, Weihua and Sun, Rob and Ying, Bidi",
  abstract = "Next-generation wireless networks (NGWN) aim to support
              diversified smart applications that require frequent data
              exchanges and collaborative data processing among multiple
              stakeholders. Data management (DM), including data collection,
              storage, sharing, and computation, plays an essential role in
              empowering NGWN. However, DM for NGWN faces two significant
              challenges: stakeholders' data cannot be easily managed across
              different trust domains under a distributed network architecture;
              and privacy preservation requirements of personal data become
              more rigorous under new privacy regulations. To explore possible
              solutions to address the challenges, we first investigate the
              state-of-the-art architecture designs for DM and emphasize
              advantages of a blockchain-based DM architecture. Then we
              summarize existing privacy-preserving techniques in terms of
              advantages and challenges when being applied to DM. In addition,
              we review recent privacy regulations with their impacts on DM and
              discuss the existing solutions with privacy regulation compliance
              based on blockchain. Finally, we identify further research
              directions for achieving DM with privacy preservation.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  1,
  pages    = "8--15",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/S/Shen et al. 2021 - Data Management for Future Wireless Networks - Architecture, Privacy Preservation, and Regulation.pdf",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.011.2000666"
}

@ARTICLE{Mai2021-xt,
  title    = "{In-Network} Computing Powered Mobile Edge: Toward High
              Performance Industrial {IoT}",
  author   = "Mai, Tianle and Yao, Haipeng and Guo, Song and Liu, Yunjie",
  abstract = "Recently, the industrial Internet of Things (IoT) has quickly
              become a disruptive force reshaping how we live and work.
              Compared to the consumer Internet, the industrial IoT puts
              forward much higher performance requirements in terms of network
              and computing capacity. The industrial IoT system needs to
              process tons of data generated by millions of IoT sensors in
              real-time. Recently, with the advent of programmable network
              devices (e.g., SmartNIC, programmable switch), the in-network
              computing (INC) paradigm has received a large amount of
              attention. INC refers to offload application-specific tasks from
              end-host to network devices. Benefiting from the line-rate
              processing capacity of network devices, the INC paradigm presents
              superior performance in terms of high-throughput low-latency
              computing. Therefore, INC is considered a promising technique to
              meet performance requirements in the industrial IoT. However,
              while network devices are capable of performing
              application-specific tasks, they are still far from the universal
              computing platform. In this article, we propose an INC powered
              mobile edge architecture, where we offload lightweight critical
              tasks to the INC devices and leave the rest to the MEC. To
              identify critical tasks, we introduce the complex event
              processing (CEP) tool in our architecture. Also, we present two
              industrial IoT use cases to evaluate the feasibility of our
              architecture.",
  journal  = "IEEE Netw.",
  volume   =  35,
  number   =  1,
  pages    = "289--295",
  month    =  jan,
  year     =  2021,
  keywords = "Computer architecture;Switches;Task analysis;Performance
              evaluation;Sensors;Pipelines;Control systems",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.021.2000318"
}

@ARTICLE{Li2018-vj,
  title    = "Learning {IoT} in Edge: Deep Learning for the Internet of Things
              with Edge Computing",
  author   = "Li, He and Ota, Kaoru and Dong, Mianxiong",
  abstract = "Deep learning is a promising approach for extracting accurate
              information from raw sensor data from IoT devices deployed in
              complex environments. Because of its multilayer structure, deep
              learning is also appropriate for the edge computing environment.
              Therefore, in this article, we first introduce deep learning for
              IoTs into the edge computing environment. Since existing edge
              nodes have limited processing capability, we also design a novel
              offloading strategy to optimize the performance of IoT deep
              learning applications with edge computing. In the performance
              evaluation, we test the performance of executing multiple deep
              learning tasks in an edge computing environment with our
              strategy. The evaluation results show that our method outperforms
              other optimization solutions on deep learning for IoT.",
  journal  = "IEEE Netw.",
  volume   =  32,
  number   =  1,
  pages    = "96--101",
  month    =  jan,
  year     =  2018,
  file     = "All Papers/L/Li et al. 2018 - Learning IoT in Edge - Deep Learning for the Internet of Things with Edge Computing.pdf",
  keywords = "Machine learning;Edge computing;Cloud computing;Feature
              extraction;Task analysis;Computational modeling;Servers",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.2018.1700202"
}

@ARTICLE{Condoluci2019-ox,
  title    = "{Fixed-Mobile} Convergence in the {5G} Era: From Hybrid Access to
              Converged Core",
  author   = "Condoluci, Massimo and Johnson, Stephen H and Ayadurai, Vicknesan
              and Lema, Maria A and Cuevas, Maria A and Dohler, Mischa and
              Mahmoodi, Toktam",
  abstract = "The availability of different paths to communicate to a user or
              device introduces several benefits, from boosting end-user
              performance to improving network utilization. Hybrid access is a
              first step in enabling convergence of mobile and fixed networks;
              however, despite traffic optimization, this approach is limited
              as fixed and mobile are still two separate core networks
              inter-connected through an aggregation point. On the road to 5G
              networks, the design trend is moving toward an aggregated
              network, where different access technologies share a common
              anchor point in the core. This enables further network
              optimization in addition to hybrid access; examples are
              user-specific policies for aggregation and improved traffic
              balancing across different accesses according to user, network,
              and service context. This article aims to discuss the ongoing
              work around hybrid access and network convergence by the
              Broadband Forum and 3GPP. We present some testbed results on
              hybrid access and analyze some primary performance indicators
              such as achievable data rates, link utilization for aggregated
              traffic and session setup latency. We finally discuss future
              directions for network convergence to enable future scenarios
              with enhanced configuration capabilities for fixed and mobile
              convergence.",
  journal  = "IEEE Netw.",
  volume   =  33,
  number   =  2,
  pages    = "138--145",
  month    =  mar,
  year     =  2019,
  file     = "All Papers/C/Condoluci et al. 2019 - Fixed-Mobile Convergence in the 5G Era - From Hybrid Access to Converged Core.pdf",
  keywords = "3GPP;Convergence;5G mobile
              communication;Switches;Optimization;Logic gates;Quality of
              service;Mobile\_Wireless",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.2018.1700462"
}

@ARTICLE{Feng2018-sf,
  title    = "{DeepTP}: An {End-to-End} Neural Network for Mobile Cellular
              Traffic Prediction",
  author   = "Feng, J and Chen, X and Gao, R and Zeng, M and Li, Y",
  abstract = "The past 10 years have witnessed the rapid growth of global
              mobile cellular traffic demands due to the popularity of mobile
              devices. While accurate traffic prediction becomes extremely
              important for stable and high-quality Internet service, the
              performance of existing methods is still poor due to three
              challenges: complicated temporal variations including burstiness
              and long periods, multi-variant impact factors such as the point
              of interest and day of the week, and potential spatial
              dependencies introduced by the movement of population. While
              existing traditional methods fail in characterizing these
              features, especially the latter two, deep learning models with
              powerful representation ability give us a chance to consider
              these from a new perspective. In this article, we propose Deep
              Traffic Predictor (DeepTP), a deep-learning-based end-toend
              model, which forecasts traffic demands from spatial-dependent and
              long-period cellular traffic. DeepTP consists of two components:
              a general feature extractor for modeling spatial dependencies and
              encoding the external information, and a sequential module for
              modeling complicated temporal variations. In the general feature
              extractor, we introduce a correlation selection mechanism for a
              spatial modeling and embedding mechanism to encode external
              information. Moreover, we apply a seq2seq model with attention
              mechanism to build the sequential model. Extensive experiments
              based on large-scale mobile cellular traffic data demonstrate
              that our model outperforms the state-of-the-art traffic
              prediction models by more than 12.31 percent.",
  journal  = "IEEE Netw.",
  volume   =  32,
  number   =  6,
  pages    = "108--115",
  month    =  nov,
  year     =  2018,
  file     = "All Papers/F/Feng et al. 2018 - DeepTP - An End-to-End Neural Network for Mobile Cellular Traffic Prediction.pdf",
  keywords = "cellular radio;feature extraction;Internet;learning (artificial
              intelligence);mobile radio;neural nets;telecommunication
              computing;telecommunication traffic;DeepTP;end-to-end neural
              network;deep traffic predictor;mobile cellular traffic
              prediction;traffic prediction models;seq2seq model;general
              feature extractor;deep learning models;high-quality Internet
              service;mobile devices;Feature extraction;Predictive
              models;Networked control systems;Poles and
              towers;Forecasting;Base stations;Neural networks;Cellular
              networks;Telecommunication traffic;Traffic
              control;GeneralNetworking",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.2018.1800127"
}

@ARTICLE{Wang2019-dq,
  title    = "{In-Edge} {AI}: Intelligentizing Mobile Edge Computing, Caching
              and Communication by Federated Learning",
  author   = "Wang, X and Han, Y and Wang, C and Zhao, Q and Chen, X and Chen,
              M",
  abstract = "Recently, along with the rapid development of mobile
              communication technology, edge computing theory and techniques
              have been attracting more and more attention from global
              researchers and engineers, which can significantly bridge the
              capacity of cloud and requirement of devices by the network
              edges, and thus can accelerate content delivery and improve the
              quality of mobile services. In order to bring more intelligence
              to edge systems, compared to traditional optimization
              methodology, and driven by the current deep learning techniques,
              we propose to integrate the Deep Reinforcement Learning
              techniques and Federated Learning framework with mobile edge
              systems, for optimizing mobile edge computing, caching and
              communication. And thus, we design the ``In-Edge AI'' framework
              in order to intelligently utilize the collaboration among devices
              and edge nodes to exchange the learning parameters for a better
              training and inference of the models, and thus to carry out
              dynamic system-level optimization and application-level
              enhancement while reducing the unnecessary system communication
              load. ``In-Edge AI'' is evaluated and proved to have near-optimal
              performance but relatively low overhead of learning, while the
              system is cognitive and adaptive to mobile communication systems.
              Finally, we discuss several related challenges and opportunities
              for unveiling a promising upcoming future of ``In-Edge AI.''",
  journal  = "IEEE Netw.",
  volume   =  33,
  number   =  5,
  pages    = "156--165",
  month    =  sep,
  year     =  2019,
  file     = "All Papers/W/Wang et al. 2019 - In-Edge AI - Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning.pdf",
  keywords = "cache storage;cloud computing;learning (artificial
              intelligence);mobile communication;mobile
              computing;optimisation;In-Edge AI framework;edge nodes;dynamic
              system-level optimization;application-level
              enhancement;unnecessary system communication load;near-optimal
              performance;mobile communication technology;edge computing
              theory;network edges;mobile services;federated learning
              framework;intelligentizing mobile edge computing;deep learning
              techniques;caching;cloud;Wireless communication;Task
              analysis;Artificial intelligence;Edge
              computing;Optimization;Resource management;Training
              data;ATOS;EdgeFogCloudIoT;MLNetworking",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.2019.1800286"
}

@ARTICLE{Yang2019-yj,
  title    = "{6G} Wireless Communications: Vision and Potential Techniques",
  author   = "Yang, Ping and Xiao, Yue and Xiao, Ming and Li, Shaoqian",
  abstract = "With the fast development of smart terminals and emerging new
              applications (e.g., real-time and interactive services), wireless
              data traffic has drastically increased, and current cellular
              networks (even the forthcoming 5G) cannot completely match the
              quickly rising technical requirements. To meet the coming
              challenges, the sixth generation (6G) mobile network is expected
              to cast the high technical standard of new spectrum and
              energy-efficient transmission techniques. In this article, we
              sketch the potential requirements and present an overview of the
              latest research on the promising techniques evolving to 6G, which
              have recently attracted considerable attention. Moreover, we
              outline a number of key technical challenges as well as the
              potential solutions associated with 6G, including physical-layer
              transmission techniques, network designs, security approaches,
              and testbed developments.",
  journal  = "IEEE Netw.",
  volume   =  33,
  number   =  4,
  pages    = "70--75",
  month    =  jul,
  year     =  2019,
  file     = "All Papers/Y/Yang et al. 2019 - 6G Wireless Communications - Vision and Potential Techniques.pdf",
  keywords = "5G mobile communication;Wireless communication;MIMO
              communication;OFDM;Time-frequency analysis;Wireless sensor
              networks;Internet of Things;6G mobile
              communication;5G6G;Mobile\_Wireless",
  issn     = "0890-8044, 1558-156X",
  doi      = "10.1109/MNET.2019.1800418"
}

@ARTICLE{Li2020-dx,
  title    = "Federated Learning: Challenges, Methods, and Future Directions",
  author   = "Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith,
              Virginia",
  abstract = "Federated learning involves training statistical models over
              remote devices or siloed data centers, such as mobile phones or
              hospitals, while keeping data localized. Training in
              heterogeneous and potentially massive networks introduces novel
              challenges that require a fundamental departure from standard
              approaches for large-scale machine learning, distributed
              optimization, and privacy-preserving data analysis. In this
              article, we discuss the unique characteristics and challenges of
              federated learning, provide a broad overview of current
              approaches, and outline several directions of future work that
              are relevant to a wide range of research communities.",
  journal  = "IEEE Signal Process. Mag.",
  volume   =  37,
  number   =  3,
  pages    = "50--60",
  month    =  may,
  year     =  2020,
  file     = "All Papers/L/Li et al. 2020 - Federated Learning - Challenges, Methods, and Future Directions.pdf",
  keywords = "Distributed databases;Data models;Training data;Data
              privacy;Privacy;Predictive models;Machine learning",
  issn     = "1558-0792",
  doi      = "10.1109/MSP.2020.2975749"
}

@ARTICLE{Chang2018-bi,
  title    = "Learn to Cache: Machine Learning for Network Edge Caching in the
              Big Data Era",
  author   = "Chang, Z and Lei, L and Zhou, Z and Mao, S and Ristaniemi, T",
  abstract = "The unprecedented growth of wireless data traffic not only
              challenges the design and evolution of the wireless network
              architecture, but also brings about profound opportunities to
              drive and improve future networks. Meanwhile, the evolution of
              communications and computing technologies can make the network
              edge, such as BSs or UEs, become intelligent and rich in terms of
              computing and communications capabilities, which intuitively
              enables big data analytics at the network edge. In this article,
              we propose to explore big data analytics to advance edge caching
              capability, which is considered as a promising approach to
              improve network efficiency and alleviate the high demand for the
              radio resource in future networks. The learning-based approaches
              for network edge caching are discussed, where a vast amount of
              data can be harnessed for content popularity estimation and
              proactive caching strategy design. An outlook of research
              directions, challenges, and opportunities is provided and
              discussed in depth. To validate the proposed solution, a case
              study and a performance evaluation are presented. Numerical
              studies show that several gains are achieved by employing
              learning- based schemes for edge caching.",
  journal  = "IEEE Wirel. Commun.",
  volume   =  25,
  number   =  3,
  pages    = "28--35",
  month    =  jun,
  year     =  2018,
  file     = "All Papers/C/Chang et al. 2018 - Learn to Cache - Machine Learning for Network Edge Caching in the Big Data Era.pdf",
  keywords = "Big Data;learning (artificial intelligence);telecommunication
              traffic;big data analytics;edge caching capability;network
              efficiency;network edge caching;proactive caching strategy
              design;machine learning;wireless data traffic;wireless network
              architecture;Machine learning;Big Data;Hidden Markov
              models;Device-to-device communication;Analytical models;Data
              models;Wireless networks;5G mobile
              communication;EdgeFogCloudIoT;MLNetworking",
  issn     = "1558-0687",
  doi      = "10.1109/MWC.2018.1700317"
}

@ARTICLE{Wang2019-cl,
  title    = "{In-Network} Caching: An Efficient Content Distribution Strategy
              for Mobile Networks",
  author   = "Wang, S and Wang, T and Cao, X",
  abstract = "The sharp increase in wireless devices yields a huge amount of
              mobile data traffic, which has made either the radio access
              network or the core network of current mobile communication
              systems seriously overloaded. In-network caching arises as a
              promising solution to this burning issue. By introducing content
              centric networking infrastructure, popular content files can be
              intelligently stored in the radio access network so that
              redundant transmissions through the core network can be
              significantly reduced, which can substantially alleviate the load
              of both the core network and the backhauls between the radio
              access network and the core network. In this article, we discuss
              what to cache, how to cache and how to evaluate the performance
              of a cache-enabled mobile network. We first discuss the
              instructive caching policies, then propose reasonable performance
              evaluation metrics for these caching policies. We present
              detailed numerical results demonstrating remarkable gains by the
              in-network caching technique. Finally, we discuss related
              research directions, opportunities and challenges.",
  journal  = "IEEE Wirel. Commun.",
  volume   =  26,
  number   =  5,
  pages    = "84--90",
  month    =  oct,
  year     =  2019,
  file     = "All Papers/W/Wang et al. 2019 - In-Network Caching - An Efficient Content Distribution Strategy for Mobile Networks.pdf",
  keywords = "mobile radio;numerical analysis;efficient content distribution
              strategy;mobile data traffic;radio access network;mobile
              communication systems;content centric networking
              infrastructure;cache-enabled mobile network;in-network caching
              technique;performance evaluation;Radio access networks;5G mobile
              communication;Internet;Quality of
              experience;Interference;Computer architecture;EdgeFogCloudIoT",
  issn     = "1558-0687",
  doi      = "10.1109/MWC.2019.1800449"
}

@INPROCEEDINGS{Mijumbi2015-eh,
  title     = "Design and evaluation of algorithms for mapping and scheduling
               of virtual network functions",
  booktitle = "Proceedings of the 2015 1st {IEEE} Conference on Network
               Softwarization ({NetSoft})",
  author    = "Mijumbi, R and Serrat, J and Gorricho, J and Bouten, N and De
               Turck, F and Davy, S",
  abstract  = "Network function virtualization has received attention from both
               academia and industry as an important shift in the deployment of
               telecommunication networks and services. It is being proposed as
               a path towards cost efficiency, reduced time-to-markets, and
               enhanced innovativeness in telecommunication service
               provisioning. However, efficiently running virtualized services
               is not trivial as, among other initialization steps, it requires
               first mapping virtual networks onto physical networks, and
               thereafter mapping and scheduling virtual functions onto the
               virtual networks. This paper formulates the online virtual
               function mapping and scheduling problem and proposes a set of
               algorithms for solving it. Our main objective is to propose
               simple algorithms that may be used as a basis for future work in
               this area. To this end, we propose three greedy algorithms and a
               tabu search-based heuristic. We carry out evaluations of these
               algorithms considering parameters such as successful service
               mappings, total service processing times, revenue, cost etc,
               under varying network conditions. Simulations show that the tabu
               search-based algorithm performs only slightly better than the
               best greedy algorithm.",
  pages     = "1--9",
  month     =  apr,
  year      =  2015,
  file      = "All Papers/M/Mijumbi et al. 2015 - Design and evaluation of algorithms for mapping and scheduling of virtual network functions.pdf",
  keywords  = "computer networks;greedy algorithms;scheduling;search
               problems;virtualisation;virtual network function
               scheduling;virtual network function mapping;network function
               virtualization;telecommunication service;online virtual function
               mapping;scheduling problem;greedy algorithm;tabu search-based
               heuristic;Job shop scheduling;Resource
               management;Delays;Virtualization;Algorithm design and
               analysis;Servers;Network function
               virtualization;mapping;scheduling;placement;chaining;tabu
               search;resource allocation;NFV",
  doi       = "10.1109/NETSOFT.2015.7116120"
}

@INPROCEEDINGS{Quintuna_Rodriguez2016-nr,
  title     = "Performance analysis of resource pooling for network function
               virtualization",
  booktitle = "2016 17th International Telecommunications Network Strategy and
               Planning Symposium (Networks)",
  author    = "Quintuna Rodriguez, V K and Guillemin, F",
  abstract  = "In the framework of network function virtualization, we consider
               in this paper the execution of Virtualized Network Functions
               (VNFs) in data centers whose computing capacities are limited.
               We assume that each VNF is composed of sub-functions to be
               executed on general purpose hardware, each sub-function
               requiring a random amount of processing time. Because of limited
               processing capacity, we investigate the relevance of resource
               pooling where available cores in a data center are shared by
               active VNFs. We study by simulation various algorithms for
               scheduling sub-functions composing active VNFs (namely Greedy,
               Round Robin and Dedicated Core algorithms). We additionally
               introduce an execution deadline criterion, which means that VNFs
               can renege if their sojourn time in the system exceeds by a
               certain factor their service time. This feature is especially
               relevant when considering the processing of real-time VNF.
               Simulations show that sub-functions chaining is critical with
               regard to performance. When sub-functions have to be executed in
               series, the simple Dedicated Core algorithm is the most
               efficient. When sub-functions can be executed in parallel,
               Greedy or Round Robin algorithms offer similar performance and
               outperform the Dedicated Core algorithm. Enabling as much as
               possible parallelism and avoiding chaining when designing a VNF
               are fundamental principles to gain from the available computing
               resources.",
  pages     = "158--163",
  month     =  sep,
  year      =  2016,
  file      = "All Papers/Q/Quintuna Rodriguez and Guillemin 2016 - Performance analysis of resource pooling for network function virtualization.pdf",
  keywords  = "computer centres;greedy algorithms;telecommunication
               scheduling;performance analysis;resource pooling;network
               function virtualization;virtualized network functions;dedicated
               core algorithms;greedy algorithms;round robin
               algorithms;computing
               resources;Cryptography;Modulation;Scheduling;virtualization;VNF;cloud
               computing;fog computing;ToRead;NFV",
  doi       = "10.1109/NETWKS.2016.7751169"
}

@INPROCEEDINGS{Wang2016-st,
  title     = "Load balancing - towards balanced delay guarantees in {NFV/SDN}",
  booktitle = "2016 {IEEE} Conference on Network Function Virtualization and
               Software Defined Networks ({NFV-SDN})",
  author    = "Wang, H and Schmitt, J",
  abstract  = "The goals of load balancing are diverse. We may distribute the
               load to servers in order to achieve the same utilizations or
               average latencies. However, these goods are not a perfect fit in
               virtualized or software-defined networks. First, it is more
               difficult to assume homogeneous server capacities. Even for two
               (virtualized) functions with the same capacities, the capacities
               seen by the customer might be heterogeneous simply because they
               belong to different providers, are shared by others, or locate
               themselves differently and the communication costs are
               different. Heterogeneous server capacity will blur the aim of
               keeping the same utilizations. Second, usually the metric of
               latency in those networks is the (stochastic) bound instead of
               average value. In this paper, we parameterize the server
               capacities, and use the stochastic latency bound as the metric
               to further support inferring load balancing. We also model the
               load balancing process as a Markov-modulated process and observe
               the influence of its parameters onto achieving balance. The
               proposed model will benefit the load balancing function
               implementation and infrastructure design in virtualized or
               software-defined networks.",
  pages     = "240--245",
  month     =  nov,
  year      =  2016,
  file      = "All Papers/W/Wang and Schmitt 2016 - Load balancing - towards balanced delay guarantees in NFV - SDN.pdf",
  keywords  = "Markov processes;resource allocation;software defined
               networking;virtualisation;balanced delay guarantees;NFV;load
               distribution;software-defined networks;homogeneous server
               capacities;virtualized functions;communication
               costs;heterogeneous server capacity;latency metric;average value
               server capacities;stochastic latency bound;load balancing
               inference;Markov-modulated process;network function
               virtualization;SDN;Load management;Servers;Delays;Load
               modeling;Uncertainty;Stochastic processes;NetworkTraffic",
  doi       = "10.1109/NFV-SDN.2016.7919504"
}

@INPROCEEDINGS{Jalodia2019-fd,
  title     = "Deep Reinforcement Learning for {Topology-Aware} {VNF} Resource
               Prediction in {NFV} Environments",
  booktitle = "2019 {IEEE} Conference on Network Function Virtualization and
               Software Defined Networks ({NFV-SDN})",
  author    = "Jalodia, N and Henna, S and Davy, A",
  abstract  = "Network Function Virtualisation (NFV) has emerged as a key
               paradigm in network softwarisation, enabling virtualisation in
               future generation networks. Once deployed, the Virtual Network
               Functions (VNFs) in an NFV application's Service Function Chain
               (SFC) experience dynamic fluctuations in network traffic and
               requests, which necessitates dynamic scaling of resource
               instances. Dynamic resource management is a critical challenge
               in virtualised environments, specifically while balancing the
               trade-off between efficiency and reliability. Since provisioning
               of virtual infrastructures is time-consuming, this negates the
               Quality of Service (QoS) requirements and reliability criterion
               in latency-critical applications such as autonomous driving.
               This calls for predictive scaling decisions to balance the
               provisioning time sink, with a methodology that preserves the
               topological dependencies between the nodes in an SFC for
               effective resource forecasting. To address this, we propose the
               model for an Asynchronous Deep Reinforcement Learning (DRL)
               enhanced Graph Neural Networks (GNN) for topology-aware VNF
               resource prediction in dynamic NFV environments.",
  pages     = "1--5",
  month     =  nov,
  year      =  2019,
  file      = "All Papers/J/Jalodia et al. 2019 - Deep Reinforcement Learning for Topology-Aware VNF Resource Prediction in NFV Environments.pdf",
  keywords  = "cloud computing;graph theory;learning (artificial
               intelligence);neural nets;quality of service;resource
               allocation;virtualisation;asynchronous deep
               reinforcement;virtual network functions;provisioning time
               sink;predictive scaling decisions;latency-critical
               applications;reliability criterion;virtual
               infrastructures;trade-off between efficiency;virtualised
               environments;dynamic resource management;resource
               instances;dynamic scaling;network traffic;NFV application;future
               generation networks;network softwarisation;key paradigm;network
               function virtualisation;dynamic NFV environments;topology-aware
               VNF resource prediction;graph neural networks;effective resource
               forecasting;topological dependencies;Dynamic
               scheduling;Mathematical model;Network function
               virtualization;Encoding;Reinforcement
               learning;Reliability;Topology;NFV;Graph Neural Networks;Deep
               Reinforcement Learning;Asynchronous Deep Q-Learning;Dynamic
               Resource Prediction;Future Generation Networks;Topology
               Awareness;Prediction;Machine Learning;Deep
               Learning;ToRead;Important;MLNetworking;NFV",
  doi       = "10.1109/NFV-SDN47374.2019.9040154"
}

@INPROCEEDINGS{Mijumbi2014-ko,
  title     = "Design and evaluation of learning algorithms for dynamic
               resource management in virtual networks",
  booktitle = "2014 {IEEE} Network Operations and Management Symposium ({NOMS})",
  author    = "Mijumbi, R and Gorricho, J and Serrat, J and Claeys, M and De
               Turck, F and Latr{\'e}, S",
  abstract  = "Network virtualisation is considerably gaining attention as a
               solution to ossification of the Internet. However, the success
               of network virtualisation will depend in part on how efficiently
               the virtual networks utilise substrate network resources. In
               this paper, we propose a machine learning-based approach to
               virtual network resource management. We propose to model the
               substrate network as a decentralised system and introduce a
               learning algorithm in each substrate node and substrate link,
               providing self-organization capabilities. We propose a
               multiagent learning algorithm that carries out the substrate
               network resource management in a coordinated and decentralised
               way. The task of these agents is to use evaluative feedback to
               learn an optimal policy so as to dynamically allocate network
               resources to virtual nodes and links. The agents ensure that
               while the virtual networks have the resources they need at any
               given time, only the required resources are reserved for this
               purpose. Simulations show that our dynamic approach
               significantly improves the virtual network acceptance ratio and
               the maximum number of accepted virtual network requests at any
               time while ensuring that virtual network quality of service
               requirements such as packet drop rate and virtual link delay are
               not affected.",
  pages     = "1--9",
  month     =  may,
  year      =  2014,
  file      = "All Papers/M/Mijumbi et al. 2014 - Design and evaluation of learning algorithms for dynamic resource management in virtual networks.pdf",
  keywords  = "computer network management;computer network performance
               evaluation;Internet;learning (artificial
               intelligence);multi-agent
               systems;virtualisation;Internet;network virtualisation;machine
               learning-based approach;virtual network resource
               management;substrate network;decentralised system;multiagent
               learning algorithm;quality of service
               requirements;Substrates;Resource management;Dynamic
               scheduling;Delays;Learning (artificial
               intelligence);Bandwidth;Heuristic algorithms;Network
               virtualization;Dynamic Resource Allocation;Virtual Network
               Embedding;Artificial Intelligence;Machine Learning;Reinforcement
               Learning;Multiagent Systems;ToRead;Important;NFV;MLNetworking",
  issn      = "2374-9709",
  doi       = "10.1109/NOMS.2014.6838258"
}

@INPROCEEDINGS{Qu2016-zl,
  title     = "Network function virtualization scheduling with transmission
               delay optimization",
  booktitle = "{NOMS} 2016 - 2016 {IEEE/IFIP} Network Operations and Management
               Symposium",
  author    = "Qu, L and Assi, C and Shaban, K",
  abstract  = "To accelerate the implementation of network functions/middle
               boxes and reduce the deployment cost, recently the concept of
               Network Function Virtualization (NFV) has emerged and became a
               topic of much interest attracting the attention of researchers
               from both industry and academia. Unlike the traditional
               implementation of network functions, a software-oriented
               approach for network functions create more flexible and dynamic
               network services to meet a more diversified demand. In this
               paper, we study the Virtual Network Function (VNF) chaining
               scheduling problem with limited network resources. We consider
               VNF transmission and processing delays, and formulate the VNFs
               chaining scheduling as a new Mixed Integer Linear Programming
               (MILP) problem. Our objective is to minimize the latency of the
               overall VNFs' schedule. Reducing the scheduling latency enables
               cloud operators to service (and admit) more customers, thereby
               increasing operators' revenues. Owing to the complexity of the
               problem, we develop a Genetic Algorithm (GA) based method for
               solving the problem efficiently. Finally, the effectiveness of
               our heuristic algorithm is verified through numerical results.",
  pages     = "638--644",
  month     =  apr,
  year      =  2016,
  file      = "All Papers/Q/Qu et al. 2016 - Network function virtualization scheduling with transmission delay optimization.pdf",
  keywords  = "computer networks;genetic algorithms;integer programming;linear
               programming;telecommunication scheduling;virtualisation;network
               function virtualization scheduling;transmission delay
               optimization;network function-middle boxe;deployment cost
               reduction;NFV;software-oriented approach;dynamic network
               service;virtual network function chaining scheduling problem;VNF
               chaining scheduling problem;mixed integer linear programming
               problem;MILP problem;latency minimization;genetic algorithm
               based method;GA based method;heuristic
               algorithm;Delays;Bandwidth;Schedules;Optimal scheduling;Joining
               processes;Job shop scheduling;Dynamic scheduling;NFV",
  issn      = "2374-9709",
  doi       = "10.1109/NOMS.2016.7502870"
}

@INPROCEEDINGS{Schmidt2010-hs,
  title     = "Efficient Distribution of Virtual Machines for Cloud Computing",
  booktitle = "2010 18th Euromicro Conference on Parallel, Distributed and
               Network-based Processing",
  author    = "Schmidt, Matthias and Fallenbeck, Niels and Smith, Matthew and
               Freisleben, Bernd",
  abstract  = "The commercial success of Cloud computing and recent
               developments in Grid computing have brought platform
               virtualization technology into the field of high performance
               computing. Virtualization offers both more flexibility and
               security through custom user images and user isolation. In this
               paper, we deal with the problem of distributing virtual machine
               (VM) images to a set of distributed compute nodes in a
               Cross-Cloud computing environment, i.e., the connection of two
               or more Cloud computing sites. Ambrust et al. identified data
               transfer bottlenecks as one of the obstacles Cloud computing has
               to solve to be a commercial success. Several methods for
               distributing VM images are presented, and optimizations based on
               copy on write layers are discussed. The performance of the
               presented solutions and the security overhead is evaluated.",
  pages     = "567--574",
  month     =  feb,
  year      =  2010,
  keywords  = "Virtual machining;Cloud computing;Data security;Virtual
               manufacturing;Distributed computing;Grid computing;Platform
               virtualization;Isolation technology;High performance
               computing;Optimization methods;Cloud;Virtualization",
  issn      = "2377-5750",
  doi       = "10.1109/PDP.2010.39"
}

@INPROCEEDINGS{Bui2010-ai,
  title     = "An integrated system for secure code distribution in Wireless
               Sensor Networks",
  booktitle = "2010 8th {IEEE} International Conference on Pervasive Computing
               and Communications Workshops ({PERCOM} Workshops)",
  author    = "Bui, Nicola and Ugus, Osman and Dissegna, Moreno and Rossi,
               Michele and Zorzi, Michele",
  abstract  = "This paper presents a Secure Code Update (SCU) system for
               Wireless Sensor Networks (WSNs). This solution achieves
               different security goals. First, through a dedicated
               authentication protocol it provides protection against the
               corruption of code images during their dissemination.
               Authentication routines exploit a lightweight asymmetric T-time
               signature algorithm and allow the out-of-order reception of data
               blocks. Second, confidentiality is provided through the
               implementation of an optimized symmetric encryption suite,
               designed to leverage the processing capabilities offered by
               typical IEEE 802.15.4 radios. Last, our solution offers
               protection against denial of service attacks. In the first part
               of the paper we present the integration of this security system
               with the SYNAPSE++ reprogramming protocol, focusing on the
               description of the security suite as well as on its salient
               implementation aspects. After this, we present experimental
               results that demonstrate the effectiveness against security
               attacks and quantify the loss in performance due to the addition
               of security components to SYNAPSE++.",
  pages     = "575--581",
  month     =  mar,
  year      =  2010,
  keywords  = "Wireless sensor networks;Data
               security;Authentication;Protocols;Protection;Out of order;Design
               optimization;Cryptography;Computer crime;Performance
               loss;Wireless Sensor Networks;security;authentication;Denial of
               Service;Secure Code Update",
  doi       = "10.1109/PERCOMW.2010.5470503"
}

@INPROCEEDINGS{Zabolotnyi2013-xv,
  title     = "Dynamic program code distribution in
               {Infrastructure-as-a-Service} clouds",
  booktitle = "2013 5th International Workshop on Principles of Engineering
               {Service-Oriented} Systems ({PESOS})",
  author    = "Zabolotnyi, Rostyslav and Leitner, Philipp and Dustdar, Schahram",
  abstract  = "Elastically scaling cloud computing applications are becoming
               more and more prevalent in today's IT landscapes. One problem of
               building such applications in an Infrastructure-as-a-Service
               cloud is the runtime distribution of program code, configuration
               files and other resources. While it is possible to include all
               required program code in the used IaaS base images, this
               severely restricts the achievable dynamicity at runtime. In this
               paper, we present a framework for dynamic program code
               distribution. Our approach handles code distribution entirely
               transparently on middleware layer. We base our solution on an
               existing middleware, CloudScale. The paper discusses the design
               and implementation of our code distribution approach on top of
               CloudScale, and numerically evaluates the practicability and
               performance of the approach based on an illustrative case study.",
  pages     = "29--36",
  month     =  may,
  year      =  2013,
  file      = "All Papers/Z/Zabolotnyi et al. 2013 - Dynamic program code distribution in Infrastructure-as-a-Service clouds.pdf",
  keywords  = "Logic gates;Software",
  issn      = "2156-793X",
  doi       = "10.1109/PESOS.2013.6635974"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Guo2013-yl,
  title     = "Joint scheduling and association for $\alpha$-fairness Network
               Utility Maximization in cellular networks",
  booktitle = "2013 {IEEE} 24th Annual International Symposium on Personal,
               Indoor, and Mobile Radio Communications ({PIMRC})",
  author    = "Guo, C and Sheng, M and Wang, X and Zhang, Y",
  abstract  = "Enhancing system throughput and improving user fairness are two
               basic but contradictory objectives for resource allocation in
               wireless cellular networks. To obtain an efficient tradeoff
               between these two goals, Network Utility Maximization (NUM)
               framework has been adopted with log-utility to obtain
               proportional fairness among all the users in the network.
               However, such tradeoff can not control the bias towards
               throughput or fairness. In this paper, we focus on
               $\alpha$-fairness NUM in Soft Frequency Reuse (SFR) based
               cellular networks, where SFR is an attractive frequency reuse
               technique to mitigate Inter-Cell-Interference (ICI) and $\alpha$
               can be utilized to adjust the tradeoff. The difficulty of the
               considered issue comes from that it is a Mixed Integer
               Programming (MIP) problem taking into account both intra-cell
               user scheduling and inter-cell user association. To overcome
               this challenge, the $\alpha$-fairness NUM problem is decomposed
               into two subproblems, which are dealt with one by one. First,
               maximize intra-cell utility by user scheduling and second,
               maximize network utility by distributed user association.
               Numerical results show that the proposed algorithm approaches
               the optimal solution of the $\alpha$-fairness NUM problem. Also,
               we get a better tradeoff between throughput and fairness, where
               fairness is measured by Jain's index. Particularly, we improve
               the maximum Jain's index from about 0.3 to about 1.",
  pages     = "1769--1773",
  month     =  sep,
  year      =  2013,
  file      = "All Papers/G/Guo et al. 2013 - Joint scheduling and association for α-fairness Network Utility Maximization in cellular networks.pdf",
  keywords  = "adjacent channel interference;cellular radio;frequency
               allocation;integer programming;interference
               suppression;scheduling;maximum Jain index;distributed user
               association;intracell utility maximization;$\alpha$-fairness NUM
               problem;intercell user association;intracell user scheduling;MIP
               problem;mixed integer programming problem;ICI
               mitigation;intercell interference mitigation;SFR-based cellular
               networks;soft frequency reuse-based cellular
               networks;proportional fairness;log-utility;wireless cellular
               networks;resource allocation;user fairness improvement;system
               throughput enhancement;$\alpha$-fairness network utility
               maximization;joint
               scheduling-association;Throughput;Indexes;Interference;Resource
               management;Handover;Wireless communication;Wireless",
  issn      = "2166-9589",
  doi       = "10.1109/PIMRC.2013.6666429"
}

@INPROCEEDINGS{Huang2017-vt,
  title     = "A study of deep learning networks on mobile traffic forecasting",
  booktitle = "2017 {IEEE} 28th Annual International Symposium on Personal,
               Indoor, and Mobile Radio Communications ({PIMRC})",
  author    = "Huang, C and Chiang, C and Li, Q",
  abstract  = "With evolution toward the fifth generation (5G) cellular
               technologies, forecasting and understanding of mobile Internet
               traffic based on big data is the foundation to enable
               intelligent management features. To take full advantage of
               machine learning, a more comprehensive investigation on a mobile
               traffic dataset with the latest deep learning models is desired.
               Therefore, a multitask learning architecture using deep learning
               networks for mobile traffic forecasting is presented in this
               work. State-of-the-art deep learning models are studied,
               including 1) recurrent neural network (RNN), 2)
               three-dimensional convolutional neural network (3D cNn), and 3)
               combination of CNN and RNN (CNN-RNN). The experiments reveal
               that CNN and RNN can extract geographical and temporal traffic
               features respectively. Comparing with either deep or non-deep
               learning approaches, CNN-RNN is a reliable model leading in all
               tasks with 70 to 80\% forecasting accuracy.",
  pages     = "1--6",
  month     =  oct,
  year      =  2017,
  file      = "All Papers/H/Huang et al. 2017 - A study of deep learning networks on mobile traffic forecasting.pdf",
  keywords  = "5G mobile communication;Big Data;convolution;Internet;learning
               (artificial intelligence);mobile computing;recurrent neural
               nets;telecommunication traffic;fifth generation cellular
               technologies;mobile Internet traffic;intelligent management
               features;machine learning;mobile traffic dataset;deep learning
               networks;mobile traffic forecasting;geographical traffic
               features;temporal traffic features;CNN-RNN;3D convolutional
               neural network;recurrent neural network;Big Data;5G cellular
               technologies;Computer architecture;Mobile communication;Feature
               extraction;Machine learning;Task
               analysis;Forecasting;Three-dimensional displays;Deep
               learning;mobile traffic forecasting;multitask learning;big
               data;GeneralNetworking",
  issn      = "2166-9589",
  doi       = "10.1109/PIMRC.2017.8292737"
}

@INPROCEEDINGS{Trinh2018-mh,
  title     = "Mobile Traffic Prediction from Raw Data Using {LSTM} Networks",
  booktitle = "2018 {IEEE} 29th Annual International Symposium on Personal,
               Indoor and Mobile Radio Communications ({PIMRC})",
  author    = "Trinh, H D and Giupponi, L and Dini, P",
  abstract  = "Predictive analysis on mobile network traffic is becoming of
               fundamental importance for the next generation cellular network.
               Proactively knowing the user demands, allows the system for an
               optimal resource allocation. In this paper, we study the mobile
               traffic of an LTE base station and we design a system for the
               traffic prediction using Recurrent Neural Networks. The mobile
               traffic information is gathered from the Physical Downlink
               Control CHannel (PDCCH) of the LTE using the passive tool
               presented in [1]. Using this tool we are able to collect all the
               control information at 1 ms resolution from the base station.
               This information comprises the resource blocks, the transport
               block size and the modulation scheme assigned to each user
               connected to the eNodeB. The design of the prediction system
               includes long short term memory units. With respect to a
               Multilayer Perceptron Network, or other artificial neurons
               structures, recurrent networks are advantageous for problems
               with sequential data (e.g. language modeling) [2]. In our case,
               we state the problem as a supervised multivariate prediction of
               the mobile traffic, where the objective is to minimize the
               prediction error given the information extracted from the PDCCH.
               We evaluate the one-step prediction and the long-term prediction
               errors of the proposed methodology, considering different
               numbers for the duration of the observed values, which
               determines the memory length of the LSTM network and how much
               information must be stored for a precise traffic prediction.",
  pages     = "1827--1832",
  month     =  sep,
  year      =  2018,
  file      = "All Papers/T/Trinh et al. 2018 - Mobile Traffic Prediction from Raw Data Using LSTM Networks.pdf",
  keywords  = "learning (artificial intelligence);Long Term
               Evolution;minimisation;mobile computing;multilayer
               perceptrons;recurrent neural nets;resource
               allocation;telecommunication traffic;artificial neurons
               structures;cellular network;multilayer perceptron
               network;recurrent neural networks;short term memory
               units;resource blocks;Physical Downlink Control CHannel;LTE base
               station;optimal resource allocation;mobile traffic
               prediction;LSTM network;long-term prediction
               errors;PDCCH;supervised multivariate prediction;Computer
               architecture;Long Term Evolution;Microprocessors;Logic
               gates;Downlink;Base stations;Cellular networks;GeneralNetworking",
  issn      = "2166-9589",
  doi       = "10.1109/PIMRC.2018.8581000"
}

@INPROCEEDINGS{Balcan2005-ha,
  title     = "Mechanism Design via Machine Learning",
  booktitle = "Proceedings of the 46th Annual {IEEE} Symposium on Foundations
               of Computer Science",
  author    = "Balcan, Maria-Florina and Blum, Avrim",
  abstract  = "We use techniques from sample-complexity in machine learning to
               reduce problems of incentive-compatible mechanism design to
               standard algorithmic questions, for a wide variety of
               revenue-maximizing pricing problems. Our reductions imply that
               for these problems, given an optimal (or
               \textbackslashbeta-approximation) algorithm for the standard
               algorithmic problem, we can convert it into a (1+
               \textbackslashin)-approximation (or \textbackslashbeta(1+
               \textbackslashin)-approximation) for the
               incentive-compatiblemechanism design problem, so long as the
               number of bidders is sufficiently large as a function of an
               appropriate measure of complexity of the comparison class of
               solutions. We apply these results to the problem of auctioning a
               digital good, the attribute auction problem, and to the problem
               of itempricing in unlimited-supply combinatorial auctions. From
               a learning perspective, these settings present several
               challenges: in particular, the loss function is discontinuous
               and asymmetric, and the range of bidders valuations may be
               large.",
  publisher = "IEEE Computer Society",
  pages     = "605--614",
  series    = "FOCS '05",
  month     =  oct,
  year      =  2005,
  address   = "USA",
  isbn      = "9780769524689",
  doi       = "10.1109/SFCS.2005.50"
}

@INPROCEEDINGS{Ferrari2013-ak,
  title     = "Virtual Synchrony Guarantees for Cyber-physical Systems",
  booktitle = "2013 {IEEE} 32nd International Symposium on Reliable Distributed
               Systems",
  author    = "Ferrari, Federico and Zimmerling, Marco and Mottola, Luca and
               Thiele, Lothar",
  abstract  = "By integrating computational and physical elements through
               feedback loops, CPSs implement a wide range of safety-critical
               applications, from high-confidence medical systems to critical
               infrastructure control. Deployed systems must therefore provide
               highly dependable operation against unpredictable real-world
               dynamics. However, common CPS hardware-comprising
               battery-powered and severely resource-constrained devices
               interconnected via low-power wireless-greatly complicates
               attaining the required communication guarantees. VIRTUS fills
               this gap by providing atomic multicast and view management atop
               resource-constrained devices, which together provide virtually
               synchronous executions that developers can leverage to apply
               established concepts from the dependable distributed systems
               literature. We build VIRTUS upon an existing best-effort
               communication layer, and formally prove the functional
               correctness of our mechanisms. We further show, through
               extensive real-world experiments, that VIRTUS incurs a limited
               performance penalty compared with best-effort communication. To
               the best of our knowledge, VIRTUS is the first system to provide
               virtual synchrony guarantees atop resource-constrained CPS
               hardware.",
  pages     = "20--30",
  month     =  sep,
  year      =  2013,
  keywords  = "Receivers;Schedules;Computer crashes;Wireless
               communication;Protocols;Reliability;Sensors",
  issn      = "1060-9857",
  doi       = "10.1109/SRDS.2013.11"
}

@ARTICLE{Adams2013-nz,
  title    = "Active Queue Management: A Survey",
  author   = "Adams, R",
  abstract = "Since its formal introduction to IP networks in 1993 as a viable
              complementary approach for congestion control, there has been a
              steady stream of research output with respect to Active Queue
              Management (AQM). This survey attempts to travel the trajectory
              of AQM research from 1993 with the first algorithm, Random Early
              Detection (RED), to current work in 2011. In this survey we
              discuss the general attributes of AQM schemes, and the design
              approaches taken such as heuristic, control-theoretic and
              deterministic optimization. Of interest is the role of AQM in QoS
              provisioning particularly in the DiffServ context, as well as the
              role of AQM in the wireless domain. For each section, example
              algorithms from the research literature are presented.",
  journal  = "IEEE Communications Surveys Tutorials",
  volume   =  15,
  number   =  3,
  pages    = "1425--1476",
  year     =  2013,
  file     = "All Papers/A/Adams 2013 - Active Queue Management - A Survey.pdf",
  keywords = "computer network management;DiffServ networks;IP networks;quality
              of service;queueing theory;telecommunication congestion
              control;active queue management;IP networks;viable complementary
              approach;congestion control;random early detection;RED;AQM
              schemes;QoS provisioning;DiffServ context;wireless
              domain;Context;Quality of service;Delay;Wireless
              communication;Diffserv networks;Optimization;Proposals;Active
              Queue Management;AQM;quality-of-service;QoS;congestion
              control;differentiated services;DiffServ;wireless
              networks;ComputerNetworks",
  issn     = "1553-877X",
  doi      = "10.1109/SURV.2012.082212.00018"
}

@INPROCEEDINGS{Vladusic2020-ls,
  title     = "Infrastructure as Code for Heterogeneous Computing",
  booktitle = "2020 22nd International Symposium on Symbolic and Numeric
               Algorithms for Scientific Computing ({SYNASC})",
  author    = "Vladusic, Daniel and Radolovic, Dragan",
  abstract  = "Setting up an infrastructure for application deployment is a
               non-trivial task. We usually simplify the task by using the
               Infrastructure as Code (IaC) approach. Through IaC we set-up
               software defined infrastructure, able to run applications.
               Several tools and platforms have been developed to describe the
               system and to implement the actual deployment of the application
               (e.g., Puppet, Chef, Ansible, Terraform, etc.). When the
               infrastructure is heterogeneous (e.g., combination of Cloud and
               HPC) the challenges to set up a collaborative infrastructure is
               even harder, as the paradigms and environments differ
               significantly. Cloud infrastructure is focusing on servers,
               events, functions, while the HPC infrastructure is procedural
               and takes the application and merely executes it, usually on
               bare metal. As expected, the applications deployed on the
               respective systems differ in their dynamics and longevity.
               Whilst deployment of applications on the Cloud can be solved
               with the aforementioned tools, the HPC systems are in this
               respect in their infancy - the application is usually scheduled
               to be executed on a pre-defined set of processors and from that
               point on, the scripts merely gather input data, execute the
               application and then organize the output data. Merging the two
               approaches is thus currently rather hard and requires an
               explicit boundary between the Cloud and HPC parts of the
               application, posing a significant issue for the overall
               modelling and thus set-up of the system with the IaC approach.
               SODALITE, an H2020 project, is targeting simplification of the
               application deployment complexity while retaining or improving
               application performance on targeted HPC heterogeneous and cloud
               systems. The application deployment is abstracted through
               modelling of application's component relationships, policies and
               performance. The application is deployed using appropriate
               container technologies, matching the targeted heterogeneous HPC
               and cloud-based platforms. The starting point is the definition
               of the system and the application within an AI-supported IDE,
               using a straightforward, TOSCA-similar language. The smart IDE
               backed with the Graph DB knowledge base supports the user with
               the suggestions on how the system and application could (or
               should) be modelled. In the next general step, this definition
               is executed through an orchestrator, resulting in an execution
               of the application within the software-defined environment. In
               cases where the source code is available, it is optimised for
               the targeted infrastructure before execution. This step ensures
               that the application performance is not lost due to abstraction.
               However, in all cases, the execution of the application is
               monitored, as SODALITE is using machine learning and
               controltheory approaches to improve runtime performance.
               SODALITE is currently in the middle of its development thus not
               all of its functionalities are available. Whilst we first
               addressed the typical private Cloud infrastructures (e.g.
               OpenStack) and HPC (e.g. Torque job scheduler) using containers
               to encapsulate the applications, there is still work to be done
               to address public Clouds (e.g. AWS) and other HPC schedulers
               (e.g. Slurm). The IDE is functional, however it still requires
               further improvements and enhancements. Finally, not all
               optimisation approaches are developed at the moment. The aim of
               the proposed approach is to flatten the learning curve for Ops
               enabling them to concentrate on domain problems, resulting in
               lower overall costs of development and application lifecycle
               management.",
  pages     = "1--2",
  month     =  sep,
  year      =  2020,
  doi       = "10.1109/SYNASC51798.2020.00011"
}

@INPROCEEDINGS{Prevost2011-qw,
  title     = "Prediction of cloud data center networks loads using stochastic
               and neural models",
  booktitle = "2011 6th International Conference on System of Systems
               Engineering",
  author    = "Prevost, J J and Nagothu, K and Kelley, B and Jamshidi, M",
  abstract  = "The increasing demand for cloud computing resources has led to a
               commensurate increase in the operating power consumption of the
               systems that comprise the cloud. In this paper, we introduce a
               novel framework combining load demand prediction and stochastic
               state transition models. We claim that our model will lead to
               optimal cloud resource allocation by minimizing energy consumed
               while maintaining required performance levels. We characterize
               the ability of neural network and auto-regressive linear
               prediction algorithms to forecast loads in cloud data center
               applications. In this paper, the performance of our models
               against two sets of data at multiple look-ahead times is also
               presented.",
  pages     = "276--281",
  month     =  jun,
  year      =  2011,
  file      = "All Papers/P/Prevost et al. 2011 - Prediction of cloud data center networks loads using stochastic and neural models.pdf",
  keywords  = "autoregressive processes;cloud computing;computer centres;neural
               nets;resource allocation;cloud data center networks;stochastic
               model;neural model;cloud computing resource;load demand
               prediction;stochastic state transition model;cloud resource
               allocation;autoregressive linear prediction algorithm;neural
               network;Artificial neural networks;Predictive models;Load
               modeling;Servers;Data models;Clouds;Cloud computing;cloud;green
               computing;linear prediction;load forcasting;neural
               networks;optimization;MLNetworking",
  doi       = "10.1109/SYSOSE.2011.5966610"
}

@INPROCEEDINGS{Riera2014-zm,
  title     = "Virtual network function scheduling: Concept and challenges",
  booktitle = "2014 International Conference on Smart Communications in Network
               Technologies ({SaCoNeT})",
  author    = "Riera, J F and Escalona, E and Batall{\'e}, J and Grasa, E and
               Garc{\'\i}a-Esp{\'\i}n, J A",
  abstract  = "Software-Defined Networking and Network Functions Virtualization
               have initiated a new landscape within the telecom market
               landscape. Initial proof-of-concept prototypes for NFV-enabled
               solutions are being developed at the same time SDN models are
               identified as the futures solutions within the telecom realm. In
               this article, we provide a brief overview of the application and
               state-of-the-art of SDN and NFV technologies over optical
               networks. At the same time, we provide the first formalisation
               model for the VNF complex scheduling problem, using the complex
               job formalisation. The article aims at being used as starting
               point in order to optimally solve the scheduling problem of
               virtual network functions that compose network services to be
               provisioned within the SDN paradigm. Finally, we also provide an
               example of the virtualization of the routing function over an
               SDN-enabled domain.",
  pages     = "1--5",
  month     =  jun,
  year      =  2014,
  file      = "All Papers/R/Riera et al. 2014 - Virtual network function scheduling - Concept and challenges.pdf",
  keywords  = "computer networks;scheduling;telecommunication
               computing;telecommunication network
               routing;virtualisation;virtual network function
               scheduling;telecom market landscape;NFV-enabled solutions;SDN
               models;NFV technologies;SDN technologies;VNF complex scheduling
               problem;complex job formalisation model;routing function
               virtualisation;SDN-enabled domain;software-defined
               networking;network function virtualization;Servers;Optical fiber
               networks;Routing;Protocols;Telecommunication
               standards;Software;Computer architecture;NFV\_SDN",
  doi       = "10.1109/SaCoNeT.2014.6867768"
}

@ARTICLE{Li2020-fc,
  title    = "{Cross-Cloud} {MapReduce} for Big Data",
  author   = "Li, P and Guo, S and Yu, S and Zhuang, W",
  abstract = "MapReduce plays a critical role as a leading framework for big
              data analytics. In this paper, we consider a geo-distributed
              cloud architecture that provides MapReduce services based on the
              big data collected from end users all over the world. Existing
              work handles MapReduce jobs by a traditional computation-centric
              approach that all input data distributed in multiple clouds are
              aggregated to a virtual cluster that resides in a single cloud.
              Its poor efficiency and high cost for big data support motivate
              us to propose a novel data-centric architecture with three key
              techniques, namely, cross-cloud virtual cluster, data-centric job
              placement, and network coding based traffic routing. Our design
              leads to an optimization framework with the objective of
              minimizing both computation and transmission cost for running a
              set of MapReduce jobs in geo-distributed clouds. We further
              design a parallel algorithm by decomposing the original
              large-scale problem into several distributively solvable
              subproblems that are coordinated by a high-level master problem.
              Finally, we conduct real-world experiments and extensive
              simulations to show that our proposal significantly outperforms
              the existing works.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  2,
  pages    = "375--386",
  month    =  apr,
  year     =  2020,
  keywords = "Big Data;cloud computing;data analysis;data
              handling;optimisation;parallel processing;software
              architecture;big data analytics;cross-cloud
              MapReduce;optimization framework;traffic routing;data-centric job
              placement;cross-cloud virtual cluster;data-centric
              architecture;big data support;MapReduce services;geo-distributed
              cloud architecture;Cloud computing;Computer
              architecture;Distributed databases;Optimization;Big
              data;Routing;Clustering algorithms;Big
              data;MapReduce;cloud;optimization;parallel
              algorithm;Datacentre;Distributed Systems",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2015.2474385"
}

@ARTICLE{Kuang2020-bq,
  title    = "An Integration Framework on Cloud for {Cyber-Physical-Social}
              Systems Big Data",
  author   = "Kuang, L and Yang, L T and Liao, Y",
  abstract = "A tremendous challenge in the development of
              Cyber-Physical-Social Systems (CPSS) is to integrate the growing
              volume and wide variety of data generated from multiple sources.
              To address the challenge, this paper presents an integration
              framework on cloud consisting of five functionally complementary
              processes, namely data representation, dimensionality reduction,
              relation establishment, data rank and data retrieval. The
              unstructured, semi-structured and structured data in the cyber,
              physical and social space are first represented as low-order data
              tensors, and then transformed to a three-order feature tensor for
              relation establishment. A similarity-based multi-linear data rank
              approach is proposed to measure the importance of the CPSS big
              data, and an incremental method is explored to rapidly and
              accurately update the rank vector. This paper, through a smart
              home case study, illustrates a practical application of the
              proposed integration framework, and evaluates the performance of
              the multi-linear data rank approach as well as the incremental
              rank update method. The results reveal that the proposed
              framework is feasible and competitive to integrate the CPSS big
              data on cloud.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  2,
  pages    = "363--374",
  month    =  apr,
  year     =  2020,
  file     = "All Papers/K/Kuang et al. 2020 - An Integration Framework on Cloud for Cyber-Physical-Social Systems Big Data.pdf",
  keywords = "Big Data;cloud computing;cyber-physical systems;data
              structures;home computing;query
              processing;tensors;similarity-based multilinear data rank
              approach;CPSS big data;rank vector;integration
              framework;incremental rank update method;cloud;functionally
              complementary processes;data representation;relation
              establishment;social space;low-order data tensors;three-order
              feature tensor;cyber-physical-social systems Big Data;smart
              home;data retrieval;dimensionality reduction;Tensile stress;Big
              data;Data models;Cloud computing;Data mining;Matrix
              decomposition;Computational modeling;Big
              data;cyber-physical-social systems;tensor;cloud;EdgeFogCloudIoT",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2015.2511766"
}

@ARTICLE{Yu2020-dj,
  title    = "Stochastic Load Balancing for Virtual Resource Management in
              Datacenters",
  author   = "Yu, L and Chen, L and Cai, Z and Shen, H and Liang, Y and Pan, Y",
  abstract = "Cloud computing offers a cost-effective and elastic computing
              paradigm that facilitates large scale data storage and analytics.
              By deploying virtualization technologies in the datacenter, cloud
              enables efficient resource management and isolation for various
              big data applications. Since the hotspots (i.e., overloaded
              machines) can degrade the performance of these applications,
              virtual machine migration has been utilized to perform load
              balancing in the datacenters to eliminate hotspots and guarantee
              Service Level Agreements (SLAs). However, the previous load
              balancing schemes make migration decisions based on deterministic
              resource demand estimation and workload characterization, without
              considering their stochastic properties. By studying real world
              traces, we show that the resource demand and workload of virtual
              machines are highly dynamic and bursty, which can cause these
              schemes to make inefficient migrations for load balancing. To
              address this problem, in this paper we propose a stochastic load
              balancing scheme which aims to provide probabilistic guarantee
              against the resource overloading with virtual machine migration,
              while minimizing the total migration overhead. Our scheme
              effectively addresses the prediction of the distribution of
              resource demand and the multidimensional resource requirements
              with stochastic characterization. Moreover, as opposed to the
              previous works that measure the migration cost without
              considering the network topology, our scheme explicitly takes
              into account the distance between the source physical machine and
              the destination physical machine for a virtual machine migration.
              The trace-driven experiments show that our scheme outperforms the
              previous schemes in terms of SLA violation and the migration
              cost.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  2,
  pages    = "459--472",
  month    =  apr,
  year     =  2020,
  file     = "All Papers/Y/Yu et al. 2020 - Stochastic Load Balancing for Virtual Resource Management in Datacenters.pdf",
  keywords = "cloud computing;computer centres;contracts;resource
              allocation;stochastic processes;virtual
              machines;virtualisation;cloud computing;elastic computing
              paradigm;large scale data storage;virtualization
              technologies;datacenter;big data applications;overloaded
              machines;virtual machine migration;service level
              agreements;migration decisions;deterministic resource demand
              estimation;workload characterization;stochastic
              properties;stochastic load balancing scheme;total
              migration;multidimensional resource requirements;stochastic
              characterization;migration cost;source physical
              machine;destination physical machine;virtual resource
              management;probabilistic guarantee;SLA violation;Load
              management;Cloud computing;Resource management;Virtual
              machining;Dynamic scheduling;Estimation;Probabilistic
              logic;Datacenter;virtual machine migration;load
              balance;stochastic load balancing;resource management;Datacentre",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2016.2525984"
}

@ARTICLE{Tseng2020-bv,
  title    = "{Link-Aware} Virtual Machine Placement for Cloud Services based
              on {Service-Oriented} Architecture",
  author   = "Tseng, F-H and Jheng, Y-M and Chou, L-D and Chao, H-C and Leung,
              V C M",
  abstract = "Data center benefits cloud applications in providing high
              scalability and ensuring service availability. However, virtual
              machine (VM) placement in data center poses new challenges for
              service provisioning. For many cloud services such as storage and
              video streaming, present placement approaches are unable to
              support network-demanding services due to overwhelming
              communication traffic and time. Therefore VM placement concerning
              link capacity is vital to cloud data centers. In this paper, we
              define the network-aware VM placement optimization (NAVMPO)
              problem based on integer linear programming. The objective
              function of NAVMPO problem aims to minimize communication time
              for VMs of the same service type. Then we propose the
              service-oriented physical machine (PM) selection (SOPMS)
              algorithm and link-aware VM placement (LAVMP) algorithm. The
              SOPMS algorithm selects the most appropriate PM based on
              service-oriented architecture, and then the LAVMP algorithm
              deploys the most suitable VM to target PM regarding to the link
              capacity between them. Simulation results show that the proposed
              placement approach significantly decreases communication time
              compared to existing non-service-oriented and service-oriented VM
              placement algorithms, and also improves the average utility rate
              of PMs with lower power consumption.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  4,
  pages    = "989--1002",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/T/Tseng et al. 2020 - Link-Aware Virtual Machine Placement for Cloud Services based on Service-Oriented Architecture.pdf",
  keywords = "Cloud computing;Data centers;Service-oriented
              architecture;Network topology;Switches;Virtual
              machining;Bandwidth;Cloud computing;data center;virtual machine
              placement;service-oriented architecture;link capacity;Datacentre",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2017.2662226"
}

@ARTICLE{Tziritas2020-xo,
  title    = "Online {Inter-Datacenter} Service Migrations",
  author   = "Tziritas, N and Khan, S U and Loukopoulos, T and Lalis, S and Xu,
              C-Z and Li, K and Zomaya, A Y",
  abstract = "Service migration between datacenters can reduce the network
              overhead within a cloud infrastructure; thereby, also improving
              the quality of service for the clients. Most of the algorithms in
              the literature assume that the client access pattern remains
              stable for a sufficiently long period so as to amortize such
              migrations. However, if such an assumption does not hold, these
              algorithms can take arbitrarily poor migration decisions that can
              substantially degrade system performance. In this paper, we
              approach the issue of performing service migrations for an
              unknown and dynamically changing client access pattern. We
              propose an online algorithm that minimizes the inter-datacenter
              network, taking into account the network load of migrating a
              service between two datacenters, as well as the fact that the
              client request pattern may change ``quickly'', before such a
              migration is amortized. We provide a rigorous mathematical proof
              showing that the algorithm is 3.8-competitive for a cloud network
              structured as a tree of multiple datacenters. We briefly discuss
              how the algorithm can be modified to work on general graph
              networks with an $O\mathrm(log\vert V\vert)$O( log |V|)
              probabilistic approximation of the optimal algorithm. Finally, we
              present an experimental evaluation of the algorithm based on
              extensive simulations.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  4,
  pages    = "1054--1068",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/T/Tziritas et al. 2020 - Online Inter-Datacenter Service Migrations.pdf",
  keywords = "Cloud computing;Approximation algorithms;Delays;Virtual
              machining;Wireless sensor networks;Linear programming;Data
              transfer;Cloud computing;online service migrations;online virtual
              machine migrations;online algorithms;Datacentre;EdgeFogCloudIoT",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2017.2680439"
}

@ARTICLE{Pham2020-jw,
  title    = "Predicting Workflow Task Execution Time in the Cloud Using A
              {Two-Stage} Machine Learning Approach",
  author   = "Pham, T and Durillo, J J and Fahringer, T",
  abstract = "Many techniques such as scheduling and resource provisioning rely
              on performance prediction of workflow tasks for varying input
              data. However, such estimates are difficult to generate in the
              cloud. This paper introduces a novel two-stage machine learning
              approach for predicting workflow task execution times for varying
              input data in the cloud. In order to achieve high accuracy
              predictions, our approach relies on parameters reflecting runtime
              information and two stages of predictions. Empirical results for
              four real world workflow applications and several commercial
              cloud providers demonstrate that our approach outperforms
              existing prediction methods. In our experiments, our approach
              respectively achieves a best-case and worst-case estimation error
              of 1.6 and 12.2 percent, while existing methods achieved errors
              beyond 20 percent (for some cases even over 50 percent) in more
              than 75 percent of the evaluated workflow tasks. In addition, we
              show that the models predicted by our approach for a specific
              cloud can be ported with low effort to new clouds with low errors
              by requiring only a small number of executions.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  1,
  pages    = "256--268",
  month    =  jan,
  year     =  2020,
  file     = "All Papers/P/Pham et al. 2020 - Predicting Workflow Task Execution Time in the Cloud Using A Two-Stage Machine Learning Approach.pdf",
  keywords = "cloud computing;learning (artificial
              intelligence);scheduling;workflow management software;cloud
              providers;resource provisioning;scheduling;two-stage machine
              learning;workflow task execution time;Cloud
              computing;Runtime;Hardware;Computational modeling;Predictive
              models;Analytical models;Performance prediction;workflow tasks
              execution time;machine learning;EdgeFogCloudIoT",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2017.2732344"
}

@ARTICLE{Fu2020-te,
  title    = "Predicted Affinity Based Virtual Machine Placement in Cloud
              Computing Environments",
  author   = "Fu, X and Zhou, C",
  abstract = "In cloud data centers, an appropriate Virtual Machine (VM)
              placement has become an effective method to improve the resource
              utilization and reduce the energy consumption. However, most
              current solutions regard the VM placement as a bin-packing
              problem and each VM is seen as a single object. None of them have
              taken the relationships between VMs into consideration, which
              supplies us with a kind of new perspective. In this paper, we
              provide a model which explores the relationships for every two
              VMs based on the resource requirement provided by ARIMA
              prediction. This model evaluates the volatility of resource
              utilization after putting the two VMs on the same host and we
              call this model as affinity model. Based on the affinity model,
              VMs will be placed on those hosts that have the highest affinity
              with them. Therefore, we call it as Predicted Affinity based
              Virtual Machine Placement Algorithm (PAVMP). The advantages of
              PAVMP are showed by comparing it with other VM placement
              algorithms on CloudSim simulation platform with the PlanetLab and
              Google workload trace.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  1,
  pages    = "246--255",
  month    =  jan,
  year     =  2020,
  file     = "All Papers/F/Fu and Zhou 2020 - Predicted Affinity Based Virtual Machine Placement in Cloud Computing Environments.pdf",
  keywords = "bin packing;cloud computing;computer centres;energy
              consumption;power aware computing;resource allocation;virtual
              machines;predicted affinity based virtual machine placement
              algorithm;Google workload trace;PlanetLab;CloudSim simulation
              platform;VM placement algorithms;affinity model;ARIMA
              prediction;resource requirement;bin-packing problem;energy
              consumption;resource utilization;cloud data centers;cloud
              computing environments;Cloud computing;Virtual machining;Resource
              management;Energy consumption;Prediction
              algorithms;Servers;Computational modeling;Cloud computing;virtual
              machine placement;affinity model;energy
              consumption;EdgeFogCloudIoT",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2017.2737624"
}

@ARTICLE{Kherbache2020-ay,
  title    = "Scheduling Live Migration of Virtual Machines",
  author   = "Kherbache, V and Madelaine, {\'E} and Hermenier, F",
  abstract = "Every day, numerous VMs are migrated inside a datacenter to
              balance the load, save energy or prepare production servers for
              maintenance. Although VM placement problems are carefully
              studied, the underlying migration schedulers rely on vague adhoc
              models. This leads to unnecessarily long and energy-intensive
              migrations. We present mVM, a new and extensible migration
              scheduler. To provide schedules with minimal completion times,
              mVM parallelizes and sequentializes the migrations with regards
              to the memory workload and the network topology. mVM is
              implemented as a plugin of BtrPlace and its current library
              allows administrators to address temporal and energy concerns.
              Experiments on a real testbed shows mVM outperforms
              state-of-the-art migration schedulers. Compared to schedulers
              that cap the migration parallelism, mVM reduces the individual
              migration duration by 20.4 percent on average and the schedule
              completion time by 28.1 percent. In a maintenance operation
              involving 96 VMs migrated between 72 servers, mVM saves 21.5
              percent Joules against BtrPlace. Compared to the migration model
              inside the cloud simulator CloudSim, the prediction error of the
              migrations duration is about 5 times lower with mVM. By computing
              schedules involving thousands of migrations performed over
              various fat-tree network topologies, we observed that the mVM
              solving time accounts for about 1 percent of the schedule
              execution time.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  1,
  pages    = "282--296",
  month    =  jan,
  year     =  2020,
  file     = "All Papers/K/Kherbache et al. 2020 - Scheduling Live Migration of Virtual Machines.pdf",
  keywords = "computer centres;power aware computing;scheduling;virtual
              machines;migration parallelism;migrations duration;production
              servers;VM placement problems;live migration scheduling;virtual
              machines;mVM;datacenter;Schedules;Servers;Computational
              modeling;Cloud computing;Biological system modeling;Processor
              scheduling;Bandwidth;Live migration;scheduling;virtual
              machines;Datacentre",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2017.2754279"
}

@ARTICLE{Cui2020-df,
  title    = "A Reinforcement {Learning-Based} Mixed Job Scheduler Scheme for
              Grid or {IaaS} Cloud",
  author   = "Cui, D and Peng, Z and Xiong, J and Xu, B and Lin, W",
  abstract = "Job scheduling is a necessary prerequisite for performance
              optimization and resource management in the cloud computing
              system. Focusing on accurate scaled cloud computing environment
              and efficient job scheduling under Virtual Machine (VM) resource
              and Server Level Agreement (SLA) constraints, we introduce the
              architecture of cloud computing platform and optimization job
              scheduling scheme in this study. The system model is comprised of
              clearly defined separate constituent parts, including portal, job
              scheduler, and resources pool. By analyzing the execution process
              of user jobs, we designed a novel job scheduling scheme based on
              reinforcement learning to minimize the makespan and Average
              Waiting Time (AWT) under the VM resource and deadline
              constraints, and employ parallel multi-age parallel technologies
              to balance the exploration and exploitation in learning process
              and accelerate the convergence of Q-learning algorithm. Both
              simulation and real cloud platform experiment results demonstrate
              the efficiency of the proposed job scheduling scheme.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  4,
  pages    = "1030--1039",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/C/Cui et al. 2020 - A Reinforcement Learning-Based Mixed Job Scheduler Scheme for Grid or IaaS Cloud.pdf",
  keywords = "Cloud computing;Processor scheduling;Computational
              modeling;Scheduling;Quality of service;Optimization;Servers;Cloud
              computing;job scheduling;reinforcement
              learning;multi-agent;parallel learning;Q-learning;SLA;Datacentre",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2017.2773078"
}

@ARTICLE{Huang2020-aq,
  title    = "{Near-Optimal} Deployment of Service Chains by Exploiting
              Correlations Between Network Functions",
  author   = "Huang, H and Li, P and Guo, S and Liang, W and Wang, K",
  abstract = "A modern Network Function Virtualization (NFV) service is usually
              expressed in a service chain that contains a list of ordered
              network functions, each can run in one or multiple virtual
              machines. Although lots of efforts have been devoted to service
              chain deployment, the researchers normally consider a simple
              model of network functions where different service chains have
              their own network functions no matter whether some of the network
              function appliances are interdependent. In this paper, we study
              the service chain deployment by exploiting two types of
              correlations between network functions: the Coordination Effect
              due to information exchanges among multiple VMs running the same
              network function, and the Traffic-Change Effect where the volume
              of outgoing traffic is not necessarily equal to the volume of its
              incoming traffic at each network function because of packet
              manipulations such as compression and encryption. These two
              effects have not been studied simultaneously in the context of
              service chaining. With theobjective to maximize the profit
              measured by the admitted traffic minus the implementation cost,
              we first formulate a joint service-function deployment and
              traffic scheduling (SUPER) problem that is proved to be NP-hard.
              We then devise an approximation algorithm based on the Markov
              approximation technique and analyze its theoretical bound on the
              convergence time. Simulation results show that the proposed
              algorithm outperforms two existing benchmark algorithms
              significantly.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  2,
  pages    = "585--596",
  month    =  apr,
  year     =  2020,
  file     = "All Papers/H/Huang et al. 2020 - Near-Optimal Deployment of Service Chains by Exploiting Correlations Between Network Functions.pdf",
  keywords = "approximation theory;computer network management;computer network
              security;Markov processes;optimisation;telecommunication
              scheduling;telecommunication traffic;virtual
              machines;virtualisation;ordered network functions;service chain
              deployment;network function appliances;service chaining;joint
              service-function deployment;modern network function
              virtualization service;near-optimal deployment;NFV
              service;multiple virtual machines;coordination effect;information
              exchanges;traffic-change effect;outgoing traffic;packet
              manipulations;traffic scheduling problem;SUPER;NP-hard
              problem;approximation algorithm;Markov approximation
              technique;Approximation algorithms;Correlation;Markov
              processes;Network function virtualization;NFV;service
              chain;coordination effect;traffic-change effect;markov
              approximation;NFV",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2017.2780165"
}

@ARTICLE{Wu2020-jh,
  title    = "{Energy-Efficient} Decision Making for Mobile Cloud Offloading",
  author   = "Wu, H and Sun, Y and Wolter, K",
  abstract = "Mobile cloud offloading migrates heavy computation from mobile
              devices to remote cloud resources or nearby cloudlets. It is a
              promising method to alleviate the struggle between
              resource-constrained mobile devices and resource-hungry mobile
              applications. Caused by frequently changing location mobile users
              often see dynamically changing network conditions which have a
              great impact on the perceived application performance. Therefore,
              making high-quality offloading decisions at run time is difficult
              in mobile environments. To balance the energy-delay tradeoff
              based on different offloading-decision criteria (e.g., minimum
              response time or energy consumption), an energy-efficient
              offloading-decision algorithm based on Lyapunov optimization is
              proposed. The algorithm determines when to run the application
              locally, when to forward it directly for remote execution to a
              cloud infrastructure and when to delegate it via a nearby
              cloudlet to the cloud. The algorithm is able to minimize the
              average energy consumption on the mobile device while ensuring
              that the average response time satisfies a given time constraint.
              Moreover, compared to local and remote execution, the
              Lyapunov-based algorithm can significantly reduce the energy
              consumption while only sacrificing a small portion of response
              time. Furthermore, it optimizes energy better and has less
              computational complexity than the Lagrange Relaxation based
              Aggregated Cost (LARAC-based) algorithm.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  2,
  pages    = "570--584",
  month    =  apr,
  year     =  2020,
  file     = "All Papers/W/Wu et al. 2020 - Energy-Efficient Decision Making for Mobile Cloud Offloading.pdf",
  keywords = "cloud computing;computational complexity;decision making;mobile
              computing;optimisation;mobile cloud offloading;remote cloud
              resources;nearby cloudlet;resource-constrained mobile
              devices;resource-hungry mobile applications;location mobile
              users;high-quality offloading decisions;mobile
              environments;energy-delay tradeoff;energy-efficient
              offloading-decision algorithm;remote execution;cloud
              infrastructure;average energy consumption;average response
              time;Lyapunov-based algorithm;Lagrange relaxation based
              aggregated cost algorithm;energy-efficient decision
              making;offloading-decision criteria;Lyapunov
              optimization;computational complexity;LARAC-based algorithm;Cloud
              computing;Mobile communication;Mobile handsets;Time
              factors;Energy consumption;Heuristic algorithms;Batteries;Mobile
              cloud computing;cloudlet;offloading;energy-efficient;Lyapunov
              optimization;LARAC algorithm",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2789446"
}

@ARTICLE{Zhang2021-ft,
  title    = "Speeding Up {VM} Startup by Cooperative {VM} Image Caching",
  author   = "Zhang, Yifan and Niu, Kai and Wu, Weigang and Li, Keqin and Zhou,
              Yu",
  abstract = "Virtual machine (VM) management is at the core of virtualized
              cloud data centers. Among others, how to reduce the startup delay
              of VMs is a key issue for improving user experience and resource
              utility. In this paper, we study this issue by jointly
              considering VM placement and VM image caching. We formulate the
              joint placement problem and design several joint algorithms,
              including both online and offline algorithms, to speed up VM
              startup. In our design, we adopt the cooperative caching
              approach, where image cache copies are shared among physical
              machines (PMs) so as to reduce image retrieval time. The key
              point of our algorithms lies in how to appropriately place VM
              image cache among PMs so as to speed up VM startup as much as
              possible. The proposed algorithms are evaluated by extensive
              simulations via SimGrid. The results show that our algorithms can
              achieve shorter startup delay in most cases, compared with
              existing ones.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  9,
  number   =  1,
  pages    = "360--371",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/Z/Zhang et al. 2021 - Speeding Up VM Startup by Cooperative VM Image Caching.pdf",
  keywords = "Delays;Cloud computing;Image retrieval;Telecommunication
              traffic;Algorithm design and analysis;Virtualization;Cloud
              computing;data caching;data centers;resource management;VM
              placement;ToRead;Important;Datacentre",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2791509"
}

@ARTICLE{Zhao2020-gr,
  title    = "{Locality-Aware} Scheduling for Containers in Cloud Computing",
  author   = "Zhao, D and Mohamed, M and Ludwig, H",
  abstract = "The state-of-the-art scheduler of containerized cloud services
              considers load balance as the only criterion; many other
              important properties, including application performance, are
              overlooked. In the era of Big Data, however, applications evolve
              to be increasingly more data-intensive thus perform poorly when
              deployed on containerized cloud services. To that end, this paper
              aims to improve today's cloud service by taking application
              performance into account for the next-generation container
              schedulers. More specifically, in this work we build and analyze
              a new model that respects both load balance and application
              performance. Unlike prior studies, our model abstracts the
              dilemma between load balance and application performance into a
              unified optimization problem and then employs a statistical
              method to efficiently solve it. The most challenging part is that
              some sub-problems are extremely complex (for example, NP-hard),
              and heuristic algorithms have to be devised. Last but not least,
              we implement a system prototype of the proposed scheduling
              strategy for containerized cloud services. Experimental results
              show that our system can significantly boost application
              performance while preserving high load balance.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  2,
  pages    = "635--646",
  month    =  apr,
  year     =  2020,
  file     = "All Papers/Z/Zhao et al. 2020 - Locality-Aware Scheduling for Containers in Cloud Computing.pdf",
  keywords = "cloud computing;optimisation;resource
              allocation;scheduling;statistical analysis;locality-aware
              scheduling;cloud computing;containerized cloud
              services;application performance;cloud service;next-generation
              container schedulers;high load balance;big data;unified
              optimization problem;statistical method;heuristic
              algorithms;Cloud
              computing;Containers;Bandwidth;Optimization;Processor
              scheduling;Heuristic algorithms;Indexes;Cloud computing;service
              computing;containers;data management;high-performance
              computing;Distributed Systems;Datacentre;EdgeFogCloudIoT",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2794344"
}

@ARTICLE{Soualhia2020-yi,
  title    = "A Dynamic and {Failure-Aware} Task Scheduling Framework for
              Hadoop",
  author   = "Soualhia, M and Khomh, F and Tahar, S",
  abstract = "Hadoop has become a popular framework for processing
              data-intensive applications in cloud environments. A core
              constituent of Hadoop is the scheduler, which is responsible for
              scheduling and monitoring the jobs and tasks, and rescheduling
              them in case of failures. Although fault-tolerance mechanisms
              have been proposed for Hadoop, the performance of Hadoop can be
              significantly impacted by unforeseen events in the cloud
              environment. In this paper, we introduce a dynamic and
              failure-aware framework that can be integrated within Hadoop
              scheduler and adjust the scheduling decisions based on collected
              information about the cloud environment. Our framework relies on
              predictions made by machine learning algorithms and scheduling
              policies generated by a Markovian Decision Process (MDP), to
              adjust its scheduling decisions on the fly. Instead of the fixed
              heartbeat-based failure detection commonly used in Hadoop to
              track active TaskTrackers (i.e., nodes that process the scheduled
              tasks), our proposed framework implements an adaptive algorithm
              that can dynamically detect the failures of the TaskTracker. To
              deploy our proposed framework, we have built, ATLAS+, an AdapTive
              Failure-Aware Scheduler for Hadoop. To assess the performance of
              ATLAS+, we conduct a large empirical study on a 100-node Hadoop
              cluster deployed on Amazon Elastic MapReduce (EMR), comparing the
              performance of ATLAS+ with those of three Hadoop schedulers
              (FIFO, Fair, and Capacity). Results show that ATLAS+ outperforms
              FIFO, Fair, and Capacity schedulers. ATLAS+ can reduce the number
              of failed jobs by up to 43 percent and the number of failed tasks
              by up to 59 percent. On average, ATLAS+ could reduce the total
              execution time of jobs by 10 minutes, which represents 40 percent
              of the job execution times, and by up to 3 minutes for tasks,
              which represents 47 percent of the task execution time. ATLAS+
              also reduced CPU and memory usage by 22 and 20 percent,
              respectively.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  2,
  pages    = "553--569",
  month    =  apr,
  year     =  2020,
  keywords = "cloud computing;data handling;fault tolerant computing;learning
              (artificial intelligence);Markov processes;parallel
              processing;resource allocation;scheduling;cloud
              environment;dynamic failure-aware framework;Hadoop
              scheduler;Hadoop cluster;ATLAS+;capacity schedulers;adaptive
              failure-aware scheduler;heartbeat-based failure
              detection;failure-aware task scheduling framework;machine
              learning algorithms;Markovian decision
              process;MDP;TaskTracker;Task analysis;Heart beat;Cloud
              computing;Dynamic scheduling;Machine learning
              algorithms;Processor scheduling;Adaptive scheduling;failure-aware
              scheduling;hadoop;MapReduce;ATLAS+;Distributed Systems",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2805812"
}

@ARTICLE{Metwally2020-dv,
  title    = "A Distributed Auction-based Framework for Scalable {IaaS}
              Provisioning in {Geo-Data} Centers",
  author   = "Metwally, K and Jarray, A and Karmouch, A",
  abstract = "This paper proposes a Cloud Infrastructure-as-a-Service (IaaS)
              framework that allows customers to have their high performance
              computing applications hosted efficiently and Cloud Service
              Providers (CSPs) to use their resources profitably. The solution
              introduces a distributed architecture that manages geographically
              distributed Data Centers (Geo-Data Centers) logically grouped in
              regions. This framework overcomes the challenges of traditional
              centralized provisioning approaches: (a) efficient provisioning
              of IaaS demand, (b) scale with respect to the growing number of
              IaaS requests, (c) guarantee of the stringent Quality of Service
              requirements of IaaS requests, and (d) efficient use of Cloud
              Geo-Data Center computing resources. Our architecture
              incorporates two decentralized approaches, hierarchical and
              distributed, that use auctions instead of a pay-as-you-go pricing
              scheme. The two approaches use a large-scale optimization
              technique for the allocation of Geo-Data Centers computing
              resources. The results of a simulation demonstrate an efficient
              use of computing resources and a significant reduction in
              computation time. This ensures adequate scalability to meet an
              exponential growth of IaaS demand. The auction-based approaches
              are also shown to provide monetary benefits to the participants.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  3,
  pages    = "647--659",
  month    =  jul,
  year     =  2020,
  keywords = "cloud computing;computer centres;optimisation;pricing;distributed
              auction-based framework;scalable IaaS provisioning;high
              performance computing applications;cloud service
              providers;distributed architecture;IaaS demand;IaaS
              requests;quality of service requirements;decentralized
              approaches;auction-based approaches;cloud geo-data center
              computing resources;centralized provisioning approaches;cloud
              infrastructure-as-a-service framework;geographically distributed
              data centers;large-scale optimization technique;Cloud
              computing;Scalability;Data centers;Quality of
              service;Computational modeling;Computer architecture;Resource
              management;Infrastructure-as-a-Service;Geo-Data centers
              architecture;resource
              allocation;provisioning;scalability;centralized
              approach;decentralized approach;hierarchical
              approach;auction;column generation optimization",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2808531"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Zhang2020-uc,
  title    = "A Truthful {$(1-\epsilon)$(1-$\epsilon$)-Optimal} Mechanism for
              {On-Demand} Cloud Resource Provisioning",
  author   = "Zhang, X and Wu, C and Li, Z and Lau, F C M",
  abstract = "On-demand resource provisioning in cloud computing provides
              tailor-made resource packages (typically in the form of VMs) to
              meet users' demands. Public clouds nowadays provide elaborated
              types of VMs, but have yet to offer the most flexible dynamic VM
              assembly, which is partly due to the lack of a mature mechanism
              for pricing tailor-made VMs. This work proposes an efficient
              randomized auction mechanism based on a novel application of
              smoothed analysis and randomized reduction, for dynamic VM
              provisioning and pricing in geo-distributed cloud data centers.
              To the best of our knowledge, it is the first one in literature
              that achieves (i) truthfulness in expectation, (ii) polynomial
              running time in expectation, and (iii) (1 - $\epsilon$)-optimal
              social welfare in expectation for resource allocation, where
              $\epsilon$ can be arbitrarily close to0. Our mechanism consists
              of three modules: (1) an exact algorithm to solve the NP-hard
              social welfare maximization problem, which has polynomial
              run-time in expectation, (2) a perturbation-based randomized
              resource allocation scheme which produces an allocation solution
              that is (1 - $\epsilon$)-optimal and (3) an auction mechanism
              prices the customized VMs using a randomized VCG payment, with a
              guarantee in truthfulness in expectation. We validate the
              efficacy of the mechanism through theoretical analysis and
              trace-driven simulations.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  3,
  pages    = "735--748",
  month    =  jul,
  year     =  2020,
  file     = "All Papers/Z/Zhang et al. 2020 - A Truthful $(1- epsilon)$(1-ε)-Optimal Mechanism for On-Demand Cloud Resource Provisioning.pdf",
  keywords = "cloud computing;computational complexity;pricing;resource
              allocation;virtual machines;On-Demand Cloud Resource
              Provisioning;cloud computing;tailor-made resource packages;public
              clouds;flexible dynamic VM assembly;efficient randomized auction
              mechanism;smoothed analysis;randomized reduction;geo-distributed
              cloud data centers;polynomial running time;NP-hard social welfare
              maximization problem;polynomial run-time;perturbation-based
              randomized resource allocation scheme;auction mechanism
              prices;customized VMs;randomized VCG payment;Cloud
              computing;Resource management;Pricing;Approximation
              algorithms;Data centers;Perturbation methods;Heuristic
              algorithms;Cloud computing;auction;resource
              allocation;pricing;truthful mechanisms",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2822718"
}

@ARTICLE{Laalaoui2020-ma,
  title    = "A Planning Approach for Reassigning Virtual Machines in {IaaS}
              Clouds",
  author   = "Laalaoui, Y and Al-Omari, J",
  abstract = "Reassignment of virtual machines into clusters is an important
              task for the good management of cloud resources since it
              decisively affects the performance of the Service Provider
              platform. Thus, for a successful reassignment, a clear and
              careful reassignment plan should be constructed in advance. In
              this paper, we propose a planning approach to the problem of
              reassigning virtual machines in IaaS Cloud platforms. First, we
              use the well-known A* algorithm to solve this planning problem.
              Then, we propose two algorithms, called Direct Move Heuristic
              (DMH) and Iterative Direct Move Heuristic (IDMH), to bridge the
              space limitation of the A* algorithm. Also, we suggest two
              experimental studies that have been conducted on randomly
              generated problem instances. The first experimental study
              considers small sized problem instances. It aims to show the
              applicability of the described modeling and assesses the
              efficiency of the proposed algorithms. The second experimental
              study focuses on large sized problem instances. It assesses the
              scalability performance of the IDMH heuristic. Our obtained
              results show a good scalability performance on problem instances
              with up to 800 virtual machines.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  3,
  pages    = "685--697",
  month    =  jul,
  year     =  2020,
  file     = "All Papers/L/Laalaoui and Al-Omari 2020 - A Planning Approach for Reassigning Virtual Machines in IaaS Clouds.pdf",
  keywords = "cloud computing;graph theory;iterative methods;virtual
              machines;A* algorithm;randomly generated problem
              instances;planning approach;IaaS clouds;cloud resources;service
              provider platform;IaaS Cloud platforms;planning problem;virtual
              machine reassignment;iterative direct move heuristic;IDMH;Cloud
              computing;Planning;Virtual machining;Heuristic
              algorithms;Clustering algorithms;Scalability;Quality of
              service;Virtual Machines;Clusters;IaaS
              Clouds;Reassignment;Planning;EdgeFogCloudIoT",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2826548"
}

@ARTICLE{Zhang2020-wj,
  title    = "Distributed Resource Allocation for Data Center Networks: A
              Hierarchical Game Approach",
  author   = "Zhang, H and Xiao, Y and Bu, S and Yu, F R and Niyato, D and Han,
              Z",
  abstract = "The increasing demand of data computing and storage for
              cloud-based services motivates the development and deployment of
              large-scale data centers. This paper studies the resource
              allocation problem for the data center networking system when
              multiple data center operators (DCOs) simultaneously serve
              multiple service subscribers (SSs). We formulate a hierarchical
              game to analyze this system where the DCOs and the SSs are
              regarded as the leaders and followers, respectively. In the
              proposed game, each SS selects its serving DCO with preferred
              price and purchases the optimal amount of resources for the SS's
              computing requirements. Based on the responses of the SSs' and
              the other DCOs', the DCOs decide their resource prices so as to
              receive the highest profit. When the coordination among DCOs is
              weak, we consider all DCOs are noncooperative with each other,
              and propose a sub-gradient algorithm for the DCOs to approach a
              sub-optimal solution of the game. When all DCOs are sufficiently
              coordinated, we formulate a coalition game among all DCOs and
              apply Kalai-Smorodinsky bargaining as a resource division
              approach to achieve high utilities. Both solutions constitute the
              Stackelberg Equilibrium. The simulation results verify the
              performance improvement provided by our proposed approaches.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  3,
  pages    = "778--789",
  month    =  jul,
  year     =  2020,
  file     = "All Papers/Z/Zhang et al. 2020 - Distributed Resource Allocation for Data Center Networks - A Hierarchical Game Approach.pdf",
  keywords = "cloud computing;computer centres;game theory;pricing;resource
              allocation;distributed resource allocation;data center
              networks;hierarchical game approach;data computing;cloud-based
              services;large-scale data centers;resource allocation
              problem;data center networking system;data center
              operators;DCO;multiple service subscribers;resource
              prices;resource division approach;SS;Games;Delays;Data
              centers;Cloud computing;Resource management;Analytical
              models;Computational modeling;Data center;hierarchical game;game
              theory;resource management",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2829744"
}

@ARTICLE{Guo2020-ng,
  title    = "Online {VM} {Auto-Scaling} Algorithms for Application Hosting in
              a Cloud",
  author   = "Guo, Y and Stolyar, A L and Walid, A",
  abstract = "We consider the auto-scaling problem for application hosting in a
              cloud, where applications are elastic and the number of requests
              changes over time. The application requests are serviced by
              Virtual Machines (VMs), which reside on Physical Machines (PMs)
              in a cloud. We aim to minimize the number of hosting PMs by
              intelligently packing VMs into PMs, while the VMs are
              auto-scaled, i.e., dynamically acquired and released, to
              accommodate varying application needs. We consider a shadow
              routing based approach for this problem. The proposed shadow
              algorithm employs a specially constructed virtual queueing system
              to dynamically produce an optimal solution that guides the VM
              auto-scaling and the VM-to-PM packing. The proposed algorithm
              runs continuously without the need to re-solve the underlying
              optimization problem ``from scratch'', and adapts automatically
              to the changes in the application demands. We prove the
              asymptotic optimality of the shadow algorithm. The simulation
              experiments further demonstrate the algorithm's good performance
              and high adaptivity.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  3,
  pages    = "889--898",
  month    =  jul,
  year     =  2020,
  keywords = "cloud computing;computer centres;optimisation;power aware
              computing;resource allocation;virtual machines;application
              hosting;requests changes;application requests;virtual
              machines;physical machines;auto-scaled;varying application
              needs;shadow routing;shadow algorithm;specially constructed
              virtual queueing system;VM-to-PM packing;underlying optimization
              problem;application demands;online VM auto-scaling
              algorithms;auto-scaling problem;hosting PM;Cloud
              computing;Routing;Heuristic algorithms;Optimization;Electronic
              mail;Adaptation models;Virtual machining;Cloud computing;data
              centers;auto-scaling;virtual machine;dynamic stochastic bin
              packing;online algorithms;Datacentre",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2830793"
}

@ARTICLE{Florin2020-gc,
  title    = "A Tight Estimate of Job Completion Time in Vehicular Clouds",
  author   = "Florin, R and Ghazizadeh, P and Zadeh, A G and Mukkamala, R and
              Olariu, S",
  abstract = "Inspired by the success of conventional cloud services and by the
              reality of present-day vehicles endowed with powerful on-board
              computers that can act as servers in a datacenter, researchers
              have recently introduced the concept of a vehicular cloud. Our
              main contribution is to offer a tight theoretical analysis of the
              expected job completion time in vehicular clouds characterized by
              short vehicular residency times, under a redundancy-based job
              assignment strategy. We also discuss various approximations of
              the expected completion time. A comprehensive set of simulations
              have confirmed the accuracy of our theoretical predictions.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  8,
  number   =  3,
  pages    = "721--734",
  month    =  jul,
  year     =  2020,
  keywords = "cloud computing;redundancy;traffic engineering
              computing;redundancy-based job assignment;vehicular cloud;cloud
              services;present-day vehicles;expected job completion time;short
              vehicular residency times;on-board
              computers;servers;datacenter;Automobiles;Cloud
              computing;Privacy;Airports;Fault tolerance;Fault tolerant
              systems;Security;Cloud computing;vehicular cloud;datacenter;fault
              tolerance;job completion time",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2834352"
}

@ARTICLE{Mseddi2021-bl,
  title    = "Efficient Replica Migration Scheme for Distributed Cloud Storage
              Systems",
  author   = "Mseddi, Amina and Salahuddin, Mohammad A and Zhani, Mohamed Faten
              and Elbiaze, Halima and Glitho, Roch H",
  abstract = "With the wide adoption of large-scale internet services and big
              data, the cloud has become the ideal environment to satisfy the
              ever-growing storage demand. In this context, data replication
              has been touted as the ultimate solution to improve data
              availability and reduce access time. However, replica management
              systems usually need to migrate and create a large number of data
              replicas over time between and within data centers, incurring a
              large overhead in terms of network load and availability. In this
              paper, we propose CRANE, an effiCient Replica migrAtion scheme
              for distributed cloud Storage systEms. CRANE complements any
              replica placement algorithm by efficiently managing replica
              creation in geo-distributed infrastructures in order to (1)
              minimize the time needed to copy the data to the new replica
              location, (2) avoid network congestion, and (3) ensure the
              minimum desired availability for the data. Through simulation and
              experimental results, we show that CRANE provides a sub-optimal
              solution for the replica migration problem with lower
              computational complexity than its integer linear program
              formulation. We also show that, compared to OpenStack Swift,
              CRANE is able to reduce by up to 60 percent the replica creation
              and migration time and by up to 50 percent the inter-data center
              network traffic while ensuring the minimum required data
              availability.",
  journal  = "IEEE Transactions on Cloud Computing",
  volume   =  9,
  number   =  1,
  pages    = "155--167",
  month    =  jan,
  year     =  2021,
  keywords = "Cranes;Cloud computing;Data centers;Bandwidth;Electronic
              mail;Schedules;Distributed databases;Cloud storage;data
              availability;data migration;replica management",
  issn     = "2168-7161",
  doi      = "10.1109/TCC.2018.2858792"
}

@ARTICLE{Qin2020-fj,
  title    = "20 Years of Evolution From Cognitive to Intelligent
              Communications",
  author   = "Qin, Z and Zhou, X and Zhang, L and Gao, Y and Liang, Y and Li, G
              Y",
  abstract = "It has been 20 years since the concept of cognitive radio (CR)
              was proposed, which is an efficient approach to provide more
              access opportunities to connect massive wireless devices. To
              improve the spectrum efficiency, CR enables unlicensed usage of
              licensed spectrum resources. It has been regarded as the key
              enabler for intelligent communications. In this article, we will
              provide an overview on the intelligent communication in the past
              two decades to illustrate the revolution of its capability from
              cognition to artificial intelligence (AI). Particularly, this
              article starts from a comprehensive review of typical spectrum
              sensing and sharing, followed by the recent achievements on the
              AI-enabled intelligent radio. Moreover, research challenges in
              the future intelligent communications will be discussed to show a
              path to the real deployment of intelligent radio. After
              witnessing the glorious developments of CR in the past 20 years,
              we try to provide readers a clear picture on how intelligent
              radio could be further developed to smartly utilize the limited
              spectrum resources as well as to optimally configure wireless
              devices in the future communication systems.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  1,
  pages    = "6--20",
  month    =  mar,
  year     =  2020,
  file     = "All Papers/Q/Qin et al. 2020 - 20 Years of Evolution From Cognitive to Intelligent Communications.pdf",
  keywords = "artificial intelligence;cognitive radio;intelligent
              networks;radio spectrum management;future communication
              systems;future intelligent communications;AI-enabled intelligent
              radio;typical spectrum sensing;artificial
              intelligence;cognition;intelligent communication;licensed
              spectrum resources;spectrum efficiency;massive wireless
              devices;cognitive radio;Sensors;Artificial intelligence;Wireless
              sensor networks;5G mobile communication;Wireless networks;Signal
              to noise ratio;Artificial intelligence;cognitive
              radio;intelligent communications;spectrum sensing and
              sharing;Mobile\_Wireless",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2019.2949279"
}

@ARTICLE{Chen2020-kh,
  title    = "Intelligent Traffic Adaptive Resource Allocation for Edge
              {Computing-Based} {5G} Networks",
  author   = "Chen, M and Miao, Y and Gharavi, H and Hu, L and Humar, I",
  abstract = "The popularity of smart mobile devices has led to a tremendous
              increase in mobile traffic, which has put a considerable strain
              on the fifth generation of mobile communication networks (5G).
              Among the three application scenarios covered by 5G, ultra-high
              reliability and ultra-low latency (uRLLC) communication can best
              be realized with the assistance of artificial intelligence. For a
              combined 5G, edge computing and IoT-Cloud (a platform that
              integrates the Internet of Things and cloud) in particular, there
              remains many challenges to meet the uRLLC latency and reliability
              requirements despite a tremendous effort to develop smart
              data-driven methods. Therefore, this paper mainly focuses on
              artificial intelligence for controlling mobile-traffic flow. In
              our approach, we first develop a traffic-flow prediction
              algorithm that is based on long short-term memory (LSTM) with an
              attention mechanism to train mobile-traffic data in single-site
              mode. The algorithm is capable of effectively predicting the peak
              value of the traffic flow. For a multi-site case, we present an
              intelligent IoT-based mobile traffic prediction-and-control
              architecture capable of dynamically dispatching communication and
              computing resources. In our experiments, we demonstrate the
              effectiveness of the proposed scheme in reducing communication
              latency and its impact on lowering packet-loss ratio. Finally, we
              present future work and discuss some of the open issues.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  2,
  pages    = "499--508",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/C/Chen et al. 2020 - Intelligent Traffic Adaptive Resource Allocation for Edge Computing-Based 5G Networks.pdf",
  keywords = "5G mobile communication;cloud computing;Internet of Things;mobile
              computing;recurrent neural nets;resource
              allocation;telecommunication network
              reliability;telecommunication traffic;intelligent traffic
              adaptive resource allocation;edge computing-based 5G
              networks;smart mobile devices;mobile communication
              networks;ultra-high reliability;ultra-low latency;artificial
              intelligence;IoT-cloud;reliability requirements;smart data-driven
              methods;mobile-traffic flow;traffic-flow prediction
              algorithm;mobile-traffic data;computing resources;Internet of
              Things;uRLLC latency;long short-term memory;attention
              mechanism;intelligent IoT-based mobile traffic
              prediction-and-control architecture;Ultra reliable low latency
              communication;Prediction algorithms;Reliability;5G mobile
              communication;Heuristic algorithms;Computer
              architecture;Artificial intelligence;5G;artificial
              intelligence;LSTM;mobile
              traffic;uRLLC;MLNetworking;EdgeFogCloudIoT",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2019.2953061"
}

@ARTICLE{Qureshi2020-yk,
  title    = "Fast Learning for Dynamic Resource Allocation in {AI-Enabled}
              Radio Networks",
  author   = "Qureshi, M A and Tekin, C",
  abstract = "Artificial Intelligence (AI)-enabled radios are expected to
              enhance the spectral efficiency of 5th generation (5G) millimeter
              wave (mmWave) networks by learning to optimize network resources.
              However, allocating resources over the mmWave band is extremely
              challenging due to rapidly-varying channel conditions. We
              consider several resource allocation problems for mmWave radio
              networks under unknown channel statistics and without any channel
              state information (CSI) feedback: i) dynamic rate selection for
              an energy harvesting transmitter, ii) dynamic power allocation
              for heterogeneous applications, and iii) distributed resource
              allocation in a multi-user network. All of these problems exhibit
              structured payoffs which are unimodal functions over partially
              ordered arms (transmission parameters) as well as over partially
              ordered contexts (side-information). Unimodality over arms helps
              in reducing the number of arms to be explored, while unimodality
              over contexts helps in using past information from nearby
              contexts to make better selections. We model this as a structured
              reinforcement learning problem, called contextual unimodal
              multi-armed bandit (MAB), and propose an online learning
              algorithm that exploits unimodality to optimize the resource
              allocation over time, and prove that it achieves logarithmic in
              time regret. Our algorithm's regret scales sublinearly both in
              the number of arms and contexts for a wide range of scenarios. We
              also show via simulations that our algorithm significantly
              improves the performance in the aforementioned resource
              allocation problems.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  1,
  pages    = "95--110",
  month    =  mar,
  year     =  2020,
  file     = "All Papers/Q/Qureshi and Tekin 2020 - Fast Learning for Dynamic Resource Allocation in AI-Enabled Radio Networks.pdf",
  keywords = "5G mobile communication;energy harvesting;learning (artificial
              intelligence);radio networks;radio transmitters;resource
              allocation;wireless channels;dynamic resource
              allocation;AI-enabled radio networks;spectral efficiency;network
              resources;channel conditions;mmWave radio networks;channel
              statistics;channel state information feedback;energy harvesting
              transmitter;dynamic power allocation;multiuser network;structured
              reinforcement learning problem;online learning algorithm;fast
              learning;artificial intelligence-enabled radios;5th generation
              millimeter wave networks;contextual unimodal multiarmed
              bandit;Resource management;Wireless communication;Heuristic
              algorithms;Dynamic scheduling;Radio networks;Reinforcement
              learning;Millimeter wave communication;AI-enabled
              radio;mmWave;resource allocation;contextual MAB;unimodal
              MAB;regret bounds;Wireless;MLNetworking",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2019.2953607"
}

@ARTICLE{Lu2020-co,
  title    = "A {GRU-Based} Prediction Framework for Intelligent Resource
              Management at Cloud Data Centres in the Age of {5G}",
  author   = "Lu, Y and Liu, L and Panneerselvam, J and Yuan, B and Gu, J and
              Antonopoulos, N",
  abstract = "The increasing deployments of 5G mobile communication system is
              expected to bring more processing power and storage supplements
              to Internet of Things (IoT) and mobile devices. It is foreseeable
              the billions of devices will be connected and it is extremely
              likely that these devices receive compute supplements from Clouds
              and upload data to the back-end datacentres for execution.
              Increasing number of workloads at the Cloud datacentres demand
              better and efficient strategies of resource management in such a
              way to boost the socio-economic benefits of the service
              providers. To this end, this paper proposes an intelligent
              prediction framework named IGRU-SD (Improved Gated Recurrent Unit
              with Stragglers Detection) based on state-of-art data analytics
              and Artificial Intelligence (AI) techniques, aimed at predicting
              the anticipated level of resource requests over a period of time
              into the future. Our proposed prediction framework exploits an
              improved GRU neural network integrated with a resource straggler
              detection module to classify tasks based on their resource
              intensity, and further predicts the expected level of resource
              requests. Performance evaluations conducted on real-world Cloud
              trace logs demonstrate that the proposed IGRU-SD prediction
              framework outperforms the existing predicting models based on
              ARIMA, RNN and LSTM in terms of the achieved prediction accuracy.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  2,
  pages    = "486--498",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/L/Lu et al. 2020 - A GRU-Based Prediction Framework for Intelligent Resource Management at Cloud Data Centres in the Age of 5G.pdf",
  keywords = "5G mobile communication;artificial intelligence;cloud
              computing;computer centres;data analysis;Internet of
              Things;mobile computing;program diagnostics;recurrent neural
              nets;resource allocation;socio-economic effects;system
              monitoring;GRU-based prediction framework;intelligent resource
              management;cloud data centres;5G mobile communication
              system;mobile devices;back-end datacentres;Cloud
              datacentres;socio-economic benefits;intelligent prediction
              framework;data analytics;artificial intelligence;improved GRU
              neural network;resource straggler detection module;cloud trace
              logs;IGRU-SD prediction framework;Improved Gated Recurrent Unit
              with Stragglers Detection;Internet of Things;Cloud computing;Task
              analysis;Predictive models;Resource management;Internet of
              Things;Data centers;5G mobile communication;5G;Internet of
              Things;resource management;cloud
              computing;MLNetworking;Datacentre;EdgeFogCloudIoT",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2019.2954388"
}

@ARTICLE{Li2020-zw,
  title    = "Deep Reinforcement {Learning-Based} {Mobility-Aware} Robust
              Proactive Resource Allocation in Heterogeneous Networks",
  author   = "Li, J and Zhang, X and Zhang, J and Wu, J and Sun, Q and Xie, Y",
  abstract = "Proactive resource allocation (PRA) is an essential technology
              boosting intelligent communication, as it can make full use of
              prediction and significantly improve network performance.
              However, most promising gains base on perfect prediction which is
              unrealistic. How to make PRA robust against prediction
              uncertainty and maximize benefits brought by prediction becomes
              an important issue. In this paper, we tackle this problem and
              propose a mobility-aware robust PRA approach (MRPRA) in
              heterogeneous networks. MRPRA pre-allocates resources in both
              time and frequency domains among mobile users with users'
              trajectories predicted by hidden Markov model. The objective is
              to minimize service delay under constraints of different levels
              of quality-of-service (QoS) requirement and mobility intensity.
              MRPRA is robust against prediction uncertainty by exploiting
              probabilistic constraint programming to model QoS requirements in
              a probabilistic sense. To this end, the probabilistic
              distribution of achievable rate is derived. To flexibly
              coordinate resource allocation among multiple mobile users over
              time horizon, a deep reinforcement learning based multi-actor
              deep deterministic policy gradient algorithm is designed. It
              learns robust PRA policies by distributed acting and centralized
              criticizing. Extensive numerical simulations are performed to
              analyze performances of the proposed approach.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  1,
  pages    = "408--421",
  month    =  mar,
  year     =  2020,
  file     = "All Papers/L/Li et al. 2020 - Deep Reinforcement Learning-Based Mobility-Aware Robust Proactive Resource Allocation in Heterogeneous Networks.pdf",
  keywords = "gradient methods;hidden Markov models;learning (artificial
              intelligence);mobility management (mobile
              radio);optimisation;quality of service;resource
              allocation;statistical distributions;telecommunication
              computing;robust PRA policies;multiactor deep deterministic
              policy gradient algorithm;multiple mobile users;mobility
              intensity;MRPRA pre-allocates resources;mobility-aware robust PRA
              approach;prediction uncertainty;PRA robust;perfect
              prediction;promising gains base;network performance;heterogeneous
              networks;deep reinforcement learning-based mobility-aware robust
              proactive resource allocation;Resource management;Hidden Markov
              models;Uncertainty;Quality of service;Probabilistic
              logic;Predictive models;Optimization;Heterogeneous
              networks;proactive resource allocation;mobility prediction;deep
              reinforcement learning;robustness;Wireless",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2019.2954396"
}

@ARTICLE{Huang2020-fk,
  title    = "Achieving Fair {LTE/Wi-Fi} Coexistence with {Real-Time}
              Scheduling",
  author   = "Huang, Y and Chen, Y and Hou, Y T and Lou, W",
  abstract = "Carrier-Sensing Adaptive Transmission (CSAT) is a promising
              approach to address coexistence between LTE and Wi-Fi in
              unlicensed bands. Under CSAT, a key problem is the design of a
              scheduling algorithm to allocate radio resources (across multiple
              channels and a large number of sub-channels) for LTE and Wi-Fi
              users. This paper investigates this scheduling problem through an
              optimization formulation with the objective of minimizing LTE's
              adverse impact on Wi-Fi users. Special considerations of each LTE
              user's uplink/downlink rate requirements and channel conditions
              are given in this optimization formulation. We show that this
              scheduling problem is NP-hard and propose to develop a
              near-optimal solution. A major challenge here is to ensure the
              scheduler can obtain a solution on 1 ms time scale - a stringent
              timing requirement to meet LTE standard. Our main contribution is
              the development of CURT, a scheduling algorithm that can obtain
              near-optimal solution in 1 ms under standard LTE small cell
              scenarios. CURT exploits the unique structure of the underlying
              optimization problem and decomposes it into a large number of
              independent sub-problems. By taking advantage of GPU's parallel
              processors, we allow the large number of sub-problems to be run
              in parallel and independently from each other. By implementing
              CURT on Nvidia GPU/CUDA platform, we demonstrate that CURT can
              deliver near-optimal scheduling solution in 1 ms for LTE small
              cells with no more than 20 users following 3GPP's evaluation
              methodology.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  1,
  pages    = "366--380",
  month    =  mar,
  year     =  2020,
  file     = "All Papers/H/Huang et al. 2020 - Achieving Fair LTE - Wi-Fi Coexistence with Real-Time Scheduling.pdf",
  keywords = "carrier sense multiple access;graphics processing units;Long Term
              Evolution;optimisation;parallel architectures;telecommunication
              scheduling;wireless LAN;Nvidia GPU/CUDA platform;near-optimal
              scheduling solution;independent sub-problems;optimization
              problem;standard LTE small cell scenarios;CURT;LTE
              standard;stringent timing requirement;LTE user;optimization
              formulation;scheduling problem;Wi-Fi users;scheduling
              algorithm;unlicensed bands;CSAT;Carrier-Sensing Adaptive
              Transmission;real-time scheduling;time 1.0 ms;Long Term
              Evolution;Wireless fidelity;Real-time systems;Optimal
              scheduling;Scheduling;Processor
              scheduling;Coexistence;LTE;Wi-Fi;unlicensed
              spectrum;scheduling;optimization;real-time;GPU;Wireless",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2019.2957076"
}

@ARTICLE{Zhou2020-cv,
  title    = "{Human-Behavior} and {QoE-Aware} Dynamic Channel Allocation for
              {5G} Networks: A Latent Contextual Bandit Learning Approach",
  author   = "Zhou, P and Xu, J and Wang, W and Jiang, C and Wang, K and Hu, J",
  abstract = "With the rapid advance of smart wireless technologies, a plethora
              of human behavioral data are generated in 5G networks, which is
              reported capable to improve network performance by leveraging
              intelligent channel resource allocation through big data
              analytics. However, what information can be extracted for the
              network mobility management, how to exploit the knowledge for
              resource allocation and to meet the user-centric quality of
              experience (QoE) are not well understood and fully explored. To
              address this problem, we propose an online learning algorithm for
              dynamic channel allocation based on contextual multi-armed bandit
              (CMAB) theory. Especially, we divide the stochastic human
              behavioral data into two categories: the user location and the
              QoE-driven context. Noticing that the distributions of CSI vary
              spatially, we define a set of user's geographic locations that
              shares the same set of CSI distributions as a cluster, and the
              stochastic channel distributions vary across clusters. The
              problem is formulated as a novel latent SCB problem, where the
              proposed agnostic SCB algorithm could automatically find the
              underlying clusters and significantly improve the learning
              performance. We then extend our online learning algorithm into
              the practical multi-user random access scenario. We conduct
              experiments on a real dataset collected from China Mobile, which
              indicate that our algorithms outperform existing approaches
              tremendously and perform extremely well in large-scale and
              high-mobility networks.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  2,
  pages    = "436--451",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/Z/Zhou et al. 2020 - Human-Behavior and QoE-Aware Dynamic Channel Allocation for 5G Networks - A Latent Contextual Bandit Learning Approach.pdf",
  keywords = "5G mobile communication;Big Data;channel allocation;consumer
              behaviour;learning (artificial intelligence);multi-access
              systems;quality of experience;resource allocation;stochastic
              processes;telecommunication computing;CSI
              distributions;stochastic channel distributions;agnostic SCB
              algorithm;online learning algorithm;high-mobility networks;human
              behavior;QoE-aware dynamic channel allocation;smart wireless
              technologies;intelligent channel resource allocation;big data
              analytics;network mobility management;user-centric
              quality;stochastic human behavioral data;user location;QoE-driven
              context;multiuser random access scenario;latent SCB
              problem;contextual multiarmed bandit theory;CMAB theory;latent
              contextual bandit learning approach;China Mobile;quality of
              experience;user-centric QoE;Quality of experience;5G mobile
              communication;Wireless communication;Channel
              allocation;Clustering algorithms;Resource management;Stochastic
              processes;Human behavior;QoE;5G;contextual bandits;channel
              allocation;user mobility;online learning;MLNetworking;5G6G",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.2969631"
}

@ARTICLE{Hall2020-qs,
  title    = "Dynamic Scheduler Management Using Deep Learning",
  author   = "Hall, J and Moessner, K and MacKenzie, R and Carrez, F and Foh, C
              H",
  abstract = "The ability to manage the distributed functionality of large
              multi-vendor networks will be an important step towards
              ultra-dense 5G networks. Managing distributed scheduling
              functionality is particularly important, due to its influence
              over inter-cell interference and the lack of standardization for
              schedulers. In this paper, we formulate a method of managing
              distributed scheduling methods across a small cluster of cells by
              dynamically selecting schedulers to be implemented at each cell.
              We use deep reinforcement learning methods to identify suitable
              joint scheduling policies, based on the current state of the
              network observed from data already available in the RAN.
              Additionally, we also explore three methods of training the deep
              reinforcement learning based dynamic scheduler selection system.
              We compare the performance of these training methods in a
              simulated environment against each other, as well as homogeneous
              scheduler deployment scenarios, where each cell in the network
              uses the same type of scheduler. We show that, by using deep
              reinforcement learning, the dynamic scheduler selection system is
              able to identify scheduler distributions that increase the number
              of users that achieve their quality of service requirements in up
              to 77\% of the simulated scenarios when compared to homogeneous
              scheduler deployment scenarios.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  2,
  pages    = "575--585",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/H/Hall et al. 2020 - Dynamic Scheduler Management Using Deep Learning.pdf",
  keywords = "5G mobile communication;dynamic scheduling;learning (artificial
              intelligence);neural nets;quality of service;telecommunication
              computing;telecommunication network management;telecommunication
              scheduling;dynamic scheduler management;multivendor
              networks;ultra-dense 5G networks;distributed scheduling
              functionality;deep reinforcement learning;dynamic scheduler
              selection system;homogeneous scheduler deployment scenarios;joint
              scheduling policies;RAN;quality of service
              requirements;Reinforcement learning;Quality of service;5G mobile
              communication;Dynamic scheduling;Job shop scheduling;Intercell
              interference;Reinforcement learning;deep
              learning;scheduling;Wireless;MLNetworking",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.2980529"
}

@ARTICLE{Zhang2020-tb,
  title    = "{Fine-Grained} Management in 5G: {DQL} Based Intelligent Resource
              Allocation for Network Function Virtualization in {C-RAN}",
  author   = "Zhang, C and Dong, M and Ota, K",
  abstract = "Recently, the installation of 5G networks offers a variety of
              real-time, high-performance and human-oriented customized
              services. However, the current laying 5G structure is unable to
              meet all of the growing communication needs by these new emerging
              services. In this paper, we propose a DQL (Deep Q-learning
              Network) based intelligent resource management method for 5G
              architecture, to improve the quality of service (QoS) under
              limited communication resources. In the environment of network
              function virtualization (NFV), we aim at improving the efficient
              usage of spectrum resources. In this two-step solution, our first
              goal is to guarantee the maximum communication quality with the
              smallest number of infrastructures. Then, a DQL-based wireless
              resource allocation algorithm is designed to realize the
              elaborate operation. Unlike previous studies, our system can
              provide the allocation policy in a more subdivided way and
              finally maximize the usage of bandwidth resources. The simulation
              also shows that our proposed MSIO improves 3.12\% in the
              performance of the maximum coverage importance problem and the
              ARODQ algorithm improves 4.05\% than other standard solutions.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  2,
  pages    = "428--435",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/Z/Zhang et al. 2020 - Fine-Grained Management in 5G - DQL Based Intelligent Resource Allocation for Network Function Virtualization in C-RAN.pdf",
  keywords = "5G mobile communication;quality of service;resource
              allocation;telecommunication computing;telecommunication network
              management;intelligent resource allocation;network function
              virtualization;deep Q-learning network;communication
              resources;spectrum resources;maximum communication
              quality;allocation policy;bandwidth resources;DQL-based wireless
              resource allocation;5G structure;intelligent resource
              management;5G fine-grained management;DQL based intelligent
              resource allocation;5G architecture;quality of service;QoS;ARODQ
              algorithm;C-RAN;5G mobile communication;Resource
              management;Wireless communication;Computer architecture;Cloud
              computing;Optimization;Quality of service;Intelligent
              control;intelligent networks;artificial intelligence;management
              decision-making;mobile communication;Wireless",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.2982886"
}

@ARTICLE{Bunyakitanon2020-nu,
  title    = "{End-to-End} {Performance-Based} Autonomous {VNF} Placement With
              Adopted Reinforcement Learning",
  author   = "Bunyakitanon, M and Vasilakos, X and Nejabati, R and Simeonidou,
              D",
  abstract = "The autonomous placement of Virtual Network Functions (VNFs) is a
              key aspect of Zero-touch network and Service Management (ZSM) in
              Fifth Generation (5G) networking. Therefore, current
              orchestration frameworks need to be enhanced, accordingly. To
              address this need, this work presents an Adapted REinforcement
              Learning VNF Performance Prediction module for Autonomous VNF
              Placement, namely AREL3P. Our solution design bears a dual
              novelty. First, it leverages end-to-end service-level performance
              predictions for placing VNFs. Second, whereas the majority of
              other Machine Learning efforts in the literature use Supervised
              Learning (SL) techniques, AREL3P is based on a particular form of
              Reinforcement Learning adapted to predictions. This makes
              placement decisions more resilient to dynamic conditions, as well
              as portable to other network nodes, and able to generalize in
              heterogeneous network environments. Backed by a meticulous
              performance evaluation over a real 5G end-to-end testbed, we
              verify the above properties after integrating AREL3P to Open
              Source Management and Orchestration (OSM MANO) decisions. Among
              other highlights, we show increased VNF performance predictions
              accuracy by 40-45\%, and an overall improved VNF placement
              efficiency against other SL benchmarks reflected by near-optimal
              decision scores in 23 out of a total of 27 investigated
              scenarios.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  2,
  pages    = "534--547",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/B/Bunyakitanon et al. 2020 - End-to-End Performance-Based Autonomous VNF Placement With Adopted Reinforcement Learning.pdf",
  keywords = "5G mobile communication;learning (artificial
              intelligence);quality of service;telecommunication network
              management;virtualisation;end-to-end performance-based autonomous
              VNF placement;Zero-touch network and Service Management;Adapted
              REinforcement Learning VNF Performance Prediciton
              module;end-to-end service-level performance predictions;Open
              Source Management and Orchestration decisions;5G end-to-end
              testbed;improved VNF placement efficiency;meticulous performance
              evaluation;heterogeneous network environments;network
              nodes;placement decisions;Supervised Learning
              techniques;AREL3P;VNF Performance Prediction module;current
              orchestration frameworks;Fifth Generation networking;Virtual
              Network Functions;autonomous placement;adopted reinforcement
              learning;Adaptation models;5G mobile communication;Learning
              (artificial intelligence);Predictive models;Monitoring;Benchmark
              testing;Training;Machine learning;network function
              virtualization;end-to-end communication;zero-touch
              management;cloud and edge computing;NFV;MLNetworking",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.2988486"
}

@ARTICLE{Li2021-ms,
  title    = "{Delay-Aware} {VNF} Scheduling: A Reinforcement Learning Approach
              With Variable Action Set",
  author   = "Li, Junling and Shi, Weisen and Zhang, Ning and Shen, Xuemin",
  abstract = "Software defined networking (SDN) and network function
              virtualization (NFV) are the key enabling technologies for
              service customization in next generation networks to support
              various applications. In such a circumstance, virtual network
              function (VNF) scheduling plays an essential role in enhancing
              resource utilization and achieving better quality-of-service
              (QoS). In this paper, the VNF scheduling problem is investigated
              to minimize the makespan (i.e., overall completion time) of all
              services, while satisfying their different end-to-end (E2E) delay
              requirements. The problem is formulated as a mixed integer linear
              program (MILP) which is NP-hard with exponentially increasing
              computational complexity as the network size expands. To solve
              the MILP with high efficiency and accuracy, the original problem
              is reformulated as a Markov decision process (MDP) problem with
              variable action set. Then, a reinforcement learning (RL)
              algorithm is developed to learn the best scheduling policy by
              continuously interacting with the network environment. The
              proposed learning algorithm determines the variable action set at
              each decision-making state and captures different execution time
              of the actions. The reward function in the proposed algorithm is
              carefully designed to realize delay-aware VNF scheduling.
              Simulation results are presented to demonstrate the convergence
              and high accuracy of the proposed approach against other
              benchmark algorithms.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  7,
  number   =  1,
  pages    = "304--318",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/L/Li et al. 2021 - Delay-Aware VNF Scheduling - A Reinforcement Learning Approach With Variable Action Set.pdf",
  keywords = "Delays;Optimal scheduling;Scheduling;Heuristic
              algorithms;Resource management;Quality of service;Processor
              scheduling;Delay-aware VNF scheduling;SDN;NFV;resource
              allocation;reinforcement learning",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.2988908"
}

@ARTICLE{ElMossallamy2020-mw,
  title    = "Reconfigurable Intelligent Surfaces for Wireless Communications:
              Principles, Challenges, and Opportunities",
  author   = "ElMossallamy, M A and Zhang, H and Song, L and Seddik, K G and
              Han, Z and Li, G Y",
  abstract = "Recently there has been a flurry of research on the use of
              reconfigurable intelligent surfaces (RIS) in wireless networks to
              create smart radio environments. In a smart radio environment,
              surfaces are capable of manipulating the propagation of incident
              electromagnetic waves in a programmable manner to actively alter
              the channel realization, which turns the wireless channel into a
              controllable system block that can be optimized to improve
              overall system performance. In this article, we provide a
              tutorial overview of reconfigurable intelligent surfaces (RIS)
              for wireless communications. We describe the working principles
              of reconfigurable intelligent surfaces (RIS) and elaborate on
              different candidate implementations using metasurfaces and
              reflectarrays. We discuss the channel models suitable for both
              implementations and examine the feasibility of obtaining accurate
              channel estimates. Furthermore, we discuss the aspects that
              differentiate RIS optimization from precoding for traditional
              MIMO arrays highlighting both the arising challenges and the
              potential opportunities associated with this emerging technology.
              Finally, we present numerical results to illustrate the power of
              an RIS in shaping the key properties of a MIMO channel.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  3,
  pages    = "990--1002",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/E/ElMossallamy et al. 2020 - Reconfigurable Intelligent Surfaces for Wireless Communications - Principles, Challenges, and Opportunities.pdf",
  keywords = "antenna arrays;channel estimation;MIMO communication;wireless
              channels;wireless channel;smart radio environment;wireless
              networks;RIS;wireless communications;reconfigurable intelligent
              surfaces;Wireless communication;Millimeter wave
              technology;Surface waves;MIMO
              communication;Multiplexing;Millimeter wave propagation;Beyond
              5G;intelligent reflecting surfaces;large intelligent
              surfaces;passive beamforming;Reconfigurable intelligent
              surfaces;smart reflect-arrays;Wireless;Mobile\_Wireless",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.2992604"
}

@ARTICLE{Lei2020-yd,
  title    = "Deep Reinforcement {Learning-Based} Spectrum Allocation in
              Integrated Access and Backhaul Networks",
  author   = "Lei, W and Ye, Y and Xiao, M",
  abstract = "We develop a framework based on deep reinforcement learning (DRL)
              to solve the spectrum allocation problem in the emerging
              integrated access and backhaul (IAB) architecture with large
              scale deployment and dynamic environment. The available spectrum
              is divided into several orthogonal sub-channels, and the donor
              base station (DBS) and all IAB nodes have the same spectrum
              resource for allocation, where a DBS utilizes those sub-channels
              for access links of associated user equipment (UE) as well as for
              backhaul links of associated IAB nodes, and an IAB node can
              utilize all for its associated UEs. This is one of key features
              in which 5G differs from traditional settings where the backhaul
              networks are designed independently from the access networks.
              With the goal of maximizing the sum log-rate of all UE groups, we
              formulate the spectrum allocation problem into a mix-integer and
              non-linear programming. However, it is intractable to find an
              optimal solution especially when the IAB network is large and
              time-varying. To tackle this problem, we propose to use the
              latest DRL method by integrating an actor-critic spectrum
              allocation (ACSA) scheme and deep neural network (DNN) to achieve
              real-time spectrum allocation in different scenarios. The
              proposed methods are evaluated through numerical simulations and
              show promising results compared with some baseline allocation
              policies.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  3,
  pages    = "970--979",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/L/Lei et al. 2020 - Deep Reinforcement Learning-Based Spectrum Allocation in Integrated Access and Backhaul Networks.pdf",
  keywords = "cognitive radio;integer programming;learning (artificial
              intelligence);linear programming;neural nets;next generation
              networks;radio spectrum management;radiofrequency
              interference;resource allocation;telecommunication
              scheduling;deep reinforcement learning;backhaul networks;spectrum
              allocation problem;emerging integrated access;backhaul
              architecture;orthogonal sub-channels;donor base station;DBS;IAB
              node;spectrum resource;access links;associated user
              equipment;backhaul links;associated IAB nodes;associated
              UEs;access networks;UE groups;IAB network;actor-critic spectrum
              allocation scheme;deep neural network;real-time spectrum
              allocation;baseline allocation policies;Resource
              management;Satellite broadcasting;Reinforcement learning;Base
              stations;Wireless communication;Interference;Receivers;Integrated
              access and backhaul;spectrum allocation;deep reinforcement
              learning;MLNetworking",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.2992628"
}

@ARTICLE{Wu2020-kq,
  title    = "{IEEE} {TCCN} Special Section Editorial: Intelligent Resource
              Management for {5G} and Beyond",
  author   = "Wu, Y and Simeonidou, D and Wang, C and Yu, F R and Choi, S and
              Xue, G and Ksentini, A",
  abstract = "Learning from massive network data to produce cognitive knowledge
              for efficient resource management in 5G and beyond 5G (B5G) is
              still challenging. We are delighted to introduce the readers to
              this special section of the IEEE Transactions on Cognitive
              Communications and Networking (TCCN), which aims at exploring
              recent advances and addressing practical challenges in the
              intelligent resource management in 5G/B5G. We have received a
              total number of 30 submissions, and after a rigorous review
              process, 15 articles have been selected for publication, which
              are briefly discussed as follows.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  2,
  pages    = "422--427",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/W/Wu et al. 2020 - IEEE TCCN Special Section Editorial - Intelligent Resource Management for 5G and Beyond.pdf",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.2993976"
}

@ARTICLE{Liu2021-ao,
  title    = "{Data-Importance} Aware User Scheduling for
              {Communication-Efficient} Edge Machine Learning",
  author   = "Liu, Dongzhu and Zhu, Guangxu and Zhang, Jun and Huang, Kaibin",
  abstract = "With the prevalence of intelligent mobile applications, edge
              learning is emerging as a promising technology for powering fast
              intelligence acquisition for edge devices from distributed data
              generated at the network edge. One critical task of edge learning
              is to efficiently utilize the limited radio resource to acquire
              data samples for model training at an edge server. In this paper,
              we develop a novel user scheduling algorithm for data acquisition
              in edge learning, called (data) importance-aware scheduling. A
              key feature of this scheduling algorithm is that it takes into
              account the informativeness of data samples, besides
              communication reliability. Specifically, the scheduling decision
              is based on a data importance indicator (DII), elegantly
              incorporating two ``important'' metrics from communication and
              learning perspectives, i.e., the signal-to-noise ratio (SNR) and
              data uncertainty. We first derive an explicit expression for this
              indicator targeting the classic classifier of support vector
              machine (SVM), where the uncertainty of a data sample is measured
              by its distance to the decision boundary. Then, the result is
              extended to convolutional neural networks (CNN) by replacing the
              distance based uncertainty measure with the entropy. As
              demonstrated via experiments using real datasets, the proposed
              importance-aware scheduling can exploit the two-fold multi-user
              diversity, namely the diversity in both the multiuser channels
              and the distributed data samples. This leads to faster model
              convergence than the conventional scheduling schemes that exploit
              only a single type of diversity.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  7,
  number   =  1,
  pages    = "265--278",
  month    =  mar,
  year     =  2021,
  annote   = "High relevance for NICCI!",
  file     = "All Papers/L/Liu et al. 2021 - Data-Importance Aware User Scheduling for Communication-Efficient Edge Machine Learning.pdf",
  keywords = "Data models;Scheduling;Computational modeling;Wireless
              communication;Training;Uncertainty;Servers;Scheduling;resource
              management;image classification;multiuser channels;data
              acquisition",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.2999606"
}

@ARTICLE{Wu2020-lj,
  title    = "Collaborative Learning of Communication Routes in {Edge-Enabled}
              {Multi-Access} Vehicular Environment",
  author   = "Wu, C and Liu, Z and Liu, F and Yoshinaga, T and Ji, Y and Li, J",
  abstract = "Some Internet-of-Things (IoT) applications have a strict
              requirement on the end-to-end delay where edge computing can be
              used to provide a short delay for end-users by conducing
              efficient caching and computing at the edge nodes. However, a
              fast and efficient communication route creation in multi-access
              vehicular environment is an underexplored research problem. In
              this paper, we propose a collaborative learning-based routing
              scheme for multi-access vehicular edge computing environment. The
              proposed scheme employs a reinforcement learning algorithm based
              on end-edge-cloud collaboration to find routes in a proactive
              manner with a low communication overhead. The routes are also
              preemptively changed based on the learned information. By
              integrating the ``proactive'' and ``preemptive'' approach, the
              proposed scheme can achieve a better forwarding of packets as
              compared with existing alternatives. We conduct extensive and
              realistic computer simulations to show the performance advantage
              of the proposed scheme over existing baselines.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  4,
  pages    = "1155--1165",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/W/Wu et al. 2020 - Collaborative Learning of Communication Routes in Edge-Enabled Multi-Access Vehicular Environment.pdf",
  keywords = "Quality of service;Edge computing;Resource management;Task
              analysis;Delays;Routing;Reinforcement learning;Vehicular
              networks;routing protocol;collaborative learning;multi-access
              vehicular environment;fuzzy logic;reinforcement
              learning;MLNetworking;EdgeFogCloudIoT",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.3002253"
}

@ARTICLE{Li2020-lh,
  title    = "Deep Reinforcement Learning for Collaborative Edge Computing in
              Vehicular Networks",
  author   = "Li, M and Gao, J and Zhao, L and Shen, X",
  abstract = "Mobile edge computing (MEC) is a promising technology to support
              mission-critical vehicular applications, such as intelligent path
              planning and safety applications. In this paper, a collaborative
              edge computing framework is developed to reduce the computing
              service latency and improve service reliability for vehicular
              networks. First, a task partition and scheduling algorithm (TPSA)
              is proposed to decide the workload allocation and schedule the
              execution order of the tasks offloaded to the edge servers given
              a computation offloading strategy. Second, an artificial
              intelligence (AI) based collaborative computing approach is
              developed to determine the task offloading, computing, and result
              delivery policy for vehicles. Specifically, the offloading and
              computing problem is formulated as a Markov decision process. A
              deep reinforcement learning technique, i.e., deep deterministic
              policy gradient, is adopted to find the optimal solution in a
              complex urban transportation network. By our approach, the
              service cost, which includes computing service latency and
              service failure penalty, can be minimized via the optimal
              workload assignment and server selection in collaborative
              computing. Simulation results show that the proposed AI-based
              collaborative computing approach can adapt to a highly dynamic
              environment with outstanding performance.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  4,
  pages    = "1122--1135",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/L/Li et al. 2020 - Deep Reinforcement Learning for Collaborative Edge Computing in Vehicular Networks.pdf",
  keywords = "Task analysis;Servers;Collaboration;Reliability;Processor
              scheduling;Edge computing;Computational modeling;Mobile edge
              computing;Internet of Vehicles;task scheduling;deep deterministic
              policy gradient;MLNetworking;EdgeFogCloudIoT",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.3003036"
}

@ARTICLE{Ghoorchian2021-wd,
  title    = "{Multi-Armed} Bandit for {Energy-Efficient} and {Delay-Sensitive}
              Edge Computing in Dynamic Networks With Uncertainty",
  author   = "Ghoorchian, Saeed and Maghsudi, Setareh",
  abstract = "In the edge computing paradigm, mobile devices offload the
              computational tasks to an edge server by routing the required
              data over the wireless network. The full potential of edge
              computing becomes realized only if a smart device selects the
              most appropriate server in terms of the latency and energy
              consumption, among many available ones. The server selection
              problem is challenging due to the randomness of the environment
              and lack of prior information about the same. Therefore, a smart
              device, which sequentially chooses a server under uncertainty,
              aims to improve its decision based on the historical time and
              energy consumption. The problem becomes more complicated in a
              dynamic environment, where key variables might undergo abrupt
              changes. To deal with the aforementioned problem, we first
              analyze the required time and energy to data transmission and
              processing. We then use the analysis to cast the problem as a
              budget-limited multi-armed bandit problem, where each arm is
              associated with a reward and cost, with time-variant statistical
              characteristics. We propose a policy to solve the formulated
              problem and prove a regret bound. The numerical results
              demonstrate the superiority of the proposed method compared to
              several online learning algorithms.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  7,
  number   =  1,
  pages    = "279--293",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/G/Ghoorchian and Maghsudi 2021 - Multi-Armed Bandit for Energy-Efficient and Delay-Sensitive Edge Computing in Dynamic Networks With Uncertainty.pdf",
  keywords = "Servers;Task analysis;Edge computing;Computational
              modeling;Energy consumption;Wireless networks;Delays;Computation
              offloading;edge computing;non-stationary
              decision-making;multi-armed bandits;uncertainty;Mobile\_Wireless",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.3012445"
}

@ARTICLE{Qu2020-mq,
  title    = "Dynamic Resource Scaling for {VNF} over Nonstationary Traffic: A
              Learning Approach",
  author   = "Qu, K and Zhuang, W and Shen, X and Li, X and Rao, J",
  abstract = "Software defined networking (SDN) and network function
              virtualization (NFV) are key enablers for service-level
              customized network slicing in fifth generation (5G) core
              networks. Network slices are required to be isolated from each
              other in terms of service performance with traffic load
              fluctuations. In this paper, the virtual network function (VNF)
              scalability issue is studied to meet the quality-of-service (QoS)
              requirement in the presence of nonstationary traffic, through
              joint VNF migration and resource scaling. A traffic parameter
              learning method based on change point detection and Gaussian
              process regression (GPR) is proposed, to learn traffic parameters
              in a fractional Brownian motion (fBm) traffic model for each
              stationary traffic segment within a nonstationary traffic trace.
              Then, the time-varying VNF resource demand is predicted from the
              learned traffic parameters based on an fBm resource provisioning
              model. With the detected change points and predicted resource
              demands, a VNF migration problem is formulated as a Markov
              decision process (MDP) with variable-length decision epochs, to
              maximize the long-term reward integrating load balancing,
              migration cost, and resource overloading penalty. A penalty-aware
              deep Q-learning algorithm is proposed to incorporate awareness of
              resource overloading penalty, with improved performance over
              benchmarks in terms of training loss reduction and cumulative
              reward maximization.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  pages    = "1--1",
  year     =  2020,
  file     = "All Papers/Q/Qu et al. 2020 - Dynamic Resource Scaling for VNF over Nonstationary Traffic - A Learning Approach.pdf",
  keywords = "Delays;Quality of service;Time series analysis;Network function
              virtualization;Gaussian processes;Predictive models;Prediction
              algorithms;Virtual network function (VNF);resource
              scaling;migration;nonstationary traffic;change point
              detection;traffic parameter learning;Gaussian process
              regression;resource demand prediction;reinforcement
              learning.;NetworkTraffic",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.3018157"
}

@ARTICLE{Galanopoulos2020-yg,
  title    = "Cooperative Edge Computing of Data Analytics for the Internet of
              Things",
  author   = "Galanopoulos, A and Salonidis, T and Iosifidis, G",
  abstract = "Internet of Things (IoT) networks are increasingly used for edge
              data analytics, i.e., collecting and analyzing data at the
              network edge. However, the IoT devices are typically
              resource-constrained and cannot support fast and accurate
              execution of such tasks, while the involvement of distant cloud
              servers is often impractical and entails huge communication
              overheads. To address this problem, we develop a framework for
              enabling the devices to collaboratively execute their tasks,
              exploiting their proximity and resource complementarity. Our
              mechanism relies on an auction-based algorithm that optimizes the
              execution accuracy and delay for all tasks, without requiring
              information about the performance priorities of nodes. The
              algorithm yields the optimal task -- node assignment, and the
              necessary reimbursements for ensuring the devices' cooperation,
              by using the auction for dual subgradient evaluations. We further
              extend this mechanism for multi-stage analytics and explain how
              it can be implemented in a decentralized fashion, namely without
              an auctioneer. We conduct a battery of testbed experiments with
              representative data analytic applications that show gains both in
              accuracy and delay compared to heuristic and greedy policies, and
              verify the minimal computation and communication overheads of our
              solution.",
  journal  = "IEEE Transactions on Cognitive Communications and Networking",
  volume   =  6,
  number   =  4,
  pages    = "1166--1179",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/G/Galanopoulos et al. 2020 - Cooperative Edge Computing of Data Analytics for the Internet of Things.pdf",
  keywords = "Task analysis;Delays;Data analysis;Wireless communication;Face
              recognition;Bandwidth;Computational modeling;Auctions;incentive
              mechanisms;network optimization;resource allocation;data
              analytics;MLNetworking;EdgeFogCloudIoT",
  issn     = "2332-7731",
  doi      = "10.1109/TCCN.2020.3019610"
}

@ARTICLE{Kleinrock1975-iq,
  title    = "Packet Switching in Radio Channels: Part {I} - Carrier Sense
              {Multiple-Access} Modes and Their {Throughput-Delay}
              Characteristics",
  author   = "Kleinrock, L and Tobagi, F",
  abstract = "Radio communication is considered as a method for providing
              remote terminal access to computers. Digital byte streams from
              each terminal are partitioned into packets (blocks) and
              transmitted in a burst mode over a shared radio channel. When
              many terminals operate in this fashion, transmissions may
              conflict with and destroy each other. A means for controlling
              this is for the terminal to sense the presence of other
              transmissions; this leads to a new method for multiplexing in a
              packet radio environment: carrier sense multiple access (CSMA).
              Two protocols are described for CSMA and their throughput-delay
              characteristics are given. These results show the large advantage
              CSMA provides as compared to the random ALOHA access modes.",
  journal  = "IEEE Trans. Commun.",
  volume   =  23,
  number   =  12,
  pages    = "1400--1416",
  month    =  dec,
  year     =  1975,
  keywords = "Packet switching;Computer networks;Costs;Radio
              broadcasting;Multiaccess communication;Wire;Radio
              communication;Packet radio
              networks;Telephony;Bandwidth;ComputerNetworks",
  issn     = "1558-0857",
  doi      = "10.1109/TCOM.1975.1092768"
}

@ARTICLE{Bharath2016-ka,
  title    = "A {Learning-Based} Approach to Caching in Heterogenous Small Cell
              Networks",
  author   = "Bharath, B N and Nagananda, K G and Poor, H V",
  abstract = "A heterogenous network with base stations (BSs), small base
              stations (SBSs), and users distributed according to independent
              Poisson point processes is considered. SBS nodes are assumed to
              possess high storage capacity and to form a distributed caching
              network. Popular files are stored in local caches of SBSs, so
              that a user can download the desired files from one of the SBSs
              in its vicinity. The offloading-loss is captured via a cost
              function that depends on the random caching strategy proposed
              here. The popularity profile of cached content is unknown and
              estimated using instantaneous demands from users within a
              specified time interval. An estimate of the cost function is
              obtained from which an optimal random caching strategy is
              devised. The training time to achieve an $\in$ > 0 difference
              between the achieved and optimal costs is finite provided the
              user density is greater than a predefined threshold, and scales
              as N2, where N is the support of the popularity profile. A
              transfer learning-based approach to improve this estimate is
              proposed. The training time is reduced when the popularity
              profile is modeled using a parametric family of distributions;
              the delay is independent of N and scales linearly with the
              dimension of the distribution parameter.",
  journal  = "IEEE Trans. Commun.",
  volume   =  64,
  number   =  4,
  pages    = "1674--1686",
  month    =  apr,
  year     =  2016,
  file     = "All Papers/B/Bharath et al. 2016 - A Learning-Based Approach to Caching in Heterogenous Small Cell Networks.pdf",
  keywords = "cellular radio;learning (artificial intelligence);mobile
              computing;stochastic processes;heterogenous small cell
              networks;small base stations;SBS;independent Poisson point
              processes;local caches;offloading-loss;cost function;optimal
              random caching strategy;popularity profile;transfer
              learning-based approach;training time;distribution
              parameter;Training;Wireless communication;Cost function;Computer
              architecture;Streaming media;Knowledge
              engineering;Protocols;Caching;small cell networks;popularity
              profile;Caching;small cell networks;popularity profile;transfer
              learning;MLNetworking;EdgeFogCloudIoT",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2016.2536728"
}

@ARTICLE{Qu2016-yy,
  title    = "{Delay-Aware} Scheduling and Resource Optimization With Network
              Function Virtualization",
  author   = "Qu, L and Assi, C and Shaban, K",
  abstract = "To accelerate the implementation of network functions/middle
              boxes and reduce the deployment cost, recently, the concept of
              network function virtualization (NFV) has emerged and become a
              topic of much interest attracting the attention of researchers
              from both industry and academia. Unlike the traditional
              implementation of network functions, a software-oriented approach
              for virtual network functions (VNFs) creates more flexible and
              dynamic network services to meet a more diversified demand.
              Software-oriented network functions bring along a series of
              research challenges, such as VNF management and orchestration,
              service chaining, VNF scheduling for low latency and efficient
              virtual network resource allocation with NFV infrastructure,
              among others. In this paper, we study the VNF scheduling problem
              and the corresponding resource optimization solutions. Here, the
              VNF scheduling problem is defined as a series of scheduling
              decisions for network services on network functions and
              activating the various VNFs to process the arriving traffic. We
              consider VNF transmission and processing delays and formulate the
              joint problem of VNF scheduling and traffic steering as a mixed
              integer linear program. Our objective is to minimize the
              makespan/latency of the overall VNFs' schedule. Reducing the
              scheduling latency enables cloud operators to service (and admit)
              more customers, and cater to services with stringent delay
              requirements, thereby increasing operators' revenues. Owing to
              the complexity of the problem, we develop a genetic
              algorithm-based method for solving the problem efficiently.
              Finally, the effectiveness of our heuristic algorithm is verified
              through numerical evaluation. We show that dynamically adjusting
              the bandwidths on virtual links connecting virtual machines,
              hosting the network functions, reduces the schedule makespan by
              15\%-20\% in the simulated scenarios.",
  journal  = "IEEE Trans. Commun.",
  volume   =  64,
  number   =  9,
  pages    = "3746--3758",
  month    =  sep,
  year     =  2016,
  file     = "All Papers/Q/Qu et al. 2016 - Delay-Aware Scheduling and Resource Optimization With Network Function Virtualization.pdf",
  keywords = "bandwidth allocation;genetic algorithms;integer
              programming;linear programming;radio links;resource
              allocation;telecommunication network management;telecommunication
              scheduling;telecommunication services;telecommunication
              traffic;virtualisation;delay-aware scheduling;resource
              optimization;network function virtualization;network
              functions/middle boxes;software-oriented network
              functions;virtual network resource allocation;mixed integer
              linear program;genetic algorithm;virtual links;virtual
              machines;Delays;Optimal scheduling;Bandwidth;Job shop
              scheduling;Channel allocation;Network function
              virtualization;scheduling;bandwidth allocation;genetic
              algorithm;NFV",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2016.2580150"
}

@ARTICLE{Ang2020-hd,
  title    = "Robust Federated Learning With Noisy Communication",
  author   = "Ang, Fan and Chen, Li and Zhao, Nan and Chen, Yunfei and Wang,
              Weidong and Yu, F Richard",
  abstract = "Federated learning is a communication-efficient training process
              that alternate between local training at the edge devices and
              averaging of the updated local model at the center server.
              Nevertheless, it is impractical to achieve perfect acquisition of
              the local models in wireless communication due to the noise,
              which also brings serious effect on federated learning. To tackle
              this challenge in this paper, we propose a robust design for
              federated learning to decline the effect of noise. Considering
              the noise in two aforementioned steps, we first formulate the
              training problem as a parallel optimization for each node under
              the expectation-based model and worst-case model. Due to the
              non-convexity of the problem, regularizer approximation method is
              proposed to make it tractable. Regarding the worst-case model, we
              utilize the sampling-based successive convex approximation
              algorithm to develop a feasible training scheme to tackle the
              unavailable maxima or minima noise condition and the non-convex
              issue of the objective function. Furthermore, the convergence
              rates of both new designs are analyzed from a theoretical point
              of view. Finally, the improvement of prediction accuracy and the
              reduction of loss function value are demonstrated via simulation
              for the proposed designs.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  6,
  pages    = "3452--3464",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/A/Ang et al. 2020 - Robust Federated Learning With Noisy Communication.pdf",
  keywords = "Training;Robustness;Computational modeling;Wireless
              communication;Servers;Convergence;Optimization;Expectation-based
              model;federated learning;robust design;worst-case
              model;MLNetworking",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.2979149"
}

@ARTICLE{Yang2020-ni,
  title    = "Learning Automata Based {Q-Learning} for Content Placement in
              Cooperative Caching",
  author   = "Yang, Zhong and Liu, Yuanwei and Chen, Yue and Jiao, Lei",
  abstract = "An optimization problem of content placement in cooperative
              caching is formulated, with the aim of maximizing the sum mean
              opinion score (MOS) of mobile users. Firstly, as user mobility
              and content popularity have significant impacts on the user
              experience, a recurrent neural network (RNN) is invoked for user
              mobility prediction and content popularity prediction. More
              particularly, practical data collected from GPS-tracker app on
              smartphones is tackled to test the accuracy of user mobility
              prediction. Then, based on the predicted mobile users' positions
              and content popularity, a learning automata based Q-learning
              (LAQL) algorithm for cooperative caching is proposed, in which
              learning automata (LA) is invoked for Q-learning to obtain an
              optimal action selection in a random and stationary environment.
              It is proven that the LA based action selection scheme is capable
              of enabling every state to select the optimal action with
              arbitrary high probability if Q-learning is able to converge to
              the optimal Q value eventually. In the LAQL algorithm, a central
              processor acts as the intelligent agent, which allocate contents
              to BSs according to the reward or penalty from the feedback of
              the BSs and users, iteratively. To characterize the performance
              of the proposed LAQL algorithms, sum MOS of users is applied to
              define the reward function. Extensive simulation results reveal
              that: 1) the prediction error of RNNs based algorithm lessen with
              the increase of iterations and nodes; 2) the proposed LAQL
              achieves significant performance improvement against traditional
              Q-learning algorithm; and 3) the cooperative caching scheme is
              capable of outperforming non-cooperative caching and random
              caching of 3\% and 4\%, respectively.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  6,
  pages    = "3667--3680",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/Y/Yang et al. 2020 - Learning Automata Based Q-Learning for Content Placement in Cooperative Caching.pdf",
  keywords = "Cooperative caching;Wireless communication;Prediction
              algorithms;Recurrent neural networks;Learning
              automata;Optimization;Quality of experience;Learning automata
              based Q-learning;quality of experience (QoE);wireless cooperative
              caching;user mobility prediction;content popularity
              prediction;MLNetworking",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.2982136"
}

@ARTICLE{Su2020-rk,
  title    = "Energy Efficient Uplink Transmissions in {LoRa} Networks",
  author   = "Su, B and Qin, Z and Ni, Q",
  abstract = "LoRa has been recognized as one of the most promising low-power
              wide-area (LPWA) techniques. Since LoRa devices are usually
              powered by batteries, energy efficiency (EE) is an essential
              consideration. In this paper, we investigate the energy efficient
              resource allocation in LoRa networks to maximize the system EE
              (SEE) and the minimal EE (MEE) of LoRa users, respectively.
              Specifically, our objective is to maximize the corresponding EE
              by jointly exploiting user scheduling, spreading factor (SF)
              assignment, and transmit power allocations. To solve them
              efficiently, we first propose a suboptimal algorithm, including
              the low-complexity user scheduling scheme based on matching
              theory and the heuristic SF assignment approach for LoRa users
              scheduled on the same channel. Then, to deal with the power
              allocation, an optimal algorithm is proposed to maximize the SEE.
              To maximize the MEE of LoRa users assigned to the same channel,
              an iterative power allocation algorithm based on the generalized
              fractional programming and sequential convex programming is
              proposed. Numerical results show that the proposed user
              scheduling algorithm achieves near-optimal EE performance, and
              the proposed power allocation algorithms outperform the
              benchmarks.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  8,
  pages    = "4960--4972",
  month    =  aug,
  year     =  2020,
  file     = "All Papers/S/Su et al. 2020 - Energy Efficient Uplink Transmissions in LoRa Networks.pdf",
  keywords = "convex programming;energy conservation;iterative methods;resource
              allocation;telecommunication power management;telecommunication
              scheduling;wide area networks;LoRa users;iterative power
              allocation algorithm;user scheduling algorithm;energy efficient
              uplink transmissions;LoRa networks;low-power wide-area
              techniques;LoRa devices;energy efficient resource
              allocation;factor assignment;transmit power
              allocations;low-complexity user;heuristic SF assignment
              approach;LPWA techniques;spreading factor assignment;suboptimal
              algorithm;low-complexity user scheduling;matching
              theory;generalized fractional programming;sequential convex
              programming;Resource management;Uplink;Logic gates;Power
              demand;Internet of Things;Interference;Scheduling;Energy
              efficiency;loRa;low-power wide-area;matching theory;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.2993085"
}

@ARTICLE{Hosler2020-pq,
  title    = "Stable Matching for Wireless {URLLC} in {Multi-Cellular},
              {Multi-User} Systems",
  author   = "H{\"o}{\ss}ler, T and Schulz, P and Jorswieck, E A and Simsek, M
              and Fettweis, G P",
  abstract = "Ultra-Reliable Low-Latency Communications (URLLC) are considered
              as one of the key services of the upcoming fifth generation (5G)
              of wireless communications systems. Enabling URLLC is especially
              challenging due to the strict requirements in terms of latency
              and reliability. Multi-connectivity is a powerful approach to
              increase reliability. However, most of the current research is
              restricted to single-user scenarios, neglecting the challenges of
              multi-cellular, multi-user systems, i.e., interference and the
              competition for limited resources. In this article, we develop
              analytic comparisons of different connectivity approaches,
              showing that multi-connectivity may not always be optimal in the
              considered scenario. Moreover, we propose and evaluate novel
              resource allocation approaches based on stable matching theory to
              enable wireless URLLC. We extend the pure many-to-one stable
              matching procedure by utilizing the optimal connectivity approach
              for each user, optimizing the maximum number of matched
              resources, and providing a resource reservation mechanism for
              users suffering from bad channel conditions. System-level
              simulations demonstrate that the proposed algorithm outperforms
              baseline resource allocation approaches in outage probability by
              up to three orders of magnitude. Even in a highly loaded system,
              an outage probability in the range of $10^-5$ is achieved.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  8,
  pages    = "5228--5241",
  month    =  aug,
  year     =  2020,
  keywords = "cellular radio;probability;resource allocation;telecommunication
              network reliability;system-level simulations;baseline resource
              allocation approaches;wireless URLLC;Ultra-Reliable Low-Latency
              Communications;upcoming fifth generation;wireless communications
              systems;multiconnectivity;single-user scenarios;different
              connectivity approaches;stable matching theory;stable matching
              procedure;optimal connectivity approach;matched
              resources;resource reservation mechanism;URLLC;multicellular
              multiuser systems;outage probability;Resource management;5G
              mobile communication;Reliability theory;Electronic mail;Wireless
              networks;5G;multi-connectivity;reliability;wireless
              systems;stable matching;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.2995150"
}

@ARTICLE{Ren2020-wb,
  title    = "Resource Allocation for Secure {URLLC} in {Mission-Critical}
              {IoT} Scenarios",
  author   = "Ren, H and Pan, C and Deng, Y and Elkashlan, M and Nallanathan, A",
  abstract = "Ultra-reliable low latency communication (URLLC) is one of three
              primary use cases in the fifth-generation (5G) networks, and its
              research is still in its infancy due to its stringent and
              conflicting requirements in terms of extremely high reliability
              and low latency. To reduce latency, the channel blocklength for
              packet transmission is finite, which incurs transmission rate
              degradation and higher decoding error probability. In this case,
              conventional resource allocation based on Shannon capacity
              achieved with infinite blocklength codes is not optimal. Security
              is another critical issue in mission-critical internet of things
              (IoT) communications, and physical-layer security is a promising
              technique that can ensure the confidentiality for wireless
              communications as no additional channel uses are needed for the
              key exchange as in the conventional upper-layer cryptography
              method. This paper is the first work to study the resource
              allocation for a secure mission-critical IoT communication system
              with URLLC. Specifically, we adopt the security capacity formula
              under finite blocklength and consider two optimization problems:
              weighted throughput maximization problem and total transmit power
              minimization problem. Each optimization problem is non-convex and
              challenging to solve, and we develop efficient methods to solve
              each optimization problem. Simulation results confirm the fast
              convergence speed of our proposed algorithm and demonstrate the
              performance advantages over the existing benchmark algorithms.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  9,
  pages    = "5793--5807",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/R/Ren et al. 2020 - Resource Allocation for Secure URLLC in Mission-Critical IoT Scenarios.pdf",
  keywords = "channel capacity;channel coding;cryptography;decoding;error
              statistics;Internet of Things;optimisation;resource
              allocation;wireless channels;channel blocklength;packet
              transmission;transmission rate degradation;higher decoding error
              probability;conventional resource allocation;Shannon
              capacity;infinite blocklength codes;physical-layer
              security;wireless communications;upper-layer cryptography
              method;secure mission-critical IoT communication system;security
              capacity formula;finite blocklength;optimization problem;total
              transmit power minimization problem;secure URLLC;mission-critical
              IoT scenarios;ultra-reliable low latency communication;primary
              use cases;weighted throughput maximization
              problem;mission-critical internet of things
              communications;fifth-generation networks;Ultra reliable low
              latency communication;Security;Resource management;Mission
              critical systems;Decoding;Error
              probability;Reliability;URLLC;secure communications;short packet
              transmission;mission-critical applications;industrial
              40;Wireless;5G6G",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.2999628"
}

@ARTICLE{Qiu2020-uo,
  title    = "Multiple {UAV-Mounted} Base Station Placement and User
              Association With Joint Fronthaul and Backhaul Optimization",
  author   = "Qiu, C and Wei, Z and Yuan, X and Feng, Z and Zhang, P",
  abstract = "In this paper, we study a joint placement, resource allocation,
              and user association problem for UAV-assisted wireless networks
              with constrained backhaul links, where multiple UAV-mounted base
              stations (UBSs) are deployed to provide wireless services for
              ground users. We propose a novel framework to maximize the user
              throughput within the flight-time of UBSs and provides fairness
              among the users. We first obtain the optimal resource allocation
              schemes based on different fronthaul and backhaul conditions, and
              an efficient iterative algorithm is then developed to jointly
              optimize user association and UBS placement. The optimal UBS
              placement can be achieved by solving an unconstrained
              optimization problem which is a simplification of the initial
              constrained optimization problem based on the optimal resource
              allocation. We develop a dual-domain coordinated descent and
              bipartite graph matching based sub-process to identify an optimal
              user association that prefers the nearby UBSs, as the user
              association under constrained backhaul links have non-unique
              optimal solutions. Extensive simulations are conducted to verify
              the effectiveness of the proposed algorithm, and results show
              that our proposed method under constrained backhaul can improve
              both the average throughput by 49\% and the fairness among the
              users by 47\% in comparison with the method under ideal backhaul.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  9,
  pages    = "5864--5877",
  month    =  sep,
  year     =  2020,
  keywords = "autonomous aerial vehicles;iterative methods;optimisation;radio
              networks;resource allocation;multiple UAV-mounted base station
              placement;user association problem;UAV-assisted wireless
              networks;constrained backhaul links;multiple UAV-mounted base
              stations;UBS;ground users;user throughput;optimal resource
              allocation schemes;backhaul conditions;optimal UBS
              placement;unconstrained optimization problem;initial constrained
              optimization problem;optimal user association;efficient iterative
              algorithm;Resource management;Optimization;Wireless
              communication;Throughput;Base stations;Delays;Unmanned aerial
              vehicles;wireless backhaul;convex optimization",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3001136"
}

@ARTICLE{Buyukates2020-hb,
  title    = "Timely Distributed Computation With Stragglers",
  author   = "Buyukates, B and Ulukus, S",
  abstract = "We consider a status update system in which the update packets
              need to be processed to extract the embedded useful information.
              The source node sends the acquired information to a computation
              unit (CU) which consists of a master node and n worker nodes. The
              master node distributes the received computation task to the
              worker nodes. Upon computation, the master node aggregates the
              results and sends them back to the source node to keep it
              updated. We investigate the age performance of uncoded and coded
              (repetition coded, MDS coded, and multi-message MDS (MM-MDS)
              coded) schemes in the presence of stragglers under i.i.d.
              exponential transmission delays and i.i.d shifted exponential
              computation times. We show that asymptotically MM-MDS coded
              scheme outperforms the other schemes. Furthermore, we
              characterize the optimal codes such that the average age is
              minimized.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  9,
  pages    = "5273--5282",
  month    =  sep,
  year     =  2020,
  annote   = "fuer das Dryad-Projekt?",
  file     = "All Papers/B/Buyukates and Ulukus 2020 - Timely Distributed Computation With Stragglers.pdf",
  keywords = "codes;source node;multimessage MDS;stragglers;exponential
              computation times;optimal codes;status update system;update
              packets;embedded useful information;worker nodes;received
              computation task;master node;timely distributed
              computation;asymptotically MM-MDS coded scheme;Delays;Task
              analysis;Computational modeling;Information age;Random
              variables;Data mining;Age of information;distributed
              computation;coded computation;straggler mitigation",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3001873"
}

@ARTICLE{Chen2020-qa,
  title    = "Computation Over {MAC}: Achievable Function Rate Maximization in
              Wireless Networks",
  author   = "Chen, L and Zhao, N and Chen, Y and Qin, X and Yu, F R",
  abstract = "The next generation wireless network is expected to connect
              billions of nodes, which brings up the bottleneck on the
              communication speed for distributed data fusion. To overcome this
              challenge, computation over multiple access channel (CoMAC) was
              recently developed to compute the desired functions with a
              summation structure (e.g., mean, norm, etc.) by using the
              superposition property of wireless channels. This work aims to
              maximize the achievable function rate of reliable CoMAC in
              wireless networks. More specifically, considering channel fading
              and transceiver design, we derive the achievable function rate
              adopting the quantization and the nested lattice coding, which is
              determined by the number of nodes, the maximum value of messages
              and the quantization error threshold. Based on the derived
              result, the transceiver design is optimized to maximize the
              achievable function rate of the network. We first study a single
              cluster network without inter-cluster interference (ICI). Then, a
              multi-cluster network is further analyzed in which the clusters
              work in the same channel with ICI. In order to avoid the global
              channel state information (CSI) aggregation during the
              optimization, a low-complexity signaling procedure irrelevant
              with the number of nodes is proposed utilizing the channel
              reciprocity and the defined effective CSI.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  9,
  pages    = "5446--5459",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/C/Chen et al. 2020 - Computation Over MAC - Achievable Function Rate Maximization in Wireless Networks.pdf",
  keywords = "channel coding;fading channels;multi-access
              systems;optimisation;sensor fusion;achievable function rate
              maximization;next generation wireless network;distributed data
              fusion;multiple access channel;wireless channels;channel
              fading;transceiver design;single cluster network;multicluster
              network;global channel state information
              aggregation;Transceivers;Fading channels;Interference;Wireless
              networks;MIMO
              communication;Lattices;Optimization;Computation;interference;multiple
              access channel;MIMO;signaling procedure;transceiver
              design;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3005958"
}

@ARTICLE{Xiao2020-xj,
  title    = "Resource Management for {Multi-User-Centric} {V2X} Communication
              in Dynamic {Virtual-Cell-Based} {Ultra-Dense} Networks",
  author   = "Xiao, H and Zhang, X and Chronopoulos, A T and Zhang, Z and Liu,
              H and Ouyang, S",
  abstract = "The technology of static user-centric virtual cell (VC) has been
              designed in the fifth-generation (5G) ultra-dense networks (UDNs)
              for alleviating both frequent handover and inter-cell
              interference. However, to provide the user-centric services, the
              fairness of resource management must be involved when the common
              vehicular-to-X (V2X) messages are multicast to the vehicle
              groups. In this paper, a dynamic user-centric virtual cell (DUVC)
              scheme is proposed for updating adaptively the VC through the
              mobile tracking of the vehicles. Furthermore, an approximation
              algorithm is proposed for solving the max-min-fair problem of
              resource management in V2X communication in order to better
              support V2X services throughout the service VC. Numerical results
              are provided for demonstrating that the proposed DUVC scheme is
              suitable for implementing in the V2X communication. Finally, the
              proposed algorithm is shown to outperform three other existing
              algorithms in terms of fairness of resource management.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  10,
  pages    = "6346--6358",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/X/Xiao et al. 2020 - Resource Management for Multi-User-Centric V2X Communication in Dynamic Virtual-Cell-Based Ultra-Dense Networks.pdf",
  keywords = "5G mobile communication;approximation theory;cellular
              radio;intercell interference;minimax techniques;mobility
              management (mobile radio);resource
              allocation;virtualisation;dynamic user-centric virtual cell
              scheme;DUVC scheme;vehicle mobile tracking;handover;5G
              ultra-dense networks;fifth-generation ultradense
              networks;user-centric virtual cell;dynamic virtual-cell-based
              ultradense networks;V2X services;max-min-fair
              problem;user-centric services;intercell
              interference;MultiUser-Centric V2X Communication;resource
              management;Vehicle-to-everything;Resource management;Computer
              architecture;Vehicle dynamics;Approximation algorithms;Array
              signal processing;Interference;5G ultra-dense
              network;beamforming;proximal alternating linearized minimization
              (PALM);resource management;V2X communication;virtual cell;5G6G",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3007612"
}

@ARTICLE{Xiao2020-pq,
  title    = "Reinforcement {Learning-Based} Mobile Offloading for Edge
              Computing Against Jamming and Interference",
  author   = "Xiao, L and Lu, X and Xu, T and Wan, X and Ji, W and Zhang, Y",
  abstract = "Mobile edge computing systems help improve the performance of
              computational-intensive applications on mobile devices and have
              to resist jamming attacks and heavy interference. In this paper,
              we present a reinforcement learning based mobile offloading
              scheme for edge computing against jamming attacks and
              interference, which uses safe reinforcement learning to avoid
              choosing the risky offloading policy that fails to meet the
              computational latency requirements of the tasks. This scheme
              enables the mobile device to choose the edge device, the transmit
              power and the offloading rate to improve its utility including
              the sharing gain, the computational latency, the energy
              consumption and the signal-to-interference-plus-noise ratio of
              the offloading signals without knowing the task generation model,
              the edge computing model, and the jamming/interference model. We
              also design a deep reinforcement learning based mobile offloading
              for edge computing that uses an actor network to choose the
              offloading policy and a critic network to update the actor
              network weights to improve the computational performance. We
              discuss the computational complexity and provide the performance
              bound that consists of the computational latency and the energy
              consumption based on the Nash equilibrium of the mobile
              offloading game. Simulation results show that this scheme can
              reduce the computational latency and save energy consumption.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  10,
  pages    = "6114--6126",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/X/Xiao et al. 2020 - Reinforcement Learning-Based Mobile Offloading for Edge Computing Against Jamming and Interference.pdf",
  keywords = "computational complexity;game theory;jamming;learning (artificial
              intelligence);mobile computing;mobile device;energy
              consumption;signal-to-interference-plus-noise ratio;deep
              reinforcement learning;computational complexity;mobile offloading
              game;mobile offloading;mobile edge computing systems;heavy
              interference;safe reinforcement learning;computational
              latency;Task analysis;Mobile handsets;Edge
              computing;Computational modeling;Interference;Energy
              consumption;Jamming;Mobile offloading;edge
              computing;interference;jamming;reinforcement
              learning;EdgeFogCloudIoT;MLNetworking",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3007742"
}

@ARTICLE{Tegos2020-yi,
  title    = "Slotted {ALOHA} With {NOMA} for the Next Generation {IoT}",
  author   = "Tegos, S A and Diamantoulakis, P D and Lioumpas, A S and
              Sarigiannidis, P G and Karagiannidis, G K",
  abstract = "Random access (RA) has recently been revisited and considered as
              a key technology for the medium access control layer of the
              Internet of Things applications. Compared to other RA protocols,
              slotted ALOHA (SA) has the advantages of low complexity and
              elimination of partially overlapping transmissions, reducing the
              number of collisions, however it may suffer from congestion as
              the traffic load and the number of devices increase. To this end,
              two RA protocols based on SA and uplink non-orthogonal multiple
              access are proposed and applied to wireless sensor networks and
              wireless powered sensor networks. More specifically, to reduce
              the number of collisions and increase the throughput of SA, while
              maintaining low complexity, two detection techniques are used to
              mitigate the interference, when two sources transmit information
              in the same time slot, namely successive interference
              cancellation (SIC) with optimal decoding order policy and joint
              decoding (JD). To evaluate the performance of the proposed
              protocols, the outage probability of SIC and JD is derived, which
              is used to express the average throughput attained by each
              protocol in closed-form. Finally, both the analytical results and
              the simulations verify that the proposed protocols substantially
              increase the throughput and the number of connected devices
              compared to SA.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  10,
  pages    = "6289--6301",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/T/Tegos et al. 2020 - Slotted ALOHA With NOMA for the Next Generation IoT.pdf",
  keywords = "access protocols;decoding;interference suppression;Internet of
              Things;multi-access systems;next generation
              networks;probability;radio networks;telecommunication network
              reliability;telecommunication traffic;wireless sensor
              networks;outage probability;performance evaluation;interference
              mitigation;Internet of Things applications;joint
              decoding;JD;optimal decoding order policy;successive interference
              cancellation;time slot;wireless powered sensor networks;uplink
              nonorthogonal multiple access;traffic load;partially overlapping
              transmissions;RA protocols;medium access control layer;random
              access;next generation IoT;NOMA;slotted ALOHA;Protocols;Wireless
              sensor networks;NOMA;Throughput;Silicon carbide;Internet of
              Things;Wireless communication;Random access;slotted ALOHA;uplink
              NOMA;energy harvesting;wireless power transfer;outage
              probability;wireless powered sensor network;Mobile\_Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3007744"
}

@ARTICLE{Fan2020-wp,
  title    = "{Cache-Enabled} {HetNets} With Limited Backhaul: A Stochastic
              Geometry Model",
  author   = "Fan, C and Zhang, T and Liu, Y and Zeng, Z",
  abstract = "With the rapid explosion of data volume from mobile networks,
              edge caching has received significant attentions as an efficient
              approach to boost content delivery efficiency by bringing
              contents near users. In this article, cache-enabled heterogeneous
              networks (HetNets) considering the limited backhaul are analyzed
              with the aid of the stochastic geometry approach. A hybrid
              caching policy, in which the most popular contents are cached in
              the macro BSs tier with the deterministic caching strategy and
              the less popular contents are cached in the helpers tier with the
              probabilistic caching strategy, is proposed. Correspondingly, the
              content-centric association strategy is designed based on the
              comprehensive state of the access link, the cache and the
              backhaul link. Under the hybrid caching policy, new analytical
              results for successful content delivery probability, average
              successful delivery rate and energy efficiency are derived in the
              general scenario, the interference-limited scenario and the mean
              load scenario. The simulation results show that the proposed
              caching policy outperforms the most popular caching policy in
              HetNets with the limited backhaul. The performance gain is
              dramatically improved when the content popularity is less skewed,
              the cache capacity is sufficient and the helper density is
              relatively large. Furthermore, it is confirmed that there exists
              an optimal helper density to maximize the energy efficiency of
              the cache-enabled HetNets.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  11,
  pages    = "7007--7022",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/F/Fan et al. 2020 - Cache-Enabled HetNets With Limited Backhaul - A Stochastic Geometry Model.pdf",
  keywords = "Probabilistic logic;Numerical models;Stochastic
              processes;Geometry;System performance;Cellular networks;Wireless
              communication;Edge caching;heterogeneous networks;limited
              backhaul;stochastic geometry;5G6G",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3013633"
}

@ARTICLE{Pei2020-ea,
  title    = "{NOMA-Based} Coordinated Direct and Relay Transmission With a
              {Half-Duplex/} {Full-Duplex} Relay",
  author   = "Pei, X and Yu, H and Wen, M and Mumtaz, S and Al Otaibi, S and
              Guizani, M",
  abstract = "In this article, we propose a downlink non-orthogonal multiple
              access (NOMA) based coordinated direct and relay system with one
              cell-center user and multiple cell-edge users, where a
              decode-and-forward (DF) relay bridges the connection between the
              base station and the cell-edge users. Both full-duplex (FD) and
              half-duplex (HD) protocols are considered for the relay. We
              assume that the performance of the cell-edge users is subjected
              to the relay, and the cancellation of the mutual interference
              between the relay and cell-center user is imperfect. Both the
              exact analytical expression of outage probability and an
              approximate expression of the ergodic sum rate at high
              signal-to-noise ratio (SNR) are derived. Numerical results
              demonstrate that: 1) the FD relaying NOMA system outperforms the
              HD relaying NOMA system at low SNR, but the situation is exactly
              the opposite at high SNR; 2) the mutual interference can cause a
              larger performance gap than the self-interference at the relay;
              3) the power allocation coefficients for the cell-center user and
              relay can affect the performance more significantly than those
              for cell-edge users.11This article was presented in part at the
              IEEE International Workshop on Signal Processing Advances in
              Wireless Communications 2019 [1].",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  11,
  pages    = "6750--6760",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/P/Pei et al. 2020 - NOMA-Based Coordinated Direct and Relay Transmission With a Half-Duplex - Full-Duplex Relay.pdf",
  keywords = "Relays;NOMA;Signal to noise ratio;Base stations;Interference
              cancellation;Resource management;Non-orthogonal multiple
              access;ergodic sum rate;outage
              probability;decode-and-forward;full-duplex;Mobile\_Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3017002"
}

@ARTICLE{Sun2021-gl,
  title    = "Movement Aware {CoMP} Handover in Heterogeneous {Ultra-Dense}
              Networks",
  author   = "Sun, W and Wang, L and Liu, J and Kato, N and Zhang, Y",
  abstract = "The densification of base station (BS) deployments is driving the
              evolution of network structures towards heterogeneous ultra-dense
              networks (UDN), making coordinated multipoint (CoMP) a viable and
              promising transmission solution. However, the BS cooperation
              regions formed by applying CoMP in the UDN are small and
              irregular, which causes frequent handover for mobile users.
              Different from most existing work that focus on the trigger time
              of handover, we explore how to choose the appropriate BS
              cooperation set to reduce handover rate. In this paper, we
              consider movement aware CoMP handover (MACH). By estimating cell
              dwell time, a user would be intelligently assigned to macro cell
              or small cell according to its movement trend. To enhance
              reliability, we further proposed improved MACH (iMACH) to achieve
              a trade-off between BSs with long dwell time and the current best
              performed BS for multipoint cooperation while user moving. Using
              stochastic geometry method, expressions of coverage probability,
              handover probability and throughput that characterize performance
              of the proposed schemes are derived. The numerical results
              indicate that the theoretical analyses fit the simulation results
              well and the proposed schemes surpass the existing schemes in
              terms of the aforementioned metrics, and more intelligent and
              suitable for ultra-dense scenarios.",
  journal  = "IEEE Trans. Commun.",
  volume   =  69,
  number   =  1,
  pages    = "340--352",
  month    =  jan,
  year     =  2021,
  keywords = "Handover;Interference;Computer
              architecture;Microprocessors;Market
              research;Reliability;Ultra-dense networks;coordinated
              multipoint;handover;stochastic geometry;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3019388"
}

@ARTICLE{Wang2020-gi,
  title    = "Effect of Spatial and Temporal Traffic Statistics on the
              Performance of Wireless Networks",
  author   = "Wang, G and Zhong, Y and Li, R and Ge, X and Quek, T Q S and Mao,
              G",
  abstract = "The traffic in wireless networks has become diverse and
              fluctuating both spatially and temporally due to the emergence of
              new wireless applications and the complexity of scenarios. The
              purpose of this paper is to quantitatively analyze the impact of
              the wireless traffic, which fluctuates both spatially and
              temporally, on the performance of the wireless networks.
              Specially, we propose to combine the tools from stochastic
              geometry and queueing theory to model the spatial and temporal
              fluctuation of traffic, which to our best knowledge has seldom
              been evaluated analytically. We derive the spatial and temporal
              statistics, the total arrival rate, the stability of queues and
              the delay of users by considering two different spatial
              properties of traffic, i.e., the uniformly and non-uniformly
              distributed cases. The numerical results indicate that although
              the fluctuation of traffic (reflected by the variance of total
              arrival rate) when the users are clustered is much fiercer than
              that when the users are uniformly distributed, the unstable
              probability is smaller. Our work provides a useful reference for
              the design of wireless networks when the complex spatio-temporal
              fluctuation of the traffic is considered.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  11,
  pages    = "7083--7097",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/W/Wang et al. 2020 - Effect of Spatial and Temporal Traffic Statistics on the Performance of Wireless Networks.pdf",
  keywords = "Wireless networks;Delays;Queueing analysis;Geometry;Stochastic
              processes;Graphical models;Traffic;delay;queueing
              theory;stability;stochastic geometry;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3019534"
}

@ARTICLE{He2020-zn,
  title    = "Virtual Service Placement for Edge Computing Under Finite Memory
              and Bandwidth",
  author   = "He, S and Lyu, X and Ni, W and Tian, H and Liu, R P and Hossain,
              E",
  abstract = "Edge computing allows an edge server to adaptively place virtual
              instances to serve different types of data. This article presents
              a new algorithm which jointly optimizes virtual service placement
              farsightedly and service data admission instantly to maximize the
              time-average service throughput of edge computing. The data
              admission is optimized, adapting to fast-changing data arrivals
              and wireless channels. The service placement is transformed into
              a two-dimensional knapsack problem by approximating future
              arrivals and channels with past observations, and solved over a
              slow timescale to allow services to be properly installed.
              Different from existing studies, our algorithm considers
              practical aspects of edge servers, such as finite memory size and
              bandwidth. We prove that the algorithm is asymptotically optimal
              and the optimality loss resulting from the approximation
              diminishes. Simulations show that our approach can improve the
              time-average throughput of existing alternatives by 16\% for our
              considered simulation setup. The improvement becomes higher, as
              the memory size becomes increasingly tight. The number of
              services to be replaced is reduced without loss of throughput,
              after being placed farsightedly.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  12,
  pages    = "7702--7718",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/H/He et al. 2020 - Virtual Service Placement for Edge Computing Under Finite Memory and Bandwidth.pdf",
  keywords = "Servers;Random access memory;Bandwidth;Throughput;Resource
              management;Delays;Memory management;Resource allocation;edge
              computing;virtual service placement;finite memory size;finite
              memory bandwidth;NFV",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3022692"
}

@ARTICLE{Li2020-jb,
  title    = "Resource Allocation in {Buffer-Aided} Cooperative
              {Non-Orthogonal} Multiple Access Systems",
  author   = "Li, J and Lei, X and Diamantoulakis, P D and Zhou, F and
              Sarigiannidis, P and Karagiannidis, G K",
  abstract = "Cooperative non-orthogonal multiple access (C-NOMA) and buffering
              are promising techniques to improve spectrum efficiency in the
              next generation of wireless networks. In this article, a
              buffer-aided cooperative NOMA network with direct links is
              studied. The throughput maximization problem is firstly
              formulated under the assumption of fixed power allocation and
              optimally solved by designing a mode selection policy. In order
              to further improve system throughput, the problem is extended
              into the case that power allocation and mode selection are
              jointly optimized. An optimal solution is obtained, while a
              sub-optimal one is also provided in order to decrease the
              implementation complexity. Furthermore, considering the case
              where the buffer has finite size and the users are
              delay-sensitive, a throughput-delay aware strategy is also
              presented. Moreover, it is shown that the proposed schemes
              outperform the baseline one in terms of throughput. Finally,
              simulations demonstrate that the sub-optimal solution achieves
              similar performance to the optimal one, while significantly
              reduces the implementation complexity.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  12,
  pages    = "7429--7445",
  month    =  dec,
  year     =  2020,
  keywords = "NOMA;Throughput;Relays;Downlink;Resource management;Power system
              reliability;Probability;Cooperative non-orthogonal
              multi-access;buffer-aided relaying;direct links;power
              allocation;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3023458"
}

@ARTICLE{Guo2020-ic,
  title    = "On the Asynchrony of Computation Offloading in {Multi-User} {MEC}
              Systems",
  author   = "Guo, K and Quek, T Q S",
  abstract = "Mobile edge computing (MEC) can extend the computing capability
              of mobile users (MUs), by offloading computational tasks from MUs
              to proximal MEC servers. Specifically, offloaded tasks are
              firstly transmitted to access points (APs) via wireless channels,
              and then relayed to MEC servers through backhauls. Due to
              differentiated wireless channels and input data sizes, offloaded
              tasks from different MUs arrive at the AP and MEC server in an
              asynchronous manner. Considering such asynchrony, we design
              sequential execution model for backhaul transmission and task
              processing in the MEC server and present its merits in terms of
              maximum task completion time for different backhaul
              configurations: 1) With ideal backhaul, the optimal task
              scheduling order in the MEC server follows the first coming and
              first scheduling rule; 2) With limited backhaul, an effective
              two-stage task scheduling order on the backhaul and MEC server is
              determined by the Johnson's rule. Further, we make suboptimal
              decisions on wireless resource provisioning and task offloading,
              with respect to the attained task scheduling order and derived
              necessary and sufficient conditions under which full local
              processing and full offloading are optimal. Finally, simulation
              results demonstrate the superiority of our devised sequential
              execution model and computation offloading mechanisms.",
  journal  = "IEEE Trans. Commun.",
  volume   =  68,
  number   =  12,
  pages    = "7746--7761",
  month    =  dec,
  year     =  2020,
  keywords = "Task analysis;Servers;Computational modeling;Wireless
              communication;Processor scheduling;Optimal
              scheduling;Scheduling;Asynchrony of computation offloading;ideal
              and limited backhaul;sequential execution model;task
              offloading;task scheduling;EdgeFogCloudIoT",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3024577"
}

@ARTICLE{Kasgari2021-ap,
  title    = "Experienced Deep Reinforcement Learning With Generative
              Adversarial Networks ({GANs}) for {Model-Free} Ultra Reliable Low
              Latency Communication",
  author   = "Kasgari, A T Z and Saad, W and Mozaffari, M and Poor, H V",
  abstract = "In this paper, a novel experienced deep reinforcement learning
              (deep-RL) framework is proposed to provide model-free resource
              allocation for ultra reliable low latency communication
              (URLLC-6G) in the downlink of a wireless network. The goal is to
              guarantee high end-to-end reliability and low end-to-end latency,
              under explicit data rate constraints, for each wireless user
              without any models of or assumptions on the users' traffic. In
              particular, in order to enable the deep-RL framework to account
              for extreme network conditions and operate in highly reliable
              systems, a new approach based on generative adversarial networks
              (GANs) is proposed. This GAN approach is used to pre-train the
              deep-RL framework using a mix of real and synthetic data, thus
              creating an experienced deep-RL framework that has been exposed
              to a broad range of network conditions. The proposed deep-RL
              framework is particularly applied to a multi-user orthogonal
              frequency division multiple access (OFDMA) resource allocation
              system. Formally, this URLLC-6G resource allocation problem in
              OFDMA systems is posed as a power minimization problem under
              reliability, latency, and rate constraints. To solve this problem
              using experienced deep-RL, first, the rate of each user is
              determined. Then, these rates are mapped to the resource block
              and power allocation vectors of the studied wireless system.
              Finally, the end-to-end reliability and latency of each user are
              used as feedback to the deep-RL framework. It is then shown that
              at the fixed-point of the deep-RL algorithm, the reliability and
              latency of the users are near-optimal. Moreover, for the proposed
              GAN approach, a theoretical limit for the generator output is
              analytically derived. Simulation results show how the proposed
              approach can achieve near-optimal performance within the
              rate-reliability-latency region, depending on the network and
              service requirements. The results also show that the proposed
              experienced deep-RL framework is able to remove the transient
              training time that makes conventional deep-RL methods unsuitable
              for URLLC-6G. Moreover, during extreme conditions, it is shown
              that the proposed, experienced deep-RL agent can recover
              instantly while a conventional deep-RL agent takes several epochs
              to adapt to new extreme conditions.",
  journal  = "IEEE Trans. Commun.",
  volume   =  69,
  number   =  2,
  pages    = "884--899",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/K/Kasgari et al. 2021 - Experienced Deep Reinforcement Learning With Gener ... tworks (GANs) for Model-Free Ultra Reliable Low Latency Communication.pdf",
  keywords = "Reliability;Resource management;Ultra reliable low latency
              communication;Wireless communication;Training;Generative
              adversarial networks;Computer network reliability;Resource
              allocation;generative adversarial networks;model-free resource
              management;low latency communications;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3031930"
}

@ARTICLE{Testi2021-bu,
  title    = "Blind Wireless Network Topology Inference",
  author   = "Testi, E and Giorgetti, A",
  abstract = "This work proposes a framework to discover the topology of a
              non-collaborative packet-based wireless network using
              radio-frequency (RF) sensors. The methodology developed is blind,
              allowing topology sensing of a network whose key features (i.e.,
              number of nodes, physical layer signals, and medium access
              control (MAC) and routing protocols) are unknown. Because of the
              wireless medium, over-the-air signals captured by the sensors are
              mixed; therefore, blind source separation (BSS) and measurement
              association are used to separate traffic patterns. Then, to infer
              the topology, we detect directed data flows among nodes by
              identifying causal relationships between the separated
              transmitted patterns. We propose causal inference methods such as
              Granger causality (GC), transfer entropy (TE), and conditional
              transfer entropy (CTE) that use the times series of traffic
              profiles, and a solution based on a neural network (NN) that
              exploits distilled time-based features. The framework is
              validated on an ad-hoc wireless network accounting for MAC
              protocol, packet collisions, nodes mobility, the spatial density
              of sensors, and channel impairments, such as path-loss,
              shadowing, and noise. Numerical results reveal that the proposed
              approach reaches a high probability of link detection and a
              moderate false alarm rate in mild shadowing regimes and low to
              moderate network nodes mobility.",
  journal  = "IEEE Trans. Commun.",
  volume   =  69,
  number   =  2,
  pages    = "1109--1120",
  month    =  feb,
  year     =  2021,
  keywords = "Topology;Network topology;Sensors;Wireless networks;Wireless
              sensor networks;Radio frequency;Blind source separation;Topology
              inference;blind source separation;independent component
              analysis;link detection;Granger causality;transfer entropy;neural
              network",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3036058"
}

@ARTICLE{Guo2021-hz,
  title    = "Online Learning Based Computation Offloading in {MEC} Systems
              With Communication and Computation Dynamics",
  author   = "Guo, K and Gao, R and Xia, W and Quek, T Q S",
  abstract = "By offloading tasks from the mobile device (MD) to its nearby
              deployed access points (APs), each of which is connected to one
              server for task processing, computation offloading can strike a
              balance between MD's task execution delay and energy consumption
              in mobile edge computing (MEC) systems. Considering communication
              and computation dynamics in MEC systems, we aim to design online
              computation offloading mechanisms in this paper to minimize the
              time average expected task execution delay under the constraint
              of average energy consumption. Firstly, with known current
              channel gains between the MD and APs as well as available
              computing capability at MEC servers, we leverage the Lyapunov
              optimization framework to make an optimal one-slot decision on
              MD's transmit power allocation and MEC server selection. On this
              basis, we then consider a more realistic scenario, where it is
              difficult to capture current available computing capability at
              MEC servers, and combine the multi-armed bandit framework for an
              online learning based MEC server selection algorithm. Finally,
              through theoretical analyses and extensive simulations, we
              demonstrate the near-optimality and feasibility of our proposed
              algorithms, and present that our proposed algorithms fully
              explore the interplay between communication and computation with
              enriched user experience and reduced energy consumption.",
  journal  = "IEEE Trans. Commun.",
  volume   =  69,
  number   =  2,
  pages    = "1147--1162",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/G/Guo et al. 2021 - Online Learning Based Computation Offloading in MEC Systems With Communication and Computation Dynamics.pdf",
  keywords = "Task analysis;Servers;Delays;Optimization;Energy
              consumption;Resource management;Heuristic algorithms;Lyapunov
              optimization;mobile edge computing (MEC);multi-armed bandit
              (MAB);online learning;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3038875"
}

@ARTICLE{Zhang2021-tw,
  title    = "{Communications-Caching-Computing} Resource Allocation for
              Bidirectional Data Computation in Mobile Edge Networks",
  author   = "Zhang, Lyutianyang and Sun, Yaping and Chen, Zhiyong and Roy,
              Sumit",
  abstract = "A novel bidirectional computation task model has emerged as an
              important use case of 5G. For example, interactive AR/VR gaming
              service needs to render the live scene by jointly computing user
              features such as 3D positions and video data generated from the
              Internet. In this article, we consider the bidirectional
              computation task model, where each task is served via three
              mechanisms, i.e., local computing with local caching, local
              computing without local caching, and computing at the mobile edge
              computing server. To minimize the average bandwidth, we formulate
              the joint caching and computing optimization problem under the
              latency, cache size and average power constraints. In the
              homogeneous scenario, we derive the optimal policy and analytical
              expression for the minimum bandwidth. In the heterogeneous
              scenario, to reduce the computation complexity of the NP-hard
              problem, we relax some constraints of the original problem and
              propose a Lagrangian relaxation (LR) suboptimal solution, which
              may be infeasible. We then reformulate the original problem as an
              auxiliary problem based on the LR solution and solve this via
              Concave-Convex Procedure (CCCP), which outputs feasible local
              optimal solution. Simulation has shown that LR-based algorithms
              outperform the baselines including greedy and CCCP algorithms in
              the bandwidth performance and time efficiency.",
  journal  = "IEEE Trans. Commun.",
  volume   =  69,
  number   =  3,
  pages    = "1496--1509",
  month    =  mar,
  year     =  2021,
  keywords = "Task analysis;Mobile handsets;Servers;Computational
              modeling;Bandwidth;Data models;Internet;Bidirectional data
              computation;mobile edge computing;wireless caching;bandwidth
              minimization;EdgeFogCloudIoT",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2020.3041343"
}

@ARTICLE{Chiariotti2021-fa,
  title    = "Peak Age of Information Distribution for Edge Computing With
              Wireless Links",
  author   = "Chiariotti, Federico and Vikhrova, Olga and Soret, Beatriz and
              Popovski, Petar",
  abstract = "Age of Information (AoI) is a critical metric for several
              Internet of Things (IoT) applications, where sensors keep track
              of the environment by sending updates that need to be as fresh as
              possible. The development of edge computing solutions has moved
              the monitoring process closer to the sensor, reducing the
              communication delays, but the processing time of the edge node
              needs to be taken into account. Furthermore, a reliable system
              design in terms of freshness requires the knowledge of the full
              distribution of the Peak AoI (PAoI), from which the probability
              of occurrence of rare, but extremely damaging events can be
              obtained. In this work, we model the communication and
              computation delay of such a system as two First Come First Serve
              (FCFS) queues in tandem, analytically deriving the full
              distribution of the PAoI for the $M/M/1$ -- $M/D/1$ and the
              $M/M/1$ -- $M/M/1$ tandems, which can represent a wide variety of
              realistic scenarios.",
  journal  = "IEEE Trans. Commun.",
  volume   =  69,
  number   =  5,
  pages    = "3176--3191",
  month    =  may,
  year     =  2021,
  file     = "All Papers/C/Chiariotti et al. 2021 - Peak Age of Information Distribution for Edge Computing With Wireless Links.pdf",
  keywords = "Computational modeling;Queueing analysis;Internet of
              Things;Sensors;Edge computing;Reliability;Measurement;Age of
              information;peak age of information;edge computing;queuing
              networks;EdgeFogCloudIoT;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2021.3053038"
}

@ARTICLE{Weerasinghe2021-qh,
  title    = "Priority Enabled {Grant-Free} Access With Dynamic Slot Allocation
              for Heterogeneous {mMTC} Traffic in {5G} {NR} Networks",
  author   = "Weerasinghe, Thilina N and Casares-Giner, Vicente and
              Balapuwaduge, Indika A M and Li, Frank Y",
  abstract = "Although grant-based mechanisms have been a predominant approach
              for wireless access for years, the additional latency required
              for initial handshake message exchange and the extra control
              overhead for packet transmissions have stimulated the emergence
              of grant-free (GF) transmission. GF access provides a promising
              mechanism for carrying low and moderate traffic with small data
              and fits especially well for massive machine type communications
              (mMTC) applications. Despite a surge of interest in GF access,
              how to handle heterogeneous mMTC traffic based on GF mechanisms
              has not been investigated in depth. In this paper, we propose a
              priority enabled GF access scheme which performs dynamic slot
              allocation in each 5G new radio subframe to devices with
              different priority levels on a subframe-by-subframe basis. While
              high priority traffic has access privilege for slot occupancy,
              the remaining slots in the same subframe will be allocated to low
              priority traffic. To evaluate the performance of the proposed
              scheme, we develop a two-dimensional Markov chain model which
              integrates these two types of traffic via a pseudo-aggregated
              process. Furthermore, the model is validated through simulations
              and the performance of the scheme is evaluated both analytically
              and by simulations and compared with two other GF access schemes.",
  journal  = "IEEE Trans. Commun.",
  volume   =  69,
  number   =  5,
  pages    = "3192--3206",
  month    =  may,
  year     =  2021,
  file     = "All Papers/W/Weerasinghe et al. 2021 - Priority Enabled Grant-Free Access With Dynamic Slot Allocation for Heterogeneous mMTC Traffic in 5G NR Networks.pdf",
  keywords = "Performance evaluation;Wireless communication;Analytical
              models;5G mobile communication;Massive machine type
              communications;Markov processes;Dynamic scheduling;Grant-free
              access;NR numerology;mMTC traffic;dynamic slot
              allocation;two-dimensional Markov chain;pseudo-aggregated
              process;Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2021.3053990"
}

@ARTICLE{Huang2021-vs,
  title    = "Iterative Collision Resolution for Slotted {ALOHA} With {NOMA}
              for Heterogeneous Devices",
  author   = "Huang, Yu-Chih and Shieh, Shin-Lin and Hsu, Yu-Pin and Cheng,
              Hao-Ping",
  abstract = "In this paper, the problem of using uncoordinated multiple access
              (UMA) to serve a massive amount of heterogeneous users is
              investigated. Leveraging the heterogeneity, we propose a novel
              UMA protocol, called iterative collision resolution for slotted
              ALOHA (IRSA) with non-orthogonal multiple access (NOMA), to
              improve the conventional IRSA. In addition to the inter-slot
              successive interference cancellation (SIC) technique used in
              existing IRSA-based schemes, the proposed protocol further
              employs the intra-slot SIC technique that enables collision
              resolution for certain configurations of collided packets. A
              novel multi-dimensional density evolution is then proposed to
              analyze and to optimize the proposed protocol. Simulation results
              show that the proposed IRSA with NOMA protocol can efficiently
              exploit the heterogeneity among users and the multi-dimensional
              density evolution can accurately predict the throughput
              performance. Last, an extension of the proposed IRSA with NOMA
              protocol to the frame-asynchronous setting is investigated, where
              a boundary effect similar to that in spatially-coupled
              low-density parity check codes can be observed to bootstrap the
              decoding process.",
  journal  = "IEEE Trans. Commun.",
  volume   =  69,
  number   =  5,
  pages    = "2948--2961",
  month    =  may,
  year     =  2021,
  file     = "All Papers/H/Huang et al. 2021 - Iterative Collision Resolution for Slotted ALOHA With NOMA for Heterogeneous Devices.pdf",
  keywords = "NOMA;Protocols;Silicon carbide;Decoding;Optimization;Interference
              cancellation;Bipartite graph;Slotted ALOHA;diversity slotted
              ALOHA;successive interference cancellation;non-orthogonal
              multiple access;density evolution;Mobile\_Wireless",
  issn     = "1558-0857",
  doi      = "10.1109/TCOMM.2021.3059778"
}

@ARTICLE{Haeri2018-vx,
  title    = "Virtual Network Embedding via Monte Carlo Tree Search",
  author   = "Haeri, Soroush and Trajkovic, Ljiljana",
  abstract = "Network virtualization helps overcome shortcomings of the current
              Internet architecture. The virtualized network architecture
              enables coexistence of multiple virtual networks (VNs) on an
              existing physical infrastructure. VN embedding (VNE) problem,
              which deals with the embedding of VN components onto a physical
              network, is known to be -hard. In this paper, we propose two VNE
              algorithms: MaVEn-M and MaVEn-S. MaVEn-M employs the
              multicommodity flow algorithm for virtual link mapping while
              MaVEn-S uses the shortest-path algorithm. They formalize the
              virtual node mapping problem by using the Markov decision process
              (MDP) framework and devise action policies (node mappings) for
              the proposed MDP using the Monte Carlo tree search algorithm.
              Service providers may adjust the execution time of the MaVEn
              algorithms based on the traffic load of VN requests. The
              objective of the algorithms is to maximize the profit of
              infrastructure providers. We develop a discrete event VNE
              simulator to implement and evaluate performance of MaVEn-M,
              MaVEn-S, and several recently proposed VNE algorithms. We
              introduce profitability as a new performance metric that captures
              both acceptance and revenue to cost ratios. Simulation results
              show that the proposed algorithms find more profitable solutions
              than the existing algorithms. Given additional computation time,
              they further improve embedding solutions.",
  journal  = "IEEE Trans Cybern",
  volume   =  48,
  number   =  2,
  pages    = "510--521",
  month    =  feb,
  year     =  2018,
  file     = "All Papers/H/Haeri and Trajkovic 2018 - Virtual Network Embedding via Monte Carlo Tree Search.pdf",
  keywords = "MLNetworking",
  language = "en",
  issn     = "2168-2275, 2168-2267",
  pmid     = "28237939",
  doi      = "10.1109/TCYB.2016.2645123"
}

@ARTICLE{Avizienis2004-ct,
  title    = "Basic concepts and taxonomy of dependable and secure computing",
  author   = "Avizienis, A and Laprie, J-C and Randell, B and Landwehr, C",
  abstract = "This paper gives the main definitions relating to dependability,
              a generic concept including a special case of such attributes as
              reliability, availability, safety, integrity, maintainability,
              etc. Security brings in concerns for confidentiality, in addition
              to availability and integrity. Basic definitions are given first.
              They are then commented upon, and supplemented by additional
              definitions, which address the threats to dependability and
              security (faults, errors, failures), their attributes, and the
              means for their achievement (fault prevention, fault tolerance,
              fault removal, fault forecasting). The aim is to explicate a set
              of general concepts, of relevance across a wide range of
              situations and, therefore, helping communication and cooperation
              among a number of scientific and technical communities, including
              ones that are concentrating on particular types of system, of
              system failures, or of causes of system failures.",
  journal  = "IEEE Trans. Dependable Secure Comput.",
  volume   =  1,
  number   =  1,
  pages    = "11--33",
  month    =  jan,
  year     =  2004,
  file     = "All Papers/A/Avizienis et al. 2004 - Basic concepts and taxonomy of dependable and secure computing.pdf",
  keywords = "Taxonomy;Availability;Fault
              tolerance;Safety;Maintenance;Communication system
              security;Uncertainty;Standardization;Books;Index Terms-
              Dependability;security;trust;faults;errors;failures;vulnerabilities;attacks;fault
              tolerance;fault removal;fault
              forecasting.;FutureInternet;GDS;ServicesDescription",
  issn     = "1941-0018",
  doi      = "10.1109/TDSC.2004.2"
}

@ARTICLE{Xu2014-za,
  title    = "Internet of Things in Industries: A Survey",
  author   = "Xu, L D and He, W and Li, S",
  abstract = "Internet of Things (IoT) has provided a promising opportunity to
              build powerful industrial systems and applications by leveraging
              the growing ubiquity of radio-frequency identification (RFID),
              and wireless, mobile, and sensor devices. A wide range of
              industrial IoT applications have been developed and deployed in
              recent years. In an effort to understand the development of IoT
              in industries, this paper reviews the current research of IoT,
              key enabling technologies, major IoT applications in industries,
              and identifies research trends and challenges. A main
              contribution of this review paper is that it summarizes the
              current state-of-the-art IoT in industries systematically.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  10,
  number   =  4,
  pages    = "2233--2243",
  month    =  nov,
  year     =  2014,
  file     = "All Papers/X/Xu et al. 2014 - Internet of Things in Industries - A Survey.pdf",
  keywords = "engineering computing;Internet of Things;mobile
              handsets;radiofrequency identification;ubiquitous
              computing;Internet of things;industrial systems;radio-frequency
              identification;RFID;sensor devices;mobile devices;wireless
              devices;industrial IoT applications;Radiofrequency
              identification;Big data;Wireless sensor networks;Service-oriented
              architecture;Wireless communication;Big data analytics;enterprise
              systems;information and communications technology
              (ICT);industrial informatics;internet of things (IoT);near field
              communications;radio-frequency identification (RFID);wireless
              sensor networks (WSNs)",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2014.2300753"
}

@ARTICLE{Sisinni2018-sl,
  title    = "Industrial Internet of Things: Challenges, Opportunities, and
              Directions",
  author   = "Sisinni, Emiliano and Saifullah, Abusayeed and Han, Song and
              Jennehag, Ulf and Gidlund, Mikael",
  abstract = "Internet of Things (IoT) is an emerging domain that promises
              ubiquitous connection to the Internet, turning common objects
              into connected devices. The IoT paradigm is changing the way
              people interact with things around them. It paves the way for
              creating pervasively connected infrastructures to support
              innovative services and promises better flexibility and
              efficiency. Such advantages are attractive not only for consumer
              applications, but also for the industrial domain. Over the last
              few years, we have been witnessing the IoT paradigm making its
              way into the industry marketplace with purposely designed
              solutions. In this paper, we clarify the concepts of IoT,
              Industrial IoT, and Industry 4.0. We highlight the opportunities
              brought in by this paradigm shift as well as the challenges for
              its realization. In particular, we focus on the challenges
              associated with the need of energy efficiency, real-time
              performance, coexistence, interoperability, and security and
              privacy. We also provide a systematic overview of the
              state-of-the-art research efforts and potential research
              directions to solve Industrial IoT challenges.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  14,
  number   =  11,
  pages    = "4724--4734",
  month    =  nov,
  year     =  2018,
  file     = "All Papers/S/Sisinni et al. 2018 - Industrial Internet of Things - Challenges, Opportunities, and Directions.pdf",
  keywords = "Industries;Reliability;Internet of Things;Informatics;Real-time
              systems;Production;Industrial internet of things (IIoT);real-time
              communication;reliability;security;wireless sensor network
              (WSN);Wireless",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2018.2852491"
}

@ARTICLE{Tao2019-xi,
  title    = "Digital Twin in Industry: {State-of-the-Art}",
  author   = "Tao, F and Zhang, H and Liu, A and Nee, A Y C",
  abstract = "Digital twin (DT) is one of the most promising enabling
              technologies for realizing smart manufacturing and Industry 4.0.
              DTs are characterized by the seamless integration between the
              cyber and physical spaces. The importance of DTs is increasingly
              recognized by both academia and industry. It has been almost 15
              years since the concept of the DT was initially proposed. To
              date, many DT applications have been successfully implemented in
              different industries, including product design, production,
              prognostics and health management, and some other fields.
              However, at present, no paper has focused on the review of DT
              applications in industry. In an effort to understand the
              development and application of DTs in industry, this paper
              thoroughly reviews the state-of-the-art of the DT research
              concerning the key components of DTs, the current development of
              DTs, and the major DT applications in industry. This paper also
              outlines the current challenges and some possible directions for
              future work.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  15,
  number   =  4,
  pages    = "2405--2415",
  month    =  apr,
  year     =  2019,
  file     = "All Papers/T/Tao et al. 2019 - Digital Twin in Industry - State-of-the-Art.pdf",
  keywords = "cyber-physical systems;manufacturing systems;product
              design;production engineering computing;sensor fusion;digital
              twin;smart manufacturing;product design;cyber-physical
              spaces;industry 4.0;health management;prognostics management;data
              fusion;Data models;Industries;Computational modeling;Smart
              manufacturing;Patents;History;Data fusion;digital twin
              (DT);industry application;modeling",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2018.2873186"
}

@ARTICLE{Beltramelli2021-fh,
  title    = "{LoRa} Beyond {ALOHA}: An Investigation of Alternative Random
              Access Protocols",
  author   = "Beltramelli, L and Mahmood, A and {\"O}sterberg, P and Gidlund, M",
  abstract = "In this article, we present a stochastic geometry-based model to
              investigate alternative medium access choices for LoRaWAN---a
              widely adopted low-power wide-area network (LPWAN) technology for
              the Internet-of-Things. LoRaWAN adoption is driven by its
              simplified network architecture, air interface, and medium
              access. The physical layer, known as Long Range (LoRa), provides
              quasi-orthogonal virtual channels through spreading factors (SFs)
              and time-power capture gains. However, the adopted pure ALOHA
              access mechanism suffers, in terms of scalability, under the
              same-channel same-SF transmissions from a large number of
              devices. In this article, our objective is to explore access
              mechanisms beyond-ALOHA for LoRaWAN. Using recent results on
              time- and power-capture effects of LoRa, we develop a unified
              model for the comparative study of other choices, i.e., slotted
              ALOHA and carrier-sense multiple access (CSMA). The model
              includes the necessary design parameters of these access
              mechanisms, such as guard time and synchronization accuracy for
              slotted ALOHA, carrier sensing threshold for CSMA. It also
              accounts for the spatial interaction of devices in annular shaped
              regions, characteristic of LoRa, for CSMA. The performance
              metrics derived from the model in terms of coverage probability,
              channel throughput, and energy efficiency are validated using
              Monte-Carlo simulations. Our analysis shows that slotted ALOHA
              indeed has higher reliability than pure ALOHA but at the cost of
              lower energy efficiency for low device densities. Whereas, CSMA
              outperforms slotted ALOHA at smaller SFs in terms of reliability
              and energy efficiency, with its performance degrading to pure
              ALOHA at higher SFs.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  5,
  pages    = "3544--3554",
  month    =  may,
  year     =  2021,
  file     = "All Papers/B/Beltramelli et al. 2021 - LoRa Beyond ALOHA - An Investigation of Alternative Random Access Protocols.pdf",
  keywords = "Interference;Multiaccess communication;Logic
              gates;Modulation;Analytical
              models;Scalability;Synchronization;Energy
              efficiency;Internet-of-Things;LoRa;low-power wide-area
              networks;medium access",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.2977046"
}

@ARTICLE{Chang2021-un,
  title    = "Dynamic Resource Allocation and Computation Offloading for {IoT}
              Fog Computing System",
  author   = "Chang, Z and Liu, L and Guo, X and Sheng, Q",
  abstract = "Fog computing system is able to facilitate computation-intensive
              applications and emerges as one of the promising technology for
              realizing the Internet of Things (IoT). By offloading the
              computational tasks to the fog node (FN) at the network edge,
              both the service latency and energy consumption can be improved,
              which is significant for industrial IoT applications. However,
              the dynamics of computational resource usages in the FN, the
              radio environment and the energy in the battery of IoT devices
              make the offloading mechanism design become challenging.
              Therefore, in this article, we propose a dynamic optimization
              scheme for the IoT fog computing system with multiple mobile
              devices (MDs), where the radio and computational resources, and
              offloading decisions, can be dynamically coordinated and
              allocated with the variation of radio resources and computation
              demands. Specifically, with the objective to minimize the system
              cost related to latency, energy consumption, and weights of MDs,
              we propose a joint computation offloading and radio resource
              allocation algorithm based on Lyapunov optimization. Through
              minimizing the derived upper bound of the Lyapunov
              drift-plus-penalty function, we divide the main problem into
              several subproblems at each time slot and address them
              accordingly. Through performance evaluation, the effectiveness of
              the proposed scheme can be verified.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  5,
  pages    = "3348--3357",
  month    =  may,
  year     =  2021,
  keywords = "Edge computing;Resource management;Optimization;Cloud
              computing;Task analysis;Energy consumption;Computational
              modeling;Dynamic computation offloading;edge computing;energy
              harvesting;fog computing;Lyapunov optimization;resource
              allocation",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.2978946"
}

@ARTICLE{Sengupta2021-xg,
  title    = "A Secure {Fog-Based} Architecture for Industrial Internet of
              Things and Industry 4.0",
  author   = "Sengupta, J and Ruj, S and Bit, S D",
  abstract = "The advent of Industrial Internet of Things (IIoT) along with
              cloud computing has brought a huge paradigm shift in
              manufacturing industries resulting in yet another industrial
              revolution, Industry 4.0. Huge amounts of delay-sensitive data of
              diverse nature are being generated, which need to be locally
              processed and secured because of their sensitivity. However, the
              low-end Internet of Things devices are unable to handle huge
              computational overheads. In addition, the semi-trusted nature of
              cloud introduces several security concerns. To address these
              issues, this article proposes a secure fog-based IIoT
              architecture by suitably plugging a number of security features
              into it and by offloading some of the tasks judiciously to fog
              nodes. These features secure the system alongside reducing the
              trust and burden on the cloud and resource-constrained devices,
              respectively. We validate our proposed architecture through both
              theoretical overhead analysis and practical experimentation,
              including simulation study and testbed implementation.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  4,
  pages    = "2316--2324",
  month    =  apr,
  year     =  2021,
  annote   = "Yet another one...",
  file     = "All Papers/S/Sengupta et al. 2021 - A Secure Fog-Based Architecture for Industrial Internet of Things and Industry 4.0",
  keywords = "cloud computing;Internet;Internet of Things;manufacturing
              industries;production engineering computing;security of
              data;low-end Internet;huge computational overheads;security
              concerns;secure fog-based IIoT architecture;security
              features;secure fog-based architecture;cloud computing;huge
              paradigm shift;industrial revolution;delay-sensitive data;diverse
              nature;Cloud computing;Computer
              architecture;Industries;Performance
              evaluation;Authentication;Task analysis;Fog computing;homomorphic
              encryption;Industrial Internet of Things (IIoT);Industry
              4.0;proxy re-encryption;secure aggregation",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.2998105"
}

@ARTICLE{Wang2021-cj,
  title    = "Fog Nodes Deployment Based on {Space--Time} Characteristics in
              Smart Factory",
  author   = "Wang, J and Li, D and Hu, Y",
  abstract = "The emergence of fog computing has improved the performance and
              efficiency of intelligent manufacturing. In a fog computing
              platform, fog nodes can provide the distributed and proximity
              resources for the tasks related to intelligent manufacturing. Due
              to the heterogeneous characteristics of the fog nodes and the
              space-time characteristics of intelligent manufacturing, the
              effective deployment of the fog nodes is a precondition for the
              fog computing implementation and it is also a key to provide
              high-performance services with the fog computing platform.
              However, traditional deployment strategies cannot satisfy the
              performance requirements of intelligent manufacturing systems. In
              this article, the problem of the fog nodes deployment is studied
              and a fog nodes deployment strategy based on the space-time
              characteristics (TSBP) is proposed. A fog nodes deployment system
              model is set up and the objective function of the fog nodes
              deployment is built up, in which the optimization goal is to
              minimize the computing response time and realize the load
              balancing of the fog nodes. In addition, the discrete
              differential evolution algorithm is applied to search the optimal
              fog nodes deployment solution. Finally, the effectiveness of the
              proposed TSBP strategy is verified using a candy packaging
              intelligent production line prototype platform.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  5,
  pages    = "3534--3543",
  month    =  may,
  year     =  2021,
  keywords = "Edge computing;Production;Servers;Smart
              manufacturing;Optimization;Real-time systems;Fog computing;nodes
              deployment;smart factory;space--time characteristics",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.2999310"
}

@ARTICLE{Qian2021-nl,
  title    = "{NOMA} Assisted {Multi-Task} {Multi-Access} Mobile Edge Computing
              via Deep Reinforcement Learning for Industrial Internet of Things",
  author   = "Qian, Liping and Wu, Yuan and Jiang, Fuli and Yu, Ningning and
              Lu, Weidang and Lin, Bin",
  abstract = "Multiaccess mobile edge computing (MA-MEC) has been envisioned as
              one of the key approaches for enabling computation-intensive yet
              delay-sensitive services in future industrial Internet of Things
              (IoT). In this article, we exploit nonorthogonal multiple access
              (NOMA) for computation offloading in MA-MEC and propose a joint
              optimization of the multiaccess multitask computation offloading,
              NOMA transmission, and computation-resource allocation, with the
              objective of minimizing the total energy consumption of IoT
              device to complete its tasks subject to the required latency
              limit. We first focus on a static channel scenario and propose a
              distributed algorithm to solve the joint optimization problem by
              identifying the layered structure of the formulated nonconvex
              problem. Furthermore, we consider a dynamic channel scenario in
              which the channel power gains from the IoT device to the
              edge-computing servers are time varying. To tackle with the
              difficulty due to the huge number of different channel
              realizations in the dynamic scenario, we propose an online
              algorithm, which is based on deep reinforcement learning (DRL),
              to efficiently learn the near-optimal offloading solutions for
              the time-varying channel realizations. Numerical results are
              provided to validate our distributed algorithm for the static
              channel scenario and the DRL-based online algorithm for the
              dynamic channel scenario. We also demonstrate the advantage of
              the NOMA assisted multitask MA-MEC against conventional
              orthogonal multiple access scheme under both static and dynamic
              channels.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  8,
  pages    = "5688--5698",
  month    =  aug,
  year     =  2021,
  keywords = "Task analysis;NOMA;Resource management;Servers;Heuristic
              algorithms;Optimization;Edge computing;Deep reinforcement
              learning;energy consumption optimization;multi-access mobile edge
              computing;non-orthogonal multiple access;Wireless",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3001355"
}

@ARTICLE{Liu2021-xl,
  title    = "The Meta Distribution of the {Signal-to-Interference} Ratio for
              Long Range Wide Area Networks With Power Control",
  author   = "Liu, Q and Ball, E A",
  abstract = "To reduce energy consumption, a device in a long range wide area
              network (LoRaWAN) needs to adjust its transmit power according to
              the distance from its tagged gateway. It is important to measure
              the performance of the LoRaWANs uplink with power control. In
              this article, we focus on the analysis of the coverage
              probability and the meta distribution of the
              signal-to-interference ratio (SIR) for a LoRaWAN uplink with
              fractional power control (FPC). The LoRaWAN uplink is analyzed
              based on the Poisson point process. We present the possible
              reductions in transmit power of devices whilst ensuring that the
              received signal power is greater than the receiver sensitivity.
              We derive the coverage probability of a LoRaWAN uplink and show
              how power control influences it. Finally, utilizing the meta
              distribution of SIR, the fine-grained information of the LoRaWAN
              is revealed. The results show that the power control greatly
              increases the successful probability of edge-devices with little
              effect on the probability of inner-devices if an appropriate FPC
              coefficient is chosen. This is because the LoRa signal can be
              demodulated at a very low required SIR threshold.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  4,
  pages    = "2579--2586",
  month    =  apr,
  year     =  2021,
  keywords = "Power control;Uplink;Receivers;Cellular networks;Signal to noise
              ratio;Interference;Analytical models;Long range wide area network
              (LoRaWAN);meta distribution;Poisson point process (PPP);power
              control;uplink",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3003474"
}

@ARTICLE{Nie2021-qd,
  title    = "A Reinforcement {Learning-Based} Network Traffic Prediction
              Mechanism in Intelligent Internet of Things",
  author   = "Nie, L and Ning, Z and Obaidat, M S and Sadoun, B and Wang, H and
              Li, S and Guo, L and Wang, G",
  abstract = "Intelligent Internet of Things (IIoT) is comprised of various
              wireless and wired networks for industrial applications, which
              makes it complex and heterogeneous.The openness of IIoT has led
              to the intractable problems of network security and management.
              Many network security and management functions rely on network
              traffic prediction techniques, such as anomaly detection and
              predictive network planning. Predicting IIoT network traffic is
              significantly difficult because its frequently updated topology
              and diversified services lead to irregular network traffic
              fluctuations. Motivated by these observations, we proposed a
              reinforcement learning-based mechanism in this article. We
              modeled the network traffic prediction problem as a Markov
              decision process, and then, predicted network traffic by Monte
              Carlo Q-learning. Furthermore, we addressed the real-time
              requirement of the proposed mechanism and we proposed a
              residual-based dictionary learning algorithm to improve the
              complexity of Monte Carlo Q-learning. Finally, the effectiveness
              of our mechanism was evaluated using the real network traffic.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  3,
  pages    = "2169--2180",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/N/Nie et al. 2021 - A Reinforcement Learning-Based Network Traffic Prediction Mechanism in Intelligent Internet of Things.pdf",
  keywords = "communication complexity;computer network management;computer
              network security;decision theory;Internet of Things;learning
              (artificial intelligence);Markov processes;Monte Carlo
              methods;prediction theory;radio networks;telecommunication
              network planning;telecommunication network
              topology;telecommunication traffic;frequently updated
              topology;irregular network traffic fluctuations;reinforcement
              learning-based network traffic prediction mechanism;intelligent
              Internet of Things;wireless networks;network security;management
              functions;network traffic prediction techniques;predictive
              network planning;wired networks;Monte Carlo Q-learning
              method;IIoT network traffic prediction;Markov decision
              process;Feature extraction;Autoregressive processes;Predictive
              models;Prediction algorithms;Security;Communication
              networks;Industrial applications;intelligent Internet of Things
              (IIOT);network traffic prediction;reinforcement learning (RL)",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3004232"
}

@ARTICLE{Pan2021-vk,
  title    = "Timely Information Update With Nonorthogonal Multiple Access",
  author   = "Pan, H and Liang, J and Liew, S C and Leung, V C M and Li, J",
  abstract = "This article studies information freshness in information update
              systems with nonorthogonal multiple access (NOMA). Information
              freshness is characterized by age of information (AoI), defined
              as the time elapsed since the generation of the last successfully
              received update. Conventional orthogonal multiple access (OMA)
              systems, say time-division multiple access (TDMA) systems, lead
              to high average AoI when a large number of users take turns to
              transmit their latest samples to a common receiver over a
              wireless medium. In contrast to OMA, NOMA allows multiple users
              to transmit simultaneously. Although NOMA could lead to higher
              packet error rates (PER) due to the wireless interference among
              users, we show that higher PERs do not always lead to a higher
              average AoI. Specifically, our experiments on software-defined
              radio indicate that NOMA with conventional multiuser decoding
              (MUD) techniques leads to higher PERs but lower average AoI than
              OMA does in the high SNR regime. Furthermore, to improve the AoI
              performance in the medium SNR regime, we combine MUD with
              physical-layer network coding (PNC), a technique that turns
              wireless interference into useful network-coding information. PNC
              works well even when the SNRs of different NOMA users do not
              differ much. This article is the first attempt to apply PNC to
              information update systems. Experiments show that the combined
              use of PNC and MUD reduces the average AoI significantly in a
              practical network setting. Overall, PNC-enabled NOMA is a
              promising solution to information update systems.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  6,
  pages    = "4096--4106",
  month    =  jun,
  year     =  2021,
  keywords = "Multiuser detection;Wireless communication;NOMA;Time division
              multiple access;Receivers;Decoding;Signal to noise ratio;Age of
              information (AoI);information freshness;nonorthogonal multiple
              access (NOMA);physical-layer network coding
              (PNC);Mobile\_Wireless",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3006061"
}

@ARTICLE{Ghosh2021-mw,
  title    = "{Edge-Cloud} Computing for Internet of Things Data Analytics:
              Embedding Intelligence in the Edge With Deep Learning",
  author   = "Ghosh, A M and Grolinger, K",
  abstract = "Rapid growth in numbers of connected devices including sensors,
              mobile, wearable, and other Internet of Things (IoT) devices, is
              creating an explosion of data that are moving across the network.
              To carry out machine learning (ML), IoT data are typically
              transferred to the cloud or another centralized system for
              storage and processing; however, this causes latencies and
              increases network traffic. Edge computing has the potential to
              remedy those issues by moving computation closer to the network
              edge and data sources. On the other hand, edge computing is
              limited in terms of computational power, and thus, is not
              well-suited for ML tasks. Consequently, this article aims to
              combine edge and cloud computing for IoT data analytics by taking
              advantage of edge nodes to reduce data transfer. In order to
              process data close to the source, sensors are grouped according
              to locations, and feature learning is performed on the close by
              edge node. For comparison reasons, similarity-based processing is
              also considered. Feature learning is carried out with deep
              learning - the encoder part of the trained autoencoder is placed
              on the edge and the decoder part is placed on the cloud. The
              evaluation was performed on the task of human activity
              recognition from sensor data. The results show that when sliding
              windows are used in the preparation step, data can be reduced on
              the edge up to 80\% without significant loss in accuracy.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  3,
  pages    = "2191--2200",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/G/Ghosh and Grolinger 2021 - Edge-Cloud Computing for Internet of Things Data Analytics - Embedding Intelligence in the Edge With Deep Learning.pdf",
  keywords = "cloud computing;data analysis;Internet of Things;learning
              (artificial intelligence);edge-cloud computing;Internet of Things
              data analytics;embedding intelligence;deep learning;connected
              devices;Internet of Things devices;machine learning;network
              traffic;network edge;data sources;computational power;ML
              tasks;IoT data analytics;edge node;data transfer
              reduction;feature learning;similarity-based processing;sensor
              data;centralized system;trained autoencoder;human activity
              recognition;sliding windows;Cloud computing;Edge computing;Task
              analysis;Data analysis;Internet of Things;Sensors;Computational
              modeling;Autoencoders (AEs);data reduction;deep learning
              (DL);edge computing (EC);human activity recognition
              (HAR);Internet of Things (IoT);MLNetworking;EdgeFogCloudIoT",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3008711"
}

@ARTICLE{Wang2021-nk,
  title    = "{Data-Augmentation-Based} Cellular Traffic Prediction in
              {Edge-Computing-Enabled} Smart City",
  author   = "Wang, Z and Hu, J and Min, G and Zhao, Z and Wang, J",
  abstract = "With the massive deployment of 5G cellular infrastructures,
              traffic prediction has become an indispensable part of the
              cellular resource management system in order to provide reliable
              and fast communication services that can meet the increasing
              quality-of-service requirements of smart city. A promising
              approach for handling this problem is to introduce intelligent
              methods to implement a highly effective and efficient cellular
              traffic prediction model. Meanwhile, integrating the multiaccess
              edge computing framework in 5G cellular networks facilitates the
              application of intelligent traffic prediction models by enabling
              their implementation at the network edge. However, the data
              shortage and privacy issues may still be obstacles for training a
              robust and accurate prediction model at the edge. To address
              these issues, we propose a data-augmentation-based cellular
              traffic prediction model (ctGAN-S2S), where an effective data
              augmentation submodel based on generative adversarial networks is
              proposed to improve the prediction performance while protecting
              data privacy, and a long-short-term-memory-based
              sequence-to-sequence submodel is used to achieve the flexible
              multistep cellular traffic prediction. The experimental results
              on a real-world city-scale cellular traffic dataset reveal that
              our ctGAN-S2S model achieves up to 48.49\% improvement of the
              prediction accuracy compared to four typical reference models.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  6,
  pages    = "4179--4187",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/W/Wang et al. 2021 - Data-Augmentation-Based Cellular Traffic Prediction in Edge-Computing-Enabled Smart City.pdf",
  keywords = "Predictive models;Data models;5G mobile communication;Smart
              cities;Cellular networks;Computational
              modeling;Reliability;Cellular networks;data augmentation;neural
              networks;smart city;time-series prediction",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3009159"
}

@ARTICLE{Lu2021-fd,
  title    = "{Communication-Efficient} Federated Learning for Digital Twin
              Edge Networks in Industrial {IoT}",
  author   = "Lu, Yunlong and Huang, Xiaohong and Zhang, Ke and Maharjan,
              Sabita and Zhang, Yan",
  abstract = "The rapid development of artificial intelligence and 5G paradigm,
              opens up new possibilities for emerging applications in
              industrial Internet of Things (IIoT). However, the large amount
              of data, the limited resources of Internet of Things devices, and
              the increasing concerns of data privacy, are major obstacles to
              improve the quality of services in IIoT. In this article, we
              propose the digital twin edge networks (DITENs) by incorporating
              digital twin into edge networks to fill the gap between physical
              systems and digital spaces. We further leverage the federated
              learning to construct digital twin models of IoT devices based on
              their running data. Moreover, to mitigate the communication
              overhead, we propose an asynchronous model update scheme and
              formulate the federated learning scheme as an optimization
              problem. We further decompose the problem and solve the
              subproblems based on the deep neural network model. Numerical
              results show that our proposed federated learning scheme for
              DITEN improves the communication efficiency and reduces the
              transmission energy cost.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  8,
  pages    = "5709--5718",
  month    =  aug,
  year     =  2021,
  keywords = "Computational modeling;Data models;Optimization;Servers;Data
              privacy;Edge computing;Machine learning;Communication
              efficiency;digital twin;energy cost;federated learning;Industrial
              Internet of Things (IIOT);Wireless",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3010798"
}

@ARTICLE{Romanov2021-ll,
  title    = "A Precise Synchronization Method for Future Wireless {TSN}
              Networks",
  author   = "Romanov, A M and Gringoli, F and Sikora, A",
  abstract = "Time-sensitive networking (TSN) is the most promising
              time-deterministic wired communication approach for industrial
              applications. To extend TSN to ``IEEE 802.11'' wireless networks,
              two challenging problems must be solved: synchronization and
              scheduling. This article is focused on the first one. Even though
              a few solutions already meet the required synchronization
              accuracies, they are built on expensive hardware that is not
              suited for mass market products. While next Wi-Fi generation
              might support the required functionalities, this article proposes
              a novel method that makes high-precision wireless synchronization
              using commercial low-cost components possible. With the proposed
              solution, a standard deviation of synchronization error of less
              than 500 ns can be achieved for many use cases and system loads
              on both CPU and network. This performance is comparable to modern
              wired real-time field buses, which makes the developed method a
              significant contribution for the extension of the TSN protocol to
              the wireless domain.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  5,
  pages    = "3682--3692",
  month    =  may,
  year     =  2021,
  file     = "All Papers/R/Romanov et al. 2021 - A Precise Synchronization Method for Future Wireless TSN Networks.pdf",
  keywords = "Synchronization;Wireless communication;Hardware;Wireless sensor
              networks;Protocols;Field programmable gate arrays;IEEE 802.11
              Standard;Communication;field bus;firmware;field-programmable gate
              array (FPGA);jitter;real-time
              systems;synchronization;Wi-Fi;wireless",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3017016"
}

@ARTICLE{Shin2021-vm,
  title    = "An {OPC} {UA-Compliant} Interface of Data Analytics Models for
              Interoperable Manufacturing Intelligence",
  author   = "Shin, S-J",
  abstract = "The open platform communications unified architecture (OPC UA)
              has received attention as a standard for data interoperability in
              industries. In particular, OPC UA extends its applicability
              across various industrial sectors by publishing OPC UA companion
              specifications created in collaboration with other industrial
              consortiums. However, OPC UA is limited to ensure the
              interoperability of data analytics models because the relevant
              companion specifications have not been developed yet. OPC UA
              should be extended to implement such model interoperability so
              that machines seamlessly use and share data analytics models
              across the layers of manufacturing systems to predict and
              optimize their performance autonomously and collaboratively in
              terms of interoperable manufacturing intelligence. This article
              proposes an OPC UA-compliant interface for the exchange of
              predictive model markup language (PMML), a domain-independent
              standard for representing XML-based data analytics models. This
              article includes the design of mapping rules and OPC UA
              information models for the exchange between PMML and OPC UA, as
              well as the implementation of an OPC UA server-client prototype
              to publish and subscribe to OPC UA-compliant regression and
              neural network models which have been transformed from PMML-based
              models.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  5,
  pages    = "3588--3598",
  month    =  may,
  year     =  2021,
  keywords = "Data models;Analytical models;Data analysis;Predictive
              models;Interoperability;Manufacturing systems;Cyber--physical
              production systems;data analytics;manufacturing
              intelligence;model interoperability;open platform communications
              unified architecture (OPC UA);predictive model markup language
              (PMML)",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3024628"
}

@ARTICLE{Wang2021-wq,
  title    = "{MPCSM}: Microservice Placement for {Edge-Cloud} Collaborative
              Smart Manufacturing",
  author   = "Wang, Yimeng and Zhao, Cong and Yang, Shusen and Ren, Xuebin and
              Wang, Luhui and Zhao, Peng and Yang, Xinyu",
  abstract = "Latency-aware service placement is promising in reducing the
              overall service response latency of proliferating edge-cloud
              collaborative smart manufacturing systems. However, intuitive
              latency estimators used by existing service placement approaches
              cannot accurately depict the nonlinear end-to-end (E2E) latency
              of multihop microservices with complex dependencies, which is
              severely hindering the effectiveness of latency-aware service
              placement. To address this issue, in this article, we present a
              microservice placement mechanism for edge-cloud collaborative
              smart manufacturing (MPCSM), where a microservice placement
              algorithm latency-aware edge-cloud collaborative placement
              supported by an accurate data-driven E2E latency estimation
              method is proposed. We build a real-world collaborative
              prototype, and conduct a case study on semiconductor
              manufacturing to elaborate the construction of our latency
              estimator. Results of extensive experiments demonstrate that the
              error of our E2E latency estimator is up to 10$\times$ less than
              that of existing ones, and the overall service latency with MPCSM
              is up to 10$\times$ less than that with existing service
              placement approaches.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  9,
  pages    = "5898--5908",
  month    =  sep,
  year     =  2021,
  keywords = "Collaboration;Smart manufacturing;Estimation;Cloud computing;Load
              modeling;Image edge detection;Computational modeling;Edge-cloud
              collaborative intelligence;edge computing;microservice
              placement;smart manufacturing;NFV",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3036406"
}

@ARTICLE{Li2021-ef,
  title    = "{Age-of-Information} Aware Scheduling for {Edge-Assisted}
              Industrial Wireless Networks",
  author   = "Li, Mingyan and Chen, Cailian and Wu, Huaqing and Guan, Xinping
              and Shen, Xuemin",
  abstract = "Industrial wireless networks (IWNs) have attracted significant
              attention for providing time-critical delivery services, which
              can benefit from device-to-device (D2D) communication for low
              transmission delay. In this article, a distributed scheduling
              problem is investigated for D2D-enabled IWNs, where D2D links
              have various age-of-information (AoI) constraints for information
              freshness. This problem is formulated as a constrained
              optimization problem to optimize D2D packet delivery over limited
              spectrum resources, which is intractable since D2D users have no
              prior knowledge of the operating environment. To tackle this
              problem, in this article, an AoI-aware scheduling scheme is
              proposed based on primal-dual optimization and actor--critic
              reinforcement learning. In specific, multiple local actors for
              D2D devices learn AoI-aware scheduling policies to make on-site
              decisions with their stochastic AoI constraints addressed in the
              dual domain. An edge-based critic estimates the performance of
              all actors' decision-making policies from a global view, which
              can effectively address the nonstationary environment caused by
              concurrent learning of multiple local actors. Theoretical
              analysis on the convergence of learning is provided and
              simulation results demonstrate the effectiveness of the proposed
              scheme.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  8,
  pages    = "5562--5571",
  month    =  aug,
  year     =  2021,
  keywords = "Performance evaluation;Job shop scheduling;Processor
              scheduling;Wireless networks;Simulation;Reinforcement
              learning;Quality of service;Age of information
              (AoI);device-to-device (D2D) communication;edge
              computing;industrial wireless network (IWN);reinforcement
              learning;Wireless",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3037299"
}

@ARTICLE{Hao2021-fz,
  title    = "Deep Reinforcement Learning for Edge Service Placement in
              Softwarized Industrial {Cyber-Physical} System",
  author   = "Hao, Yixue and Chen, Min and Gharavi, Hamid and Zhang, Yin and
              Hwang, Kai",
  abstract = "Future industrial cyber-physical system (CPS) devices are
              expected to request a large amount of delay-sensitive services
              that need to be processed at the edge of a network. Due to
              limited resources, service placement at the edge of the cloud has
              attracted significant attention. Although there are many methods
              of design schemes, the service placement problem in industrial
              CPS has not been well studied. Furthermore, none of existing
              schemes can optimize service placement, workload scheduling, and
              resource allocation under uncertain service demands. To address
              these issues, we first formulate a joint optimization problem of
              service placement, workload scheduling, and resource allocation
              in order to minimize service response delay. We then propose an
              improved deep Q-network (DQN)-based service placement algorithm.
              The proposed algorithm can achieve an optimal resource allocation
              by means of convex optimization where the service placement and
              workload scheduling decisions are assisted by means of DQN
              technology. The experimental results verify that the proposed
              algorithm, compared with existing algorithms, can reduce the
              average service response time by 8--10\%.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  8,
  pages    = "5552--5561",
  month    =  aug,
  year     =  2021,
  keywords = "Cloud computing;Job shop scheduling;Wireless sensor
              networks;Wireless communication;Optimization;Resource
              management;Delays;Deep reinforcement learning;edge
              cloud;industrial cyber-physical system (CPS);service
              placement;NFV",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3041713"
}

@ARTICLE{Magrin2021-li,
  title    = "Performance Analysis of {LoRaWAN} in Industrial Scenarios",
  author   = "Magrin, Davide and Capuzzo, Martina and Zanella, Andrea and
              Vangelista, Lorenzo and Zorzi, Michele",
  abstract = "In this article, we evaluate the performance of a LoRaWAN network
              in industrial scenarios where different Industrial Internet of
              Things (IIoT) end nodes communicate to a central controller in
              order to provide monitoring and sensing information to optimize
              the efficiency of industrial processes and reduce costs. In
              particular, we consider confirmed and unconfirmed traffic,
              multigateway deployments, the usage of different classes of
              devices, and a nonstandard channel plan. Furthermore, we analyze
              the higher-layer impact of different models of LoRa PHY layer
              with industrial channel models. We show that, with proper
              configuration, LoRaWAN is able to serve IIoT sensing applications
              with a packet success rate over 90\%, providing at the same time
              limited communication delays.",
  journal  = "IEEE Trans. Ind. Inf.",
  volume   =  17,
  number   =  9,
  pages    = "6241--6250",
  month    =  sep,
  year     =  2021,
  keywords = "Performance evaluation;Sensors;Robustness;Frequency
              modulation;Europe;Bandwidth;Network servers;Industrial
              communication;interchannel
              interference;LoRaWAN;ns-3;simulation;telecommunication network
              reliability;wireless sensor networks",
  issn     = "1941-0050",
  doi      = "10.1109/TII.2020.3044942"
}

@ARTICLE{Viterbi1967-gv,
  title    = "Error bounds for convolutional codes and an asymptotically
              optimum decoding algorithm",
  author   = "Viterbi, A",
  abstract = "The probability of error in decoding an optimal convolutional
              code transmitted over a memoryless channel is bounded from above
              and below as a function of the constraint length of the code. For
              all but pathological channels the bounds are asymptotically
              (exponentially) tight for rates aboveR\_0, the computational
              cutoff rate of sequential decoding. As a function of constraint
              length the performance of optimal convolutional codes is shown to
              be superior to that of block codes of the same length, the
              relative improvement increasing with rate. The upper bound is
              obtained for a specific probabilistic nonsequential decoding
              algorithm which is shown to be asymptotically optimum for rates
              aboveR\_0and whose performance bears certain similarities to that
              of sequential decoding algorithms.",
  journal  = "IEEE Trans. Inf. Theory",
  volume   =  13,
  number   =  2,
  pages    = "260--269",
  month    =  apr,
  year     =  1967,
  issn     = "0018-9448, 1557-9654",
  doi      = "10.1109/TIT.1967.1054010"
}

@ARTICLE{Dustdar2022-df,
  title    = "On distributed computing continuum systems",
  author   = "Dustdar, Schahram and Casamajor Pujol, Victor and Donta, Praveen
              Kumar",
  abstract = "This article presents our vision on the need of developing new
              managing technologies to harness distributed computing continuum
              systems. These systems are concurrently executed in multiple
              computing tiers: Cloud, Fog, Edge and IoT. This simple idea
              develops manifold challenges due to the inherent complexity
              inherited from the underlying infrastructures of these systems.
              This makes inappropriate the use of current methodologies for
              managing Internet distributed systems, which are based on the
              early systems that were based on client/server architectures and
              were completely specified by the application software. We present
              a new methodology to manage distributed computing continuum
              systems. This is based on a mathematical artifact called Markov
              Blanket, which sets these systems in a Markovian space, more
              suitable to cope with their complex characteristics. Furthermore,
              we develop the concept of equilibrium for these systems,
              providing a more flexible management framework compared with the
              one based on thresholds, currently in use for Internet-based
              distributed systems. Finally, we also link the equilibrium with
              the development of adaptive mechanisms. However, we are aware
              that developing the entire methodology requires a big effort and
              the use of learning techniques, therefore, we finish this article
              with an overview of the techniques required to develop this
              methodology.",
  journal  = "IEEE Trans. Knowl. Data Eng.",
  pages    = "1--1",
  year     =  2022,
  file     = "All Papers/D/Dustdar et al. 2022 - On distributed computing continuum systems.pdf",
  keywords = "Cloud computing;Costs;Computer architecture;Processor
              scheduling;Pipelines;Medical services;Job shop
              scheduling;Distributed Systems;Computing continuum;Edge
              Computing;Markov Blanket;ServicesDescription",
  issn     = "1041-4347, 1558-2191",
  doi      = "10.1109/TKDE.2022.3142856"
}

@ARTICLE{Jiang2017-nx,
  title    = "Optimal Cooperative Content Caching and Delivery Policy for
              Heterogeneous Cellular Networks",
  author   = "Jiang, W and Feng, G and Qin, S",
  abstract = "To address the explosively growing demand for mobile data
              services in the 5th generation (5G) mobile communication system,
              it is important to develop efficient content caching and
              distribution techniques, aiming at significantly reducing
              redundant data transmissions and improving content delivery
              efficiency. In heterogeneous cellular network (HetNet), which has
              been deemed as a promising architectural technique for 5G,
              caching some popular content items at femto base-stations (FBSs)
              and even at user equipment (UE) can be exploited to alleviate the
              burden of backhaul and to reduce the costly transmissions from
              the macro base-stations to UEs. In this paper, we develop the
              optimal cooperative content caching and delivery policy, for
              which FBSs and UEs are all engaged in local content caching. We
              formulate the cooperative content caching problem as an
              integer-linear programming problem, and use hierarchical
              primal-dual decomposition method to decouple the problem into two
              level optimization problems, which are solved by using the
              subgradient method. Furthermore, we design the optimal content
              delivery policy, which is formulated as an unbalanced assignment
              problem and solved by using Hungarian algorithm. Numerical
              results have shown that the proposed cooperative content caching
              and delivery policy can significantly improve content delivery
              performance in comparison with existing caching strategies.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  16,
  number   =  5,
  pages    = "1382--1393",
  month    =  may,
  year     =  2017,
  file     = "All Papers/J/Jiang et al. 2017 - Optimal Cooperative Content Caching and Delivery Policy for Heterogeneous Cellular Networks.pdf",
  keywords = "5G mobile communication;cooperative communication;femtocellular
              radio;optimisation;telecommunication services;heterogeneous
              cellular networks;optimal cooperative content caching;delivery
              policy;mobile data services;5th generation mobile communication
              system;distribution techniques;redundant data
              transmissions;content delivery efficiency;femto
              base-stations;FBS;user equipment;local content
              caching;cooperative content caching problem;integer-linear
              programming problem;hierarchical primal-dual
              decomposition;optimization problems;subgradient method;unbalanced
              assignment problem;Hungarian algorithm;Mobile
              computing;Femtocells;Servers;Signal to noise ratio;5G mobile
              communication;5G;caching;content
              delivery;HetNets;optimization;EdgeFogCloudIoT",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2016.2597851"
}

@ARTICLE{Bui2018-uc,
  title    = "{Data-Driven} Evaluation of Anticipatory Networking in {LTE}
              Networks",
  author   = "Bui, N and Widmer, J",
  abstract = "Anticipatory networking is a recent branch of network
              optimization based on prediction of the system state. Our work
              specifically tackles prediction-driven resource allocation for
              mobile networks. While some anticipatory networking concepts have
              been proposed in the literature, understanding of the potential
              real world gains is so far very limited. Future mobile networks
              will likely integrate such mechanisms, and thus it is of
              paramount importance to understand the actual performance
              improvements and in which scenarios they can be realized.
              Analyzing a month of LTE control channel information collected in
              four locations, we show how anticipatory networking can enhance
              current LTE networks. First, we propose a comprehensive
              optimization framework encompassing different forecasting
              solutions. Then, we provide a thorough analysis of the aggregated
              network traffic and the contributions of individual users. In
              particular, we show that predictable traffic accounts for more
              than 95 percent of the total traffic volume and that simple
              prediction and optimization techniques allow network operators to
              save 50 percent of the resources and/or on average more than
              double the offered data rate in our data set.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  17,
  number   =  10,
  pages    = "2252--2265",
  month    =  oct,
  year     =  2018,
  file     = "All Papers/B/Bui and Widmer 2018 - Data-Driven Evaluation of Anticipatory Networking in LTE Networks.pdf",
  keywords = "Long Term Evolution;mobile computing;optimisation;resource
              allocation;telecommunication traffic;wireless channels;total
              traffic volume;comprehensive optimization framework;system state
              prediction;LTE networks;aggregated network traffic;LTE control
              channel information;future mobile networks;anticipatory
              networking concepts;prediction-driven resource allocation;network
              optimization;data-driven evaluation;Optimization;Long Term
              Evolution;Mobile computing;Linear programming;Resource
              management;Analytical models;Data
              models;LTE;prediction;anticipatory
              networking;measurements;optimization;mobile
              networks;sniffer;GeneralNetworking",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2018.2809750"
}

@ARTICLE{Wang2019-ne,
  title    = "{Spatio-Temporal} Analysis and Prediction of Cellular Traffic in
              Metropolis",
  author   = "Wang, X and Zhou, Z and Xiao, F and Xing, K and Yang, Z and Liu,
              Y and Peng, C",
  abstract = "Understanding and predicting cellular traffic at large-scale and
              fine-granularity is beneficial and valuable to mobile users,
              wireless carriers, and city authorities. Predicting cellular
              traffic in modern metropolis is particularly challenging because
              of the tremendous temporal and spatial dynamics introduced by
              diverse user Internet behaviors and frequent user mobility
              citywide. In this paper, we characterize and investigate the root
              causes of such dynamics in cellular traffic through a big
              cellular usage dataset covering 1.5 million users and 5,929 cell
              towers in a major city of China. We reveal intensive
              spatio-temporal dependency even among distant cell towers, which
              is largely overlooked in previous works. To explicitly
              characterize and effectively model the spatio-temporal dependency
              of urban cellular traffic, we propose a novel decomposition of
              in-cell and inter-cell data traffic, and apply a graph-based deep
              learning approach to accurate cellular traffic prediction.
              Experimental results demonstrate that our method consistently
              outperforms the state-of-the-art time-series based approaches and
              we also show through an example study how the decomposition of
              cellular traffic can be used for event inference.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  18,
  number   =  9,
  pages    = "2190--2202",
  month    =  sep,
  year     =  2019,
  file     = "All Papers/W/Wang et al. 2019 - Spatio-Temporal Analysis and Prediction of Cellular Traffic in Metropolis.pdf",
  keywords = "cellular radio;inference mechanisms;Internet;learning (artificial
              intelligence);telecommunication computing;telecommunication
              traffic;time series;spatio-temporal analysis;predicting cellular
              traffic;mobile users;tremendous temporal dynamics;spatial
              dynamics;diverse user Internet;frequent user mobility
              citywide;big cellular usage dataset;intensive spatio-temporal
              dependency;distant cell towers;urban cellular traffic;inter-cell
              data traffic;accurate cellular traffic prediction;Poles and
              towers;Urban areas;Cellular networks;Predictive
              models;Monitoring;Internet;Mobile handsets;Machine
              learning;prediction methods;predictive models;mobile
              computing;communication systems;mobile
              communication;GeneralNetworking;MLNetworking",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2018.2870135"
}

@ARTICLE{Ndikumana2020-fb,
  title    = "Joint Communication, Computation, Caching, and Control in Big
              Data {Multi-Access} Edge Computing",
  author   = "Ndikumana, A and Tran, N H and Ho, T M and Han, Z and Saad, W and
              Niyato, D and Hong, C S",
  abstract = "The concept of Multi-access Edge Computing (MEC) has been
              recently introduced to supplement cloud computing by deploying
              MEC servers to the network edge so as to reduce the network delay
              and alleviate the load on cloud data centers. However, compared
              to the resourceful cloud, MEC server has limited resources. When
              each MEC server operates independently, it cannot handle all
              computational and big data demands stemming from users devices.
              Consequently, the MEC server cannot provide significant gains in
              overhead reduction of data exchange between users devices and
              remote cloud. Therefore, joint Computing, Caching, Communication,
              and Control (4C) at the edge with MEC server collaboration is
              needed. To address these challenges, in this paper, the problem
              of joint 4C in big data MEC is formulated as an optimization
              problem whose goal is to jointly optimize a linear combination of
              the bandwidth consumption and network latency. However, the
              formulated problem is shown to be non-convex. As a result, a
              proximal upper bound problem of the original formulated problem
              is proposed. To solve the proximal upper bound problem, the block
              successive upper bound minimization method is applied. Simulation
              results show that the proposed approach satisfies computation
              deadlines and minimizes bandwidth consumption and network
              latency.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  19,
  number   =  6,
  pages    = "1359--1374",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/N/Ndikumana et al. 2020 - Joint Communication, Computation, Caching, and Control in Big Data Multi-Access Edge Computing.pdf",
  keywords = "Big Data;cloud computing;computer centres;concave
              programming;minimisation;big data multiaccess edge
              computing;cloud computing;joint communication;bandwidth
              consumption;computation deadlines;proximal upper bound
              problem;big data MEC;MEC server collaboration;remote cloud;data
              exchange;users devices;big data demands;computational data
              demands;resourceful cloud;cloud data centers;network
              edge;Servers;Big Data;Collaboration;Cloud
              computing;Delays;Optimization;Resource
              management;Communication;computation;caching;distributed
              control;multi-access edge computing;5G network;EdgeFogCloudIoT",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2908403"
}

@ARTICLE{Kim2020-lu,
  title    = "Economics of Fog Computing: Interplay Among Infrastructure and
              Service Providers, Users, and Edge Resource Owners",
  author   = "Kim, Daewoo and Lee, Hyojung and Song, Hyungseok and Choi,
              Nakjung and Yi, Yung",
  abstract = "Fog computing is a paradigm which brings computing, storage, and
              networking closer to end users and end devices for better service
              provisioning. One of the crucial factors in the success of fog
              computing is on how to incentivize the individual users' edge
              resources and provide them to end users such that fog computing
              is economically beneficial to all involved economic players. In
              this paper, we model and analyze a market of fog computing, from
              which we aim at drawing practical implications to uncover how the
              fog computing market should operate. To this end, we conduct an
              economic analysis of such user-oriented fog computing by modeling
              a market consisting of Infrastructure and Service Provider (ISP),
              end Service Users (SUs), and Edge Resource Owners (EROs) as a
              non-cooperative game. In this market, ISP, which provides a
              platform for fog computing, behaves as a mediator or a broker
              which leases EROs' edge resources and provides various services
              to SUs. In our model, a two-stage dynamic game is used where in
              each stage, there exists a dynamic game, one for between ISP and
              EROs and another for between ISP and SUs, to model the market
              more practically. Despite this complex game structure, we provide
              a closed-form equilibrium analysis which gives an insight on how
              much economic benefit is obtained by ISP, SUs, and EROs from
              user-oriented fog computing under what conditions, and we figure
              out the economic factors that have a significant impact on the
              success of fog computing.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  19,
  number   =  11,
  pages    = "2609--2622",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/K/Kim et al. 2020 - Economics of Fog Computing - Interplay Among Infrastructure and Service Providers, Users, and Edge Resource Owners.pdf",
  keywords = "Edge computing;Games;Computational modeling;Economics;Indium
              phosphide;III-V semiconductor materials;Business;Edge network;fog
              computing;game theory;network economics",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2925797"
}

@ARTICLE{Huang2020-lz,
  title    = "{Reliability-Aware} Virtualized Network Function Services
              Provisioning in Mobile Edge Computing",
  author   = "Huang, Meitian and Liang, Weifa and Shen, Xiaojun and Ma, Yu and
              Kan, Haibin",
  abstract = "Along with Network Function Virtualization (NFV), Mobile Edge
              Computing (MEC) is becoming a new computing paradigm that enables
              accommodating innovative applications and services with stringent
              response delay and resource requirements, including autonomous
              vehicles and augmented reality. Provisioning reliable network
              services for users is the top priority of most network service
              providers, as unreliable services or severe service failures can
              result in tremendous losses of users, particularly for their
              mission-critical applications. In this paper, we study
              reliability-aware VNF instances provisioning in an MEC, where
              different users request different network services with different
              reliability requirements through paying their requested services
              with the aim to maximize the network throughput. To this end, we
              first formulate a novel reliability-aware VNF instance placement
              problem by provisioning primary and secondary VNF instances at
              different cloudlets in MEC for each user while meeting the
              specified reliability requirement of the user request. We then
              show that the problem is NP-hard and formulate an Integer Linear
              Programming (ILP) solution. Due to the NP-hardness of the
              problem, we instead devise an approximation algorithm with a
              logarithmic approximation ratio for the problem. Moreover, we
              also consider two special cases of the problem. For one special
              case where each request only requests one primary and one
              secondary VNF instances, the problem is still NP-hard, and we
              devise a constant approximation algorithm for it. For another
              special case where different VNFs have the same amounts of
              computing resource demands, we show that it is polynomial-time
              solvable by developing a dynamic programming solution for it. We
              finally evaluate the performance of the proposed algorithms
              through experimental simulations. Experimental results
              demonstrate that the proposed algorithms are promising, and the
              empirical results of the algorithms outperform their analytical
              counterparts as theoretical estimations usually are very
              conservative.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  19,
  number   =  11,
  pages    = "2699--2713",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/H/Huang et al. 2020 - Reliability-Aware Virtualized Network Function Services Provisioning in Mobile Edge Computing.pdf",
  keywords = "Reliability;Cloud computing;Approximation algorithms;Heuristic
              algorithms;Mobile computing;Edge computing;Network function
              virtualization;Reliability-aware VNF instances provisioning;VNF
              instance placements;virtualized network function
              implementation;approximation algorithms;generalized assignment
              problems;dynamic programming;mobile edge computing;combinatorial
              optimization problems",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2927214"
}

@ARTICLE{Li2020-ab,
  title    = "{Learning-Aided} Computation Offloading for Trusted Collaborative
              Mobile Edge Computing",
  author   = "Li, Y and Wang, X and Gan, X and Jin, H and Fu, L and Wang, X",
  abstract = "Cooperative offloading in mobile edge computing enables
              resource-constrained edge clouds to help each other with
              computation-intensive tasks. However, the power of such
              offloading could not be fully unleashed, unless trust risks in
              collaboration are properly managed. As tasks are outsourced and
              processed at the network edge, completion latency usually
              presents high variability that can harm the offered service
              levels. By jointly considering these two challenges, we propose
              OLCD, an Online Learning-aided Cooperative offloaDing mechanism
              under the scenario where computation offloading is organized
              based on accumulated social trust. Under co-provisioning of
              computation, transmission, and trust services, trust propagation
              is performed along the multi-hop offloading path such that tasks
              are allowed to be fulfilled by powerful edge clouds. We harness
              Lyapunov optimization to exploit the spatial-temporal optimality
              of long-term system cost minimization problem. By gap-preserving
              transformation, we decouple the series of bidirectional
              offloading problems so that it suffices to solve a separate
              decision problem for each edge cloud. The optimal offloading
              control can not materialize without complete latency knowledge.
              To adapt to latency variability, we resort to the delayed online
              learning technique to facilitate completion latency prediction
              under long-duration processing, which is fed as input to
              queued-based offloading control policy. Such predictive control
              is specially designed to minimize the loss due to prediction
              errors over time. We theoretically prove that OLCD guarantees
              close-to-optimal system performance even with inaccurate
              prediction, but its robustness is achieved at the expense of
              decreased stability. Trace-driven simulations demonstrate the
              efficiency of OLCD as well as its superiorities over prior
              related work.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  19,
  number   =  12,
  pages    = "2833--2849",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/L/Li et al. 2020 - Learning-Aided Computation Offloading for Trusted Collaborative Mobile Edge Computing.pdf",
  keywords = "cloud computing;learning (artificial
              intelligence);minimisation;mobile computing;trusted
              computing;trace-driven simulations;queued-based offloading
              control policy;predictive control;online learning-aided
              cooperative offloading mechanism;online learning-aided
              computation offloading;network edge;trust
              risks;computation-intensive tasks;resource-constrained edge
              clouds;trusted collaborative mobile edge
              computing;close-to-optimal system performance;completion latency
              prediction;delayed online learning technique;latency
              variability;complete latency knowledge;optimal offloading
              control;bidirectional offloading problems;long-term system cost
              minimization problem;spatial-temporal optimality;harness Lyapunov
              optimization;multihop offloading path;trust propagation;trust
              services;accumulated social trust;OLCD;Task analysis;Cloud
              computing;Collaboration;Edge
              computing;Minimization;Optimization;Edge computing;Spread
              spectrum communication;Mobile edge computing;multi-hop
              cooperative offloading;trust propagation;completion latency
              variability",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2934103"
}

@ARTICLE{Huang2020-ji,
  title    = "{Coflow-Like} Online Data Acquisition from {Low-Earth-Orbit}
              Datacenters",
  author   = "Huang, H and Guo, S and Liang, W and Wang, K and Okabe, Y",
  abstract = "Satellite-based communication technology has gained much
              attention in the past few years, where satellites play mainly the
              supplementary roles as relay devices to terrestrial communication
              networks. Unlike previous work, we treat the low-earth-orbit
              (LEO) satellites as secure data storage mediums. We focus on data
              acquisition from a LEO satellite based data storage system (also
              referred to as the LEO based datacenters), which has been
              considered as a promising and secure paradigm on data storage.
              Under the LEO based datacenter architecture, one fundamental
              challenge is to deal with energy-efficient downloading from space
              to ground while maintaining the system stability. In this paper,
              we aim to maximize the amount of data admitted while minimizing
              the energy consumption, when downloading files from LEO based
              datacenters to meet user demands. To this end, we first formulate
              a novel optimization problem and develop an online scheduling
              framework. We then devise a novel coflow-like ``Join the first
              K-shortest Queues (JKQ)'' based job-dispatch strategy, which can
              significantly lower backlogs of queues residing in LEO
              satellites, thereby improving the system stability. We also
              analyze the optimality of the proposed approach and system
              stability. We finally evaluate the performance of the proposed
              algorithm through conducting emulator based simulations, based on
              real-world LEO constellation and user demand traces. The
              simulation results show that the proposed algorithm can
              dramatically lower the queue backlogs and achieve high energy
              efficiency.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  19,
  number   =  12,
  pages    = "2743--2760",
  month    =  dec,
  year     =  2020,
  annote   = "Not really all that relevant related work. More for the fun of
              it! :-)",
  keywords = "artificial satellites;computer centres;data acquisition;energy
              conservation;optimisation;queueing theory;satellite
              communication;telecommunication power
              management;telecommunication scheduling;LEO satellite based data
              storage system;datacenter architecture;energy-efficient
              downloading;system stability;LEO based datacenters;online
              scheduling framework;LEO satellites;real-world LEO
              constellation;online data acquisition;low-earth-orbit
              datacenters;satellite-based communication technology;terrestrial
              communication networks;low-earth-orbit satellites;secure data
              storage mediums;JKQ-based job-dispatch strategy;K-shortest
              queue;Satellite communication;Low earth orbit
              satellites;Memory;Stability analysis;Optimization;Servers;Cloud
              computing;LEO-based datacenter;online
              job-scheduling;coflow;drift-plus-penalty;energy efficiency;queue
              stability",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2936202"
}

@ARTICLE{Saputra2021-ze,
  title    = "A Novel Mobile Edge Network Architecture with Joint
              {Caching-Delivering} and Horizontal Cooperation",
  author   = "Saputra, Y M and Hoang, D T and Nguyen, D N and Dutkiewicz, E",
  abstract = "Mobile edge caching/computing (MEC) has been emerging as a
              promising paradigm to provide ultra-high rate, ultra-reliable,
              and/or low-latency communications in future wireless networks. In
              this paper, we introduce a novel MEC network architecture that
              leverages the optimal joint caching-delivering with horizontal
              cooperation among mobile edge nodes (MENs). To that end, we first
              formulate the content-access delay minimization problem by
              jointly optimizing the content caching and delivering decisions
              under various network constraints (e.g., network topology,
              storage capacity and users' demands at each MEN). However, the
              strongly mutual dependency between the decisions makes the
              problem a nested dual optimization that is proved to be NP-hard.
              To deal with it, we propose a novel transformation method to
              transform the nested dual problem to an equivalent mixed-integer
              nonlinear programming (MINLP) optimization problem. Then, we
              design a centralized solution using an improved branch-and-bound
              algorithm with the interior-point method to find the joint
              caching and delivering policy which is within 1 percent of the
              optimal solution. Since the centralized solution requires the
              full network topology and information from all MENs, to make our
              solution scalable, we develop a distributed algorithm which
              allows each MEN to make its own decisions based on its local
              observations. Extensive simulations demonstrate that the proposed
              solutions can reduce the total average delay for the whole
              network up to 40 percent compared with other current caching
              policies. Furthermore, the proposed solutions also increase the
              cache hit ratio for the network up to 4 times, thereby
              dramatically reducing the traffic load on the backhaul network.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  1,
  pages    = "19--31",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/S/Saputra et al. 2021 - A Novel Mobile Edge Network Architecture with Joint Caching-Delivering and Horizontal Cooperation.pdf",
  keywords = "Delays;Optimization;Cooperative caching;Network
              architecture;Servers;Bandwidth;Mobile edge caching;horizontal
              cooperative caching;joint
              caching-delivering;branch-and-bound;delay;interior-point",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2938510"
}

@ARTICLE{Sepulcre2021-hj,
  title    = "Heterogeneous {V2V} Communications in {Multi-Link} and
              {Multi-RAT} Vehicular Networks",
  author   = "Sepulcre, M and Gozalvez, J",
  abstract = "Connected and automated vehicles will enable advanced traffic
              safety and efficiency applications thanks to the dynamic exchange
              of information between vehicles, and between vehicles and
              infrastructure nodes. Connected vehicles can utilize IEEE 802.11p
              for vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I)
              communications. However, a widespread deployment of connected
              vehicles and the introduction of connected automated driving
              applications will notably increase the bandwidth and scalability
              requirements of vehicular networks. This paper proposes to
              address these challenges through the adoption of heterogeneous
              V2V communications in multi-link and multi-RAT vehicular
              networks. In particular, the paper proposes the first distributed
              (and decentralized) context-aware heterogeneous V2V
              communications algorithm that is technology and application
              agnostic, and that allows each vehicle to autonomously and
              dynamically select its communications technology taking into
              account its application requirements and the communication
              context conditions. This study demonstrates the potential of
              heterogeneous V2V communications, and the capability of the
              proposed algorithm to satisfy the vehicles' application
              requirements while approaching the estimated upper bound network
              capacity.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  1,
  pages    = "162--173",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/S/Sepulcre and Gozalvez 2021 - Heterogeneous V2V Communications in Multi-Link and Multi-RAT Vehicular Networks.pdf",
  keywords = "Rats;Vehicular ad hoc networks;Bandwidth;Heuristic
              algorithms;Reliability;Mobile computing;Safety;Connected
              vehicles;connected automated vehicles;cooperative
              ITS;V2V;vehicle-to-vehicle;heterogeneous
              communications;heterogeneous
              V2V;multi-RAT;multi-link;multi-channel;multi-band;VANET;vehicular
              networks",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2939803"
}

@ARTICLE{Xia2021-fl,
  title    = "A {Link-Layer} Synchronization and Medium Access Control Protocol
              for {Terahertz-Band} Communication Networks",
  author   = "Xia, Q and Hossain, Z and Medley, M and Jornet, J M",
  abstract = "In this paper, a link-layer synchronization and medium access
              control (MAC) protocol for very-high-speed wireless communication
              networks in the Terahertz (THz) band is presented. The protocol
              relies on a receiver-initiated handshake to guarantee
              synchronization between transmitter and receiver. Two scenarios
              are considered, namely, a macroscale scenario, where nodes
              utilize rotating directional antennas to periodically sweep the
              space while overcoming the distance problem at THz frequencies,
              and a nanoscale scenario, where nano-devices require energy
              harvesting systems to operate. Both scenarios are implemented on
              a centralized and an ad-hoc network architecture. A carrier-based
              physical layer is considered for the macro-scenario, whereas the
              physical layer for the nano-scenario is based on a
              femtosecond-long pulse-based modulation scheme with packet
              interleaving. The performance of the proposed MAC protocol is
              analytically investigated in terms of delay, throughput and
              probability of successful packet delivery, and compared to that
              of an adapted Carrier Sense Multiple Access with Collision
              Avoidance (CSMA/CA) with and without handshake. The results are
              validated by means of extensive simulations with ns-3, in which
              all the necessary THz elements have been implemented. The results
              show that the proposed protocol can maximize the successful
              packet delivery probability without compromising the achievable
              throughput in THz-band communication networks.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  1,
  pages    = "2--18",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/X/Xia et al. 2021 - A Link-Layer Synchronization and Medium Access Control Protocol for Terahertz-Band Communication Networks.pdf",
  keywords = "Wireless communication;Media Access
              Protocol;Synchronization;Wireless sensor networks;Nanoscale
              devices;Ad hoc networks;Terahertz communications;MAC
              protocols;link-layer synchronization;ultra-broadband
              networking;6G;Mobile\_Wireless",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2940441"
}

@ARTICLE{Li2021-cd,
  title    = "An {Incentive-Aware} Job Offloading Control Framework for
              {Multi-Access} Edge Computing",
  author   = "Li, L and Quek, T Q S and Ren, J and Yang, H H and Chen, Z and
              Zhang, Y",
  abstract = "This paper considers a scenario in which an access point (AP) is
              equipped with a server of finite computing power, and serves
              multiple resource-hungry users by charging users a price. This
              price helps to regulate users' behavior in offloading jobs to the
              AP. However, existing works on pricing are based on abstract
              concave utility functions, giving no dependence on physical layer
              parameters. To that end, we first introduce a novel utility
              function, which measures the cost reduction by offloading as
              compared with executing jobs locally. Based on this utility
              function we then formulate two offloading games, with one
              maximizing individuals interest and the other maximizing the
              overall systems interest. We analyze the structural property of
              the games and admit in closed-form the Nash Equilibrium and the
              Social Equilibrium for the homogeneous user case, respectively.
              The proposed expressions are functions of user parameters such as
              the weights of time and energy, the distance from the AP, thus
              constituting an advancement over prior economic works that have
              considered only abstract functions. Finally, we propose an
              optimal price-based scheme, with which we prove that the
              interactive decision-making process with self-interested users
              converges to a Nash Equilibrium point equal to the Social
              Equilibrium point.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  1,
  pages    = "63--75",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/L/Li et al. 2021 - An Incentive-Aware Job Offloading Control Framework for Multi-Access Edge Computing.pdf",
  keywords = "Servers;Economics;Edge computing;Cloud computing;Delays;Mobile
              handsets;Nash equilibrium;Multi-access edge
              computing;decentralized computation offloading;wireless network
              economics",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2941934"
}

@ARTICLE{Chen2021-aj,
  title    = "Collaborative Service Placement for Edge Computing in Dense Small
              Cell Networks",
  author   = "Chen, L and Shen, C and Zhou, P and Xu, J",
  abstract = "Mobile Edge Computing (MEC) pushes computing functionalities away
              from the centralized cloud to the proximity of data sources,
              thereby reducing service provision latency and saving backhaul
              network bandwidth. Although computation offloading for MEC
              systems has been extensively studied in the literature, service
              placement is an equally, if not more, important design topic of
              MEC, yet receives much less attention. Service placement refers
              to configuring the service platform and storing the related
              libraries/databases at the edge server, e.g., MEC-enabled Base
              Station (BS), which enables corresponding computation tasks to be
              executed. Due to the limited computing resource, the edge server
              can host only a small number of services and hence which services
              to host has to be judiciously decided to maximize the system
              performance. In this paper, we investigate collaborative service
              placement in MEC-enabled dense small cell networks. An efficient
              decentralized algorithm, called CSP (Collaborative Service
              Placement), is proposed where a network of small cell BSs
              optimize service placement decisions collaboratively to address a
              number of challenges in MEC systems, including service
              heterogeneity, spatial demand coupling, and decentralized
              coordination. CSP is developed based on parallel Gibbs sampling
              by exploiting the graph coloring on the small cell network. The
              algorithm significantly improves the time efficiency compared to
              conventional Gibbs sampling, yet guarantees provable convergence
              and optimality. CSP is further extended to work with selfish BSs,
              where BSs are allowed to choose ``to cooperate'' or ``not to
              cooperate.'' We employ coalitional game to investigate the
              strategic behaviors of selfish BSs and design a coalition
              formation scheme to form stable BS coalitions using
              merge-and-split rules. Simulations results show that CSP can
              effectively reduce edge system operational cost for both
              cooperative and selfish BSs.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  2,
  pages    = "377--390",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/C/Chen et al. 2021 - Collaborative Service Placement for Edge Computing in Dense Small Cell Networks.pdf",
  keywords = "cellular radio;cloud computing;computational complexity;game
              theory;mobile computing;optimisation;collaborative service
              placement;MEC-enabled dense small cell networks;cell BSs optimize
              service placement decisions;MEC systems;service
              heterogeneity;cell network;edge system operational cost;Mobile
              Edge Computing;service provision;backhaul network
              bandwidth;computation offloading;service platform;MEC-enabled
              Base Station;corresponding computation tasks;computing
              resource;edge server;Collaboration;Task analysis;Servers;Cloud
              computing;Microcell networks;Edge computing;Games;Edge
              computing;service placement;small-cell network;decentralized
              optimization",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2945956"
}

@ARTICLE{Gonzalez-Diaz2021-gk,
  title    = "Integrating Fronthaul and Backhaul Networks: Transport Challenges
              and Feasibility Results",
  author   = "Gonzalez-Diaz, S and Garcia-Saavedra, A and de la Oliva, A and
              Costa-Perez, X and Gazda, R and Mourad, A and Deiss, T and
              Mangues-Bafalluy, J and Iovanna, P and Stracca, S and Leithead, P",
  abstract = "In addition to CPRI, new functional splits have been defined in
              5G creating diverse fronthaul transport bandwidth and latency
              requirements. These fronthaul requirements shall be fulfilled
              simultaneously together with the backhaul requirements by an
              integrated fronthaul and backhaul transport solution. In this
              paper, we analyze the technical challenges to achieve an
              integrated transport solution in 5G and propose specific
              solutions to address these challenges. These solutions have been
              implemented and verified with pre-commercial equipment. Our
              results confirm that an integrated fronthaul and backhaul
              transport dubbed Crosshaul can meet all the requirements of 5G
              fronthaul and backhaul in a cost-efficient manner.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  2,
  pages    = "533--549",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/G/Gonzalez-Diaz et al. 2021 - Integrating Fronthaul and Backhaul Networks - Transport Challenges and Feasibility Results.pdf",
  keywords = "5G mobile communication;Prototypes;Bandwidth;Switching
              circuits;Copper;Protocols;Long Term
              Evolution;5G-Crosshaul;fronthaul;backhaul;5G;C-RAN;5G6G",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2948641"
}

@ARTICLE{Pratap2021-bd,
  title    = "Maximizing Fairness for Resource Allocation in Heterogeneous {5G}
              Networks",
  author   = "Pratap, A and Misra, R and Das, S K",
  abstract = "In this article, we first formulate the joint resource
              allocation, interference minimization, user-level, and cell-level
              fairness for maximum resource reuse in 5G heterogeneous small
              cell networks as an NP-hard problem. We then propose three
              algorithms---centralized, distributed, and randomized distributed
              algorithms---to efficiently solve the formulated resource
              allocation problem while minimizing interference, maximizing
              fairness, and resource reuse. Through extensive real data
              analysis and network simulations, we show that our proposed
              solutions outperform state-of-the-art schemes, namely interfering
              model (INT) and distributed random access (DRA), for both low and
              high-density 5G networks.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  2,
  pages    = "603--619",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/P/Pratap et al. 2021 - Maximizing Fairness for Resource Allocation in Heterogeneous 5G Networks.pdf",
  keywords = "Resource management;Interference;Femtocells;5G mobile
              communication;Quality of service;Microcell networks;5G cellular
              networks;randomized and distributed algorithms;heterogeneous
              small cell networks;Mobile\_Wireless",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2948877"
}

@ARTICLE{Zhang2021-ky,
  title    = "Fully Distributed Packet Scheduling Framework for Handling
              Disturbances in Lossy {Real-Time} Wireless Networks",
  author   = "Zhang, T and Gong, T and Han, S and Deng, Q and Hu, X S",
  abstract = "Along with the rapid growth of Industrial Internet-of-Things
              (IIoT) applications and their penetration into many industry
              sectors, real-time wireless networks (RTWNs) have been playing a
              more critical role in providing real-time, reliable, and secure
              communication services for such applications. A key challenge in
              RTWN management is how to ensure real-time Quality of Services
              (QoS) especially in the presence of unexpected disturbances and
              lossy wireless links. Most prior work takes centralized
              approaches for handling disturbances, which are slow and subject
              to single-point failure, and do not scale. To overcome these
              drawbacks, this article presents a fully distributed packet
              scheduling framework called FD-PaS . FD-PaS aims to provide
              guaranteed fast response to unexpected disturbances while
              achieving minimum performance degradation for meeting the timing
              and reliability requirements of all critical tasks. To combat the
              scalability challenge, FD-PaS incorporates several key advances
              in both algorithm design and data link layer protocol design to
              enable individual nodes to make on-line decisions locally without
              any centralized control. Our extensive simulation and testbed
              results have validated the correctness of the FD-PaS design and
              demonstrated its effectiveness in providing fast response for
              handling disturbances while ensuring the designated QoS
              requirements.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  2,
  pages    = "502--518",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/Z/Zhang et al. 2021 - Fully Distributed Packet Scheduling Framework for Handling Disturbances in Lossy Real-Time Wireless Networks.pdf",
  keywords = "Task analysis;Reliability;Scheduling algorithms;Real-time
              systems;Sensors;Schedules;Dynamic scheduling;Real-time wireless
              networks;disturbances;distributed and reliable packet scheduling",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2950913"
}

@ARTICLE{Liang2021-hc,
  title    = "{Interaction-Oriented} Service Entity Placement in Edge Computing",
  author   = "Liang, Y and Ge, J and Zhang, S and Wu, J and Pan, L and Zhang, T
              and Luo, B",
  abstract = "Distributed Interactive Applications (DIAs) such as virtual
              reality and multiplayer online game usually require fast
              processing of tremendous data and timely exchange of
              delay-sensitive action data and metadata. This makes traditional
              mobile-based or cloud-based solutions no longer effective. Thanks
              to edge computing, DIA Service Providers (DSPs) can rent
              resources from Edge Infrastructure Providers (EIPs) to place
              service entities that store user states and run
              computation-intensive tasks. One fundamental problem for a DSP is
              to decide where to place service entities to achieve low-delay
              pairwise interactions between DIA users, under the constraint
              that the total placement cost is no more than a specified budget
              threshold. In this article, we formally model the service entity
              placement problem and prove that it is NP-complete by a
              polynomial reduction from the set cover problem. We present GPA,
              an efficient algorithm for service entity placement, and
              theoretically analyze its performance. We evaluated GPA with both
              real-world data trace-driven simulations, and observed that GPA
              performs close to the optimal algorithm and generally outperforms
              the baseline algorithm. We also output a curve showing the
              trade-off between the weighted average interaction delay and the
              budget threshold, so that a DSP can choose the right balance.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  3,
  pages    = "1064--1075",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/L/Liang et al. 2021 - Interaction-Oriented Service Entity Placement in Edge Computing.pdf",
  keywords = "Servers;Delays;Edge computing;Mobile computing;Cloud
              computing;Games;Task analysis;Edge computing;distributed
              interactive applications;interaction delay;service entity
              placement",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2952097"
}

@ARTICLE{Wang2021-am,
  title    = "{Delay-Aware} Microservice Coordination in Mobile Edge Computing:
              A Reinforcement Learning Approach",
  author   = "Wang, S and Guo, Y and Zhang, N and Yang, P and Zhou, A and Shen,
              X",
  abstract = "As an emerging service architecture, microservice enables
              decomposition of a monolithic web service into a set of
              independent lightweight services which can be executed
              independently. With mobile edge computing, microservices can be
              further deployed in edge clouds dynamically, launched quickly,
              and migrated across edge clouds easily, providing better services
              for users in proximity. However, the user mobility can result in
              frequent switch of nearby edge clouds, which increases the
              service delay when users move away from their serving edge
              clouds. To address this issue, this article investigates
              microservice coordination among edge clouds to enable seamless
              and real-time responses to service requests from mobile users.
              The objective of this work is to devise the optimal microservice
              coordination scheme which can reduce the overall service delay
              with low costs. To this end, we first propose a dynamic
              programming-based offline microservice coordination algorithm,
              that can achieve the globally optimal performance. However, the
              offline algorithm heavily relies on the availability of the prior
              information such as computation request arrivals, time-varying
              channel conditions and edge cloud's computation capabilities
              required, which is hard to be obtained. Therefore, we reformulate
              the microservice coordination problem using Markov decision
              process framework and then propose a reinforcement learning-based
              online microservice coordination algorithm to learn the optimal
              strategy. Theoretical analysis proves that the offline algorithm
              can find the optimal solution while the online algorithm can
              achieve near-optimal performance. Furthermore, based on two
              real-world datasets, i.e., the Telecom's base station dataset and
              Taxi Track dataset from Shanghai, experiments are conducted. The
              experimental results demonstrate that the proposed online
              algorithm outperforms existing algorithms in terms of service
              delay and migration costs, and the achieved performance is close
              to the optimal performance obtained by the offline algorithm.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  3,
  pages    = "939--951",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/W/Wang et al. 2021 - Delay-Aware Microservice Coordination in Mobile Edge Computing - A Reinforcement Learning Approach.pdf",
  keywords = "Cloud computing;Delays;Image edge detection;Base stations;Edge
              computing;Heuristic algorithms;Microservice;mobile edge
              computing;coordination;delay;migration",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2957804"
}

@ARTICLE{Kadota2021-en,
  title    = "Minimizing the Age of Information in Wireless Networks with
              Stochastic Arrivals",
  author   = "Kadota, I and Modiano, E",
  abstract = "We consider a wireless network with a base station serving
              multiple traffic streams to different destinations. Packets from
              each stream arrive to the base station according to a stochastic
              process and are enqueued in a separate (per stream) queue. The
              queueing discipline controls which packet within each queue is
              available for transmission. The base station decides, at every
              time t, which stream to serve to the corresponding destination.
              The goal of scheduling decisions is to keep the information at
              the destinations fresh. Information freshness is captured by the
              Age of Information (AoI) metric. In this paper, we derive a lower
              bound on the AoI performance achievable by any given network
              operating under any queueing discipline. Then, we consider three
              common queueing disciplines and develop both an Optimal
              Stationary Randomized policy and a Max-Weight policy under each
              discipline. Our approach allows us to evaluate the combined
              impact of the stochastic arrivals, queueing discipline and
              scheduling policy on AoI. We evaluate the AoI performance both
              analytically and using simulations. Numerical results show that
              the performance of the Max-Weight policy is close to the
              analytical lower bound.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  3,
  pages    = "1173--1185",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/K/Kadota and Modiano 2021 - Minimizing the Age of Information in Wireless Networks with Stochastic Arrivals.pdf",
  keywords = "Wireless networks;Scheduling;Optimal scheduling;Mobile
              computing;Stochastic processes;Information age;Age of
              information;scheduling;wireless
              networks;optimization;Mobile\_Wireless",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2019.2959774"
}

@ARTICLE{Chen2021-vn,
  title    = "Robust Computation Offloading and Resource Scheduling in
              {Cloudlet-Based} Mobile Cloud Computing",
  author   = "Chen, Menggang and Guo, Songtao and Liu, Kai and Liao, Xiaofeng
              and Xiao, Bin",
  abstract = "Mobile cloud computing (MCC) as an emerging computing paradigm
              enables mobile devices to offload their computation tasks to
              nearby resource-rich cloudlets so as to augment computation
              capability and reduce energy consumption of mobile devices.
              However, due to the mobility of mobile devices and the admission
              of cloudlets, the connection between mobile devices and cloudlets
              may be unstable, which will affect offloading decision, even
              cause offloading failure. To address such an issue, in this
              paper, we propose a robust computation offloading strategy with
              failure recovery (RoFFR) in an intermittently connected cloudlet
              system aiming to reduce energy consumption and shorten
              application completion time. We first provide an optimal cloudlet
              selection policy when multiple cloudlets are available near
              mobile devices. Furthermore, we formulate the RoFFR problem as
              two optimization problems, i.e., local execution cost
              minimization problem and offloading execution cost minimization
              problem while satisfying the task-dependency requirement and
              application completion deadline constraint. By solving both
              optimization problems, we present a distributed RoFFR algorithm
              for CPU clock frequency configuration in local execution and
              transmission power allocation and data rate control in cloudlet
              execution. Experimental results in a real testbed show that our
              distributed RoFFR algorithm outperforms several baseline policies
              and existing offloading schemes in terms of application
              completion cost and offloading data rate.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  5,
  pages    = "2025--2040",
  month    =  may,
  year     =  2021,
  keywords = "Cloud computing;Mobile handsets;Task analysis;Energy
              consumption;Resource management;Clocks;Computational
              modeling;Mobile cloud computing;computation offloading;resource
              scheduling;offloading failure;unstable connectivity",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2020.2973993"
}

@ARTICLE{Yang2021-ea,
  title    = "Optimizing Information Freshness in Wireless Networks: A
              Stochastic Geometry Approach",
  author   = "Yang, Howard H and Arafa, Ahmed and Quek, Tony Q S and Poor, H
              Vincent",
  abstract = "Optimization of information freshness in wireless networks has
              usually been performed based on queueing analysis that captures
              only the temporal traffic dynamics associated with the
              transmitters and receivers. However, the effect of interference,
              which is mainly dominated by the interferers' geographic
              locations, is not well understood. In this paper, we leverage a
              spatiotemporal model, which allows one to characterize the age of
              information (AoI) from a joint queueing-geometry perspective, for
              the design of a decentralized scheduling policy that exploits
              local observation to make transmission decisions that minimize
              the AoI. To quantify the performance, we also derive accurate and
              tractable expressions for the peak AoI. Numerical results reveal
              that: i) the packet arrival rate directly affects the service
              process due to queueing interactions, ii) the proposed scheme can
              adapt to traffic variations and largely reduce the peak AoI, and
              iii) the proposed scheme scales well as the network grows in
              size. This is done by adaptively adjusting the radio access
              probability at each transmitter to the change of the ambient
              environment.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  6,
  pages    = "2269--2280",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/Y/Yang et al. 2021 - Optimizing Information Freshness in Wireless Networks - A Stochastic Geometry Approach.pdf",
  keywords = "Transmitters;Receivers;Interference;Wireless
              networks;Protocols;Stochastic processes;Geometry;Poisson bipolar
              network;age of information;scheduling policy;spatiotemporal
              analysis;stochastic geometry;Wireless",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2020.2977010"
}

@ARTICLE{Wang2021-dk,
  title    = "Predictability and Prediction of Human Mobility Based on
              {Application-Collected} Location Data",
  author   = "Wang, Huandong and Zeng, Sihan and Li, Yong and Jin, Depeng",
  abstract = "In the modern information society, analysis of human mobility
              becomes increasingly essential in various areas such as city
              planning and resource management. With users' historical
              trajectories, the inherent patterns of their movements can be
              extracted and utilized to accurately predict the future
              movements. Plenty of previous work adopted traditional Markov
              model, which suffers when the trajectory becomes sparse or it
              shows distinct mobility patterns in different time of day. In
              this paper, based on an app-collected dataset of 100,000
              individuals' actively uploaded location information, we
              comprehensively analyze the mobility and predictability of each
              user. To approach the theoretical predictability and overcome the
              shortcomings of traditional Markov model, we propose a
              time-variant Markov model based on Gibbs sampling for mobility
              prediction. Specifically, we model human mobility as several
              interconnected Markov chains, each chain corresponds to a
              movement pattern of a period of time. Then, we adopt Gibbs
              sampling method to simultaneously recover the missing part of
              trajectories and train the Markov chains, in order to solve the
              unevenly distribution and the high missing rate. Results show
              that our prediction algorithm can achieve 11.2 percent higher
              prediction accuracy than the benchmark method, especially on
              sparse trajectories. In addition, we discover a high correlation
              between prediction accuracy and predictability, with correlation
              coefficient reaching 0.81. Finally, we investigate various
              factors including spatial and temporal resolution, orders of
              Markov models, and radius of gyration, in order to further
              explore the predictability under different circumstances.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  7,
  pages    = "2457--2472",
  month    =  jul,
  year     =  2021,
  keywords = "Trajectory;Markov processes;Predictive models;Kernel;Benchmark
              testing;Urban areas;Mobility prediction;time varying Markov
              model;predictability analysis;ActualTraffic;Mobility",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2020.2981441"
}

@ARTICLE{Gedawy2021-ym,
  title    = "{RAMOS}: A {Resource-Aware} {Multi-Objective} System for Edge
              Computing",
  author   = "Gedawy, Hend and Habak, Karim and Harras, Khaled A and Hamdi,
              Mounir",
  abstract = "Mobile and IoT devices are becoming increasingly capable
              computing platforms that are often underutilized. In this paper,
              we propose RAMOS, a system that leverages the idle compute cycles
              in a group of heterogeneous mobile and IoT devices that can be
              clustered to form an edge FemtoCloud. At the heart of this
              system, we formulate a multi-objective, resource-aware task
              assignment and scheduling problem. The scheduler runs in two main
              modes; latency-minimization and energy-efficiency. Under the
              latency-minimization mode, it strives to maximize the
              computational throughput of the constructed FemtoCloud while
              maintaining the energy consumption below an operator specified
              threshold. Under the energy-efficient mode, it minimizes the
              total energy consumed in the FemtoCloud while meeting defined
              tasks deadlines. Due to the NP-Completeness of this scheduling
              problem, we design a set of heuristics to solve it. We implement
              a prototype of our system and use it to evaluate its performance
              and efficiency. Our results demonstrate the system's ability to
              meet different scheduling objectives while adhering to
              pre-specified time and energy constraints. Compared to other
              schedulers, RAMOS achieves 10 to 40 percent completion time
              improvement under latency minimization mode and up to 30 percent
              more energy-efficiency under the energy-efficient mode.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  8,
  pages    = "2654--2670",
  month    =  aug,
  year     =  2021,
  keywords = "Task analysis;Processor scheduling;Energy
              consumption;Schedules;Performance evaluation;Systems
              architecture;Batteries;Edge computing;FemtoCloud;internet of
              things;IoT cloud;mobile cloud;mobile computing;EdgeFogCloudIoT",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2020.2984134"
}

@ARTICLE{Yang2021-iy,
  title    = "Computation Offloading in {Multi-Access} Edge Computing: A
              {Multi-Task} Learning Approach",
  author   = "Yang, Bo and Cao, Xuelin and Bassey, Joshua and Li, Xiangfang and
              Qian, Lijun",
  abstract = "Multi-access edge computing (MEC) has already shown great
              potential in enabling mobile devices to bear the
              computation-intensive applications by offloading some computing
              jobs to a nearby access point (AP) integrated with a MEC server
              (MES). However, due to the varying network conditions and limited
              computational resources of the MES, the offloading decisions
              taken by a mobile device and the computational resources
              allocated by the MES can be formulated as a mixed-integer
              nonlinear programming (MINLP) problem, which may not be optimized
              with the lowest cost. In this paper, we propose a novel
              offloading framework for the multi-server MEC network where each
              AP is equipped with an MES assisting mobile users (MUs) in
              executing computation-intensive jobs via offloading.
              Specifically, we formulate the offloading decision problem as a
              multiclass classification problem and formulate the MES
              computational resource allocation problem as a regression
              problem. Then a multi-task learning based feedforward neural
              network (MTFNN) model is designed and trained to jointly optimize
              the offloading decision and computational resource allocation.
              Numerical results show that the proposed MTFNN outperforms the
              conventional optimization method in terms of inference accuracy
              and computational complexity.",
  journal  = "IEEE Trans. Mob. Comput.",
  volume   =  20,
  number   =  9,
  pages    = "2745--2762",
  month    =  sep,
  year     =  2021,
  keywords = "Optimization;Resource management;Computational
              modeling;Delays;Training;Mobile computing;Mobile
              handsets;Multi-access edge computing;neural networks;multi-task
              learning;mixed-integer nonlinear programming (MINLP);computation
              offloading",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2020.2990630"
}

@ARTICLE{Yu2020-mn,
  title    = "{STEP}: A {Spatio-Temporal} {Fine-Granular} User Traffic
              Prediction System for Cellular Networks",
  author   = "Yu, L and Li, M and Jin, W and Guo, Y and Wang, Q and Yan, F and
              Li, P",
  abstract = "While traffic modeling and prediction are at the heart of
              providing high-quality telecommunication services in cellular
              networks and attract much attention, they have been approved as
              an extremely challenging task. Due to the diverse network demand
              of Internet-based apps, the cellular traffic from an individual
              user can have a wide dynamic range. Most existing methods, on the
              other hand, model traffic patterns as probabilistic distributions
              or stochastic processes and impose stringent assumptions over
              these models. Such assumptions may be beneficial at providing
              closed-form formula in evaluating prediction performances, but
              fall short for practice use. In this paper we propose STEP, a
              spatio-temporal fine-granular user traffic prediction mechanism
              for cellular networks. A deep graph convolution network, called
              GCGRN, is constructed. It is a novel combination of the graph
              convolution network (GCN) and gated recurrent units (GRU), which
              exploits graph neural network to learn an efficient
              spatio-temporal model from a user's massive dataset for traffic
              prediction. Extensive experimental results demonstrate that our
              model outperforms the state-of-the-art time-series based
              approaches. Besides, STEP merely incurs mild energy consumption,
              communication overhead and system resource occupancy to mobile
              devices. NS-3 based simulations validate the efficacy of STEP in
              reducing session dropping ratio in cellular networks.",
  journal  = "IEEE Trans. Mob. Comput.",
  pages    = "1--1",
  year     =  2020,
  file     = "All Papers/Y/Yu et al. 2020 - STEP - A Spatio-Temporal Fine-Granular User Traffic Prediction System for Cellular Networks.pdf",
  keywords = "Cellular networks;Mobile computing;Predictive
              models;Convolution;Urban areas;Data collection;Task
              analysis;Fine-granular traffic prediction;deep graph convolution
              network;cellular networks;MLNetworking",
  issn     = "1558-0660",
  doi      = "10.1109/TMC.2020.3001225"
}

@ARTICLE{Kuo2018-oo,
  title    = "Deploying Chains of Virtual Network Functions: On the Relation
              Between Link and Server Usage",
  author   = "Kuo, T and Liou, B and Lin, K C and Tsai, M",
  abstract = "Recently, network function virtualization has been proposed to
              transform from network hardware appliances to software
              middleboxes. Normally, a demand needs to invoke several virtual
              network functions (VNFs) following the order determined by the
              service chain along a routing path. In this paper, we study the
              joint problem of the VNF placement and path selection to better
              utilize the network. We discover that the relation between the
              link and server usage plays a crucial role in the problem.
              Inspired by stress testing, we first propose a systematic way to
              elastically tune the link and server usage of each demand based
              on the network status and properties of demands. In particular,
              we compute a proper routing path length, and decide, for each VNF
              in the service chain, whether to use additional server resources
              or to reuse resources provided by existing servers. We then
              propose a chain deployment algorithm that follows the guidance of
              this link and server usage. Via simulations, we show that our
              design effectively adapts resource allocation to network dynamics
              and, hence, serves more demands than other heuristics.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  26,
  number   =  4,
  pages    = "1562--1576",
  month    =  aug,
  year     =  2018,
  file     = "All Papers/K/Kuo et al. 2018 - Deploying Chains of Virtual Network Functions - On the Relation Between Link and Server Usage.pdf",
  keywords = "computer networks;network servers;resource
              allocation;telecommunication network routing;telecommunication
              services;virtualisation;network hardware appliances;virtual
              network functions;service chain;path selection;server usage;chain
              deployment algorithm;network dynamics;network function
              virtualization;link usage;routing path length;software
              middleboxes;VNF placement;stress testing;resource
              allocation;server resources;Servers;Stress;Testing;Resource
              management;Linear programming;Network function
              virtualization;Heuristic algorithms;Network function
              virtualization;network function deployment;routing;NFV",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2018.2842798"
}

@ARTICLE{Lotfi2020-hs,
  title    = "Is {Non-Neutrality} Profitable for the Stakeholders of the
              Internet Market?",
  author   = "Lotfi, M H and Sarkar, S and Kesidis, G",
  abstract = "We consider a system in which there exists two ISPs, one ``big''
              Content Provider (CP), and a continuum of End-Users (EUs). One of
              the ISPs is neutral and the other is non-neutral. We consider
              that the CP can differentiate between ISPs by controlling the
              quality of the content she is offering on each one. We also
              consider that EUs have different levels of innate preferences for
              ISPs. We formulate a sequential game, and explicitly characterize
              all the possible Sub-game Perfect Nash Equilibria (SPNE) of the
              game. We prove that if an SPNE exists, it would be one of the
              five possible strategies each of which we explicitly
              characterize. We prove that when EUs have sufficiently low innate
              preferences for ISPs, a unique SPNE exists in which the neutral
              ISP would be driven out of the market. We also prove that when
              these preferences are sufficiently high, there exists a unique
              SPNE with a non-neutral outcome in which both ISPs are active.
              Numerical results reveal that the neutral ISP receives a lower
              payoff and the non-neutral ISP receives a higher payoff (most of
              the time) in a non-neutral scenario. However, we identify
              scenarios in which the non-neutral ISP loses payoff by adopting
              non-neutrality. We also show that a non-neutral regime yields a
              higher welfare for EUs in comparison to a neutral one if the
              market power of the non-neutral ISP is small, the sensitivity of
              EUs (respectively, the CP) to the quality is low (respectively,
              high), or a combinations of these factors.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  4,
  pages    = "1435--1448",
  month    =  aug,
  year     =  2020,
  file     = "All Papers/L/Lotfi et al. 2020 - Is Non-Neutrality Profitable for the Stakeholders of the Internet Market.pdf",
  keywords = "game theory;Internet;EUs;sufficiently low innate
              preferences;ISPs;unique SPNE;neutral ISP;nonneutral
              outcome;nonneutral ISP;nonneutral
              scenario;nonneutrality;nonneutral regime yields;possible Sub-game
              Perfect Nash
              Equilibria;Internet;Games;Sensitivity;FCC;Switches;IEEE
              transactions;Stakeholders;Heterogeneous networks;mathematical
              models;optimization;ComputerNetworks",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.2981259"
}

@ARTICLE{Dang2020-wd,
  title    = "P4xos: Consensus as a Network Service",
  author   = "Dang, H T and Bressana, P and Wang, H and Lee, K S and Zilberman,
              N and Weatherspoon, H and Canini, M and Pedone, F and Soul{\'e},
              R",
  abstract = "In this paper, we explore how a programmable forwarding plane
              offered by a new breed of network switches might naturally
              accelerate consensus protocols, specifically focusing on Paxos.
              The performance of consensus protocols has long been a concern.
              By implementing Paxos in the forwarding plane, we are able to
              significantly increase throughput and reduce latency. Our
              P4-based implementation running on an ASIC in isolation can
              process over 2.5 billion consensus messages per second, a four
              orders of magnitude improvement in throughput over a widely-used
              software implementation. This effectively removes consensus as a
              bottleneck for distributed applications in data centers. Beyond
              sheer performance, our approach offers several other important
              benefits: it readily lends itself to formal verification; it does
              not rely on any additional network hardware; and as a full Paxos
              implementation, it makes only very weak assumptions about the
              network.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  4,
  pages    = "1726--1738",
  month    =  aug,
  year     =  2020,
  file     = "All Papers/D/Dang et al. 2020 - P4xos - Consensus as a Network Service.pdf",
  keywords = "computer centres;fault tolerant computing;formal
              verification;message passing;protocols;programmable forwarding
              plane;network switches;consensus protocols;implementation
              running;consensus messages;software implementation;sheer
              performance;additional network hardware;Paxos
              implementation;network
              service;Protocols;Throughput;Hardware;Performance evaluation;IEEE
              transactions;Acceleration;Software;Fault
              tolerance;reliability;availability;network programmability
              (SDN/NFV/in-network computing);Distributed Systems;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.2992106"
}

@ARTICLE{Fan2020-li,
  title    = "Shuffle Scheduling for {MapReduce} Jobs Based on Periodic Network
              Status",
  author   = "Fan, Y and Liu, W and Guo, D and Wu, W and Du, D",
  abstract = "MapReduce jobs need to shuffle a large amount of data over the
              network between mapper and reducer nodes. The shuffle time
              accounts for a big part of the total running time of the
              MapReduce jobs. Therefore, optimizing the makespan of shuffle
              phase can greatly improve the performance of MapReduce jobs. A
              large fraction of production jobs in data centers are recurring
              with predictable characteristics, and the recurring jobs split
              the network into periodic busy and idle time slots, which allows
              us to better schedule the shuffle data in order to reduce the
              makespan of shuffle phase with the future predictable network
              status available. In this paper, we formulate the shuffle
              scheduling problem with the aim to minimize the makespan of
              MapReduce shuffle phase by leveraging the predictable periodic
              network status. We then propose a simple yet effective
              network-aware shuffle scheduling algorithm (NAS) to reduce the
              number of idle time slots required to transfer the shuffle data
              so as to reduce the shuffle makespan. We also prove that the
              proposed algorithm NAS is a 3/2-approximation algorithm to the
              shuffle scheduling problem when all the future idle time slots
              have the same duration. We finally conduct experiments through
              simulations. Experimental results demonstrate the proposed
              algorithm can effectively reduce the makespan of MapReduce
              shuffle phase and increase network utilization.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  4,
  pages    = "1832--1844",
  month    =  aug,
  year     =  2020,
  keywords = "approximation theory;computational complexity;data
              handling;optimisation;parallel processing;power aware
              computing;scheduling;MapReduce jobs;production jobs;recurring
              jobs;shuffle data;future predictable network status;shuffle
              scheduling problem;MapReduce shuffle phase;predictable periodic
              network status;effective network-aware shuffle scheduling
              algorithm;shuffle makespan;future idle time slots;increase
              network utilization;mapper;reducer nodes;shuffle time
              accounts;total running time;Task analysis;Data centers;Data
              communication;Approximation algorithms;Bandwidth;Dynamic
              scheduling;MapReduce;network-aware;shuffle
              scheduling;makespan;Distributed Systems",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.2993945"
}

@ARTICLE{Talak2020-ok,
  title    = "Improving Age of Information in Wireless Networks With Perfect
              Channel State Information",
  author   = "Talak, R and Karaman, S and Modiano, E",
  abstract = "Age of information (AoI), defined as the time that elapsed since
              the last received update was generated, is a newly proposed
              metric to measure the timeliness of information updates in a
              network. We consider AoI minimization problem for a network with
              general interference constraints, and time varying channels. We
              propose two policies, namely, virtual-queue based policy and
              age-based policy when the channel state is available to the
              network scheduler at each time step. We prove that the
              virtual-queue based policy is nearly optimal, up to a constant
              additive factor, and the age-based policy is at-most a factor of
              4 away from optimality. Comparison with previous work, which
              derived age optimal policies when channel state information is
              not available to the scheduler, demonstrates significant
              improvement in age due to the availability of channel state
              information. Our analysis relies on the age conservation law and
              age-square conservation law developed in this paper, which hold
              more generally and may be of independent interest.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  4,
  pages    = "1765--1778",
  month    =  aug,
  year     =  2020,
  file     = "All Papers/T/Talak et al. 2020 - Improving Age of Information in Wireless Networks With Perfect Channel State Information.pdf",
  keywords = "conservation laws;optimisation;queueing theory;radio
              networks;scheduling;telecommunication traffic;time-varying
              channels;general interference constraints;virtual-queue based
              policy;age-based policy;network scheduler;age optimal
              policies;age conservation law;age-square conservation
              law;improving age;wireless networks;perfect channel state
              information;received update;information updates;AoI minimization
              problem;Scheduling;Wireless networks;Channel state
              information;Minimization;Information age;Interference
              constraints;Age of information (AoI);wireless
              networks;scheduling;information freshness;channel state
              information;Mobile\_Wireless;Wireless",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.2996237"
}

@ARTICLE{Wang2020-nb,
  title    = "A Scalable, {High-Performance}, and {Fault-Tolerant} Network
              Architecture for Distributed Machine Learning",
  author   = "Wang, S and Li, D and Cheng, Y and Geng, J and Wang, Y and Wang,
              S and Xia, S and Wu, J",
  abstract = "In large-scale distributed machine learning (DML), the network
              performance between machines significantly impacts the speed of
              iterative training. In this paper we propose BML, a scalable,
              high-performance and fault-tolerant DML network architecture on
              top of Ethernet and commodity devices. BML builds on BCube
              topology, and runs a fully-distributed gradient synchronization
              algorithm. Compared to a Fat-Tree network with the same size, a
              BML network is expected to take much less time for gradient
              synchronization, for both low theoretical synchronization time
              and its benefit to RDMA transport. With server/link failures, the
              performance of BML degrades in a graceful way. Experiments of
              MNIST and VGG-19 benchmarks on a testbed with 9 dual-GPU servers
              show that, BML reduces the job completion time of DML training by
              up to 56.4\% compared with Fat-Tree running state-of-the-art
              gradient synchronization algorithm.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  4,
  pages    = "1752--1764",
  month    =  aug,
  year     =  2020,
  file     = "All Papers/W/Wang et al. 2020 - A Scalable, High-Performance, and Fault-Tolerant Network Architecture for Distributed Machine Learning.pdf",
  keywords = "cloud computing;computer centres;computer networks;controller
              area networks;fault tolerance;fault tolerant computing;graphics
              processing units;learning (artificial intelligence);local area
              networks;parallel processing;synchronisation;tree data
              structures;trees (mathematics);fault-tolerant network
              architecture;large-scale distributed machine learning;Fat-Tree
              running state-of-the-art gradient synchronization algorithm;DML
              training;job completion time;BML degrades;low theoretical
              synchronization time;BML network;Fat-Tree network;BCube
              topology;commodity devices;scalable performance;iterative
              training;network
              performance;Synchronization;Servers;Training;Computational
              modeling;Fault tolerance;Fault tolerant systems;Data
              models;Distributed machine learning;gradient synchronization
              time;scalability;fault-tolerant;Distributed
              Systems;FutureInternet;MLNetworking",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.2999377"
}

@ARTICLE{Hawari2020-ip,
  title    = "{High-Accuracy} Packet Pacing on Commodity Servers for
              {Constant-Rate} Flows",
  author   = "Hawari, M and Cordero-Fuertes, J-A and Clausen, T",
  abstract = "This paper addresses the problem of high-quality packet pacing
              for constant-rate packet consumption systems, with strict
              buffering limitations. A mostly-software pacing architecture is
              developed, which has minimal hardware requirements, satisfied by
              commodity servers - rendering the proposed solution easily
              deployable in existing (data-centre) infrastructures. Two
              algorithms (free-running and frequency-controlled pacing, for
              explicitly and implicitly indicated target rates, respectively)
              are specified, and formally analysed. The proposed solution,
              including both algorithms, is implemented, and is tested on real
              hardware and under real conditions. The performance of these
              implementations is experimentally evaluated and compared to
              existing mechanisms, available in general-purpose hardware.
              Results of both exhaustive experiments, and of an analytical
              modeling, indicate that the proposed approach is able to perform
              low-jitter packet pacing on commodity hardware, being thus
              suitable for constant rate transmission and consumption in media
              production scenarios.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "1953--1967",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/H/Hawari et al. 2020 - High-Accuracy Packet Pacing on Commodity Servers for Constant-Rate Flows.pdf",
  keywords = "computer centres;file servers;jitter;constant rate
              transmission;low-jitter packet pacing;data-centre;mostly-software
              pacing architecture;constant-rate packet consumption
              systems;constant-rate flows;commodity servers;high-accuracy
              packet pacing;Receivers;Hardware;Streaming
              media;Software;SMPTE;Delays;Production;Packet
              pacing;jitter;constant rate;SMPTE;implementation;buffering;pacing
              assistant;systems architecture;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3001672"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Karakoc2020-um,
  title    = "{Multi-Layer} Decomposition of Network Utility Maximization
              Problems",
  author   = "Karako{\c c}, N and Scaglione, A and Nedi{\'c}, A and Reisslein,
              M",
  abstract = "We describe a distributed framework for resource sharing problems
              that arise in communications, micro-economics, and various
              networking applications. In particular, we consider a
              hierarchical multi-layer decomposition for network utility
              maximization (ML-NUM), where functionalities are assigned to
              different layers. The proposed methodology creates solutions with
              central management and distributed computations to the resource
              allocation problems. In non-stationary environments, the
              technique aims to respond quickly to the dynamics of the network
              by decreasing delay by partially shifting the communication and
              computational burden to the network edges. Our main contribution
              is a detailed analysis under the assumption that the network
              changes are on the same time-scale as the convergence time of the
              algorithms used for local computations. Moreover, assuming strong
              concavity and smoothness of the users' objective functions, and
              under some stability conditions for each layer, we present
              convergence rates and optimality bounds for the ML-NUM framework.
              In addition, the main benefits of the proposed method are
              demonstrated with numerical examples.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "2077--2091",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/K/Karakoç et al. 2020 - Multi-Layer Decomposition of Network Utility Maximization Problems.pdf",
  keywords = "computational complexity;distributed algorithms;network theory
              (graphs);resource allocation;distributed optimization
              algorithm;network utility maximization;ML-NUM framework;network
              edges;resource allocation problems;multilayer
              decomposition;resource sharing problems;Resource
              management;Convergence;Heuristic algorithms;Indexes;Gradient
              methods;Distributed algorithms;IEEE transactions;Distributed
              computation;network resource allocation;GeneralInterest",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3003925"
}

@ARTICLE{Sexton2020-bi,
  title    = "On Provisioning Slices and Overbooking Resources in Service
              Tailored Networks of the Future",
  author   = "Sexton, C and Marchetti, N and DaSilva, L A",
  abstract = "There is a trade-off in network slicing between the twin goals of
              providing tailored performance and increasing resource
              utilisation through increased opportunities for sharing. To
              balance this trade-off, we propose a system consisting of assured
              resources, which are available if needed over the lifetime of a
              slice, and auxiliary resources, which are offered on a
              probabilistic basis in the short-term based on forecasted
              resource demand. We employ the practice of overbooking, which is
              widely used in many industries such as airlines and hotels, to
              increase resource utilisation when offering auxiliary resources.
              After deriving probabilistic results relating to the availability
              of auxiliary resources, we then design an algorithm to determine
              how much to overbook by. We also propose several ways of
              distributing auxiliary resource offers among slices, as well as
              providing an approach for dealing with overbookings. Finally, we
              highlight the conditions that are conducive to effective
              overbooking by examining the effects of varying several key
              system parameters, providing both analytical and numerical
              results.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "2106--2119",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/S/Sexton et al. 2020 - On Provisioning Slices and Overbooking Resources in Service Tailored Networks of the Future.pdf",
  keywords = "mobile radio;resource allocation;network slicing;forecasted
              resource demand;auxiliary resource;resource utilisation;service
              tailored networks;overbooking resources;provisioning
              slices;Resource management;Network
              slicing;Contracts;Forecasting;Dynamic scheduling;Numerical
              models;IEEE transactions;Network slicing;resource
              allocation;overbooking;5G6G",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3004443"
}

@ARTICLE{Yuan2020-hc,
  title    = "Optimal and Approximation Algorithms for Joint Routing and
              Scheduling in {Millimeter-Wave} Cellular Networks",
  author   = "Yuan, Dingwen and Lin, Hsuan-Yin and Widmer, J{\"o}rg and
              Hollick, Matthias",
  abstract = "Millimeter-wave (mmWave) communication is a promising technology
              to cope with the exponential increase in 5G data traffic. Such
              networks typically require a very dense deployment of base
              stations. A subset of those, so-called macro base stations,
              feature high-bandwidth connection to the core network, while
              relay base stations are connected wirelessly. To reduce cost and
              increase flexibility, wireless backhauling is needed to connect
              both macro to relay as well as relay to relay base stations. The
              characteristics of mmWave communication mandates new paradigms
              for routing and scheduling. The paper investigates scheduling
              algorithms under different interference models. To showcase the
              scheduling methods, we study the maximum throughput fair
              scheduling problem. Yet the proposed algorithms can be easily
              extended to other problems. For a full-duplex network under the
              no interference model, we propose an efficient polynomial-time
              scheduling method, the schedule-oriented optimization. Further,
              we prove that the problem is NP-hard if we assume pairwise link
              interference model or half-duplex radios. Fractional weighted
              coloring based approximation algorithms are proposed for these
              NP-hard cases. Moreover, the approximation algorithm parallel
              data stream scheduling is proposed for the case of half-duplex
              network under the no interference model. It has better
              approximation ratio than the fractional weighted coloring based
              algorithms and even attains the optimal solution for the special
              case of uniform orthogonal backhaul networks.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "2188--2202",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/Y/Yuan et al. 2020 - Optimal and Approximation Algorithms for Joint Routing and Scheduling in Millimeter-Wave Cellular Networks.pdf",
  keywords = "Scheduling;Relays;Interference;Approximation
              algorithms;Routing;Base stations;Radio
              frequency;Millimeter-wave;5G;backhaul;max-min
              fairness;full-duplex;half-duplex;matching;coloring;conflict
              graph;pairwise link interference;single user spatial multiplexing",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3006312"
}

@ARTICLE{Liu2020-fv,
  title    = "Network Utility Maximization Under Maximum Delay Constraints and
              Throughput Requirements",
  author   = "Liu, Q and Zeng, H and Chen, M",
  abstract = "We consider a multi-path routing problem of maximizing the
              aggregate user utility over a multi-hop network, subject to link
              capacity constraints, maximum end-to-end delay constraints, and
              user throughput requirements. A user's utility is a concave
              function of the achieved throughput or the experienced maximum
              delay. The problem is important for supporting real-time
              multimedia traffic and is uniquely challenging due to the need of
              simultaneously considering maximum delay constraints and
              throughput requirements. In this paper, we first show that it is
              NP-complete either (i) to construct a feasible solution strictly
              meeting all constraints, or (ii) to obtain an optimal solution
              after relaxing either the maximum delay constraints or the
              throughput requirements. We then develop a polynomial-time
              approximation algorithm named PASS. The design of PASS leverages
              a novel understanding between non-convex maximum-delay-aware
              problems and their convex average-delay-aware counterparts, which
              can be of independent interest and suggests a new avenue for
              solving maximum-delay-aware network optimization problems. We
              prove that PASS always obtains approximate solutions (i.e., with
              theoretical performance guarantees), at the cost of violating
              both the maximum delay constraints and the throughput
              requirements by up to constant ratios. We also develop two
              variants of PASS, named PASS-M and PASS-T, to generate
              approximate solutions at the cost of violating either the maximum
              delay constraints or the throughput requirements by up to
              problem-dependent ratios. We evaluate our solutions using
              extensive simulations on Amazon EC2 datacenters supporting
              video-conferencing traffic. Compared to the existing algorithms
              and a conceivable baseline, our solutions obtain up to 100\%
              improvement of utilities, by meeting the throughput requirements
              but relaxing the maximum delay constraints to the extent
              acceptable for practical video conferencing applications.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "2132--2145",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/L/Liu et al. 2020 - Network Utility Maximization Under Maximum Delay Constraints and Throughput Requirements.pdf",
  keywords = "approximation theory;computational complexity;concave
              programming;convex programming;delays;multimedia
              communication;multipath channels;telecommunication network
              routing;telecommunication traffic;teleconferencing;video
              communication;multipath routing problem;link capacity
              constraints;multihop network;real-time multimedia
              traffic;video-conferencing traffic;Amazon EC2
              datacenters;PASS-T;PASS-M;NP-complete problem;network utility
              maximization;maximum-delay-aware network optimization
              problems;nonconvex maximum-delay-aware problems;maximum
              end-to-end delay constraints;throughput
              requirements;Delays;Throughput;Approximation
              algorithms;Routing;Unicast;Aggregates;Optimization;Delay-sensitive
              multiple-unicast network flow;delay-aware multi-path
              routing;network utility maximization;GeneralInterest",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3007842"
}

@ARTICLE{Chkirbene2020-lx,
  title    = "{LaScaDa}: A Novel Scalable Topology for Data Center Network",
  author   = "Chkirbene, Z and Hadjidj, R and Foufou, S and Hamila, R",
  abstract = "The growth of cloud-based services is mainly supported by the
              core networking infrastructures of large-scale data centers,
              while the scalability of these services is influenced by the
              performance and dependability characteristics of data centers.
              Hence, the data center network must be agile and reconfigurable
              in order to respond quickly to the ever-changing application
              demands and service requirements. The network must also be able
              to interconnect the big number of nodes, and provide an efficient
              and fault-tolerant routing service to upper-layer applications.
              In response to these challenges, the research community began
              exploring novel interconnect topologies, namely: Flecube, DCell,
              Ficonn, HyperFlaNet and BCube. However, these topologies either
              scale too fast (grows exponentially in size), or too slow, and
              therefore suffer from performance bottlenecks. In this paper, we
              propose a novel data center topology called LaScaDa (Layered
              Scalable Data Center) as a new solution for building scalable and
              cost-effective data center networking infrastructures. The
              proposed topology organizes nodes in clusters of similar
              structure, then interconnect these clusters in a well-crafted
              pattern and system of coordinates for nodes to reduce the number
              of redundant connections between clusters, while maximizing
              connectivity. LaScaDa forwards packets between nodes using a new
              hierarchical row-based routing algorithm. The algorithm
              constructs the route to the source based on the modular
              difference between the source and destination coordinates.
              Furthermore, the proposed topology interconnects a large number
              of nodes using a small node degree. This strategy increases the
              number of directly connected clusters and avoids redundant
              connections. As a result, we get a good quality of nodes in terms
              of average path length (APL), bisection bandwidth, and aggregated
              bottleneck throughput. Experimental results show that LaScaDa has
              better performance than DCell, BCube, and HyperBcube in terms of
              scalability, while providing a good quality of service.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "2051--2064",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/C/Chkirbene et al. 2020 - LaScaDa - A Novel Scalable Topology for Data Center Network.pdf",
  keywords = "cloud computing;computer centres;computer network
              management;quality of service;telecommunication network
              routing;telecommunication network topology;aggregated bottleneck
              throughput;bisection bandwidth;average path length;directly
              connected clusters;destination coordinates;source
              coordinates;hierarchical row-based routing algorithm;well-crafted
              pattern;BCube interconnect topology;HyperFlaNet interconnect
              topology;Ficonn interconnect topology;DCell interconnect
              topology;Flecube interconnect topology;upper-layer
              application;fault-tolerant routing service;layered scalable data
              center;data center networking infrastructures;service
              requirements;large-scale data centers;core networking
              infrastructures;cloud-based services;scalable
              topology;LaScaDa;data center topology;upper-layer
              applications;quality of service;Topology;Network topology;Data
              centers;Routing;Scalability;Fault tolerance;Fault tolerant
              systems;Data center network;network topology;average path
              length;bisection bandwidth;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3008512"
}

@ARTICLE{Key2020-df,
  title    = "Pricing, Competition and Content for Internet Service Providers",
  author   = "Key, P and Steinberg, R",
  abstract = "We examine competition between two Internet Service Providers
              (ISPs), where the first ISP provides basic Internet service,
              while the second ISP provides Internet service plus content,
              i.e., enhanced service, where the first ISP can partner with a
              Content Provider to provide the same content as the second ISP.
              When such a partnering arrangement occurs, the Content Provider
              pays the first ISP a transfer price for delivering the content.
              Users have heterogeneous preferences, and each in general faces
              three options: (1) buy basic Internet service from the first ISP;
              (2) buy enhanced service from the second ISP; or (3) buy enhanced
              service jointly from the first ISP and the Content Provider. We
              derive results on the existence and uniqueness of a Nash
              equilibrium, and provide closed-form expressions for the prices,
              user masses, and profits of the two ISPs and the Content
              Provider. When the first ISP has the ability to choose the
              transfer price, then when congestion is linear in the load, it is
              never optimal for the first ISP to set a negative transfer price
              in the hope of attracting more revenue from additional customers
              desiring enhanced service. Conversely, when congestion is
              sufficiently super-linear, the optimal strategy for the first ISP
              is either to set a negative transfer price (subsidizing the
              Content Provider) or to set a high transfer price that shuts the
              Content Provider out of the market.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "2285--2298",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/K/Key and Steinberg 2020 - Pricing, Competition and Content for Internet Service Providers.pdf",
  keywords = "game theory;Internet;pricing;Internet Service
              Providers;ISP;Content Provider;Nash equilibrium;closed-form
              expressions;transfer price;optimal strategy;Web and internet
              services;Pricing;Nash equilibrium;Bandwidth;Games;Broadband
              communication;Communication networks;competition;content
              provider;optimal pricing;Nash equilibrium;profit;ComputerNetworks",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3010550"
}

@ARTICLE{Kogan2020-hv,
  title    = "Towards {Software-Defined} Buffer Management",
  author   = "Kogan, K and Menikkumbura, D and Petri, G and Noh, Y and
              Nikolenko, S I and Sirotkin, A and Eugster, P",
  abstract = "Buffering architectures and policies for their efficient
              management are core ingredients of a network architecture.
              However, despite strong incentives to experiment with and deploy
              new policies, opportunities for changing anything beyond minor
              elements are limited. We introduce a new specification language,
              OpenQueue, that allows to express virtual buffering architectures
              and management policies representing a wide variety of economic
              models. OpenQueue allows users to specify entire buffering
              architectures and policies conveniently through several
              comparators and simple functions. We show examples of buffer
              management policies in OpenQueue and empirically demonstrate its
              impact on performance in various settings.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "2337--2349",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/K/Kogan et al. 2020 - Towards Software-Defined Buffer Management.pdf",
  keywords = "buffer storage;computer network management;software defined
              networking;specification languages;software-defined buffer
              management;core ingredients;network architecture;specification
              language;OpenQueue;virtual buffering architectures;buffer
              management policies;Admission control;Computer
              architecture;Process control;Optimized production
              technology;Hardware;IEEE transactions;Economics;Admission
              control;computer buffers;scheduling algorithms;ComputerNetworks",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3011048"
}

@ARTICLE{Wang2020-ut,
  title    = "Joint Server Assignment and Resource Management for {Edge-Based}
              {MAR} System",
  author   = "Wang, C and Zhang, S and Qian, Z and Xiao, M and Wu, J and Ye, B
              and Lu, S",
  abstract = "Mobile Augmented Reality (MAR) applications usually contain
              computation-intensive tasks which far outstrip the capability of
              mobile devices. One way to overcome this is offloading
              computation-intensive MAR tasks to remote clouds. However, the
              wide area network delay is hard to reduce. Thanks to edge
              computing, we can offload MAR tasks to nearby servers. Prior
              studies focus on either single-task MAR applications offloading
              or dependent tasks offloading for a single user. In this article,
              we study the offloading decision of MAR applications from
              multiple users, each of which is comprised of a chain of
              dependent tasks, over a generic cloud-edge system consisting of a
              group of heterogeneous edge servers and remote clouds. We
              formulate the Multi-user Multi-task MAR Application Scheduling
              (M3AS) problem, which is NP-hard. We present Mutas, an efficient
              scheduling algorithm that jointly optimizes server assignment and
              resource management. We also consider the online version of M3AS
              and present OnMutas. Extensive evaluations demonstrate that both
              Mutas and OnMutas can significantly reduce the service delays of
              MAR applications when compared to three other heuristics.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "2378--2391",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/W/Wang et al. 2020 - Joint Server Assignment and Resource Management for Edge-Based MAR System.pdf",
  keywords = "augmented reality;cloud computing;mobile
              computing;scheduling;mobile augmented reality
              applications;computation-intensive tasks;mobile
              devices;computation-intensive MAR tasks;remote clouds;wide area
              network delay;nearby servers;single-task MAR
              applications;dependent tasks;offloading decision;multiple
              users;generic cloud-edge system;heterogeneous edge
              servers;resource management;joint server assignment;edge-based
              MAR system;multiuser multitask MAR application scheduling;Task
              analysis;Servers;Delays;Cloud computing;Resource
              management;Mobile handsets;Edge computing;Mobile augmented
              reality;edge computing;multi-task application;scheduling;resource
              management;EdgeFogCloudIoT",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3012410"
}

@ARTICLE{Zhang2020-jr,
  title    = "Rethinking Fast and Friendly Transport in Data Center Networks",
  author   = "Zhang, T and Huang, J and Chen, K and Wang, J and Chen, J and
              Pan, Y and Min, G",
  abstract = "The sustainable growth of bandwidth has been an inevitable
              tendency in current Data Center Networks (DCN). However, the
              dramatic expansion of link capacity offers a remarkable challenge
              to the transport layer protocols of DCN, i.e., how to converge
              fast and enable data flow to utilize the high bandwidth
              effectively. Meanwhile, the new protocol should be compatible to
              the traditional TCP because the applications with old TCP
              versions are still widely deployed. Therefore, it is important to
              achieve a trade-off between the aggressiveness and
              TCP-friendliness in protocol design. In this article, we first
              empirically investigate why the existing typical data center TCP
              variants naturally fail to guarantee both fast convergence and
              TCP friendliness. Then, we design a new transport protocol for
              DCN, namely Fast and Friendly Converging (FFC), which makes
              independent decisions and self-adjustment through retrieving the
              two-dimensional congestion notification from both RTT and ECN. We
              further present a mathematic model to analyze its competing
              behavior and converging process. The results from simulation
              experiments and real implementation show that FFC can achieve
              fast convergence, thus benefiting the flow completion time.
              Moreover, when coexisting with the traditional TCP, FFC also
              presents a moderate behavior, while introducing trivial
              deployment overhead only at the end-hosts.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  5,
  pages    = "2364--2377",
  month    =  oct,
  year     =  2020,
  file     = "All Papers/Z/Zhang et al. 2020 - Rethinking Fast and Friendly Transport in Data Center Networks.pdf",
  keywords = "computer centres;Internet;telecommunication congestion
              control;transport protocols;current Data Center
              Networks;DCN;dramatic expansion;link capacity;transport layer
              protocols;TCP-friendliness;protocol design;fast convergence;TCP
              friendliness;transport protocol;Friendly
              Converging;FFC;converging process;flow completion time;Friendly
              transport;typical data center TCP;Convergence;Data
              centers;Switches;Bandwidth;Transport protocols;Computer
              science;Data center;convergence;TCP
              friendliness;RTT;ECN;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3012556"
}

@ARTICLE{Munir2020-yr,
  title    = "Network Scheduling and Compute Resource Aware Task Placement in
              Datacenters",
  author   = "Munir, A and He, T and Raghavendra, R and Le, F and Liu, A X",
  abstract = "To improve the performance of data-intensive applications,
              existing datacenter schedulers optimize either the placement of
              tasks or the scheduling of network flows. The task scheduler
              strives to place tasks close to their input data (i.e., maximize
              data locality) to minimize network traffic, while assuming fair
              sharing of the network. The network scheduler strives to finish
              flows as quickly as possible based on their sources and
              destinations determined by the task scheduler, while the
              scheduling is based on flow properties (e.g., size, deadline, and
              correlation) and not bound to fair sharing. Inconsistent
              assumptions of the two schedulers can compromise the overall
              application performance. In this paper, we propose NEAT+, a task
              scheduling framework that leverages information from the
              underlying network scheduler and available compute resources to
              make task placement decisions. The core of NEAT+ is a task
              completion time predictor that estimates the completion time of a
              task under given network condition and a given network scheduling
              policy. NEAT+ leverages the predicted task completion times to
              minimize the average completion time of active tasks. Evaluation
              using ns2 simulations and real-testbed shows that NEAT+ improves
              application performance by up to $3.7\times$ for the suboptimal
              network scheduling policies and up to 33\% for the optimal
              network scheduling policy.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  6,
  pages    = "2435--2448",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/M/Munir et al. 2020 - Network Scheduling and Compute Resource Aware Task Placement in Datacenters.pdf",
  keywords = "Task analysis;Processor scheduling;IEEE transactions;Data
              transfer;Schedules;Correlation;Computer science;Datacenter
              networks;network scheduling;task placement;cloud
              computing;Datacentre;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3013548"
}

@ARTICLE{Quan2020-up,
  title    = "Counterintuitive Characteristics of Optimal Distributed {LRU}
              Caching Over Unreliable Channels",
  author   = "Quan, G and Tan, J and Eryilmaz, A",
  abstract = "Least-recently-used (LRU) caching and its variants have
              conventionally been used as a fundamental and critical method to
              ensure fast and efficient data access in computer and
              communication systems. Emerging data-intensive applications over
              unreliable channels, e.g., mobile edge computing and wireless
              content delivery networks, have imposed new challenges in
              optimizing LRU caching in environments prone to failures. Most
              existing studies focus on reliable channels, e.g., on wired Web
              servers and within data centers, which have already yielded good
              insights and successful algorithms. Surprisingly, we show that
              these insights do not necessarily hold true for unreliable
              channels. We consider a single-hop multi-cache distributed system
              with data items being dispatched by random hashing. The objective
              is to design efficient cache organization and data placement that
              minimize the miss probability. The former allocates the total
              memory space to each of the involved caches. The latter decides
              data routing and replication strategies. Analytically, we
              characterize the asymptotic miss probabilities for unreliable LRU
              caches, and optimize the system design. Remarkably, these results
              sometimes are counterintuitive, differing from the ones obtained
              for reliable caches. We discover an interesting phenomenon:
              allocating the cache space unequally can achieve a better
              performance, even when channel reliability levels are equal. In
              addition, we prove that splitting the total cache space into
              separate LRU caches can achieve a lower asymptotic miss
              probability than organizing the total space in a single LRU
              cache. These results provide new and even counterintuitive
              insights that motivate novel designs for caching systems over
              unreliable channels.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  6,
  pages    = "2461--2474",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/Q/Quan et al. 2020 - Counterintuitive Characteristics of Optimal Distributed LRU Caching Over Unreliable Channels.pdf",
  keywords = "Resource management;Reliability theory;Hash functions;Nickel;IEEE
              transactions;Organizations;LRU caching;reliability;memory space
              allocation;Distributed Systems",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3015474"
}

@ARTICLE{Mohammadkhan2020-ot,
  title    = "{CleanG---Improving} the Architecture and Protocols for Future
              Cellular Networks With {NFV}",
  author   = "Mohammadkhan, A and Ramakrishnan, K K and Jain, V A",
  abstract = "With the rapid increase in the number of users and changing
              pattern of network usage, cellular networks will continue to be
              challenged meeting bandwidth and latency requirements. A
              significant contributor to latency and overhead is cellular
              network's complex control-plane. We propose CleanG, a new packet
              core architecture and significantly more efficient control-plane
              protocol, that exploits the capabilities of modern-day Network
              Function Virtualization (NFV) platforms. CleanG is a single
              component NFV-based architecture. With the elastic scalability
              offered by NFV, the data and control sub-components of the core
              can scale, adapting to workload demand. CleanG eliminates the use
              of GTP tunnels for data packets and the associated complex
              protocol for coordination across multiple, distributed components
              for setting up and managing them. We carefully examine the use of
              each protocol message exchange (and the component fields of those
              messages) in developing a substantially simplified protocol,
              while retaining similar essential functionality for security,
              mobility, and air-interface resource management. We have
              implemented CleanG on the OpenNetVM platform and perform an
              apples-to-apples comparison with the existing 3GPP LTE
              architecture and an architecture that separates the control and
              user plane (the CUPS-based architecture like the 5G
              architecture). Measurements on our testbed show that CleanG
              substantially reduces both control and data plane latency, and
              significantly increases system capacity.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  6,
  pages    = "2559--2572",
  month    =  dec,
  year     =  2020,
  keywords = "Computer architecture;Protocols;5G mobile communication;Long Term
              Evolution;Cellular networks;3GPP;Hardware;Cellular
              networks;network protocols;network architecture;network function
              virtualization;software defined networking;5G6G",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3015946"
}

@ARTICLE{Zhang2020-rr,
  title    = "Prophet: Toward Fast, {Error-Tolerant} {Model-Based} Throughput
              Prediction for Reactive Flows in {DC} Networks",
  author   = "Zhang, J and Gao, K and Yang, Y R and Bi, J",
  abstract = "As modern network applications (e.g., large data analytics)
              become more distributed and can conduct application-layer traffic
              adaptation, they demand better network visibility to better
              orchestrate their data flows. As a result, the ability to predict
              the available bandwidth for a set of flows has become a
              fundamental requirement of today's networking systems. While
              there are previous studies addressing the case of non-reactive
              flows, the prediction for reactive flows, e.g., flows managed by
              TCP congestion control algorithms, still remains an open problem.
              In this paper, we take the first step to solving this problem in
              a data center network. To address both theoretical and practical
              challenges, we introduce a novel learning-based prediction system
              based on the NUM model, with two key techniques named fast factor
              learning (FFL) and efficient flow sampling. We adopt novel
              techniques to overcome practical concerns such as scalability,
              convergence and unknown system parameters. A system, Prophet, is
              proposed leveraging the emerging technologies of Software Defined
              Networking (SDN) to realize the model. Evaluations demonstrate
              that our solution achieves significant accuracy in a wide range
              of settings.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  6,
  pages    = "2475--2488",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/Z/Zhang et al. 2020 - Prophet - Toward Fast, Error-Tolerant Model-Based Throughput Prediction for Reactive Flows in DC Networks.pdf",
  keywords = "Throughput;Predictive models;Bandwidth;Data centers;Computational
              modeling;Convergence;Computer science;Data center
              networks;throughput prediction;network utility
              maximization;Datacentre",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3016838"
}

@ARTICLE{Gao2020-lo,
  title    = "Congestion Minimization for Service Chain Routing Problems With
              Path Length Considerations",
  author   = "Gao, L and Rouskas, G N",
  abstract = "Network function virtualization (NFV), with its perceived
              potential to accelerate service deployment and to introduce
              flexibility in service provisioning, has drawn a growing interest
              from industry and academia alike over the past few years. One of
              the key challenges in realizing NFV is the service chain routing
              problem, whereby traffic must be routed so as to traverse the
              various components of a network service that have been mapped
              onto the underlying network. In this work, we consider the online
              service chain routing problem. We route the service chain with
              the goal of jointly minimizing the maximum network congestion and
              the number of hops from the source to the destination. To this
              end, we present a simple yet effective online algorithm in which
              the routing decision is irrevocably made without prior knowledge
              of future requests. We prove that our algorithm is $O(\log m)$
              -competitive in terms of congestion minimization, where $m$ is
              the number of edges of the underlying network topology, and we
              show that this ratio is asymptotically optimal.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  6,
  pages    = "2643--2656",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/G/Gao and Rouskas 2020 - Congestion Minimization for Service Chain Routing Problems With Path Length Considerations.pdf",
  keywords = "Routing;Heuristic algorithms;Minimization;Network function
              virtualization;Resource management;IEEE transactions;Noise
              measurement;Network function virtualization;virtual network
              functions;NFV orchestration;online algorithm;resource
              allocation;NFV",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3017792"
}

@ARTICLE{Xia2020-ra,
  title    = "{FTrack}: Parallel Decoding for {LoRa} Transmissions",
  author   = "Xia, X and Zheng, Y and Gu, T",
  abstract = "LoRa has emerged as a promising Low-Power Wide Area Network
              (LP-WAN) technology to connect a huge number of
              Internet-of-Things (IoT) devices. The dense deployment and an
              increasing number of IoT devices lead to intense collisions due
              to uncoordinated transmissions. However, the current MAC/PHY
              design of LoRaWAN fails to recover collisions, resulting in
              degraded performance as the system scales. This article presents
              FTrack, a novel communication paradigm that enables demodulation
              of collided LoRa transmissions. FTrack resolves LoRa collisions
              at the physical layer and thereby supports parallel decoding for
              LoRa transmissions. We propose a novel technique to separate
              collided transmissions by jointly considering both the time
              domain and the frequency domain features. The proposed technique
              is motivated from two key observations: (1) the symbol edges of
              the same frame exhibit periodic patterns, while the symbol edges
              of different frames are usually misaligned in time; (2) the
              frequency of LoRa signal increases continuously in between the
              edges of symbol, yet exhibits sudden changes at the symbol edges.
              We detect the continuity of signal frequency to remove
              interference and further exploit the time-domain information of
              symbol edges to recover symbols of all collided frames. We
              substantially optimize computation-intensive tasks and meet the
              real-time requirements of parallel LoRa decoding. We implement
              FTrack on a low-cost software defined radio. Our testbed
              evaluations show that FTrack demodulates collided LoRa frames
              with low symbol error rates in diverse SNR conditions. It
              increases the throughput of LoRaWAN in real usage scenarios by up
              to 3 times.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  6,
  pages    = "2573--2586",
  month    =  dec,
  year     =  2020,
  keywords = "Chirp;Image edge detection;Frequency
              modulation;Decoding;Time-domain analysis;Time-frequency
              analysis;Internet of Things;LoRaWAN;collision resolving;parallel
              decoding",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3018020"
}

@ARTICLE{Yan2020-jk,
  title    = "Service Function Path Provisioning With Topology Aggregation in
              {Multi-Domain} Optical Networks",
  author   = "Yan, B and Zhao, Y and Yu, X and Li, Y and Rahman, S and He, Y
              and Xin, X and Zhang, J",
  abstract = "Traffic flows are often processed by a chain of Service Functions
              (SFs) (known as Service Function Chaining (SFC)) to satisfy
              service requirements. The deployed path for a SFC is called
              Service Function Path (SFP). SFs can be virtualized and migrated
              to datacenters, thanks to the evolution of Software Defined
              Network (SDN) and Network Function Virtualization (NFV). In such
              a scenario, provisioning of paths (i.e., SFPs) between
              virtualized network functions is an important problem. SFP
              provisioning becomes more complex in a multi-domain network
              topology. `Topology aggregation' helps to create a single-domain
              view of such a network by abstracting multi-domain networks.
              However, traditional `topology aggregation' methods are unable to
              abstract SF resources properly, which is required for SFP
              provisioning. In this paper, we propose an SFC-Oriented Topology
              Aggregation (SOTA) method to enable abstraction for SFs in
              multi-domain optical networks. This study explores the node and
              the link aggregation degree to evaluate information compression
              during the `Topology aggregation' process. Additionally, we also
              propose a new data structure named wheel matrix and related
              operations to store routing information in the aggregated
              topology. Based on SOTA, we propose two cross-domain SFP
              provisioning algorithms named Ordered Anchor Selection (OAS) and
              $k$ -paths OAS (K-OAS), and a benchmark named Global OAS (GOAS).
              Simulation results show that SOTA could aggregate large-scale
              multi-domain optical networks into a small network that contains
              only 6.9\% of the nodes and 10.1\% of the links. Both OAS and
              K-OAS can calculate SFPs efficiently and reduce blocking
              probability up to 52.10\% compared to the benchmark.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  6,
  pages    = "2755--2767",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/Y/Yan et al. 2020 - Service Function Path Provisioning With Topology Aggregation in Multi-Domain Optical Networks.pdf",
  keywords = "Topology;Optical fiber networks;Network
              topology;Routing;Bandwidth;Aggregates;Optical network;service
              function chain;service function path;topology aggregation;NFV",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3019708"
}

@ARTICLE{Wang2020-pl,
  title    = "On the Tussle Between {Over-the-Top} and Internet Service
              Providers: Analysis of the Netflix- Comcast Type of Deals",
  author   = "Wang, X and Ma, R T B",
  abstract = "Over-the-top (OTT) services reach users via the open Internet
              without dedicated infrastructures and have experienced enormous
              growth in recent years. Netflix, an OTT streaming provider, now
              accounts for more than one-third of peak U.S. downstream traffic
              and causes cord-cutting of traditional cable pay-TV services from
              incumbent Internet service providers (ISPs). However, the service
              quality of video streaming is still influenced by the last-mile
              Internet access providers, who do not have incentives to deploy
              enough capacity and want to charge OTT service providers (OSPs)
              for direct connection. Although Netflix has reached deals with
              ISPs such as Comcast and Verizon to improve service quality,
              their undisclosed agreements have raised concerns about net
              neutrality. In this article, we study the economics of the
              Netflix-Comcast type of deals and derive the conditions under
              which an OSP and an ISP would reach such a deal. We analyze the
              impact of a deal transaction on the revenue of providers, the
              utility of users and the social welfare. Based on these results,
              we further classify different policy regimes and draw regulatory
              implications that depend on the intensity of ex-post deal
              competition and the cost of the deal. Our results can help
              understand how existing deals were made, how future deals might
              emerge, and how regulators should respond to various market
              conditions and scenarios.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  28,
  number   =  6,
  pages    = "2823--2835",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/W/Wang and Ma 2020 - On the Tussle Between Over-the-Top and Internet Service Providers - Analysis of the Netflix- Comcast Type of Deals.pdf",
  keywords = "Over-the-top media services;Internet;Throughput;Nash
              equilibrium;IEEE transactions;Network neutrality;Internet
              economics;Nash equilibrium;OSP-ISP deal;regulation
              policy;ComputerNetworks",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3024770"
}

@ARTICLE{Giotsas2021-ij,
  title    = "{O} Peer, Where Art Thou? Uncovering Remote Peering
              Interconnections at {IXPs}",
  author   = "Giotsas, V and Nomikos, G and Kotronis, V and Sermpezis, P and
              Gigis, P and Manassakis, L and Dietzel, C and Konstantaras, S and
              Dimitropoulos, X",
  abstract = "Internet eXchange Points (IXPs) are Internet hubs that mainly
              provide the switching infrastructure to interconnect networks and
              exchange traffic. While the initial goal of IXPs was to bring
              together networks residing in the same city or country, and thus
              keep local traffic local, this model is gradually shifting. Many
              networks connect to IXPs without having physical presence at
              their switching infrastructure. This practice, called Remote
              Peering, is changing the Internet topology and economy, and has
              become the subject of a contentious debate within the network
              operators' community. However, despite the increasing attention
              it attracts, the understanding of the characteristics and impact
              of remote peering is limited. In this work, we introduce and
              validate a heuristic methodology for discovering remote peers at
              IXPs. We (i) identify critical remote peering inference
              challenges, (ii) infer remote peers with high accuracy (>97\%)
              and coverage (94\%) per IXP, and (iii) characterize different
              aspects of the remote peering ecosystem by applying our
              methodology to 30 large IXPs. We observe that remote peering is a
              significantly common practice in all the studied IXPs; for the
              largest IXPs, remote peers account for 40\% of their member base.
              We also show that today, IXP growth is mainly driven by remote
              peering, which contributes two times more than local peering.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  1,
  pages    = "1--16",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/G/Giotsas et al. 2021 - O Peer, Where Art Thou - Uncovering Remote Peering Interconnections at IXPs.pdf",
  keywords = "Internet;Switches;Ecosystems;Resilience;Routing;IEEE
              transactions;Urban areas;Communications technology;communication
              systems;communication networks;computer networks;Internet;IP
              networks;metropolitan area networks;Internet
              topology;telecommunications;telecommunication network
              topology;ComputerNetworks",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3025945"
}

@ARTICLE{Wang2021-tm,
  title    = "Service Placement for Collaborative Edge Applications",
  author   = "Wang, L and Jiao, L and He, T and Li, J and Bal, H",
  abstract = "Edge computing is emerging as a promising computing paradigm for
              supporting next-generation applications that rely on low-latency
              network connections in the Internet-of-Things (IoT) era. Many
              edge applications, such as multi-player augmented reality (AR)
              gaming and federated machine learning, require that distributed
              clients work collaboratively for a common goal through message
              exchanges. Given an edge network, it is an open problem how to
              deploy such collaborative edge applications to achieve the best
              overall system performance. This paper presents a formal study of
              this problem. We first provide a mix of cost models to capture
              the system. Based on a thorough formulation, we propose an
              iterative algorithm dubbed ITEM, where in each iteration, we
              construct a graph to encode all the costs and convert the cost
              optimization problem into a graph cut problem. By obtaining the
              minimum $s-t$ cut via existing max-flow algorithms, we address
              the original problem via solving a series of graph cuts. We
              rigorously prove that ITEM has a parameterized constant
              approximation ratio. Inspired by the optimal stopping theory, we
              further design an online algorithm called OPTS, based on
              optimally alternating between partial and full placement updates.
              Our evaluations with real-world data traces demonstrate that ITEM
              performs close to the optimum (within 5\%) and converges fast.
              OPTS achieves a bounded performance as expected while reducing
              full updates by more than 67\% of the time.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  1,
  pages    = "34--47",
  month    =  feb,
  year     =  2021,
  keywords = "Collaboration;Synchronization;Edge computing;Optimized production
              technology;Cloud computing;Quality of service;Edge
              computing;service placement;performance
              optimization;approximation;NFV;EdgeFogCloudIoT",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3025985"
}

@ARTICLE{Guo2021-zk,
  title    = "{AggreFlow}: Achieving Power Efficiency, Load Balancing, and
              Quality of Service in Data Center Networks",
  author   = "Guo, Z and Xu, Y and Liu, Y-F and Liu, S and Chao, H J and Zhang,
              Z-L and Xia, Y",
  abstract = "Power-efficient Data Center Networks (DCNs) have been proposed to
              save power of DCNs using OpenFlow. In these DCNs, the OpenFlow
              controller adaptively turns on/off links and OpenFlow switches to
              form a minimum-power subnet that satisfies the traffic demand. As
              the subnet changes, flows are dynamically routed and rerouted to
              the routes composed of active switches and links. However,
              existing flow scheduling schemes could cause undesired results:
              (1) power inefficiency: due to unbalanced traffic allocation on
              active routes, extra switches and links may be activated to cater
              to bursty traffic surges on congested routes, and (2) Quality of
              Service (QoS) fluctuation: because of the limited flow entry
              processing ability, switches may not be able to timely
              install/delete/update flow entries to properly route/reroute
              flows. In this paper, we propose AggreFlow, a dynamic flow
              scheduling scheme that achieves power efficiency and QoS
              improvement using three techniques: Flow-set Routing, Lazy
              Rerouting, and Adaptive Rerouting. Flow-set Routing achieves load
              balancing with a small number of flow entry operations by routing
              flows in a coarse-grained flow-set fashion. Lazy Rerouting
              spreads rerouting operations over a relatively long period of
              time, reducing the burstiness of entry operation on switches.
              Adaptive Rerouting selectively reroutes flow-sets to maintain
              load balancing. We built an NS3 based fat-tree network simulation
              platform to evaluate AggreFlow's performance. The simulation
              results show that AggreFlow reduces power consumption by about
              18\%, yet achieving load balancing and improved QoS (low packet
              loss rate and reducing the number of processing entries for flow
              scheduling by 98\%), compared with baseline schemes.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  1,
  pages    = "17--33",
  month    =  feb,
  year     =  2021,
  keywords = "Load management;Control systems;Quality of service;Data
              centers;Power demand;Dynamic scheduling;Flow
              scheduling;power-efficient data center networks;power
              saving;OpenFlow;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3026015"
}

@ARTICLE{Ao2021-so,
  title    = "Joint Workload Distribution and Capacity Augmentation in Hybrid
              Datacenter Networks",
  author   = "Ao, W C and Huang, P-H and Psounis, K",
  abstract = "In hybrid datacenter networks, wired connections are augmented
              with wireless links to facilitate data transfers between racks.
              The usage of mmWave/FSO wireless links enables dynamic
              bandwidth/capacity allocation with extremely small
              reconfiguration delay. Also, on-demand workload distribution,
              where the workload of a job is divided into multiple tasks that
              can be distributed/routed to different racks to be processed in
              parallel, allows better utilization of computational resources in
              data centers. In prior work, the dynamic wireless capacity
              augmentation and workload distribution decisions were mostly made
              independently and in a heuristic manner for serving distributed
              and parallel computing jobs. In this paper, we propose a novel
              analytical framework and algorithms to jointly optimize both the
              wireless capacity augmentation and the workload distribution, to
              minimize the job completion time. We consider workload that is
              not amenable to pipelining, fully amenable to pipelining, and
              partially amenable to pipelining. With extensive simulation
              studies, we show that the gain (in terms of the reduction in the
              job completion time) can be very substantial when allowing such
              joint optimization.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  1,
  pages    = "120--133",
  month    =  feb,
  year     =  2021,
  keywords = "Wireless communication;Task analysis;Pipeline
              processing;Optimization;Resource management;Data
              centers;Servers;On-demand workload distribution;wireless capacity
              augmentation;biconvex optimization;distributed and parallel
              computing;hybrid datacenter networks;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3027607"
}

@ARTICLE{Zou2021-qa,
  title    = "{Flow-Aware} Adaptive Pacing to Mitigate {TCP} Incast in Data
              Center Networks",
  author   = "Zou, S and Huang, J and Wang, J and He, T",
  abstract = "In data center networks, many network-intensive applications
              leverage large fan-in and many-to-one communication to achieve
              high performance. However, the special traffic patterns, such as
              micro-burst and high concurrency, easily cause TCP Incast problem
              and seriously degrade the application performance. To address the
              TCP Incast problem, we first reveal theoretically and empirically
              that alleviating packet burstiness is much more effective in
              reducing the Incast probability than controlling the congestion
              window. Inspired by the findings and insights from our
              experimental observations, we further propose a general
              supporting scheme Adaptive Pacing (AP), which dynamically adjusts
              burstiness according to the flow concurrency without any change
              on switch. Additionally, a sender-based approach is proposed to
              estimate the flow concurrency. Another feature of AP is its broad
              applicability. We integrate AP transparently into different TCP
              protocols (i.e., DCTCP, L2DCT and D2TCP). Through a series of
              large-scale NS2 simulations and testbed experiments, we show that
              AP significantly reduces the Incast probability across different
              TCP protocols and the network goodput can be increased
              consistently by on average $7\times $ under severe congestion.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  1,
  pages    = "134--147",
  month    =  feb,
  year     =  2021,
  keywords = "Switches;Data centers;Microsoft Windows;Protocols;Servers;Packet
              loss;Data center;pacing;TCP;Incast;congestion
              control;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3027749"
}

@ARTICLE{Psychasand2021-jz,
  title    = "{High-Throughput} Bin Packing: Scheduling Jobs With Random
              Resource Demands in Clusters",
  author   = "Psychasand, K and Ghaderi, J",
  abstract = "We consider a natural scheduling problem which arises in many
              distributed computing frameworks. Jobs with diverse resource
              demands (e.g. memory requirements) arrive over time and must be
              served by a cluster of servers. To improve throughput and delay,
              the scheduler can pack as many jobs as possible in each server,
              however the sum of the jobs' resource demands cannot exceed the
              server's capacity. Motivated by the increasing complexity of
              workloads in shared clusters, we consider a setting where jobs'
              resource demands belong to a very large set of diverse types, or
              in the extreme case even infinitely many types, i.e. resource
              demands are drawn from a general unknown distribution over a
              possibly continuous support. The application of classical
              scheduling approaches that crucially rely on a predefined finite
              set of types is discouraging in this high (or infinite) type
              setting. We first characterize a fundamental limit on the maximum
              throughput in such setting. We then develop oblivious scheduling
              algorithms, based on Best-Fit and Universal Partitioning, that
              have low complexity and can achieve at least 1/2 and 2/3 of the
              maximum throughput respectively, without the knowledge of the
              resource demand distribution. Extensive simulation results, using
              both synthetic and real traffic traces, are presented to verify
              the performance of our algorithms.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  1,
  pages    = "220--233",
  month    =  feb,
  year     =  2021,
  keywords = "Servers;Throughput;Scheduling;Memory management;Complexity
              theory;Partitioning algorithms;Scheduling algorithms;Scheduling
              algorithms;stability;queues;bin packing;data
              centers;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3034022"
}

@ARTICLE{Dinh2021-jv,
  title    = "Federated Learning Over Wireless Networks: Convergence Analysis
              and Resource Allocation",
  author   = "Dinh, C T and Tran, N H and Nguyen, M N H and Hong, C S and Bao,
              W and Zomaya, A Y and Gramoli, V",
  abstract = "There is an increasing interest in a fast-growing machine
              learning technique called Federated Learning (FL), in which the
              model training is distributed over mobile user equipment (UEs),
              exploiting UEs' local computation and training data. Despite its
              advantages such as preserving data privacy, FL still has
              challenges of heterogeneity across UEs' data and physical
              resources. To address these challenges, we first propose FEDL, a
              FL algorithm which can handle heterogeneous UE data without
              further assumptions except strongly convex and smooth loss
              functions. We provide a convergence rate characterizing the
              trade-off between local computation rounds of each UE to update
              its local model and global communication rounds to update the FL
              global model. We then employ FEDL in wireless networks as a
              resource allocation optimization problem that captures the
              trade-off between FEDL convergence wall clock time and energy
              consumption of UEs with heterogeneous computing and power
              resources. Even though the wireless resource allocation problem
              of FEDL is non-convex, we exploit this problem's structure to
              decompose it into three sub-problems and analyze their
              closed-form solutions as well as insights into problem design.
              Finally, we empirically evaluate the convergence of FEDL with
              PyTorch experiments, and provide extensive numerical results for
              the wireless resource allocation sub-problems. Experimental
              results show that FEDL outperforms the vanilla FedAvg algorithm
              in terms of convergence rate and test accuracy in various
              settings.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  1,
  pages    = "398--409",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/D/Dinh et al. 2021 - Federated Learning Over Wireless Networks - Convergence Analysis and Resource Allocation.pdf",
  keywords = "Convergence;Computational modeling;Training;Data models;Resource
              management;Wireless communication;Wireless networks;Distributed
              machine learning;federated learning;optimization
              decomposition;MLNetworking",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3035770"
}

@ARTICLE{Zhao2021-ec,
  title    = "Joint Reducer Placement and Coflow Bandwidth Scheduling for
              Computing Clusters",
  author   = "Zhao, Y and Tian, C and Fan, J and Guan, T and Zhang, X and Qiao,
              C",
  abstract = "Reducing Coflow Completion Time (CCT) has a significant impact on
              application performance in data-parallel frameworks. Most
              existing works assume that the endpoints of constituent flows in
              each coflow are predetermined. We argue that CCT can be further
              optimized by treating flows' destinations as an additional
              optimization dimension via reducer placement. In this article, we
              propose and implement RPC, a joint online Reducer Placement and
              Coflow bandwidth scheduling framework, to minimize the average
              CCT in cloud clusters. We first develop a 2-approximation
              algorithm to minimize the CCT of a single coflow, and then
              schedule all the coflows following the Shortest Remaining Time
              First (SRTF) principle. We use real testbed experiments and
              extensive large-scale simulations to demonstrate that RPC can
              reduce the average CCT by 64.98\% compared with the
              state-of-the-art technologies.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  1,
  pages    = "438--451",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/Z/Zhao et al. 2021 - Joint Reducer Placement and Coflow Bandwidth Scheduling for Computing Clusters.pdf",
  keywords = "Bandwidth;Task analysis;Processor scheduling;Schedules;Data
              transfer;Optimization;IEEE transactions;Computing
              clusters;reducer placement;coflow scheduling;Datacentre",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3037064"
}

@ARTICLE{Xu2021-bo,
  title    = "An {Actor-Critic-Based} Transfer Learning Framework for
              {Experience-Driven} Networking",
  author   = "Xu, Z and Yang, D and Tang, J and Tang, Y and Yuan, T and Wang, Y
              and Xue, G",
  abstract = "Experience-driven networking has emerged as a new and highly
              effective approach for resource allocation in complex
              communication networks. Deep Reinforcement Learning (DRL) has
              been shown to be a useful technique for enabling
              experience-driven networking. In this paper, we focus on a
              practical and fundamental problem for experience-driven
              networking: when network configurations are changed, how to train
              a new DRL agent to effectively and quickly adapt to the new
              environment. We present an Actor-Critic-based Transfer learning
              framework for the Traffic Engineering (TE) problem using policy
              distillation, which we call ACT-TE. ACT-TE effectively and
              quickly trains a new DRL agent to solve the TE problem in a new
              network environment, using both old knowledge (i.e., distilled
              from the existing agent) and new experience (i.e., newly
              collected samples). We implement ACT-TE in ns-3, and compare it
              with commonly-used baselines using packet-level simulations on
              three representative network topologies: NSFNET, ARPANET and
              random topology. The extensive simulation results show that 1)
              The existing well-trained DRL agents do not work well in new
              network environments; 2) ACT-TE significantly outperforms both
              two straightforward methods (training from scratch and
              fine-tuning based on an existing DRL agent) and several
              widely-used traditional methods in terms of network utility,
              throughput and delay.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  1,
  pages    = "360--371",
  month    =  feb,
  year     =  2021,
  keywords = "Training;Network topology;Topology;Routing;Knowledge
              engineering;Throughput;Mathematical model;Experience-driven
              networking;deep reinforcement learning and transfer
              learning;MLNetworking",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3037231"
}

@ARTICLE{Laki2021-xx,
  title    = "{Core-Stateless} Forwarding With {QoS} Revisited: Decoupling
              Delay and Bandwidth Requirements",
  author   = "Laki, S{\'a}ndor and N{\'a}das, Szilveszter and Gombos, Gerg{\H
              o} and Fejes, Ferenc and Hudoba, P{\'e}ter and Tur{\'a}nyi,
              Zolt{\'a}n and Kiss, Zolt{\'a}n and Keszei, Csaba",
  abstract = "Network QoS, fairness and resource sharing control are not
              completely solved problems. Available solutions lack scalability
              due to maintaining flow state, require re-tuning if traffic
              changes, focus on a limited set of networking scenarios or
              require complex, centralized controllers and feedback loops. In
              this paper, we propose a core-stateless solution for closed
              network domains like access, enterprise and data center networks
              that handles resource sharing and provides guarantees for per-hop
              latency, independently. The proposed method enables controlled
              resource sharing by encoding the utility function of flows to
              Packet Value markings. This allows expressing resource sharing
              policies for all possible congestion situations, while operation
              is completely flow unaware inside the network. In addition, it
              also satisfies per-hop delay requirements for traffic flows
              independently. The separation of the delay requirements of the
              packets from their importance has not generally been possible by
              existing methods so far. The performance of the proposed method
              has thoroughly been analyzed by large number of simulations
              covering both static and dynamic scenarios and was implemented in
              a cloud-native virtual router implementing all the policies
              needed for a Broadband Network Gateway, showing good performance
              and better scalability than existing weighted queuing-based
              solutions.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  2,
  pages    = "503--516",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/L/Laki et al. 2021 - Core-Stateless Forwarding With QoS Revisited - Decoupling Delay and Bandwidth Requirements.pdf",
  keywords = "Delays;Resource management;Quality of
              service;Bandwidth;Proposals;Aggregates;Scalability;Resource
              sharing;QoS;congestion control;core-stateless;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3041235"
}

@ARTICLE{Zeng2021-bi,
  title    = "{CoEdge}: Cooperative {DNN} Inference With Adaptive Workload
              Partitioning Over Heterogeneous Edge Devices",
  author   = "Zeng, Liekang and Chen, Xu and Zhou, Zhi and Yang, Lei and Zhang,
              Junshan",
  abstract = "Recent advances in artificial intelligence have driven increasing
              intelligent applications at the network edge, such as smart home,
              smart factory, and smart city. To deploy computationally
              intensive Deep Neural Networks (DNNs) on resource-constrained
              edge devices, traditional approaches have relied on either
              offloading workload to the remote cloud or optimizing computation
              at the end device locally. However, the cloud-assisted approaches
              suffer from the unreliable and delay-significant wide-area
              network, and the local computing approaches are limited by the
              constrained computing capability. Towards high-performance edge
              intelligence, the cooperative execution mechanism offers a new
              paradigm, which has attracted growing research interest recently.
              In this paper, we propose CoEdge, a distributed DNN computing
              system that orchestrates cooperative DNN inference over
              heterogeneous edge devices. CoEdge utilizes available computation
              and communication resources at the edge and dynamically
              partitions the DNN inference workload adaptive to devices'
              computing capabilities and network conditions. Experimental
              evaluations based on a realistic prototype show that CoEdge
              outperforms status-quo approaches in saving energy with close
              inference latency, achieving up to 25.5\% 66.9\% energy reduction
              for four widely-adopted CNN models.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  2,
  pages    = "595--608",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/Z/Zeng et al. 2021 - CoEdge - Cooperative DNN Inference With Adaptive Workload Partitioning Over Heterogeneous Edge Devices.pdf",
  keywords = "Image edge detection;Computational modeling;Smart
              homes;Performance evaluation;Task analysis;Feature
              extraction;Runtime;Edge intelligence;cooperative DNN
              inference;distributed computing;energy efficiency",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3042320"
}

@ARTICLE{Xiang2021-sf,
  title    = "Optimizing in the Dark: Learning Optimal Network Resource
              Reservation Through a Simple Request Interface",
  author   = "Xiang, Qiao and Yu, Haitao and Aspnes, James and Le, Franck and
              Guok, Chin and Kong, Linghe and Yang, Y Richard",
  abstract = "Network resource reservation systems are being developed and
              deployed, driven by the demand and substantial benefits of
              providing performance predictability for modern distributed
              applications. However, existing systems suffer limitations: They
              either are inefficient in finding the optimal resource
              reservation, or cause private information (e.g., from the network
              infrastructure) to be exposed (e.g., to the user). In this paper,
              we design BoxOpt, a novel system that leverages efficient oracle
              construction techniques in optimization and learning theory to
              automatically, and swiftly learn the optimal resource
              reservations without exchanging any private information between
              the network and the user. In BoxOpt, we first model the simple
              reservation interface adopted in most reservation systems as a
              resource membership oracle. Second, we develop an efficient
              algorithm that constructs a resource separation oracle by a
              linear number of calls on resource membership oracle. Third, we
              develop a generic framework to construct a resource optimization
              oracle by iteratively calling the resource separation oracle, and
              then develop three novel, efficient algorithms under this generic
              framework, the best of which computes the optimal resource
              reservation by a linear number of calls on resource separation
              oracle. As such, BoxOpt can discover the optimal resource
              reservation with $O(n^2)$ calls on the resource membership
              oracle. We implement a prototype of BoxOpt with and demonstrate
              its efficiency and efficacy via extensive experiments using real
              network topology and a 7-day trace from a large operational
              federation network. Results show that (1) BoxOpt has a 100\%
              correctness ratio by comparing with a state-of-the-art
              optimization solver, and (2) for 90\% of requests, BoxOpt learns
              the optimal resource reservation within 10 seconds.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  2,
  pages    = "571--584",
  month    =  apr,
  year     =  2021,
  keywords = "Bandwidth;Optimization;Computer science;Network topology;Linear
              programming;Large Hadron Collider;Tuning;Machine learning;network
              optimization;resource orchestration;bandwidth
              reservation;MLNetworking",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3045595"
}

@ARTICLE{Farhadi2021-bs,
  title    = "Service Placement and Request Scheduling for {Data-Intensive}
              Applications in Edge Clouds",
  author   = "Farhadi, Vajiheh and Mehmeti, Fidan and He, Ting and Porta,
              Thomas F La and Khamfroush, Hana and Wang, Shiqiang and Chan,
              Kevin S and Poularakis, Konstantinos",
  abstract = "Mobile edge computing provides the opportunity for wireless users
              to exploit the power of cloud computing without a large
              communication delay. To serve data-intensive applications (e.g.,
              video analytics, machine learning tasks) from the edge, we need,
              in addition to computation resources, storage resources for
              storing server code and data as well as network bandwidth for
              receiving user-provided data. Moreover, due to time-varying
              demands, the code and data placement needs to be adjusted over
              time, which raises concerns of system stability and operation
              cost. In this paper, we address these issues by proposing a
              two-time-scale framework that jointly optimizes service (code and
              data) placement and request scheduling, while considering
              storage, communication, computation, and budget constraints.
              First, by analyzing the hardness of various cases, we completely
              characterize the complexity of our problem. Next, we develop a
              polynomial-time service placement algorithm by formulating our
              problem as a set function optimization, which attains a
              constant-factor approximation under certain conditions.
              Furthermore, we develop a polynomial-time request scheduling
              algorithm by computing the maximum flow in a carefully
              constructed auxiliary graph, which satisfies hard resource
              constraints and is provably optimal in the special case where
              requests have homogeneous resource demands. Extensive synthetic
              and trace-driven simulations show that the proposed algorithms
              achieve 90\% of the optimal performance.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  2,
  pages    = "779--792",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/F/Farhadi et al. 2021 - Service Placement and Request Scheduling for Data-Intensive Applications in Edge Clouds.pdf;All Papers/F/Farhadi et al. 2021 - Service Placement and Request Scheduling for Data-Intensive Applications in Edge Clouds.pdf",
  keywords = "Processor scheduling;Cloud computing;Servers;Bandwidth;Complexity
              theory;Edge computing;Computational modeling;Mobile edge
              computing;service placement;workload scheduling;complexity
              analysis;algorithm design;EdgeFogCloudIoT",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2020.3048613"
}

@ARTICLE{Bae2021-db,
  title    = "Learning to Schedule Network Resources Throughput and Delay
              Optimally Using {Q+-Learning}",
  author   = "Bae, Jeongmin and Lee, Joohyun and Chong, Song",
  abstract = "As network architecture becomes complex and the user requirement
              gets diverse, the role of efficient network resource management
              becomes more important. However, existing throughput-optimal
              scheduling algorithms such as the max-weight algorithm suffer
              from poor delay performance. In this paper, we present
              reinforcement learning-based network scheduling algorithms for a
              single-hop downlink scenario which achieve throughput-optimality
              and converge to minimal delay. To this end, we first formulate
              the network optimization problem as a Markov decision process
              (MDP) problem. Then, we introduce a new state-action value
              function called $Q^+$ -function and develop a reinforcement
              learning algorithm called $Q^+$ -learning with UCB (Upper
              Confidence Bound) exploration which guarantees small performance
              loss during a learning process. We also derive an upper bound of
              the sample complexity in our algorithm, which is more efficient
              than the best known bound from Q-learning with UCB exploration by
              a factor of $\gamma ^2$ where $\gamma $ is the discount factor of
              the MDP problem. Finally, via simulation, we verify that our
              algorithm shows a delay reduction of up to 40.8\% compared to the
              max-weight algorithm over various scenarios. We also show that
              the Q+-learning with UCB exploration converges to an $\epsilon $
              -optimal policy 10 times faster than Q-learning with UCB.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  2,
  pages    = "750--763",
  month    =  apr,
  year     =  2021,
  keywords = "Throughput;Delays;Optimization;Reinforcement learning;Wireless
              networks;Heuristic algorithms;Complexity theory;Network resource
              management;throughput and delay optimality;reinforcement
              learning;upper confidence bound;MLNetworking",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3051663"
}

@ARTICLE{Shabtai2021-cw,
  title    = "Risk Aware Stochastic Placement of Cloud Services",
  author   = "Shabtai, Galia and Raz, Danny and Shavitt, Yuval",
  abstract = "Allocating the right amount of resources to each service in any
              of the datacenters in a cloud environment is a very difficult
              task. This task becomes much harder due to the dynamic nature of
              the workload and the fact that while long term statistics about
              the demand may be known, it is impossible to predict the exact
              demand in each point in time. As a result, service providers
              either over allocate resources and hurt the service cost
              efficiency, or run into situation where the allocated local
              resources are insufficient to support the current demand. In
              these cases, the service providers deploy overflow mechanisms
              such as redirecting traffic to a remote datacenter or temporarily
              leasing additional resources (at a higher price) from the cloud
              infrastructure owner. The additional cost is in many cases
              proportional to the amount of overflow demand. In this paper we
              study this approach and develop a novel mechanism to assign
              services to datacenters based on the available resources in each
              datacenter and the distribution of the demand for each service.
              We use comprehensive analysis to prove that the overall overflow
              cost is almost optimal for arbitrary demand distributions, as
              long as there are no dependencies among the services. We further
              show, using simulation based on real data that the scheme
              performs very well on realistic service workloads.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  2,
  pages    = "805--820",
  month    =  apr,
  year     =  2021,
  keywords = "Random variables;Stochastic processes;Gaussian distribution;Cost
              function;Approximation algorithms;Cloud computing;Resource
              management;Cloud computing technologies;stochastic
              optimization;resource sharing;NFV",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3052962"
}

@ARTICLE{Wu2021-eo,
  title    = "Deep Reinforcement Learning With {Spatio-Temporal} Traffic
              Forecasting for {Data-Driven} Base Station Sleep Control",
  author   = "Wu, Qiong and Chen, Xu and Zhou, Zhi and Chen, Liang and Zhang,
              Junshan",
  abstract = "To meet the ever increasing mobile traffic demand in 5G era, base
              stations (BSs) have been densely deployed in radio access
              networks (RANs) to increase the network coverage and capacity.
              However, as the high density of BSs is designed to accommodate
              peak traffic, it would consume an unnecessarily large amount of
              energy if BSs are on during off-peak time. To save the energy
              consumption of cellular networks, an effective way is to
              deactivate some idle base stations that do not serve any traffic
              demand. In this paper, we develop a traffic-aware dynamic BS
              sleep control framework, named DeepBSC, which presents a novel
              data-driven learning approach to determine the BS active/sleep
              modes while meeting lower energy consumption and satisfactory
              Quality of Service (QoS) requirements. Specifically, the traffic
              demands are predicted by the proposed GS-STN model, which
              leverages the geographical and semantic spatial-temporal
              correlations of mobile traffic. With accurate mobile traffic
              forecasting, the BS sleep control problem is cast as a Markov
              Decision Process that is solved by Actor-Critic reinforcement
              learning methods. To reduce the variance of cost estimation in
              the dynamic environment, we propose a benchmark transformation
              method that provides robust performance indicator for policy
              update. To expedite the training process, we adopt a Deep
              Deterministic Policy Gradient (DDPG) approach, together with an
              explorer network, which can strengthen the exploration further.
              Extensive experiments with a real-world dataset corroborate that
              our proposed framework significantly outperforms the existing
              methods.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  2,
  pages    = "935--948",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/W/Wu et al. 2021 - Deep Reinforcement Learning With Spatio-Temporal Traffic Forecasting for Data-Driven Base Station Sleep Control.pdf",
  keywords = "Base stations;Forecasting;Correlation;Cellular networks;Quality
              of service;Energy consumption;Switches;Base station sleep
              control;spatio-temporal traffic forecasting;deep reinforcement
              learning",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3053771"
}

@ARTICLE{Huang2021-ps,
  title    = "Mitigating Packet Reordering for Random Packet Spraying in Data
              Center Networks",
  author   = "Huang, Jiawei and Lyu, Wenjun and Li, Weihe and Wang, Jianxin and
              He, Tian",
  abstract = "Modern data center networks are usually constructed in
              multi-rooted tree topologies, which require the highly efficient
              multi-path load balancing to achieve high link utilization.
              Recent packet-level load balancer obtains high throughput by
              spraying packets to all paths, but it easily leads to the packet
              reordering under network asymmetry. The flow-level or
              flowlet-level load balancer avoids the packet reordering, while
              reducing the link utilization due to their inflexibility. To
              solve these problems, we design a Queueing Delay Aware Packet
              Spraying (QDAPS), that effectively mitigates the packet
              reordering for packet-level load balancer. QDAPS selects paths
              for packets according to the queueing delay of output buffer, and
              lets the packet arriving earlier be forwarded before the later
              packets to avoid packet reordering. Moreover, we adopt the
              ``power-of- n-choices'' paradigm on QDAPS to alleviate the impact
              of herd behavior under multiple forwarding engines. We compare
              QDAPS with ECMP, LetFlow and RPS through NS2 simulation and
              Mininet implementation. The test results show that QDAPS reduces
              flow completion time (FCT) by 30\%-50\% over the state-of-the-art
              load balancing mechanism.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  3,
  pages    = "1183--1196",
  month    =  jun,
  year     =  2021,
  keywords = "Load management;Switches;Delays;Data centers;Out of
              order;Topology;Spraying;Data center;multi-path;load
              balancing;DataCenter",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3056601"
}

@ARTICLE{Sallam2021-bg,
  title    = "Joint Placement and Allocation of {VNF} Nodes With Budget and
              Capacity Constraints",
  author   = "Sallam, Gamal and Ji, Bo",
  abstract = "With the advent of Network Function Virtualization (NFV), network
              services that traditionally run on proprietary dedicated hardware
              can now be realized using Virtual Network Functions (VNFs) that
              are hosted on general-purpose commodity hardware. This new
              network paradigm offers a great flexibility to Internet service
              providers (ISPs) for efficiently operating their networks
              (collecting network statistics, enforcing management policies,
              etc.). However, introducing NFV requires an investment to deploy
              VNFs at certain network nodes (called VNF-nodes), which has to
              account for practical constraints such as the deployment budget
              and the VNF-node capacity. To that end, it is important to design
              a joint VNF-nodes placement and capacity allocation algorithm
              that can maximize the total amount of network flows that are
              fully processed by the VNF-nodes while respecting such practical
              constraints. In contrast to most prior work that often neglects
              either the budget constraint or the capacity constraint, we
              explicitly consider both of them. We prove that accounting for
              these constraints introduces several new challenges.
              Specifically, we prove that the studied problem is not only
              NP-hard but also non-submodular. To address these challenges, we
              introduce a novel relaxation method such that the objective
              function of the relaxed placement subproblem becomes submodular.
              Leveraging this useful submodular property, we propose two
              algorithms that achieve an approximation ratio of
              \textbackslashfrac 12(1-1/e) and \textbackslashfrac 13(1-1/e) for
              the original non-relaxed problem, respectively. Finally, we
              corroborate the effectiveness of the proposed algorithms through
              extensive evaluations using trace-driven simulations.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  3,
  pages    = "1238--1251",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/S/Sallam and Ji 2021 - Joint Placement and Allocation of VNF Nodes With Budget and Capacity Constraints.pdf",
  keywords = "Approximation algorithms;Resource management;Network function
              virtualization;Routing;Planning;Linear
              programming;Hardware;Network Function Virtualization
              (NFV);resource allocation;submodular optimization;Important;NFV",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3058378"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Alasmar2021-vx,
  title    = "Internet Traffic Volumes are Not {Gaussian---They} are
              {Log-Normal}: An 18-Year Longitudinal Study With Implications for
              Modelling and Prediction",
  author   = "Alasmar, Mohammed and Clegg, Richard and Zakhleniuk, Nickolay and
              Parisis, George",
  abstract = "Getting good statistical models of traffic on network links is a
              well-known, often-studied problem. A lot of attention has been
              given to correlation patterns and flow duration. The distribution
              of the amount of traffic per unit time is an equally important
              but less studied problem. We study a large number of traffic
              traces from many different networks including academic,
              commercial and residential networks using state-of-the-art
              statistical techniques. We show that traffic obeys the log-normal
              distribution which is a better fit than the Gaussian distribution
              commonly claimed in the literature. We also investigate an
              alternative heavy-tailed distribution (the Weibull) and show that
              its performance is better than Gaussian but worse than
              log-normal. We examine anomalous traces which exhibit a poor fit
              for all distributions tried and show that this is often due to
              traffic outages or links that hit maximum capacity. We
              demonstrate that the data we look at is stationary if we consider
              samples of 15-minute long or even 1-hour long. This gives
              confidence that we can use the distributions for estimation and
              modelling purposes. We demonstrate the utility of our findings in
              two contexts: predicting that the proportion of time traffic will
              exceed a given level (for service level agreement or link
              capacity estimation) and predicting 95th percentile pricing. We
              also show that the log-normal distribution is a better predictor
              than Gaussian or Weibull distributions in both contexts.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  3,
  pages    = "1266--1279",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/A/Alasmar et al. 2021 - Internet Traffic Volumes are Not Gaussian—They are ... ear Longitudinal Study With Implications for Modelling and Prediction.pdf",
  keywords = "Internet;Log-normal distribution;Monitoring;IP networks;Gaussian
              distribution;Pricing;Planning;Traffic modelling;network
              planning;bandwidth provisioning;traffic
              billing;NetworkTraffic;FutureInternet",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3059542"
}

@ARTICLE{Abdelmoniem2021-tm,
  title    = "{T-RACKs}: A Faster Recovery Mechanism for {TCP} in Data Center
              Networks",
  author   = "Abdelmoniem, Ahmed M and Bensaou, Brahim",
  abstract = "Cloud interactive data-driven applications generate swarms of
              small TCP flows that compete for the small switch buffer space in
              data-center. Such applications require a small flow completion
              time (FCT) to be effective. Unfortunately, TCP is myopic with
              respect to the composite nature of application data. In addition
              it tends to artificially inflate the FCT of individual flows by
              several orders of magnitude, because of its Internet-centric
              design, that fixes the retransmission timeout (RTO) to be at
              least hundreds of milliseconds. To better understand this
              problem, in this paper, we use empirical measurements in a small
              data center testbed to study, at a microscopic level, the effects
              of various types of packet losses on TCP's performance. In
              particular, we single out packet losses that impact the tail end
              of small flows, as well as bursty losses that span a significant
              fraction of small TCP congestion windows, and show a
              non-negligible effect of such losses on the FCT. Based on this,
              we propose the so-called, timely-retransmitted ACKs (or T-RACKs),
              a simple loss recovery mechanism that conceals the drawbacks of
              the long RTO even in the presence of heavy packet losses.
              Interestingly enough, T-RACKS achieves this transparently to TCP
              itself as it does not require any change to TCP in the tenant's
              virtual machine (VM) or container. T-RACKs can be implemented as
              a software shim layer in the hypervisor between the VMs and the
              server's NIC or in hardware as a networking function in a
              SmartNIC. Simulation and real testbed results show remarkable
              performance improvements.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  3,
  pages    = "1074--1087",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/A/Abdelmoniem and Bensaou 2021 - T-RACKs - A Faster Recovery Mechanism for TCP in Data Center Networks.pdf",
  keywords = "Packet loss;Delays;Cloud computing;Switches;Microscopy;IP
              networks;Data centers;Cross layer;data-center;fast
              recovery;tCP-incast;testbed;timeouts;CongestionControl",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3059913"
}

@ARTICLE{Behrouzi-Far2021-fw,
  title    = "Efficient Replication for Fast and Predictable Performance in
              Distributed Computing",
  author   = "Behrouzi-Far, Amir and Soljanin, Emina",
  abstract = "Master-worker distributed computing systems use task replication
              to mitigate the effect of slow workers on job compute time. The
              master node groups tasks into batches and assigns each batch to
              one or more workers. We first assume that the batches do not
              overlap. Using majorization theory, we show that a balanced
              replication of batches minimizes the average job compute time for
              a general class of service time distributions. We then show that
              the balanced assignment of non-overlapping batches achieves a
              lower average job compute time than the overlapping schemes
              proposed in the literature. Next, we derive the optimum
              redundancy level as a function of the task service time
              distribution. We show that the redundancy level that minimizes
              the average job compute time may not coincide with the redundancy
              level that maximizes job compute time predictability. Therefore,
              there is a trade-off in optimizing the two metrics. By running
              experiments on Google cluster traces, we observe that redundancy
              can reduce the job compute time by one order of magnitude. The
              optimum level of redundancy depends on the distribution of task
              service time.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  4,
  pages    = "1467--1476",
  month    =  aug,
  year     =  2021,
  keywords = "Task analysis;Redundancy;Computational modeling;Machine
              learning;Internet;Computer
              architecture;Training;Redundancy;replication;distributed
              systems;distributed computing;latency;coefficient of
              variations;Distributed Systems",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3062215"
}

@ARTICLE{Guan2021-id,
  title    = "{WNOS}: Enabling Principled {Software-Defined} Wireless
              Networking",
  author   = "Guan, Zhangyu and Bertizzolo, Lorenzo and Demirors, Emrecan and
              Melodia, Tommaso",
  abstract = "This article investigates the basic design principles for a new
              Wireless Network Operating System (WNOS), a radically different
              approach to software-defined networking (SDN) for
              infrastructure-less wireless networks. Departing from
              well-understood approaches inspired by OpenFlow, WNOS provides
              the network designer with an abstraction hiding (i) the
              lower-level details of the wireless protocol stack and (ii) the
              distributed nature of the network operations. Based on this
              abstract representation, the WNOS takes network control programs
              written on a centralized, high-level view of the network and
              automatically generates distributed cross-layer control programs
              based on distributed optimization theory that are executed by
              each individual node on an abstract representation of the radio
              hardware. We first discuss the main architectural principles of
              WNOS. Then, we discuss a new approach to automatically generate
              solution algorithms for each of the resulting subproblems in an
              automated fashion. Finally, we illustrate a prototype
              implementation of WNOS on software-defined radio devices and test
              its effectiveness by considering specific cross-layer control
              problems. Experimental results indicate that, based on the
              automatically generated distributed control programs, WNOS
              achieves 18\%, 56\% and 80.4\% utility gain in networks with low,
              medium and high levels of interference; maybe more importantly,
              we illustrate how the global network behavior can be controlled
              by modifying a few lines of code on a centralized abstraction.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  3,
  pages    = "1391--1407",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/G/Guan et al. 2021 - WNOS - Enabling Principled Software-Defined Wireless Networking.pdf",
  keywords = "Protocols;Wireless networks;Optimization;Decentralized
              control;Hardware;Routing;Operating systems;Software-defined
              networking;distributed optimization;automated control program
              generation;wireless ad hoc networks;ProgrammableNetworks",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3064824"
}

@ARTICLE{Ye2021-vz,
  title    = "Combating Bufferbloat in {Multi-Bottleneck} Networks: Theory and
              Algorithms",
  author   = "Ye, Jiancheng and Leung, Ka-Cheong and Low, Steven H",
  abstract = "Bufferbloat is a phenomenon in computer networks where large
              router buffers are frequently filled up, resulting in high
              queueing delay and delay variation. More and more delay-sensitive
              applications on the Internet have made this phenomenon a pressing
              issue. Interacting with the Transmission Control Protocol (TCP),
              active queue management (AQM) algorithms run on routers play an
              important role in combating bufferbloat. However, AQM algorithms
              have not been widely deployed due to complicated manual parameter
              tuning. Moreover, they are often designed and analyzed based on
              network models with a single bottleneck link, rendering their
              performance and stability unclear in multi-bottleneck networks.
              In this paper, we propose a general framework to combat
              bufferbloat in multi-bottleneck networks. We first present an
              equilibrium analysis for a general multi-bottleneck TCP/AQM
              system and provide sufficient conditions for the uniqueness of an
              equilibrium point in the system. We then decompose the system
              into single-bottleneck subsystems and derive sufficient
              conditions for the local asymptotic stability of the subsystems.
              Using our framework, we develop an algorithm to compute the
              equilibrium point of the system. We further present a case study
              to analyze the stability of the recently proposed Controlled
              Delay (CoDel) in multi-bottleneck networks and devise Self-Tuning
              CoDel to improve the system stability. Extensive numerical and
              packet-level simulation results not only verify our theoretical
              studies but also show that our proposed Self-Tuning CoDel
              significantly stabilizes queueing delay in multi-bottleneck
              networks, thereby mitigating bufferbloat.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  4,
  pages    = "1477--1493",
  month    =  aug,
  year     =  2021,
  file     = "All Papers/Y/Ye et al. 2021 - Combating Bufferbloat in Multi-Bottleneck Networks - Theory and Algorithms.pdf",
  keywords = "Asymptotic stability;Delays;Stability criteria;Numerical
              stability;Internet;Tuning;Mathematical model;Active queue
              management;bufferbloat;congestion
              control;equilibrium;stability;CongestionControl",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3066505"
}

@ARTICLE{Huang2021-cp,
  title    = "Online {VNF} Chaining and Predictive Scheduling: Optimality and
              {Trade-Offs}",
  author   = "Huang, Xi and Bian, Simeng and Gao, Xin and Wu, Weijie and Shao,
              Ziyu and Yang, Yang and Lui, John C S",
  abstract = "For NFV systems, the key design space includes the function
              chaining for network requests and the resource scheduling for
              servers. The problem is challenging since NFV systems usually
              require multiple (often conflicting) design objectives and the
              computational efficiency of real-time decision making with
              limited information. Furthermore, the benefits of predictive
              scheduling to NFV systems still remain unexplored. In this
              article, we propose POSCARS, an efficient predictive and online
              service chaining and resource scheduling scheme that achieves
              tunable trade-offs among various system metrics with stability
              guarantee. Through a careful choice of granularity in system
              modeling, we acquire a better understanding of the trade-offs in
              our design space. By a non-trivial transformation, we decouple
              the complex optimization problem into a series of online
              sub-problems to achieve the optimality with only limited
              information. By employing randomized load balancing techniques,
              we propose three variants of POSCARS to reduce the overheads of
              decision making. Theoretical analysis and simulations show that
              POSCARS and its variants require only mild-value of future
              information to achieve near-optimal system cost with an ultra-low
              request response time.",
  journal  = "IEEE/ACM Trans. Netw.",
  volume   =  29,
  number   =  4,
  pages    = "1867--1880",
  month    =  aug,
  year     =  2021,
  file     = "All Papers/H/Huang et al. 2021 - Online VNF Chaining and Predictive Scheduling - Optimality and Trade-Offs.pdf",
  keywords = "Servers;Dynamic scheduling;Processor scheduling;Optimization;Time
              factors;Resource management;Decision making;NFV;service
              chaining;resource allocation;predictive scheduling",
  issn     = "1063-6692, 1558-2566",
  doi      = "10.1109/TNET.2021.3072423"
}

@ARTICLE{Chen2020-pr,
  title    = "Flow Scheduling of Service Chain Processing in a {NFV-based}
              Network",
  author   = "Chen, Y and Wu, J",
  abstract = "Network Function Virtualization (NFV) is changing the way we
              implement the network functions from expensive hardwares to
              software middleboxes, called Virtual Network Functions (VNFs).
              Flows always request to be processed by several middleboxes in a
              specific order, which is known as a service chain. Most
              researches study the middlebox placement problem and few of them
              pay attention to the flow scheduling of a deployed service chain,
              resulting in poor control of flow completion times. However, the
              flow completion time is an extreme metric to evaluate the
              performance of a network. Therefore, we focus on the service
              chain scheduling problem. We aim to minimize the flow completion
              time in two aspects: the longest completion time (makespan) and
              the average completion time. When there are only two middleboxes
              in the service chain, we propose one optimal solution for each
              aspect, respectively. For a service chain with an arbitrary
              length, we prove the NP-hardness of our problem in both aspects
              and two corresponding heuristic algorithms are designed, which
              are extended from our proposed optimal solutions for a service
              chain with a length of two. Real testbed experiments and
              extensive simulations are conducted to evaluate the performance
              of our proposed algorithms.",
  journal  = "IEEE Transactions on Network Science and Engineering",
  pages    = "1--1",
  year     =  2020,
  file     = "All Papers/C/Chen and Wu 2020 - Flow Scheduling of Service Chain Processing in a NFV-based Network.pdf",
  keywords = "Middleboxes;Delays;Servers;Schedules;Heuristic
              algorithms;Software;Data centers;Middlebox;transmission
              delay;processing time;service chain;ToRead;NFV",
  issn     = "2327-4697",
  doi      = "10.1109/TNSE.2020.3038783"
}

@ARTICLE{Gil_Herrera2016-rf,
  title    = "Resource Allocation in {NFV}: A Comprehensive Survey",
  author   = "Gil Herrera, J and Botero, J F",
  abstract = "Network functions virtualization (NFV) is a new network
              architecture framework where network function that traditionally
              used dedicated hardware (middleboxes or network appliances) are
              now implemented in software that runs on top of general purpose
              hardware such as high volume server. NFV emerges as an initiative
              from the industry (network operators, carriers, and
              manufacturers) in order to increase the deployment flexibility
              and integration of new network services with increased agility
              within operator's networks and to obtain significant reductions
              in operating expenditures and capital expenditures. NFV promotes
              virtualizing network functions such as transcoders, firewalls,
              and load balancers, among others, which were carried out by
              specialized hardware devices and migrating them to software-based
              appliances. One of the main challenges for the deployment of NFV
              is the resource allocation of demanded network services in
              NFV-based network infrastructures. This challenge has been called
              the NFV resource allocation (NFV-RA) problem. This paper presents
              a comprehensive state of the art of NFV-RA by introducing a novel
              classification of the main approaches that pose solutions to
              solve it. This paper also presents the research challenges that
              are still subject of future investigation in the NFV-RA realm.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  13,
  number   =  3,
  pages    = "518--532",
  month    =  sep,
  year     =  2016,
  file     = "All Papers/G/Gil Herrera and Botero 2016 - Resource Allocation in NFV - A Comprehensive Survey.pdf",
  keywords = "computer networks;resource allocation;virtualisation;network
              function virtualization;middleboxes;network appliances;network
              operators;deployment flexibility;network services;operator
              networks;capital expenditures;operating expenditures;hardware
              devices;software-based appliances;NFV-based network
              infrastructures;NFV resource allocation problem;NFV-RA
              problem;Hardware;Virtualization;Resource
              management;Middleboxes;Software;Servers;Network function
              virtualization;virtual network functions;resource allocation;NFV
              orchestration;VNF forwarding graph;scheduling;service chaining
              and placement;NFV\_SDN",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2016.2598420"
}

@ARTICLE{Alameddine2017-wb,
  title    = "On the Interplay Between Network Function Mapping and Scheduling
              in {VNF-Based} Networks: A Column Generation Approach",
  author   = "Alameddine, H A and Sebbah, S and Assi, C",
  abstract = "Middleboxes (i.e., firewall, cache, proxy, etc.) are hardware
              appliances designed to enforce security and performance policies.
              Being an integral part of today's cloud and enterprise networks,
              these middleboxes are expensive, hard to manage and to maintain.
              Network function virtualization has emerged as a promising
              technology that replaces these hardware appliances by software
              ones known as virtual network functions (VNFs). Unlike hardware
              middleboxes, VNFs can be instantiated and deployed on virtual
              machines running on commodity servers which ensures their
              flexibility, manageability, cost-efficiency, and reduce their
              time-to-market. However, efficiently processing services through
              an ordered chain of VNFs, called service function chaining (SFC),
              is not trivial. It requires solving three inter-related
              sub-problems; the network functions (NFs) mapping sub-problem,
              the traffic routing sub-problem and the service scheduling
              sub-problem. This paper first highlights the existing interplay
              between the three sub-problems and then presents a formulation of
              the SFC scheduling (SFCS) which exploits interactions between NFs
              mapping onto VNFs, service scheduling and traffic routing. Given
              the complexity of the SFCS problem, we present a novel
              primal-dual decomposition using column generation that solves
              exactly a relaxed version of the problem and can serve as a
              benchmark approach. We enhance our solution methodology with a
              diversification technique to help improve the quality of the
              obtained solutions. We evaluate numerically our method and show
              that it can attain optimal solutions substantially faster.
              Finally, we present several engineering insights for improving
              the network performance.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  14,
  number   =  4,
  pages    = "860--874",
  month    =  dec,
  year     =  2017,
  file     = "All Papers/A/Alameddine et al. 2017 - On the Interplay Between Network Function Mapping and Scheduling in VNF-Based Networks - A Column Generation Approach.pdf",
  keywords = "cloud computing;operating systems
              (computers);optimisation;scheduling;virtual
              machines;virtualisation;network function mapping;column
              generation approach;hardware appliances;integral
              part;cloud;enterprise networks;network function
              virtualization;virtual network functions;VNFs;hardware
              middleboxes;virtual machines;network functions mapping
              sub-problem;traffic routing sub-problem;service scheduling
              sub-problem;SFC scheduling;NFs mapping;SFCS problem;network
              performance;service function
              chaining;SFC;Routing;Hardware;Middleboxes;Network function
              virtualization;Delays;Cloud
              computing;Scheduling;Bandwidth;Network function
              virtualization;service function chaining;virtual network
              functions;scheduling;bandwidth guarantees;cloud
              networks;optimization;column generation;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2017.2757266"
}

@ARTICLE{Choudhury2019-qf,
  title    = "A Proactive {Context-Aware} Service Replication Scheme for Adhoc
              {IoT} Scenarios",
  author   = "Choudhury, B and Choudhury, S and Dutta, A",
  abstract = "We consider a smart-city IoT scenario where large crowd may
              gather temporarily rendering the existing infrastructure
              inadequate for service consumption. This necessitates a service
              replication framework over quasi-adhoc scenario using the
              available computing resources carried by the users such as
              smart-phones. Such framework can offer fog computing solution in
              addition to enabling consumption of plethora of new services
              available with the crowd. In this paper, we propose a service
              replication scheme that achieves improved service availability,
              service response-time and system-wide resource utilization
              compared to the existing ones. The scheme uses a
              dual-threshold-based proactive sensing mechanism to identify the
              services which are required to be replicated in immediate future
              and a multi-agent-based optimal task assignment scheme that
              enables batch-wise decision making. These mechanisms acting
              together reduces the service drop rate and improves the
              system-wide resource utilization. The service response time and
              the overhead involved in making assignment decisions, are
              markedly reduced by applying a strategy that combines the
              benefits of both physical and functional contexts together. An
              integrated model for analyzing the performance of various generic
              service replication schemes, is also developed in this paper.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1797--1811",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/C/Choudhury et al. 2019 - A Proactive Context-Aware Service Replication Scheme for Adhoc IoT Scenarios.pdf",
  keywords = "decision making;distributed processing;Internet of
              Things;multi-agent systems;resource allocation;smart cities;smart
              phones;service response-time;system-wide resource
              utilization;dual-threshold-based proactive sensing
              mechanism;multiagent-based optimal task assignment
              scheme;batch-wise decision making;service drop rate;service
              response time;service replication schemes;proactive context-aware
              service replication scheme;ad hoc IoT Scenarios;smart-city IoT
              scenario;service replication framework;smart-phones;fog
              computing;Resource management;Sensors;Servers;Quality of
              service;Middleware;Edge computing;Time
              factors;Context-aware;DCDP;Internet of things;proactive
              sensing;service replication",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2928698"
}

@ARTICLE{Li2019-dn,
  title    = "Availability Aware {VNF} Deployment in Datacenter Through Shared
              Redundancy and {Multi-Tenancy}",
  author   = "Li, D and Hong, P and Xue, K and Pei, J",
  abstract = "By means of network function virtualization (NFV), dedicated
              proprietary network devices can be implemented as software and
              instantiated flexibly on common-off-the-shelf servers, in the
              form of virtual network functions (VNF). NFV can bring great cost
              reduction as well as operation flexibility. However, it also
              brings new problems, one of which is how to meet the availability
              of network services in the VNF deployment process, because of the
              error prone nature of software. The availability aware VNF
              deployment problem has attracted attention by academics, and
              reserving redundancy has been treated as the de facto technology.
              Compared with traditional backup schemes for physical machines,
              resource orchestration in NFV is more flexible and the
              characteristics of software should be considered to improve
              resource utilization efficiency. Based on the above
              considerations, in this paper we further study the availability
              aware VNF deployment problem in datacenter networks. To improve
              the resource utilization efficiency, the sharing mechanism of
              redundancy and multi-tenancy technology are taken into account.
              Then we formulate the problem mathematically and propose a joint
              deployment and backup scheme (JDBS). Finally, we conduct a
              numerical simulation in detail and compare it with four
              contrasting schemes in the existing literature. The simulation
              results show that JDBS is obviously superior to the contrasting
              schemes and can save about 40\% resources at most.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1651--1664",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/L/Li et al. 2019 - Availability Aware VNF Deployment in Datacenter Through Shared Redundancy and Multi-Tenancy.pdf",
  keywords = "computer centres;computer network reliability;numerical
              analysis;virtualisation;numerical simulation;JDBS;joint
              deployment and backup scheme;mathematical analysis;multitenancy
              technology;datacenter networks;resource utilization
              efficiency;availability aware VNF deployment problem;virtual
              network functions;dedicated proprietary network
              devices;NFV;network function virtualization;shared
              redundancy;Redundancy;Software;Resource
              management;Servers;Simulation;Virtualization;Network function
              virtualization;VNF deployment;SFC;availability
              aware;multi-tenancy;redundancy sharing",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2936505"
}

@ARTICLE{Castellano2019-ba,
  title    = "A {Service-Defined} Approach for Orchestration of Heterogeneous
              Applications in {Cloud/Edge} Platforms",
  author   = "Castellano, G and Esposito, F and Risso, F",
  abstract = "Edge Computing is moving resources toward the network borders,
              thus enabling the deployment of a pool of new applications that
              benefit from the new distributed infrastructure. However, due to
              the heterogeneity of such applications, specific orchestration
              strategies need to be adopted for each deployment request. Each
              application can potentially require different optimization
              criteria and may prefer particular reactions upon the occurrence
              of the same event. This paper presents a Service-Defined approach
              for orchestrating cloud/edge services in a distributed fashion,
              where each application can define its own orchestration strategy
              by means of declarative statements, which are parsed into a
              Service-Defined Orchestrator (SDO). Moreover, to coordinate the
              coexistence of a variety of SDOs on the same infrastructure while
              preserving the resource assignment optimality, we present DRAGON,
              a Distributed Resource AssiGnment and OrchestratioN algorithm
              that seeks optimal partitioning of shared resources between
              different actors. We evaluate the advantages of our novel
              Service-Defined orchestration approach over some representative
              edge use cases, as well as measure convergence and performance of
              DRAGON on a prototype implementation, assessing the benefits
              compared to conventional orchestration approaches.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1404--1418",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/C/Castellano et al. 2019 - A Service-Defined Approach for Orchestration of Heterogeneous Applications in Cloud - Edge Platforms.pdf",
  keywords = "business data processing;cloud computing;optimisation;resource
              allocation;service-oriented architecture;Web services;distributed
              resource assignment;service-defined orchestration approach;edge
              computing;orchestration strategy;optimization criteria;deployment
              request;specific orchestration strategies;distributed
              infrastructure;network borders;heterogeneous
              applications;service-defined approach;conventional orchestration
              approaches;representative edge use cases;shared resources;optimal
              partitioning;resource assignment optimality;Service-Defined
              Orchestrator;Optimization;Streaming media;Computer
              architecture;Resource management;Cloud computing;Partitioning
              algorithms;Convergence;Orchestration;mathematical
              optimization;distributed algorithms;distributed management",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2941639"
}

@ARTICLE{Van_Rossem2019-ku,
  title    = "{Profile-Based} Resource Allocation for Virtualized Network
              Functions",
  author   = "Van Rossem, S and Tavernier, W and Colle, D and Pickavet, M and
              Demeester, P",
  abstract = "The virtualization of compute and network resources enables an
              unseen flexibility for deploying network services. A wide
              spectrum of emerging technologies allows an ever-growing range of
              orchestration possibilities in cloud-based environments. But in
              this context it remains challenging to rhyme dynamic cloud
              configurations with deterministic performance. The service
              operator must somehow map the performance specification in the
              Service Level Agreement (SLA) to an adequate resource allocation
              in the virtualized infrastructure. We propose the use of a VNF
              profile to alleviate this process. This is illustrated by
              profiling the performance of four example network functions (a
              virtual router, switch, firewall and cache server) under varying
              workloads and resource configurations. We then compare several
              methods to derive a model from the profiled datasets. We select
              the most accurate method to further train a model which predicts
              the services' performance, in function of incoming workload and
              allocated resources. Our presented method can offer the service
              operator a recommended resource allocation for the targeted
              service, in function of the targeted performance and maximum
              workload specified in the SLA. This helps to deploy the
              softwarized service with an optimal amount of resources to meet
              the SLA requirements, thereby avoiding unnecessary scaling steps.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1374--1388",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/V/Van Rossem et al. 2019 - Profile-Based Resource Allocation for Virtualized Network Functions.pdf",
  keywords = "cloud computing;resource allocation;software defined
              networking;virtualisation;profile-based resource
              allocation;virtualized network functions;network resources;unseen
              flexibility;network services;orchestration
              possibilities;cloud-based environments;rhyme dynamic cloud
              configurations;deterministic performance;service
              operator;performance specification;service level
              agreement;SLA;adequate resource allocation;virtualized
              infrastructure;VNF profile;example network functions;virtual
              router;firewall;cache server;resource configurations;profiled
              datasets;incoming workload;recommended resource
              allocation;targeted service;targeted performance;softwarized
              service;Measurement;Hardware;Monitoring;Resource
              management;Virtualization;Cloud computing;Servers;Network
              function virtualization;performance analysis;performance
              profiling",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2943779"
}

@ARTICLE{Farkiani2019-sk,
  title    = "A Fast {Near-Optimal} Approach for {Energy-Aware} {SFC}
              Deployment",
  author   = "Farkiani, B and Bakhshi, B and MirHassani, S A",
  abstract = "Service function chaining along with network function
              virtualization enable flexible and rapid provisioning of network
              services to meet increasing demand for short-lived services with
              diverse requirements. In this paradigm, the main question to be
              answered is how to deploy the requested services by means of
              creating virtual network function (VNF) instances and routing the
              traffic between them, according to the services specifications.
              In this paper, we define the energy aware service deployment
              problem, and present the ILP formulation of it by considering
              limited traffic processing capacity of VNF instances and
              management concerns. We apply the Benders decomposition technique
              to decompose the problem into two smaller problems: master and
              sub-problem. As it is NP-Hard to find a non-trivial solution to
              the ILP master problem, we resort to the relaxed LP version of
              the problem. Then, we design methods based on the feasibility
              pump and duality theorem to rapidly calculate a near-optimal
              integer solution. The extensive simulation results show even in a
              network with 24 switches and 40 servers, our algorithm can deploy
              35 requests in less than 3 seconds while the total power
              consumption is only about 1.3 times of the optimal solution
              obtained by the exhaustive exact approach. Moreover, it
              significantly outperforms the prominent SFC deployment algorithms
              in the fat-tree topology.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1360--1373",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/F/Farkiani et al. 2019 - A Fast Near-Optimal Approach for Energy-Aware SFC Deployment.pdf",
  keywords = "cloud computing;computer centres;integer programming;linear
              programming;resource allocation;telecommunication power
              management;telecommunication traffic;trees
              (mathematics);virtualisation;prominent SFC deployment
              algorithms;optimal solution;near-optimal integer solution;ILP
              master problem;nontrivial solution;smaller problems;Benders
              decomposition technique;management concerns;traffic processing
              capacity;ILP formulation;energy aware service deployment
              problem;services specifications;virtual network function
              instances;requested services;main question;diverse
              requirements;short-lived services;network services;rapid
              provisioning;network function virtualization;service
              function;energy-aware SFC deployment;near-optimal approach;time
              3.0 s;Servers;Power demand;Energy consumption;Bandwidth;Heuristic
              algorithms;Computational modeling;Virtualization;Service function
              chaining;network function virtualization;decomposition;duality",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2944023"
}

@ARTICLE{Ma2019-ea,
  title    = "Placing {Traffic-Changing} and {Partially-Ordered} {NFV}
              Middleboxes via {SDN}",
  author   = "Ma, W and Beltran, J and Pan, D and Pissinou, N",
  abstract = "Network Function Virtualization (NFV) enables flexible
              implementation of network functions, also called middleboxes, as
              virtual machines running on standard servers. However, the
              flexibility also makes it a challenge to optimally place
              middleboxes, because a middlebox may be hosted by different
              servers at different locations. The middlebox placement challenge
              is further complicated by additional constraints, including the
              capability of middleboxes to change traffic volumes and
              dependency between them. In this paper, we address the optimal
              placement challenge of NFV middleboxes for the data plane using a
              software-defined networking (SDN) approach. First, we formulate
              the optimization problem to place traffic-changing and
              interdependent middleboxes. When the flow path is predetermined,
              we design optimal algorithms to place a non-ordered or
              totally-ordered middlebox set, and propose a low-complexity
              solution for the general scenario of a partially-ordered
              middlebox set after proving its NP-hardness. When the flow path
              is not predetermined, we show that the problem is NP-hard even
              for a non-ordered or totally-ordered middlebox set, and propose
              an efficient traffic and space aware routing algorithm. We have
              evaluated the proposed algorithms using large scale simulations
              and a real application based SDN prototype, and present extensive
              evaluation results to demonstrate the superiority of our design
              over benchmark solutions.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1303--1317",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/M/Ma et al. 2019 - Placing Traffic-Changing and Partially-Ordered NFV Middleboxes via SDN.pdf",
  keywords = "computational complexity;optimisation;software defined
              networking;telecommunication network routing;telecommunication
              traffic;virtual machines;virtualisation;optimal placement
              challenge;NFV middleboxes;software-defined networking
              approach;interdependent middleboxes;network function
              virtualization;network functions;middlebox placement
              challenge;Middleboxes;Approximation
              algorithms;Servers;Optimization;Prototypes;Mice;Virtualization;Network
              function virtualization;software-defined networking;middlebox",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2946347"
}

@ARTICLE{Kherraf2019-cv,
  title    = "Latency and {Reliability-Aware} Workload Assignment in {IoT}
              Networks With Mobile Edge Clouds",
  author   = "Kherraf, N and Sharafeddine, S and Assi, C M and Ghrayeb, A",
  abstract = "Along with the dramatic increase in the number of IoT devices,
              different IoT services with heterogeneous QoS requirements are
              evolving with the aim of making the current society smarter and
              more connected. In order to deliver such services to the end
              users, the network infrastructure has to accommodate the
              tremendous workload generated by the smart devices and their
              heterogeneous and stringent latency and reliability requirements.
              This would only be possible with the emergence of ultra reliable
              low latency communications (uRLLC) promised by 5G. Mobile Edge
              Computing (MEC) has emerged as an enabling technology to help
              with the realization of such services by bringing the remote
              computing and storage capabilities of the cloud closer to the
              users. However, integrating uRLLC with MEC would require the
              network operator to efficiently map the generated workloads to
              MEC nodes along with resolving the trade-off between the latency
              and reliability requirements. Thus, we study in this paper the
              problem of Workload Assignment (WA) and formulate it as a Mixed
              Integer Program (MIP) to decide on the assignment of the
              workloads to the available MEC nodes. Due to the complexity of
              the WA problem, we decompose the problem into two subproblems;
              Reliability Aware Candidate Selection (RACS) and Latency Aware
              Workload Assignment (LAWA-MIP). We evaluate the performance of
              the decomposition approach and propose a more scalable approach;
              Tabu meta-heuristic (WA-Tabu). Through extensive numerical
              evaluation, we analyze the performance and show the efficiency of
              our proposed approach under different system parameters.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1435--1449",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/K/Kherraf et al. 2019 - Latency and Reliability-Aware Workload Assignment in IoT Networks With Mobile Edge Clouds.pdf",
  keywords = "cloud computing;distributed processing;integer
              programming;Internet of Things;mobile computing;quality of
              service;reliability-aware workload assignment;IoT networks;mobile
              edge clouds;IoT devices;different IoT services;heterogeneous QoS
              requirements;network infrastructure;smart devices;reliability
              requirements;ultra reliable low latency communications;uRLLC;5G
              mobile edge computing;remote computing;MEC nodes;mixed integer
              program;MIP;reliability aware candidate selection;RACS;latency
              aware workload assignment;LAWA-MIP;Tabu
              meta-heuristic;WA-Tabu;Reliability;Internet of Things;5G mobile
              communication;Task analysis;Energy consumption;Cloud
              computing;Internet of Things;mobile edge computing;ultra reliable
              low latency communications;5G;optimization;workload assignment",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2946467"
}

@ARTICLE{Hosseini2019-ja,
  title    = "Probabilistic Virtual Link Embedding Under Demand Uncertainty",
  author   = "Hosseini, F and James, A and Ghaderi, M",
  abstract = "This paper considers the problem of mapping virtual links to
              physical network paths, referred to as Virtual Link Embedding
              (VLE), under the condition that bandwidth demands of virtual
              links are uncertain. To realize virtual links with predictable
              performance, the mapping is required to guarantee a bound on the
              congestion probability of the physical paths that embed the
              virtual links. To this end, we consider a general uncertainty
              model in which bandwidth demands of virtual links are expressed
              by random variables for which only the mean and variance (or a
              range) are known. We formulate the VLE problem as a nonlinear
              optimization program and design an algorithm called Equal
              Partition VLE (epVLE) to solve the problem by employing an
              approximate formulation that results in a second-order cone
              program (SOCP) that can be solved efficiently even for large
              networks. We then provide simulation results as well as
              model-driven and trace-driven experimental results from an SDN
              testbed to show the utility and efficiency of the epVLE algorithm
              in various network scenarios. We apply epVLE to commonly studied
              small networks as well as randomly generated large networks. Our
              results show that epVLE is able to satisfy the required link
              congestion constraint, and that it produces results that are very
              close to those obtained from the exact optimization model.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1552--1566",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/H/Hosseini et al. 2019 - Probabilistic Virtual Link Embedding Under Demand Uncertainty.pdf",
  keywords = "convex programming;probability;resource allocation;software
              defined networking;virtualisation;SDN testbed;epVLE;equal
              partition VLE;second-order cone program;SOCP;demand
              uncertainty;virtual link embedding;link congestion
              constraint;bandwidth demands;physical network paths;mapping
              virtual links;probabilistic virtual
              link;Uncertainty;Optimization;Bandwidth;Computational
              modeling;Substrates;Approximation algorithms;Delays;Network
              virtualization;virtual link embedding;uncertain
              demands;Congestion probability",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2946949"
}

@ARTICLE{Quang2019-xr,
  title    = "A Deep Reinforcement Learning Approach for {VNF} Forwarding Graph
              Embedding",
  author   = "Quang, P T A and Hadjadj-Aoul, Y and Outtagarts, A",
  abstract = "Network Function Virtualization (NFV) and service orchestration
              simplify the deployment and management of network and
              telecommunication services. The deployment of these services
              requires, typically, the allocation of Virtual Network Function -
              Forwarding Graph (VNF-FG), which implies not only the fulfillment
              of the service's requirements in terms of Quality of Service
              (QoS), but also considering the constraints of the underlying
              infrastructure. This topic has been well-studied in existing
              literature, however, its complexity and uncertainty of available
              information unveil challenges for researchers and engineers. In
              this paper, we explore the potential of reinforcement learning
              techniques for the placement of VNF-FGs. However, it turns out
              that even the most well-known learning technique is ineffective
              in the context of a large-scale action space. In this respect, we
              propose approaches to find out feasible solutions while improving
              significantly the exploration of the action space. The simulation
              results clearly show the effectiveness of the proposed learning
              approach for this category of problems. Moreover, thanks to the
              deep learning process, the performance of the proposed approach
              is improved over time.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1318--1331",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/Q/Quang et al. 2019 - A Deep Reinforcement Learning Approach for VNF Forwarding Graph Embedding.pdf",
  keywords = "graph theory;learning (artificial intelligence);quality of
              service;telecommunication services;virtualisation;VNF-FG;VNF
              forwarding graph;network function
              virtualization;NFV;telecommunication services;virtual network
              function;deep reinforcement learning;quality of service;Resource
              management;Optimization;Space exploration;Reinforcement
              learning;Quality of service;Complexity theory;Convergence;Network
              function virtualization;VNF-FG embedding;Deep reinforcement
              learning;Quality of Services;NFV;MLNetworking",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2947905"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Zeljkovic2019-ki,
  title    = "{ABRAHAM}: Machine Learning Backed Proactive Handover Algorithm
              Using {SDN}",
  author   = "Zeljkovi{\'c}, E and Slamnik-Krije{\v s}torac, N and Latr{\'e}, S
              and Marquez-Barja, J M",
  abstract = "An important aspect of managing multi access point (AP) IEEE
              802.11 networks is the support for mobility management by
              controlling the handover process. Most handover algorithms,
              residing on the client station (STA), are reactive and take a
              long time to converge, and thus severely impact Quality of
              Service (QoS) and Quality of Experience (QoE). Centralized
              approaches to mobility and handover management are mostly
              proprietary, reactive and require changes to the client STA. In
              this paper, we first created an Software-Defined Networking (SDN)
              modular handover management framework called HuMOR, which can
              create, validate and evaluate handover algorithms that preserve
              QoS. Relying on the capabilities of HuMOR, we introduce ABRAHAM,
              a machine learning backed, proactive, handover algorithm that
              uses multiple metrics to predict the future state of the network
              and optimize the load to ensure the preservation of QoS. We
              compare ABRAHAM to a number of alternative handover algorithms in
              a comprehensive QoS study, and demonstrate that it outperforms
              them with an average throughput improvement of up to 139\%, while
              statistical analysis shows that there is significant statistical
              difference between ABRAHAM and the rest of the algorithms.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  16,
  number   =  4,
  pages    = "1522--1536",
  month    =  dec,
  year     =  2019,
  file     = "All Papers/Z/Zeljković et al. 2019 - ABRAHAM - Machine Learning Backed Proactive Handover Algorithm Using SDN.pdf",
  keywords = "mobility management (mobile radio);optimisation;quality of
              experience;quality of service;software defined
              networking;statistical analysis;wireless LAN;ABRAHAM;machine
              learning;proactive handover algorithm;SDN;multiaccess
              point;mobility management;handover process;QoS;client
              STA;alternative handover algorithms;software-defined networking
              modular handover management
              framework;Handover;Measurement;Quality of service;Quality of
              experience;Prediction algorithms;IEEE 802.11 Standard;IEEE
              80211;SDN;handover algorithm",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2948883"
}

@ARTICLE{Geissler2020-lq,
  title    = "The Power of Composition: Abstracting a {Multi-Device} {SDN} Data
              Path Through a Single {API}",
  author   = "Geissler, S and Herrnleben, S and Bauer, R and Grigorjew, A and
              Zinner, T and Jarschel, M",
  abstract = "Software Defined Networking aims to separate network control and
              data plane by moving the control logic from network elements into
              a logically-centralized controller. Using a well-defined, unified
              control-channel protocol, such as OpenFlow, the controller is
              able to configure the forwarding behavior of data plane devices.
              Here, the OpenFlow protocol is translated to vendor- and
              device-specific instructions that, for instance, manipulate the
              flow table entries of a switch. In practice, SDN-enabled switches
              often feature different hardware capabilities and configurations
              with respect to the number of flow tables, their implementation,
              and which kind of data plane features they support. This leads to
              device heterogeneity within the SDN landscape, thereby
              obstructing the increased scalability and flexibility promised by
              the SDN paradigm. To overcome this challenge we propose
              TableVisor, a transparent proxy-layer for the SDN control channel
              that enables the flexible abstraction of heterogeneous data plane
              devices into a single emulated data plane switch. In this paper,
              we extend our previous work by introducing features to integrate
              modern P4 devices into an existing SDN environment and perform a
              detailed performance evaluation to quantify the overhead induced
              by our approach.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  2,
  pages    = "722--735",
  month    =  jun,
  year     =  2020,
  keywords = "application program interfaces;protocols;software defined
              networking;telecommunication control;telecommunication
              switching;multidevice SDN data path;single API;Software Defined
              Networking;network control;control logic;network
              elements;logically-centralized controller;unified control-channel
              protocol;forwarding behavior;OpenFlow protocol;device-specific
              instructions;flow table entries;SDN-enabled switches;data plane
              features;device heterogeneity;SDN landscape;SDN paradigm;SDN
              control channel;flexible abstraction;heterogeneous data plane
              devices;single emulated data plane switch;modern P4 devices;SDN
              environment;Switches;Hardware;Pipeline
              processing;Protocols;Performance
              evaluation;Scalability;Software;Software-defined networks;data
              plane abstraction;programmable hardware;p4;SDN",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2951834"
}

@ARTICLE{Zhang2020-mp,
  title    = "Network Service Scheduling With Resource Sharing and Preemption",
  author   = "Zhang, Y and He, F and Sato, T and Oki, E",
  abstract = "Network function virtualization enables network operators to
              implement network functions in a software-oriented manner and
              makes network services (NSes) provisioning much simpler. This
              paper proposes an optimization model to schedule delay sensitive
              NSes with deadlines allowing resource sharing and preemption.
              Unlike conventional NS scheduling models with static resource
              allocation for virtualized network function (VNF) instances, the
              proposed model ensures that VNF instances deployed on the same
              node share computation resources of the node and are able to
              scale up/down to change their process rate at runtime. NSes
              mapped to the same VNF instance of the same node share
              computation resources of the VNF instance and are able to be
              processed in parallel by the VNF instance. Preemption is allowed,
              which means that rescheduling the order of NS processing at
              runtime is possible and the process duration of each function of
              an NS is allowed to be discrete. We formulate the proposed model
              as an integer linear programming problem to maximize the number
              of admissible NSes. Due to the complexity of the problem, we
              develop a genetic algorithm to solve it efficiently. We evaluate
              the proposed model with conventional models in the static and
              dynamic scenarios. The numerical results show that the proposed
              model outperforms conventional models in terms of acceptance
              ratio in both static and dynamic scenarios.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  2,
  pages    = "764--778",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/Z/Zhang et al. 2020 - Network Service Scheduling With Resource Sharing and Preemption.pdf",
  keywords = "genetic algorithms;integer programming;linear
              programming;resource allocation;scheduling;software defined
              networking;virtualisation;NS scheduling models;static resource
              allocation;virtualized network function;VNF instance;node share
              computation resources;process rate;admissible NSes;network
              service scheduling;network function virtualization;network
              operators;network services provisioning;integer linear
              programming problem;genetic algorithm;resource sharing;delay
              sensitive NSes;optimization model;software-oriented
              manner;Computational modeling;Scheduling;Resource
              management;Numerical models;Processor scheduling;Genetic
              algorithms;Runtime;Network function virtualization;network
              services;scheduling;resource sharing;preemption;genetic
              algorithm;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2956949"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Comsa2020-dn,
  title    = "{5MART}: A {5G} {SMART} Scheduling Framework for Optimizing {QoS}
              Through Reinforcement Learning",
  author   = "Comșa, I and Trestian, R and Muntean, G and Ghinea, G",
  abstract = "The massive growth in mobile data traffic and the heterogeneity
              and stringency of Quality of Service (QoS) requirements of
              various applications have put significant pressure on the
              underlying network infrastructure and represent an important
              challenge even for the very anticipated 5G networks. In this
              context, the solution is to employ smart Radio Resource
              Management (RRM) in general and innovative packet scheduling in
              particular in order to offer high flexibility and cope with both
              current and upcoming QoS challenges. Given the increasing demand
              for bandwidth-hungry applications, conventional scheduling
              strategies face significant problems in meeting the heterogeneous
              QoS requirements of various application classes under dynamic
              network conditions. This paper proposes 5MART, a 5G smart
              scheduling framework that manages the QoS provisioning for
              heterogeneous traffic. Reinforcement learning and neural networks
              are jointly used to find the most suitable scheduling decisions
              based on current networking conditions. Simulation results show
              that the proposed 5MART framework can achieve up to 50\%
              improvement in terms of time fraction (in sub-frames) when the
              heterogeneous QoS constraints are met with respect to other
              state-of-the-art scheduling solutions.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  2,
  pages    = "1110--1124",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/C/Comșa et al. 2020 - 5MART - A 5G SMART Scheduling Framework for Optimizing QoS Through Reinforcement Learning.pdf",
  keywords = "5G mobile communication;learning (artificial intelligence);neural
              nets;quality of service;resource allocation;telecommunication
              computing;telecommunication network management;telecommunication
              scheduling;telecommunication traffic;5G SMART scheduling
              framework;reinforcement learning;mobile data traffic;quality of
              service;anticipated 5G networks;smart Radio Resource
              Management;general packet scheduling;bandwidth-hungry
              applications;heterogeneous QoS requirements;dynamic network
              conditions;QoS provisioning;heterogeneous traffic;neural
              networks;5MART framework;RRM;Quality of service;Frequency-domain
              analysis;5G mobile communication;Delays;Time-domain
              analysis;Dynamic scheduling;5G;radio resource management;machine
              learning;scheduling;traffic prioritization;QoS
              optimization;Wireless",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2960849"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gambin2020-dv,
  title    = "A Sharing Framework for Energy and Computing Resources in
              {Multi-Operator} Mobile Networks",
  author   = "Gamb{\'\i}n, {\'A} F and Rossi, M",
  abstract = "Energy Harvesting (EH) and Multi-access Edge Computing (MEC) are
              here combined to build energy-sustainable mobile networks. We
              consider an edge infrastructure shared among several mobile
              operators and equipped with a solar EH farm for energy efficiency
              purposes together with an edge MEC server for low-latency
              computation, where two main goals are pursued: (i) to maximally
              and fairly exploit the available resources at the edge, allotting
              them among Base Stations (BSs) belonging to different operators;
              and (ii) to decrease the monetary cost incurred by energy
              purchases from the power grid. To do so, we devise an online
              framework combining Artificial Neural Network (ANN)-based pattern
              forecasting that learns energy harvesting and traffic load
              profiles over time, and Model Predictive Control (MPC)-based
              adaptive algorithms. Numerical results, obtained with real-world
              harvested energy, traffic load, and energy price traces, show
              that our proposal effectively reduces the amount of purchased
              energy from the electrical grid by more than 50\% with respect to
              the case where no EH is considered, and by about 30\% with
              respect to the case where the optimization is performed
              disregarding future energy and traffic load forecasts. Moreover,
              it is capable of reducing the energy consumption related to edge
              computation by about 20\% with respect to two benchmark policies.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  2,
  pages    = "1140--1152",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/G/Gambín and Rossi 2020 - A Sharing Framework for Energy and Computing Resources in Multi-Operator Mobile Networks.pdf",
  keywords = "adaptive control;distributed processing;energy
              conservation;energy consumption;energy harvesting;load
              forecasting;mobile computing;neural nets;optimisation;power
              engineering computing;power grids;power markets;power system
              control;predictive control;pricing;solar power;telecommunication
              traffic;traffic load forecasts;energy consumption;edge
              computation;multioperator mobile networks;multiaccess edge
              computing;energy-sustainable mobile networks;edge
              infrastructure;mobile operators;solar EH farm;energy
              efficiency;electrical grid;model predictive control-based
              adaptive algorithms;artificial neural network-based pattern
              forecasting;energy and computing resources;energy price
              traces;traffic load profiles;energy harvesting;power grid;energy
              purchases;base stations;low-latency computation;edge MEC
              server;Power grids;Optimization;Servers;Forecasting;Energy
              consumption;Computer architecture;Resource management;Energy
              harvesting;edge computing;multi-operator networks;pattern
              forecasting;adaptive control;energy self-sustainability;mobile
              networks;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2019.2962725"
}

@ARTICLE{Yao2020-fd,
  title    = "A {Continuous-Decision} Virtual Network Embedding Scheme Relying
              on Reinforcement Learning",
  author   = "Yao, H and Ma, S and Wang, J and Zhang, P and Jiang, C and Guo, S",
  abstract = "Network Virtualization (NV) techniques allow multiple virtual
              network requests to beneficially share resources on the same
              substrate network, such as node computational resources and link
              bandwidth. As the most famous family member of NV techniques,
              virtual network embedding is capable of efficiently allocating
              the limited network resources to the users on the same substrate
              network. However, traditional heuristic virtual network embedding
              algorithms generally follow a static operating mechanism, which
              cannot adapt well to the dynamic network structures and
              environments, resulting in inferior nodes ranking and embedding
              strategies. Some reinforcement learning aided embedding
              algorithms have been conceived to dynamically update the
              decision-making strategies, while the node embedding of the same
              request is discretized and its continuity is ignored. To address
              this problem, a Continuous-Decision virtual network embedding
              scheme relying on Reinforcement Learning (CDRL) is proposed in
              our paper, which regards the node embedding of the same request
              as a time-series problem formulated by the classic seq2seq model.
              Moreover, two traditional heuristic embedding algorithms as well
              as the classic reinforcement learning aided embedding algorithm
              are used for benchmarking our proposed CDRL algorithm. Finally,
              simulation results show that our proposed algorithm is superior
              to the other three algorithms in terms of long-term average
              revenue, revenue to cost and acceptance ratio.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  2,
  pages    = "864--875",
  month    =  jun,
  year     =  2020,
  file     = "All Papers/Y/Yao et al. 2020 - A Continuous-Decision Virtual Network Embedding Scheme Relying on Reinforcement Learning.pdf",
  keywords = "computer network management;decision making;learning (artificial
              intelligence);resource allocation;virtualisation;traditional
              heuristic virtual network embedding algorithms;reinforcement
              learning;node embedding;network virtualization
              techniques;multiple virtual network requests;node computational
              resources;heuristic embedding algorithms;continuous-decision
              virtual network embedding scheme;Substrates;Heuristic
              algorithms;Bandwidth;Reinforcement learning;Machine learning
              algorithms;Computational modeling;Prediction
              algorithms;Reinforcement learning;virtual network
              embedding;continuous decision;time-series;seq2seq;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.2971543"
}

@ARTICLE{Huang2020-xr,
  title    = "A Scalable Approach to {SDN} Control Plane Management: High
              Utilization Comes With Low Latency",
  author   = "Huang, V and Chen, G and Zhang, P and Li, H and Hu, C and Pan, T
              and Fu, Q",
  abstract = "One major research challenge for Software-Defined Networking is
              to properly deploy and efficiently utilize multiple controllers
              to improve resource utilization and maintain high network
              performance. While addressing this Controller Placement Problem
              (CPP), many existing studies overlooked the importance and
              influence of the Controller Scheduling Problem (CSP) with the
              central focus on proper distribution of requests from all
              switches among all controllers. In this paper, we define a new
              Controller Placement and Scheduling Problem (CPSP), emphasizing
              on the necessity and importance of tackling both CPP and CSP
              simultaneously in a coherent framework. To solve CPSP, we must
              seek a combination of solutions to both problems. Particularly,
              CSP is addressed based on a given solution to CPP and a
              Gradient-Descent-based (GD-based) scheduling algorithm is
              developed to optimize the probabilistic distribution of requests
              among all controllers. Built on the GD-based approach for
              controller scheduling, a Clustering-based Genetic Algorithm with
              Cooperative Clusters (CGA-CC) is further proposed to address CPP.
              In comparison to the majority of heuristic methods developed in
              the past, CGA-CC has two unique strengths. Specifically, it
              partitions a large network to substantially reduce the search
              space of the Genetic Algorithm (GA), resulting in fast
              identification of high-quality CPP solutions. Moreover, a greedy
              load re-distribution mechanism is developed to handle unexpected
              demand variations by dynamically forwarding bursting requests to
              neighboring sub-networks. Extensive simulations showed that our
              algorithms can significantly outperform several existing
              algorithms, including a recently proposed approach called
              Multi-controller Selection and Placement Algorithm (MSPA), in
              terms of both response time and controller utilization.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  2,
  pages    = "682--695",
  month    =  jun,
  year     =  2020,
  keywords = "genetic algorithms;gradient methods;greedy algorithms;pattern
              clustering;resource allocation;software defined
              networking;telecommunication scheduling;scalable approach;SDN
              control plane management;high utilization comes;research
              challenge;software-defined networking;multiple
              controllers;resource utilization;high network
              performance;controller placement problem;central focus;proper
              distribution;CPSP;coherent framework;probabilistic
              distribution;GD-based approach;CGA-CC;high-quality CPP
              solutions;greedy load re-distribution mechanism;neighboring
              sub-networks;controller utilization;clustering-based genetic
              algorithm;gradient-descent-based scheduling algorithm;controller
              scheduling problem;Control systems;Time factors;Genetic
              algorithms;Scheduling algorithms;Process control;Resource
              management;Heuristic algorithms;Software-defined
              networking;distributed controller architectures;controller
              placement;controller scheduling;SDN",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.2973222"
}

@ARTICLE{Yu2020-wk,
  title    = "{Fine-Grained} Cloud Resource Provisioning for Virtual Network
              Function",
  author   = "Yu, H and Yang, J and Fung, C",
  abstract = "The deployment of Virtualized Network Functions is expected to be
              dynamic and swift when using Network Function Virtualization
              technology. The dynamic nature of workload from users requires
              the resource allocation of underlying infrastructure to be
              flexible to cope with the changes. Existing works investigated
              elastic NFV solutions by dynamically creating and dismantling
              Virtual Machine (VM) replicas, while maintaining balanced
              workload among VMs. However, those solutions are coarse-grained
              which may cause unnecessary resource over-provisioning as
              different network functions consume different amount of
              resources. In this paper, we present ElasticNFV, a dynamic and
              fine-grained cloud resource provisioning solution for VNF.
              ElasticNFV takes real-time resource demand of multiple service
              chains and allocates resources through an elastic provision
              mechanism. When a scaling conflict occurs, ElasticNFV provides a
              two-phase minimal migration algorithm to optimize the migration
              time and embedding cost of VNF instances. We implement ElasticNFV
              on top of the KVM platform to provide elastic VM for each VNF
              instance and Open vSwitch to form elastic intra-cloud network
              with virtual links between VNF instances. Our evaluation results
              show that ElasticNFV can improve VNF performance significantly,
              and achieve high resource utilization and fast migration time
              with low cost.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1363--1376",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/Y/Yu et al. 2020 - Fine-Grained Cloud Resource Provisioning for Virtual Network Function.pdf",
  keywords = "cloud computing;elasticity;resource allocation;virtual
              machines;virtualisation;high resource utilization;virtual
              links;elastic intra-cloud network;elastic VM;VNF instance;elastic
              provision mechanism;allocates resources;multiple service
              chains;real-time resource demand;dynamic cloud
              resource;ElasticNFV;different network functions;unnecessary
              resource over-provisioning;balanced workload;Virtual Machine
              replicas;elastic NFV solutions;resource allocation;Network
              Function Virtualization technology;Virtualized Network
              Functions;Virtual Network Function;fine-grained cloud resource
              provisioning;Cloud computing;Resource management;Dynamic
              scheduling;Heuristic algorithms;Middleboxes;Bandwidth;Network
              function virtualization;Middlebox;network function virtualization
              (NFV);resource allocation;service chain;cloud computing;resource
              scaling;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.2986223"
}

@ARTICLE{Alamro2020-fk,
  title    = "Shed+: Optimal Dynamic Speculation to Meet Application Deadlines
              in Cloud",
  author   = "Alamro, S and Xu, M and Lan, T and Subramaniam, S",
  abstract = "With the growing deadline-sensitivity of cloud applications,
              adherence to specific deadlines is becoming increasingly crucial,
              particularly in shared clusters. A few slow tasks called
              stragglers can potentially adversely affect job execution times.
              Equally, inadequate slotting of data analytics applications could
              result in inappropriate resource deployment, ultimately damaging
              system performance. Against this backdrop, one effective way of
              tackling stragglers is by making extra attempts (or clones)1 for
              every single straggler after the submission of a job. This paper
              proposes Shed+, which is an optimization framework utilizing
              dynamic speculation that aims to maximize the jobs' PoCD
              (Probability of Completion before Deadline) by making full use of
              available resources. Notably, our work encompasses a new online
              scheduler that dynamically recomputes and reallocates resources
              during the course of a job's execution. According to our
              findings, Shed+ successfully leverages cloud resources and
              maximizes the percentage of jobs meeting their deadlines. In our
              experiments, we have seen this percentage for heavy load going up
              to 98\% for Shed+ as opposed to nearly 68\%, 40\%, 35\% and 37\%
              for Shed, Dolly, Hopper and Hadoop with speculation enabled,
              respectively.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1515--1526",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/A/Alamro et al. 2020 - Shed+ - Optimal Dynamic Speculation to Meet Application Deadlines in Cloud.pdf",
  keywords = "cloud computing;data analysis;data handling;optimisation;parallel
              processing;resource allocation;scheduling;optimal dynamic
              speculation;meet application deadlines;growing
              deadline-sensitivity;cloud applications;adherence;specific
              deadlines;shared clusters;slow tasks;stragglers;job execution
              times;inadequate slotting;data analytics
              applications;inappropriate resource deployment;damaging system
              performance;extra attempts;single straggler;optimization
              framework;jobs;dynamically recomputes;reallocates resources;cloud
              resources;Task analysis;Cloud computing;Optimization;Dynamic
              scheduling;Cloning;Computational modeling;Electronic
              mail;Cloud;mapreduce;stragglers;scheduling;cloning;speculation;deadlines;Datacentre",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.2986477"
}

@ARTICLE{Salhab2020-jl,
  title    = "Optimization of Virtualization Cost, Processing Power and Network
              Load of {5G} {Software-Defined} Data Centers",
  author   = "Salhab, N and Rahim, R and Langar, R",
  abstract = "Virtualization is getting unprecedented attention from Mobile
              Network Operators (MNOs) as it provides agility in deployment,
              especially when coupled with the Cloud that offers inherent
              elasticity and load-balancing of resources. MNOs have to ensure
              operational excellence by meeting several objectives. In this
              context, we propose in this paper, a framework for optimizing the
              mapping of next Generation Node-Bs (gNBs) to Software-Defined 5G
              Core (5GC) delay tolerant Network Functions (NFs). These NFs are
              considered to be deployed as a Virtual Machine (VM) pool, or
              containers, in order to minimize cloud computing cost, processing
              power and at the same time maximize network load. First, we
              formulate this problem as an integer linear program, while taking
              into account multiple constraints including Virtual Central
              Processing Unit (vCPU) capacity, central processing load limits
              and integrality of mapping relations between gNBs and 5GC NFs.
              Then, we propose an algorithm to solve large problem instances
              based on Branch, Cut and Price (BCP) combining all of ``Branch
              and Price'', ``Branch and Cut'' and ``Branch and Bound''
              frameworks. We present several schemes reflecting different
              optimization goals that the MNO can foster: virtualization cost,
              power minimization, network load or all. Simulation results
              demonstrate the good performance of our proposed algorithm to
              solve the gNBs-VM pool mapping for all evaluated schemes, while
              also emphasizing the advantages of a particular one (EWoS-333 for
              Equal Weight optimization Scheme) that can decrease
              virtualization cost by almost one order of magnitude compared to
              a static selection scheme, while considering the other two
              objectives.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1542--1553",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/S/Salhab et al. 2020 - Optimization of Virtualization Cost, Processing Power and Network Load of 5G Software-Defined Data Centers.pdf",
  keywords = "5G mobile communication;cloud computing;computer centres;integer
              programming;linear programming;resource allocation;software
              defined networking;virtual machines;virtualisation;virtualization
              cost;processing power;network load;Mobile Network
              Operators;inherent elasticity;operational excellence;5GC;tolerant
              Network Functions;NFs;central processing load limits;different
              optimization goals;power minimization;gNBs-VM pool mapping;Equal
              Weight optimization Scheme;virtual machine
              pool;NF;software-defined 5G Core;mobile network
              operators;generation Node-B;Cloud computing;Optimization;5G
              mobile communication;Data
              centers;Minimization;Virtualization;Heuristic
              algorithms;Multi-objective optimization;branch;cut and price
              (BCP);5G;mobile network operator (MNO);virtualized network
              functions;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.2990664"
}

@ARTICLE{Shi2020-eo,
  title    = "{IntFlow}: Integrating {Per-Packet} and {Per-Flowlet} Switching
              Strategy for Load Balancing in Datacenter Networks",
  author   = "Shi, Q and Wang, F and Feng, D",
  abstract = "Datacenter network load balancing schemes handle network traffic
              generated by massive different applications. Some packet-based or
              flowlet-based schemes capture traffic bursts for load balancing.
              But frequent rerouting within a flow can mix ACKs belonging to
              different paths in congestion control protocols, which adversely
              affects flow rate control. Besides, performance optimization
              effect of flowlet-based schemes may be less noticeable under
              smoother workloads. And several packet-based mechanisms
              implemented at end hosts can proactively reroute congested flows
              based on flow status even under a smooth workload, but fail to
              improve performance with the bursty nature of traffic. Therefore,
              existing schemes cannot adapt to different burst levels of
              dynamic traffic in datacenter networks and still have significant
              performance flaws in some ways. This paper proposes IntFlow, a
              novel load balancing scheme that integrates end-host based
              per-packet monitoring of flow status with flowlet switching in
              programable switches. IntFlow proactively reroutes flows
              experiencing network congestion or failures and avoids doing
              flowlet switching for small flows with high sending rate. IntFlow
              can provide excellent performance under both high burst and
              smooth workloads. Finally experimental results show IntFlow
              achieves up to 32\% and 28\% better performance than CONGA and
              Hermes under asymmetries, respectively.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1377--1388",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/S/Shi et al. 2020 - IntFlow - Integrating Per-Packet and Per-Flowlet Switching Strategy for Load Balancing in Datacenter Networks.pdf",
  keywords = "computer centres;resource allocation;telecommunication congestion
              control;telecommunication traffic;transport protocols;smoother
              workloads;packet-based mechanisms;end hosts;congested flows;flow
              status;smooth workload;different burst levels;dynamic
              traffic;datacenter networks;significant performance
              flaws;IntFlow;end-host;per-packet monitoring;programable
              switches;network congestion;high sending rate;high
              burst;per-flowlet switching strategy;load balancing;datacenter
              network load;network traffic;massive different
              applications;schemes capture traffic bursts;frequent
              rerouting;congestion control protocols;flow rate
              control;performance optimization effect;flowlet-based
              schemes;Switches;Load
              management;Topology;Bandwidth;Throughput;Protocols;Monitoring;Datacenter
              networks;load balancing;programmable switches;Datacentre",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.2990868"
}

@ARTICLE{Cao2020-wm,
  title    = "Reliable and Efficient Multimedia Service Optimization for Edge
              {Computing-Based} {5G} Networks: Game Theoretic Approaches",
  author   = "Cao, T and Xu, C and Du, J and Li, Y and Xiao, H and Gong, C and
              Zhong, L and Niyato, D",
  abstract = "The edge computing-based 5G networks have the advantages in
              efficiently offloading the large-scale Internet traffic, which is
              considered to be a promising architecture to alleviate the
              conflict between transmission performance and quality of
              experience (QoE). However, due to the unreliability of service
              providers and the mutual interference between wireless channels
              in 5G networks, it is still difficult for existing solutions to
              provide satisfactory multimedia services for mobile users. In
              response to these crucial challenges, this paper proposes a
              reliable and efficient multimedia service optimization framework
              named ``REMSO'' hereby, including a two-stage joint optimization
              procedure. Specifically, a reliable video service mechanism is
              first constructed to help the mobile users distinguish the
              credible and economic service BSs. Afterwards, an efficient
              wireless resource allocation strategy is established to achieve
              low latency and energy efficient video service optimization. In
              particular, the Stackelberg and potential game models are
              leveraged to achieve these optimization objectives. Finally,
              extensive simulations corroborate that our REMSO framework can
              deliver prominent performance advantages in terms of the
              reliability and efficiency when comparing with the
              state-of-the-art solutions.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1610--1625",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/C/Cao et al. 2020 - Reliable and Efficient Multimedia Service Optimization for Edge Computing-Based 5G Networks - Game Theoretic Approaches.pdf",
  keywords = "cellular radio;game theory;Internet;multimedia
              communication;optimisation;quality of service;resource
              allocation;telecommunication traffic;wireless channels;edge
              computing-based 5G networks;game theoretic approaches;large-scale
              Internet traffic;service providers;satisfactory multimedia
              services;mobile users;reliable multimedia service optimization
              framework;efficient multimedia service optimization
              framework;two-stage joint optimization procedure;reliable video
              service mechanism;credible service BSs;economic service
              BSs;efficient wireless resource allocation strategy;energy
              efficient video service optimization;optimization
              objectives;reliability;5G mobile communication;Resource
              management;Optimization;Reliability;Games;Pricing;Wireless
              communication;Multimedia services;reliability;efficiency;5G
              networks;game theoretic;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.2993886"
}

@ARTICLE{Rahman2020-aj,
  title    = "{Auto-Scaling} Network Service Chains Using Machine Learning and
              Negotiation Game",
  author   = "Rahman, S and Ahmed, T and Huynh, M and Tornatore, M and
              Mukherjee, B",
  abstract = "Network Function Virtualization (NFV) enables Network Operators
              (NOs) to efficiently respond to the increasing dynamicity of
              network services. Virtual Network Functions (VNFs) running on
              commercial off-the-shelf servers are easy to deploy, update,
              monitor, and manage. Such virtualized services are often deployed
              as Service Chains (SCs), which require in-sequence placement of
              computing and memory resources as well as routing of traffic
              flows. Due to the ongoing migration towards cloudification of
              networks, the concept of auto-scaling which originated in Cloud
              Computing, is now receiving attention from networks professionals
              too. Prior studies on auto-scaling use measured load to
              dynamically react to traffic changes. Moreover, they often focus
              on only one of the resources (e.g., compute only, or network
              capacity only). In this study, we consider three different
              resource types: compute, memory, and network bandwidth. In prior
              studies, NO takes auto-scaling decisions, assuming tenants are
              always willing to auto-scale, and Quality of Service (QoS)
              requirements are homogeneous. Our study proposes a
              negotiation-game-based auto-scaling method where tenants and NO
              both engage in the auto-scaling decision, based on their
              willingness to participate, heterogeneous QoS requirements, and
              financial gain (e.g., cost savings). In addition, we propose a
              proactive Machine Learning (ML) based prediction method to
              perform SC auto-scaling in dynamic traffic scenario. Numerical
              examples show that our proposed SC auto-scaling methods powered
              by ML present a win-win situation for both NO and tenants (in
              terms of cost savings).",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1322--1336",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/R/Rahman et al. 2020 - Auto-Scaling Network Service Chains Using Machine Learning and Negotiation Game.pdf",
  keywords = "cloud computing;learning (artificial intelligence);quality of
              service;resource allocation;telecommunication
              traffic;virtualisation;network capacity;different resource
              types;network bandwidth;auto-scaling decision;auto-scale;Service
              requirements;negotiation-game-based auto-scaling
              method;heterogeneous QoS requirements;dynamic traffic scenario;SC
              auto-scaling methods;auto-scaling Network Service Chains;Network
              Function Virtualization;Network Operators;increasing
              dynamicity;network services;Virtual Network Functions;commercial
              off-the-shelf servers;virtualized services;memory
              resources;traffic flows;Cloud Computing;networks
              professionals;auto-scaling use;Cloud computing;Quality of
              service;Prediction methods;Bandwidth;Prediction
              algorithms;Heuristic algorithms;Economics;Auto-scaling;service
              chains;virtual network functions;machine learning;negotiation
              game;cost savings;QoS;resource disaggregation;edge
              datacenters;NFV;MLNetworking",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.2995900"
}

@ARTICLE{Gholipoor2020-ck,
  title    = "{E2E} {QoS} Guarantee for the Tactile Internet via Joint {NFV}
              and Radio Resource Allocation",
  author   = "Gholipoor, N and Saeedi, H and Mokari, N and Jorswieck, E A",
  abstract = "The Tactile Internet (TI) is one of the next generation wireless
              network services with end to end (E2E) delay as low as 1 ms.
              Since this ultra low E2E delay cannot be met in the current 4G
              network architecture, it is necessary to investigate this service
              in the next generation wireless network by considering new
              technologies such as networks function virtualization (NFV). On
              the other hand, given the importance of E2E delay in the TI
              service, it is crucial to consider the delay of all parts of the
              network, including the radio access part and the NFV core part.
              In this paper, for the first time, we investigate the joint radio
              resource allocation (R-RA) and NFV resource allocation (NFV-RA)
              in a heterogeneous network where queuing delays, transmission
              delays, and delays resulting from virtual network function (VNF)
              execution are jointly considered. For this setup, we formulate a
              new resource allocation (RA) problem to minimize the total cost
              function subject to guaranteeing E2E delay of each connection.
              Since the proposed optimization problem is highly non-convex, we
              exploit alternative search method (ASM), successive convex
              approximation (SCA), and heuristic algorithms to solve it.
              Besides, for the NFV-RA, we propose an online heuristic
              algorithm, and analyze its performance for the TI service.
              Simulation results reveal that the proposed scheme can
              significantly reduce the network costs compared to the case where
              the two problems are optimized separately. Moreover, we compare
              the online algorithm with its offline counterpart as well as a
              baseline approach and it is shown that the online algorithm
              outperforms both of them.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1788--1804",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/G/Gholipoor et al. 2020 - E2E QoS Guarantee for the Tactile Internet via Joint NFV and Radio Resource Allocation.pdf",
  keywords = "4G mobile communication;computer network management;convex
              programming;DiffServ networks;Internet;optimisation;quality of
              service;radio access networks;resource
              allocation;telecommunication network routing;telecommunication
              traffic;virtualisation;E2E QoS Guarantee;Tactile Internet;joint
              NFV;generation wireless network services;virtualization;TI
              service;radio access part;NFV core part;joint radio resource
              allocation;R-RA;heterogeneous network;queuing delays;transmission
              delays;virtual network function execution;resource allocation
              problem;total cost function subject;guaranteeing E2E;optimization
              problem;successive convex approximation;NFV-RA;online heuristic
              algorithm;online algorithm;time 1.0 ms;Delays;Resource
              management;Servers;Ultra reliable low latency
              communication;Quality of service;Wireless networks;Next
              generation networking;Tactile Internet (TI);network function
              virtualization (NFV);virtualized network function (VNF);queuing
              delay;transmission delay;network service (NS);end-to-end (E2E)
              delay;NFV;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3001359"
}

@ARTICLE{Redhu2020-gs,
  title    = "Cooperative Network Model for Joint Mobile Sink Scheduling and
              Dynamic Buffer Management Using {Q-Learning}",
  author   = "Redhu, S and Hegde, R M",
  abstract = "Development of energy-efficient wireless sensor networks is
              crucial in the deployment of IoT and IIoT for modern day
              applications like smart home, smart vehicles, and smart
              industries. Several methods like network clustering, mobile sink
              deployment and dynamic sensing rate have been used in improving
              the energy-efficiency of wireless sensor networks in IoT
              framework. However, these methods have been developed
              independently which can lead to certain network issues like
              reduced lifetime, network breakdown among others. In this work,
              an energy-efficient method that optimizes mobile sink scheduling
              while concurrently providing dynamic buffer management is
              proposed. A cooperative network model that incorporates node
              clustering and mobile sink deployment in variable node sensing
              rate scenario is first developed. However, in such cooperative
              network models, mobile sink scheduling and buffer overflow
              management which causes information loss become challenging. This
              is primarily due to limited buffer size, variable sensing rate of
              the nodes, and the unavailability of mobile sink at all times
              near a cluster. Therefore, a reinforcement Q-learning framework
              is developed for scheduling the mobile sink while minimizing the
              information loss caused by buffer overflow in each cluster of a
              clustered WSN. More specifically, the network behaviour is learnt
              in the context of buffer overflow using Q-learning approach. The
              proposed method computes the adaptive halt-times for the mobile
              sink based on information loss and buffer overflow in each
              cluster. Performance of the proposed joint mobile sink scheduling
              and dynamic buffer management method is evaluated on a medium
              scale WSN. A clustered wireless sensor network with a total of
              600 sensor nodes is considered for performance evaluation. The
              proposed method is shown to learn the variable node sensing rate
              in a reasonable amount of time using convergence analysis.
              Numeric evaluations indicate that the proposed method minimizes
              the information loss in a medium scale wireless sensor network
              while improving the network lifetime simultaneously. The proposed
              cooperative network model also outperforms in terms of
              energy-efficiency when compared to conventional WSN. The results
              are motivating enough for the use of cooperative network model in
              practical WSNs for IoT applications.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1853--1864",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/R/Redhu and Hegde 2020 - Cooperative Network Model for Joint Mobile Sink Scheduling and Dynamic Buffer Management Using Q-Learning.pdf",
  keywords = "buffer storage;cooperative communication;energy
              conservation;Internet of Things;learning (artificial
              intelligence);optimisation;pattern clustering;sensor
              placement;storage management;telecommunication
              computing;telecommunication network planning;telecommunication
              power management;telecommunication scheduling;wireless sensor
              networks;joint mobile sink scheduling;energy-efficient wireless
              sensor networks;network clustering;mobile sink deployment;dynamic
              sensing rate;clustered wireless sensor network;cooperative
              network model;dynamic buffer management;IoT;mobile sink
              scheduling;buffer overflow management;reinforcement
              Q-learning;WSN;Wireless sensor networks;Sensors;Cooperative
              systems;Protocols;Job shop scheduling;Dynamic
              scheduling;Clustering;mobile sink scheduling;variable sensing
              rate;buffer management;wireless sensor
              networks;Wireless;MLNetworking",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3002828"
}

@ARTICLE{Wu2020-li,
  title    = "On Virtual Network Embedding: Paths and Cycles",
  author   = "Wu, H and Zhou, F and Chen, Y and Zhang, R",
  abstract = "Network virtualization provides a promising solution to overcome
              the ossification of current networks, allowing multiple Virtual
              Network Requests (VNRs) to be embedded on a common
              infrastructure. The major challenge in network virtualization is
              the Virtual Network Embedding (VNE) problem, which is to embed
              VNRs onto a shared substrate network and known to be
              $\lambda$/P-hard. The topological heterogeneity of VNRs is one
              important factor hampering the performance of the VNE. However,
              in many specialized applications and infrastructures, VNRs are of
              some common structural features, e.g., paths and cycles. To
              achieve better outcomes, it is thus critical to design dedicated
              algorithms for these applications and infrastructures by taking
              into accounting topological characteristics. Besides, paths and
              cycles are two of the most fundamental topologies that all
              network structures consist of. Exploiting the characteristics of
              path and cycle embeddings is vital to tackle the general VNE
              problem. In this paper, we investigate the path and cycle
              embedding problems. For path embedding, we prove its
              $\lambda$/P-hardness and inapproximability. Then, by utilizing
              Multiple Knapsack Problem (MKP) and Multi-Dimensional Knapsack
              Problem (MDKP), we propose an efficient and effective
              MKP-MDKP-based algorithm. For cycle embedding, we propose a
              Weighted Directed Auxiliary Graph (WDAG) to develop a
              polynomial-time algorithm to determine the
              least-resource-consuming embedding. Numerical results show our
              customized algorithms can boost the acceptance ratio and revenue
              compared to generic embedding algorithms in the literature.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1487--1500",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/W/Wu et al. 2020 - On Virtual Network Embedding - Paths and Cycles.pdf",
  keywords = "computational complexity;computer networks;directed
              graphs;embedded systems;graph theory;knapsack problems;network
              theory (graphs);optimisation;telecommunication network
              topology;virtualisation;generic embedding
              algorithms;least-resource-consuming embedding;effective
              MKP-MDKP-based algorithm;efficient MKP-MDKP-based
              algorithm;MultiDimensional Knapsack Problem;Multiple Knapsack
              Problem;path embedding;cycle embedding;general VNE problem;cycle
              embeddings;network structures;accounting topological
              characteristics;specialized applications;topological
              heterogeneity;shared substrate network;Virtual Network Embedding
              problem;common infrastructure;VNRs;multiple Virtual Network
              Requests;network virtualization;Substrates;Topology;Network
              topology;Virtualization;Indium phosphide;III-V semiconductor
              materials;Bandwidth;Virtual network embedding (VNE);path and
              cycles embeddings;topology decomposition;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3002849"
}

@ARTICLE{Detti2020-ov,
  title    = "{Sub-Linear} Scalability of {MQTT} Clusters in {Topic-Based}
              {Publish-Subscribe} Applications",
  author   = "Detti, A and Funari, L and Blefari-Melazzi, N",
  abstract = "Message Queuing Telemetry Transport (MQTT) is a widespread
              protocol for topic-based publish-subscribe architectures
              supporting IoT and social networks applications. MQTT brokers are
              logical entities that couple publishers and subscribers and play
              a critical role in such architectures. MQTT brokers can be
              implemented either as standalone servers or in a cluster
              configuration. Clusters of brokers increase reliability,
              availability and overall performance, since operations can be
              highly parallelized among the brokers that form the cluster. The
              load-balancing strategy in a cluster usually consists in
              connecting an incoming client to a randomly selected broker. This
              random-attach strategy, it is very simple, but generates a
              significant amount of inter-broker traffic, as we demonstrate
              through theoretical and experimental evaluations. Inter-broker
              traffic is an overhead for the system and it increases the CPU
              load of the brokers, compromising the scaling behavior of the
              whole cluster. Indeed, we found that a linear increase of the
              number of brokers forming a cluster does not necessarily provide
              an equivalent linear gain in performance, and such a scaling
              penalty can be surprisingly significant, in the order of 40\%. To
              solve this issue and improve performance, we propose a novel
              load-balancing strategy that envisages the use of multiple MQTT
              sessions per client to reduce inter-broker traffic and that can
              be implemented by means of a greedy algorithm. We show
              feasibility and effectiveness of our strategy for IoT and
              social-network applications by means of simulations and real
              measurements. The resulting scaling penalty is reduced to 10\%.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  3,
  pages    = "1954--1968",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/D/Detti et al. 2020 - Sub-Linear Scalability of MQTT Clusters in Topic-Based Publish-Subscribe Applications.pdf",
  keywords = "greedy algorithms;Internet;Internet of Things;message
              passing;middleware;protocols;queueing theory;resource
              allocation;social networking (online);telemetry;transport
              protocols;linear scalability;MQTT clusters;topic-based
              publish-subscribe applications;Message Queuing Telemetry
              Transport;social networks applications;MQTT brokers;logical
              entities that couple publishers;cluster configuration;randomly
              selected broker;inter-broker traffic;novel load-balancing
              strategy;multiple MQTT sessions;social-network
              applications;Quality of
              service;Routing;Scalability;Publish-subscribe;Protocols;Social
              networking (online);Virtual machining;Topic-based
              publish-subscribe;MQTT;cluster;scalability;IoT;social
              networks;Distributed Systems",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3003535"
}

@ARTICLE{Lu2020-xc,
  title    = "A Dynamic and Collaborative {Multi-Layer} Virtual Network
              Embedding Algorithm in {SDN} Based on Reinforcement Learning",
  author   = "Lu, M and Gu, Y and Xie, D",
  abstract = "Most of existing virtual network embedding (VNE) algorithms only
              consider how to construct virtual networks more efficiently on a
              physical infrastructure, without considering the possibility that
              the constructed virtual networks may be further virtualized to
              multiple smaller ones. We define the former scenario as
              single-layer VNE and the later as multi-layer VNE. As the
              increasing popularity of deploying large datacenter networks and
              wide area networks with Software Defined Network (SDN)
              architectures, it becomes a new requirement and possibility to
              provide multi-layer encapsulated network services for large
              tenants who have hierarchical organizational structures or need
              fine-grained service isolation. However, existing VNE algorithm
              are not specifically designed for the above requirement and not
              flexible enough to deal with mapping virtual network requirements
              (VNRs) to a physical network and smaller VNRs to a mapped virtual
              network. In this paper, we aim to propose a unified and flexible
              multi-layer VNE algorithm combining with reinforcement learning
              to solve the embedding of multi-layer VNRs, which can better
              distinguish the differences between VNRs and physical networks.
              Simulation results show that our algorithm achieves good
              performance both in single-layer and multi-layer VNE scenarios.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2305--2317",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/L/Lu et al. 2020 - A Dynamic and Collaborative Multi-Layer Virtual Network Embedding Algorithm in SDN Based on Reinforcement Learning.pdf",
  keywords = "Heuristic algorithms;Substrates;Approximation algorithms;Learning
              (artificial
              intelligence);Switches;Bandwidth;Collaboration;Dynamic and
              collaborative embedding;multi-layer virtual network
              embedding;multi-dimensional attributes;reinforcement learning;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3012588"
}

@ARTICLE{Zuo2020-lm,
  title    = "Intelligent and Flexible Bandwidth Scheduling for Data Transfers
              in Dedicated {High-Performance} Networks",
  author   = "Zuo, L",
  abstract = "High-demanding transfers of extremely large amounts of data have
              been increasingly supported by the bandwidth reservation services
              in dedicated high-performance networks (HPNs). To use the
              bandwidth reservation service, a user needs to initialize a
              bandwidth reservation request (BRR) containing parameters and
              requirements of the bandwidth reservation or data transfer, such
              as the time interval. Upon receiving one BRR, existing researches
              and bandwidth reservation service providers return either the
              best-case bandwidth reservation (BR) or a reject message if not
              all the BRR requirements can be satisfied. In this paper, we
              study the intelligent and flexible bandwidth scheduling of two
              common types of BRRs: direct bandwidth reservation and indirect
              bandwidth reservation (data transfer). When not all the
              requirements of the first type can be satisfied, instead of a
              direct reject message, we identify and return two alternative BRs
              made within the closest time intervals before and after the
              user-specified time interval; for the second type, besides the
              above two alternative BRs, we also identify and return the
              alternative BR made within the closest time interval crossing the
              user-specified time interval. For each problem, we design a
              flexible bandwidth scheduling algorithm with rigorous optimality
              proofs to compute both the best-case and alternative BRs. For
              comparison, we also design two heuristic algorithms adapted from
              existing bandwidth scheduling algorithms. We then conduct
              extensive simulations to compare their overall performance, and
              the simulation results show that the proposed optimal algorithms
              have superior performance to those in comparison. To the best of
              our knowledge, our work in this paper is among the first to study
              intelligent and adaptive bandwidth scheduling with alternative
              BRs in HPNs.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2364--2379",
  month    =  dec,
  year     =  2020,
  annote   = "Probably mostly interesting as background",
  file     = "All Papers/Z/Zuo 2020 - Intelligent and Flexible Bandwidth Scheduling for Data Transfers in Dedicated High-Performance Networks.pdf",
  keywords = "Bandwidth;Data transfer;Scheduling;Scheduling
              algorithms;Heuristic algorithms;Computational modeling;Bandwidth
              reservation;dynamic provisioning;high-performance
              networks;quality of service;FutureInternet",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3012888"
}

@ARTICLE{Chen2020-ny,
  title    = "Deploying Virtual Network Functions With {Non-Uniform} Models in
              {Tree-Structured} Networks",
  author   = "Chen, Y and Wu, J and Ji, B",
  abstract = "Network Function Virtualization (NFV) has promoted the
              implementation of network functions from expensive hardwares to
              software middleboxes. These software middleboxes, also called
              Virtual Network Functions (VNFs), are executed on
              switch-connected servers. Efficiently deploying such VNFs on
              servers is challenging because the traffic rate of flows must be
              fully processed by their requested VNFs when they reach
              destinations, and the deployed positions of VNFs are restricted
              by the server capacity. In addition, each network function offers
              non-uniform VNF models (types) with different configurations of
              processing volumes and costs. This paper focuses on minimizing
              the total cost of deploying VNFs for providing a specific network
              function to all flows in tree-structured networks. First, we
              prove the NP-hardness of non-uniform VNF deployment in a tree
              topology and propose a dynamic programming based solution with a
              pseudo-polynomial time complexity. Then we narrow it down to
              three simplified cases by focusing on either uniform VNFs or the
              linear line topology. Specifically, three algorithms are
              introduced: an improved dynamic programming based algorithm for
              deploying uniform VNFs in a tree topology, a
              performance-guaranteed algorithm for deploying non-uniform VNFs
              in a linear line topology, and an optimal greedy algorithm for
              deploying uniform VNFs in a linear line topology. Additionally,
              we generalize our approach to a case of deploying a service
              chain, which consists of multiple network functions applied to
              flows in a specific order. We propose two solutions: one is
              optimal but time-consuming while another is heuristic but
              efficient. Extensive simulations are conducted to evaluate our
              algorithms.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2260--2274",
  month    =  dec,
  year     =  2020,
  keywords = "Topology;Network topology;Servers;Software;Dynamic
              programming;Time complexity;Heuristic
              algorithms;Deployment;NFV;tree-structured networks;VNFs;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3013252"
}

@ARTICLE{Gupta2020-tz,
  title    = "Online Sparse {BLSTM} Models for Resource Usage Prediction in
              Cloud Datacentres",
  author   = "Gupta, S and Dileep, A D and Gonsalves, T A",
  abstract = "Real time resource usage prediction is an important part of
              resource provisioning in a cloud data centre. As cloud workloads
              vary dynamically, effective resource provisioning requires
              prediction of future resource usage trends. The problem is highly
              complicated because of highly time varying nature of cloud
              resource workloads. Training the future resource usage prediction
              models once, using a fixed set of observations is not sufficient
              to capture the variability in cloud workloads. In this work, we
              propose to use gradient descent (GD) and Levenberg-Marquardt (LM)
              adaptation algorithms for dynamic adaptation of resource
              utilization prediction models. We also propose a novel sparse
              framework for fast online adaptation of resource usage prediction
              models. We propose to analyze different algorithms such as $\ell
              ^1$ regularization, $\ell ^2$ regularization, optimal brain
              damage (OBD), optimal brain surgeon (OBS) for introducing
              sparsity. The proposed sparse framework for online adaptation of
              multivariate resource usage prediction models is validated for
              CPU usage prediction in the Google cluster trace and PlanetLab
              workload trace. A comparative analysis of different sparse
              frameworks shows that OBD-based LM adaptation algorithm performs
              better than other frameworks for online multivariate resource
              usage prediction in a cloud.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2335--2349",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/G/Gupta et al. 2020 - Online Sparse BLSTM Models for Resource Usage Prediction in Cloud Datacentres.pdf",
  keywords = "Predictive models;Adaptation models;Autoregressive
              processes;Cloud computing;Time series analysis;Market
              research;Analytical models;Resource usage prediction;cloud
              computing;supervised control;sparse;long short term memory
              (LSTM);MLNetworking",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3013922"
}

@ARTICLE{Porcu2020-bn,
  title    = "Estimation of the Quality of Experience During Video Streaming
              From Facial Expression and Gaze Direction",
  author   = "Porcu, S and Floris, A and Voigt-Antons, J-N and Atzori, L and
              M{\"o}ller, S",
  abstract = "This article investigates the possibility to estimate the
              perceived Quality of Experience (QoE) automatically and
              unobtrusively by analyzing the face of the consumer of video
              streaming services, from which facial expression and gaze
              direction are extracted. If effective, this would be a valuable
              tool for the monitoring of personal QoE during video streaming
              services without asking the user to provide feedback, with great
              advantages for service management. Additionally, this would
              eliminate the bias of subjective tests and would avoid bothering
              the viewers with questions to collect opinions and feedback. The
              performed analysis relies on two different experiments: i) a
              crowdsourcing test, where the videos are subject to impairments
              caused by long initial delays and re-buffering events; ii) a
              laboratory test, where the videos are affected by blurring
              effects. The facial Action Units (AU) that represent the
              contractions of specific facial muscles together with the
              position of the eyes' pupils are extracted to identify the
              correlation between perceived quality and facial expressions. An
              SVM with a quadratic kernel and a k-NN classifier have been
              tested to predict the QoE from these features. These have also
              been combined with measured application-level parameters to
              improve the quality prediction. From the performed experiments,
              it results that the best performance is obtained with the k-NN
              classifier by combining all the described features and after
              training it with both the datasets, with a prediction accuracy as
              high as 93.9\% outperforming the state of the art achievements.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2702--2716",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/P/Porcu et al. 2020 - Estimation of the Quality of Experience During Video Streaming From Facial Expression and Gaze Direction.pdf",
  keywords = "Streaming media;Quality of experience;Feature
              extraction;Electroencephalography;Two dimensional
              displays;Face;Brain modeling;Video streaming;quality of
              experience;facial expressions;gaze direction;machine
              learning;video key quality indicators;QoE
              estimation;GeneralInterest",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3018303"
}

@ARTICLE{Patounas2020-th,
  title    = "Characterization and Identification of Cloudified Mobile Network
              Performance Bottlenecks",
  author   = "Patounas, G and Foukas, X and Elmokashfi, A and Marina, M K",
  abstract = "This study is a first attempt to experimentally explore the range
              of performance bottlenecks that 5G mobile networks can
              experience. To this end, we leverage a wide range of measurements
              obtained with a prototype testbed that captures the key aspects
              of a cloudified mobile network. We investigate the relevance of
              the metrics and a number of approaches to accurately and
              efficiently identify bottlenecks across the different locations
              of the network and layers of the system architecture. Our
              findings validate the complexity of this task in the
              multi-layered architecture and highlight the need for novel
              monitoring approaches that intelligently fuse metrics across
              network layers and functions. In particular, we find that
              distributed analytics performs reasonably well both in terms of
              bottleneck identification accuracy and incurred computational and
              communication overhead.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2567--2583",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/P/Patounas et al. 2020 - Characterization and Identification of Cloudified Mobile Network Performance Bottlenecks.pdf",
  keywords = "5G mobile communication;Monitoring;Cloud computing;Task
              analysis;Complexity theory;Computer architecture;Prototypes;5G
              mobile communication;network function
              virtualization;monitoring;measurement techniques;performance
              loss;fault diagnosis;prototypes;machine learning;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3018538"
}

@ARTICLE{Wei2020-jv,
  title    = "Network Slice Reconfiguration by Exploiting Deep Reinforcement
              Learning With Large Action Space",
  author   = "Wei, F and Feng, G and Sun, Y and Wang, Y and Qin, S and Liang,
              Y-C",
  abstract = "It is widely acknowledged that network slicing can tackle the
              diverse usage scenarios and connectivity services that the
              5G-and-beyond system needs to support. To guarantee performance
              isolation while maximizing network resource utilization under
              dynamic traffic load, network slice needs to be reconfigured
              adaptively. However, it is commonly believed that the
              fine-grained resource reconfiguration problem is intractable due
              to the extremely high computational complexity caused by numerous
              variables. In this article, we investigate the reconfiguration
              within a core network slice with aim of minimizing long-term
              resource consumption by exploiting Deep Reinforcement Learning
              (DRL). This problem is also intractable by using conventional
              Deep Q Network (DQN), as it has a multi-dimensional discrete
              action space which is difficult to explore efficiently. To
              address the curse of dimensionality, we propose to exploit
              Branching Dueling Q-network which incorporates the action
              branching architecture into DQN to drastically decrease the
              number of estimated actions. Based on the discrete BDQ network,
              we develop an intelligent network slice reconfiguration algorithm
              (INSRA). Extensive simulation experiments are conducted to
              evaluate the performance of INSRA and the numerical results
              reveal that INSRA can minimize the long-term resource consumption
              and achieve high resource efficiency compared with several
              benchmark algorithms.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2197--2211",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/W/Wei et al. 2020 - Network Slice Reconfiguration by Exploiting Deep Reinforcement Learning With Large Action Space.pdf",
  keywords = "Network slicing;Uncertainty;Resource
              management;Optimization;Machine learning;Substrates;Numerical
              models;Network slice reconfiguration;deep reinforcement
              learning;core network slicing;branch dueling
              q-network;5G6G;MLNetworking",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3019248"
}

@ARTICLE{Elgendy2020-mb,
  title    = "Efficient and Secure {Multi-User} {Multi-Task} Computation
              Offloading for {Mobile-Edge} Computing in Mobile {IoT} Networks",
  author   = "Elgendy, I A and Zhang, W-Z and Zeng, Y and He, H and Tian, Y-C
              and Yang, Y",
  abstract = "Mobile edge computing (MEC) is a new paradigm to alleviate
              resource limitations of mobile IoT networks through computation
              offloading with low latency. This article presents an efficient
              and secure multi-user multi-task computation offloading model
              with guaranteed performance in latency, energy, and security for
              mobile-edge computing. It does not only investigate offloading
              strategy but also considers resource allocation, compression and
              security issues. Firstly, to guarantee efficient utilization of
              the shared resource in multi-user scenarios, radio and
              computation resources are jointly addressed. In addition, JPEG
              and MPEG4 compression algorithms are used to reduce the transfer
              overhead. To fulfill security requirements, a security layer is
              introduced to protect the transmitted data from cyber-attacks.
              Furthermore, an integrated model of resource allocation,
              compression, and security is formulated as an integer nonlinear
              problem with the objective of minimizing the weighted sum of
              energy under a latency constraint. As this problem is considered
              as NP-hard, linearization and relaxation approaches are applied
              to transform the problem into a convex one. Finally, an efficient
              offloading algorithm is designed with detailed processes to make
              the computation offloading decision for computation tasks of
              mobile users. Simulation results show that our model not only
              saves about 46\% of system overhead consumption in comparison
              with local execution but also scale well for large-scale IoT
              networks.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2410--2422",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/E/Elgendy et al. 2020 - Efficient and Secure Multi-User Multi-Task Computation Offloading for Mobile-Edge Computing in Mobile IoT Networks.pdf",
  keywords = "Computational modeling;Cloud computing;Security;Task
              analysis;Energy consumption;Mobile handsets;Resource
              management;Computation offloading;compression;Internet of Things
              (IoT);mobile-edge computing;optimization;security;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3020249"
}

@ARTICLE{Arzo2020-lc,
  title    = "Study of Virtual Network Function Placement in {5G} Cloud Radio
              Access Network",
  author   = "Arzo, S T and Bassoli, R and Granelli, F and Fitzek, F H P",
  abstract = "5G and beyond need to meet stringent requirements of latency,
              reliability, and support for heterogeneous devices. However, the
              existing wireless network architecture is limited to fulfill
              these constraints. Cloud radio access network, along with network
              function virtualization, is suggested to provide flexibility and
              network agility. It decouples network functions, such as firewall
              and packet gateway, from hardware to software deployed in the
              cloud. Thus comprehensive end-to-end formulation of this
              architecture is required for virtual network function placement.
              Most of existing works focus on virtual functions placement with
              different objectives, addressing different service requirements
              separately. In this article, six 5G constraints are considered
              simultaneously to find optimal virtual network function placement
              with service differentiation. The selected six parameters reflect
              services' requirements, network constraints and computing
              constraints. We first model the overall cloud radio access
              network as a multi-layer loopless-random hypergraph and we
              provide the overall formulation of the system. Then, we
              reformulate such model considering backup virtual functions and
              CPU over-provisioning techniques to improve both virtual
              function's reliability and processing latency. Finally, we
              propose service differentiation to reduce CPU utilization and
              energy consumption, while using the above techniques. The results
              suggest that the application of service differentiation can
              significantly improve assignment of computing resources and
              energy efficiency.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2242--2259",
  month    =  dec,
  year     =  2020,
  keywords = "Data centers;5G mobile communication;Reliability;Computer
              architecture;Energy consumption;Nonhomogeneous
              media;Delays;Network function virtualization;virtual baseband
              unit;C-RAN;5G;5G6G;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3020390"
}

@ARTICLE{Faticanti2020-ek,
  title    = "{Throughput-Aware} Partitioning and Placement of Applications in
              Fog Computing",
  author   = "Faticanti, F and De Pellegrini, F and Siracusa, D and Santoro, D
              and Cretti, S",
  abstract = "Fog computing promises to extend cloud computing to match
              emerging demands for low latency, location-awareness and dynamic
              computation. It thus brings data processing close to the edge of
              the network by leveraging on devices with different computational
              characteristics. However, the heterogeneity, the geographical
              distribution, and the data-intensive profiles of IoT deployments
              render the placement of fog applications a fundamental problem to
              guarantee target performance figures. This is a core challenge
              for fog computing providers to offer fog infrastructure as a
              service, while satisfying the requirements of this new class of
              microservices-based applications. In this article we root our
              analysis on the throughput requirements of the applications while
              exploiting offloading towards different regions. The resulting
              resource allocation problem is developed for a fog-native
              application architecture based on containerised microservice
              modules. An algorithmic solution is designed to optimise the
              placement of applications modules either in cloud or in fog.
              Finally, the overall solution consists of two cascaded
              algorithms. The first one performs a throughput-oriented
              partitioning of fog application modules. The second one rules the
              orchestration of applications over a region-based infrastructure.
              Extensive numerical experiments validate the performance of the
              overall scheme and confirm that it outperforms state-of-the-art
              solutions adapted to our context.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2436--2450",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/F/Faticanti et al. 2020 - Throughput-Aware Partitioning and Placement of Applications in Fog Computing.pdf",
  keywords = "Cloud computing;Edge computing;Servers;Throughput;Resource
              management;Computer architecture;Data models;Fog
              computing;IoT;applications partitioning;resource
              allocation;microservices;EdgeFogCloudIoT",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3023011"
}

@ARTICLE{Hardegen2020-xf,
  title    = "Predicting Network Flow Characteristics Using Deep Learning and
              {Real-World} Network Traffic",
  author   = "Hardegen, C and Pf{\"u}lb, B and Rieger, S and Gepperth, A",
  abstract = "We present a processing pipeline for flow-based traffic
              classification using a machine learning component leveraging Deep
              Neural Networks (DNNs). The system is trained to predict likely
              characteristics of real-world traffic flows from a campus network
              ahead of time, e.g., a flow's throughput or duration. Training
              and evaluation of DNN models are continuously performed on a flow
              data stream collected from a university data center. Instead of
              the common binary classification into ``mice'' and ``elephant''
              (throughput) or ``short-term'' and ``long-term'' (duration)
              flows, predicted flow characteristics are quantized into three
              classes. Various communication contexts (subset of network
              traffic, e.g., only TCP) and flow feature groups (subset of flow
              features, e.g., only a flow's 5-tuple), which are supported
              through an enrichment strategy, are considered and investigated.
              An in-depth description of the data acquisition process,
              including preprocessing steps and anonymization used to protect
              sensitive information, is given. Additionally, we employ an
              accelerated variant of t-distributed Stochastic Neighbor
              Embedding (t-SNE) to visualize network traffic data. This enables
              the understanding of traffic characteristics and relations
              between communication flows at a glance. Furthermore, possible
              use-cases and a high-level architecture for flow-based routing
              scenarios utilizing the developed pipeline are proposed.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2662--2676",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/H/Hardegen et al. 2020 - Predicting Network Flow Characteristics Using Deep Learning and Real-World Network Traffic.pdf",
  keywords = "Routing;Machine learning;Throughput;Pipelines;Data centers;Data
              models;Mice;Traffic engineering;network management;flow
              prediction;machine learning;deep
              learning;NetFlow;5G6G;MLNetworking",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3025131"
}

@ARTICLE{Marvi2020-kg,
  title    = "Toward an Automated Data Offloading Framework for {Multi-RAT}
              {5G} Wireless Networks",
  author   = "Marvi, M and Aijaz, A and Khurram, M",
  abstract = "Offloading among multiple radio access technologies (RATs) is an
              effective solution to tackle some of the key challenges faced by
              5G wireless networks. While making an offloading decision, in
              user-centric offloading schemes, the network context is not taken
              into account, which limits the full utilization of the potential
              offered by these techniques. Therefore, in this work, we develop
              a data offloading framework wherein a user defines a tuple, which
              includes a request for data service and the associated delay
              limit, and the user equipment (UE) translates this tuple into an
              optimal offloading policy by taking into account the network
              context. The data offloading problem is formulated by assuming a
              finite horizon Markov decision process (MDP), and an analytical
              data offloading model is derived by using stochastic geometry for
              modeling the spatial domain of a multi-RAT wireless network, and
              by employing Markov process for capturing mobility of users. In
              order to validate the performance of derived model, we also solve
              the offloading problem by using policy iteration (PI) algorithm.
              The results show that the analytical algorithm outperforms the
              iterative algorithm. We also benchmark the proposed framework
              against standard data offloading methods.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2584--2597",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/M/Marvi et al. 2020 - Toward an Automated Data Offloading Framework for Multi-RAT 5G Wireless Networks.pdf",
  keywords = "Wireless fidelity;Rats;5G mobile communication;Wireless
              networks;Analytical models;Data
              models;Delays;Cellular;Wi-Fi;delay;data offloading;stochastic
              geometry;Markov process;coverage zones;multi-RAT;5G;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3026948"
}

@ARTICLE{Kumari2020-nh,
  title    = "An Incentive {Mechanism-Based} Stackelberg Game for Scheduling of
              {LoRa} Spreading Factors",
  author   = "Kumari, P and Gupta, H P and Dutta, T",
  abstract = "Wireless Local Area Networks (WLANs) are one of the most popular
              networks for the Internet-of-Things (IoT) applications. Among
              various WLAN technologies, the Long-Range WAN (LoRaWAN) has
              gained a high demand in recent years because of its low power
              consumption and long-range communication. However, the Long-Range
              (LoRa) network suffers from interference problem among LoRa
              Devices (LDs) that are connected to the LoRa gateway by using the
              same Spreading Factors (SFs). In this article, we propose a game
              theory-based approach for estimating the time duration of
              transmission of data on suitable SFs such that interference
              problem is reduced and network devices maximize their utilities.
              We next propose a scheduling algorithm that schedules the
              allocated time duration on the SFs such that the waiting time of
              the network can be minimized. We finally use the network
              simulator-3 for validating the propose work. Various experiments
              are performed which demonstrate the improvement in the network
              performance.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2598--2609",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/K/Kumari et al. 2020 - An Incentive Mechanism-Based Stackelberg Game for Scheduling of LoRa Spreading Factors.pdf",
  keywords = "Interference;Games;Protocols;Resource management;Scheduling;Logic
              gates;Performance evaluation;Long-range;Nash
              equilibrium;scheduler",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3027730"
}

@ARTICLE{Nguyen2020-cm,
  title    = "{Deadline-Aware} {SFC} Orchestration Under Demand Uncertainty",
  author   = "Nguyen, M and Dolati, M and Ghaderi, M",
  abstract = "In network function virtualization, a service function chain
              (SFC) specifies a sequence of virtual network functions that user
              traffic has to traverse to realize a network service. The problem
              of SFC orchestration has been extensively studied in the
              literature. However, most existing works assume deterministic
              demands and resort to costly runtime resource reprovisioning to
              deal with dynamic demands. In this work, we formulate the
              deadline-aware co-located and geo-distributed SFC orchestration
              with demand uncertainty as robust optimization problems and
              develop exact and approximate algorithms to solve them. A key
              feature of our formulation is the consideration of end-to-end
              delay in service chains by carefully modeling load-independent
              propagation delay as well as load-dependent queueing and
              processing delays. To avoid frequent resource reprovisioning, our
              algorithms utilize uncertain demand knowledge to compute
              proactive SFC orchestrations that can withstand fluctuations in
              dynamic service demands. Extensive simulations are conducted to
              evaluate the performance of our algorithms in terms of ability to
              cope with demand fluctuations, scalability, and relative
              performance against other recent algorithms.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2275--2290",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/N/Nguyen et al. 2020 - Deadline-Aware SFC Orchestration Under Demand Uncertainty.pdf",
  keywords = "Delays;Servers;Substrates;Approximation
              algorithms;Uncertainty;Heuristic algorithms;Optimization;Service
              function chain;service orchestration;demand
              uncertainty;end-to-end delay;ToRead;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3029749"
}

@ARTICLE{Sun2020-qq,
  title    = "Efficient Handover Mechanism for Radio Access Network Slicing by
              Exploiting Distributed Learning",
  author   = "Sun, Y and Jiang, W and Feng, G and Klaine, P V and Zhang, L and
              Imran, M A and Liang, Y-C",
  abstract = "Network slicing is identified as a fundamental architectural
              technology for future mobile networks since it can logically
              separate networks into multiple slices and provide tailored
              quality of service (QoS). However, the introduction of network
              slicing into radio access networks (RAN) can greatly increase
              user handover complexity in cellular networks. Specifically, both
              physical resource constraints on base stations (BSs) and logical
              connection constraints on network slices (NSs) should be
              considered when making a handover decision. Moreover, various
              service types call for an intelligent handover scheme to
              guarantee the diversified QoS requirements. As such, in this
              article, a multi-agent reinforcement LEarning based Smart
              handover Scheme, named LESS, is proposed, with the purpose of
              minimizing handover cost while maintaining user QoS. Due to the
              large action space introduced by multiple users and the data
              sparsity caused by user mobility, conventional reinforcement
              learning algorithms cannot be applied directly. To solve these
              difficulties, LESS exploits the unique characteristics of slicing
              in designing two algorithms: 1) LESS-DL, a distributed $Q$
              -learning algorithm to make handover decisions with reduced
              action space but without compromising handover performance; 2)
              LESS-QVU, a modified $Q$ -value update algorithm which exploits
              slice traffic similarity to improve the accuracy of $Q$ -value
              evaluation with limited data. Thus, LESS uses LESS-DL to choose
              the target BS and NS when a handover occurs, while $Q$ -values
              are updated by using LESS-QVU. The convergence of LESS is
              theoretically proved in this article. Simulation results show
              that LESS can significantly improve network performance. In more
              detail, the number of handovers, handover cost and outage
              probability are reduced by around 50\%, 65\%, and 45\%,
              respectively, when compared with traditional methods.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2620--2633",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/S/Sun et al. 2020 - Efficient Handover Mechanism for Radio Access Network Slicing by Exploiting Distributed Learning.pdf",
  keywords = "Handover;Quality of service;Network slicing;Radio access
              networks;Cellular networks;Handover;RAN slicing;multi-agent
              reinforcement learning;quality of service;MLNetworking;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3031079"
}

@ARTICLE{Gijon2020-xr,
  title    = "Estimating Pole Capacity From Radio Network Performance
              Statistics by Supervised Learning",
  author   = "Gij{\'o}n, C and Toril, M and Luna-Ram{\'\i}rez, S and
              Bejarano-Luque, J L and Mar{\'\i}-Altozano, M L",
  abstract = "Network dimensioning is a critical task for cellular operators to
              avoid degraded user experience and unnecessary upgrades of
              network resources with changing mobile traffic patterns. For this
              purpose, smart network planning tools require accurate cell and
              user capacity estimates. In these tools, throughput is often used
              as a capacity metric due to its close relationship with user
              satisfaction. In this work, a comprehensive analysis is carried
              out to compare different well-known Supervised Learning (SL)
              algorithms for estimating cell and user throughput in the
              DownLink in busy hours from radio measurements collected on a
              cell basis in the Operation Support System (OSS). The considered
              SL approaches include random forest, shallow multi-layer
              perceptron, support vector regression and k-nearest neighbors.
              Such algorithms are compared with classical multiple linear
              regression and deep learning approaches considered in previous
              works. All these algorithms are tested in two radio access
              technologies: High Speed DownLink Packet Access (HSDPA) and Long
              Term Evolution (LTE). To this end, two datasets with the most
              relevant performance indicators per technology are collected from
              live cellular networks. Results show that non-deep SL algorithms
              are the most appropriate option for applications with storage
              constraints, such as network planning tools, since they provide a
              higher accuracy with reduced datasets.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2090--2101",
  month    =  dec,
  year     =  2020,
  keywords = "Throughput;Multiaccess communication;Spread spectrum
              communication;Estimation;Long Term Evolution;Measurement;Linear
              regression;Cellular network;supervised learning;pole
              capacity;user throughput;network
              measurements;MLNetworking;5G6G;Wireless",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3031333"
}

@ARTICLE{Pastor-Galindo2020-vf,
  title    = "Spotting Political Social Bots in Twitter: A Use Case of the 2019
              Spanish General Election",
  author   = "Pastor-Galindo, J and Zago, M and Nespoli, P and Bernal, S L and
              Celdr{\'a}n, A H and P{\'e}rez, M G and Ruip{\'e}rez-Valiente, J
              A and P{\'e}rez, G M and M{\'a}rmol, F G",
  abstract = "While social media has been proved as an exceptionally useful
              tool to interact with other people and massively and quickly
              spread helpful information, its great potential has been
              ill-intentionally leveraged as well to distort political
              elections and manipulate constituents. In this article at hand,
              we analyzed the presence and behavior of social bots on Twitter
              in the context of the November 2019 Spanish general election.
              Throughout our study, we classified involved users as social bots
              or humans, and examined their interactions from a quantitative
              (i.e., amount of traffic generated and existing relations) and
              qualitative (i.e., user's political affinity and sentiment
              towards the most important parties) perspectives. Results
              demonstrated that a non-negligible amount of those bots actively
              participated in the election, supporting each of the five
              principal political parties.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2156--2170",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/P/Pastor-Galindo et al. 2020 - Spotting Political Social Bots in Twitter - A Use Case of the 2019 Spanish General Election.pdf",
  keywords = "Twitter;Voting;Tools;Market research;Data mining;Media;Data
              mining and (big) data analysis;election manipulation;fake
              news;machine learning;information technology services;information
              visualization;political social bots;GeneralInterest",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3031573"
}

@ARTICLE{Aoki2020-go,
  title    = "{Few-Shot} Learning and {Self-Training} for {eNodeB} Log Analysis
              for {Service-Level} Assurance in {LTE} Networks",
  author   = "Aoki, S and Shiomoto, K and Eng, C L",
  abstract = "With the increasing network topology complexity and continuous
              evolution of the new wireless technology, it is challenging to
              address the network service outage with traditional methods. In
              the long-term evolution (LTE) networks, a large number of base
              stations called eNodeBs are deployed to cover the entire service
              areas spanning various kinds of geographical regions. Each eNodeB
              generates a large number of key performance indicators (KPIs).
              Hundreds of thousands of eNodeBs are typically deployed to cover
              a nation-wide service area. Operators need to handle hundreds of
              millions of KPIs to cover the areas. It is impractical to handle
              manually such a huge amount of KPI data, and automation of data
              processing is therefore desired. To improve network operation
              efficiency, a suitable machine learning technique is used to
              learn and classify individual eNodeBs into different states based
              on multiple performance metrics during a specific time window.
              However, an issue with supervised learning requires a large
              amount of labeled dataset, which takes costly human-labor and
              time to annotate data. To mitigate the cost and time issues, we
              propose a method based on few-shot learning that uses
              Prototypical Networks algorithm to complement the eNodeB states
              analysis. Using a dataset from a live LTE network that consists
              of thousand of eNodeB, our experiment results show that the
              proposed technique provides high performance while using a low
              number of labeled data.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2077--2089",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/A/Aoki et al. 2020 - Few-Shot Learning and Self-Training for eNodeB Log Analysis for Service-Level Assurance in LTE Networks.pdf",
  keywords = "Machine learning;Task analysis;Training;Key performance
              indicator;Long Term Evolution;Training
              data;LTE;eNodeB;KPI;service assurance;few-shot
              learning;prototypical network;self
              training;Useful\_Data;Mobile\_Wireless",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3032156"
}

@ARTICLE{Abdullah2020-lt,
  title    = "Diminishing Returns and Deep Learning for Adaptive {CPU} Resource
              Allocation of Containers",
  author   = "Abdullah, M and Iqbal, W and Bukhari, F and Erradi, A",
  abstract = "Containers provide a lightweight runtime environment for
              microservices applications while enabling better server
              utilization. Automatic optimal allocation of CPU pins to the
              containers serving specific workloads can help to minimize the
              completion time of jobs. Most of the existing state-of-the-art
              focused on building new efficient scheduling algorithms for
              placing the containers on the infrastructure, and the resources
              to the containers are allocated manually and statically. An
              automatic method to identify and allocate optimal CPU resources
              to the containers can help to improve the efficiency of the
              scheduling algorithms. In this article, we introduce a new deep
              learning-based approach to allocate optimal CPU resources to the
              containers automatically. Our approach uses the law of
              diminishing marginal returns to determine the optimal number of
              CPU pins for containers to gain maximum performance while
              maximizing the number of concurrent jobs. The proposed method is
              evaluated using real workloads on a Docker-based containerized
              infrastructure. The results demonstrate the effectiveness of the
              proposed solution in reducing the completion time of the jobs by
              23\% to 74\% compared to commonly used static CPU allocation
              methods.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2052--2063",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/A/Abdullah et al. 2020 - Diminishing Returns and Deep Learning for Adaptive CPU Resource Allocation of Containers.pdf",
  keywords = "Pins;Resource management;Containers;Data centers;Scheduling
              algorithms;Cloud computing;Performance gain;Containers;CPU
              pin;job completion time;CPU allocation;diminishing
              marginal;Distributed Systems",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3033025"
}

@ARTICLE{Saidi2020-rm,
  title    = "Exploring {Network-Wide} Flow Data With Flowyager",
  author   = "Saidi, S J and Maghsoudlou, A and Foucard, D and Smaragdakis, G
              and Poese, I and Feldmann, A",
  abstract = "Many network operations, ranging from attack investigation and
              mitigation to traffic management, require answering network-wide
              flow queries in seconds. Although flow records are collected at
              each router, using available traffic capture utilities, querying
              the resulting datasets from hundreds of routers across sites and
              over time, remains a significant challenge due to the sheer
              traffic volume and distributed nature of flow records. In this
              article, we investigate how to improve the response time for a
              priori unknown network-wide queries. We present Flowyager, a
              system that is built on top of existing traffic capture
              utilities. Flowyager generates and analyzes tree data structures,
              that we call Flowtrees, which are succinct summaries of the raw
              flow data available by capture utilities. Flowtrees are
              self-adjusted data structures that drastically reduce space and
              transfer requirements, by 75\% to 95\%, compared to raw flow
              records. Flowyager manages the storage and transfers of
              Flowtrees, supports Flowtree operators, and provides a structured
              query language for answering flow queries across sites and time
              periods. By deploying a Flowyager prototype at both a large
              Internet Exchange Point and a Tier-1 Internet Service Provider,
              we showcase its capabilities for networks with hundreds of router
              interfaces. Our results show that the query response time can be
              reduced by an order of magnitude when compared with alternative
              data analytics platforms. Thus, Flowyager enables interactive
              network-wide queries and offers unprecedented drill-down
              capabilities to, e.g., identify DDoS culprits, pinpoint the
              involved sites, and determine the length of the attack.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "1988--2006",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/S/Saidi et al. 2020 - Exploring Network-Wide Flow Data With Flowyager.pdf",
  keywords = "Task analysis;Monitoring;Time factors;Distributed databases;Data
              structures;Tools;Switches;Network data summarization;network
              monitoring;network-wide traffic analytics;FutureInternet",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3034278"
}

@ARTICLE{Zhao2020-mm,
  title    = "Mobility Management With Transferable Reinforcement Learning
              Trajectory Prediction",
  author   = "Zhao, Z and Karimzadeh, M and Pacheco, L and Santos, H and
              Ros{\'a}rio, D and Braun, T and Cerqueira, E",
  abstract = "Future mobile networks will enable the massive deployment of
              mobile multimedia applications anytime and anywhere. In this
              context, mobility management schemes, such as handover and
              proactive multimedia service migration, will be essential to
              improve network performance. In this article, we propose a
              proactive mobility management approach based on group user
              trajectory prediction. Specifically, we introduce a mobile user
              trajectory prediction algorithm by combining the Long-Short Term
              Memory networks (LSTM) with Reinforcement Learning (RL) to
              automate the model training procedure. We further develop a group
              user trajectory predictor to reduce prediction calculation
              overheads of users with similar movement patterns. To validate
              the impact of the proposed mobility management approach, we
              present a virtual reality (VR) service migration scheme built on
              the top of the proactive handover mechanism that benefits from
              trajectory predictions. Experiment results validate our
              predictor's outstanding accuracy and its impacts on enhancing
              handover and service migration performance to provide quality of
              service assurance.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2102--2116",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/Z/Zhao et al. 2020 - Mobility Management With Transferable Reinforcement Learning Trajectory Prediction.pdf",
  keywords = "Trajectory;Mobile handsets;Streaming media;Handover;Computer
              architecture;Prediction algorithms;Mobility management;trajectory
              prediction;reinforcement learning;transfer learning;service
              migration;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3034482"
}

@ARTICLE{Mohammadjafari2020-hg,
  title    = "Machine {Learning-Based} Radio Coverage Prediction in Urban
              Environments",
  author   = "Mohammadjafari, S and Roginsky, S and Kavurmacioglu, E and Cevik,
              M and Ethier, J and Bener, A B",
  abstract = "AIM: Having a reliable prediction model of radio signal strength
              is an essential tool for planning and designing a radio network.
              Given a geographic region, and associated power estimates linked
              to the transmitter placements, our objective is to develop
              machine learning models to predict the strength of the radio
              signals. BACKGROUND: The propagation model is often used to
              determine the optimal location of radio transmitters in order to
              optimize the power coverage in a geographic area of interest.
              However, it is often a costly operation to obtain the exact power
              measurements over a region for a given set of transmitter
              locations. Therefore, fast prediction methods are needed to
              estimate the power values given limited data. METHODOLOGY: We
              consider a dataset consisting of simulated power at each point in
              an environment for a given set of transmitter locations. We
              experiment with various machine learning models, namely,
              generalized linear models (GLMs), neural networks (NNs), and
              k-nearest neighbor (KNN), to estimate the power values for a
              given transmitter placement. We investigate various feature
              engineering approaches to enhance the predictive performance of
              the machine learning models. RESULTS: We observe that employed
              feature engineering methods such as polynomial degrees and
              transmitter to cluster distances significantly improve the
              prediction accuracy. In particular, GLM model performance notably
              improves thanks to these extracted features, where mean absolute
              error (MAE) is reduced around 77\% from 11.37 dB to 2.55 dB. We
              note that KNN with $k =$ 2 and DNN models perform better than NN
              and GLM. KNN has the best performance with an average MAE of
              0.65dB and also substantially faster to train than NN/DNN models.
              In addition, our analysis shows that, to train a well-performing
              machine learning model, it is sufficient to use a dataset
              consisting of measurements at a fraction of the potential
              transmitter locations in a given region. CONCLUSIONS: Machine
              learning methods are highly effective for the coverage prediction
              task. Using carefully engineered features, simple models such as
              GLMs and KNNs can be as effective as more complex ones,
              especially for small datasets.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  17,
  number   =  4,
  pages    = "2117--2130",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/M/Mohammadjafari et al. 2020 - Machine Learning-Based Radio Coverage Prediction in Urban Environments.pdf",
  keywords = "Predictive models;Radio transmitters;Computational
              modeling;Machine learning;Artificial neural networks;Mathematical
              model;Machine learning;radio coverage;transmitter
              placement;k-nearest neighbor;generalized linear models;neural
              networks;Wireless;MLNetworking;Mobile\_Wireless",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3035442"
}

@ARTICLE{Zhang2021-zb,
  title    = "{NFV} Platforms: Taxonomy, Design Choices and Future Challenges",
  author   = "Zhang, Tianzhu and Qiu, Han and Linguaglossa, Leonardo and
              Cerroni, Walter and Giaccone, Paolo",
  abstract = "Due to the intrinsically inefficient service provisioning in
              traditional networks, Network Function Virtualization (NFV) keeps
              gaining attention from both industry and academia. By replacing
              the purpose-built, expensive, proprietary network equipment with
              software network functions consolidated on commodity hardware,
              NFV envisions a shift towards a more agile and open service
              provisioning paradigm. During the last few years, a large number
              of NFV platforms have been implemented to facilitate the
              development, deployment, and management of Virtual Network
              Functions (VNFs). Nonetheless, just like any complex system, such
              platforms commonly consist of abounding software and hardware
              components and usually incorporate disparate design choices based
              on distinct motivations or use cases. This broad collection of
              convoluted alternatives makes it extremely arduous for network
              operators to make proper choices. Although numerous efforts have
              been devoted to investigating different aspects of NFV, none of
              them specifically focused on NFV platforms or attempted to
              explore their design space. In this article, we present a
              comprehensive survey on the NFV platform design. Our study solely
              targets existing NFV platform implementations. We begin with a
              top-down architectural view of the standard reference NFV
              platform and present our taxonomy of existing NFV platforms based
              on what features they provide in terms of a typical network
              function life cycle. Then we thoroughly explore the design space
              and elaborate on the implementation choices each platform opts
              for. We also envision future challenges for NFV platform design
              in the incoming 5G era. We believe that our study gives a
              detailed guideline for network operators or service providers to
              choose the most appropriate NFV platform based on their
              respective requirements. Our work also provides guidelines for
              implementing new NFV platforms.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  18,
  number   =  1,
  pages    = "30--48",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/Z/Zhang et al. 2021 - NFV Platforms - Taxonomy, Design Choices and Future Challenges.pdf",
  keywords = "Hardware;Software;Middleboxes;Space
              exploration;Monitoring;Switches;Virtualization;Network function
              virtualization;service function chaining;service management and
              orchestration;NFV infrastructure;VNF life
              cycle;ToRead;Good;Important;NFV\_SDN",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3045381"
}

@ARTICLE{Diez2021-ho,
  title    = "Understanding the Performance of Flexible Functional Split in
              {5G} {vRAN} Controllers: A Markov {Chain-Based} Model",
  author   = "Diez, Luis and Hervella, Cristina and Ag{\"u}ero, Ram{\'o}n",
  abstract = "We study Flexible Functional Split functionality of 5G vRAN
              controllers in 5G networks. We propose an innovative model, based
              on a Markov Chain, which can be used to characterize their
              performance. We consider both infinite and finite-buffer
              controllers. In the former, frames would not be lost (provided
              the system works in a stable regime), and we thus focus on the
              time frames stay at the controller. For the finite-buffer
              controller, there might be losses, and we analyze the trade-off
              between time at the controller (which might hinder the stringent
              delay requirements of 5G services), and loss probability.
              Matrix-geometric techniques are used to resolve the corresponding
              Quasi-Birth-Death process. The validity of the proposed model is
              assessed by means of an extensive experiment campaign carried out
              over an ad-hoc event-driven simulator, which is also used to
              broaden the analysis, considering different service rate
              distributions, as well as the variability of the studied
              performance indicators. The results show that the proposed model
              can be effectively exploited to tackle the dimensioning of these
              systems, as it sheds light on how their configuration impacts the
              expected delay and loss rate.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  18,
  number   =  1,
  pages    = "456--468",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/D/Diez et al. 2021 - Understanding the Performance of Flexible Functional Split in 5G vRAN Controllers - A Markov Chain-Based Model.pdf",
  keywords = "Delays;Computational modeling;5G mobile communication;Computer
              architecture;Quality of service;Markov processes;Analytical
              models;Flexible functional split;vRAN;controller;Markov
              chain;quasi-birth-death process;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2020.3045968"
}

@ARTICLE{Pham2021-zm,
  title    = "Optimized {IoT} Service Chain Implementation in Edge Cloud
              Platform: A Deep Learning Framework",
  author   = "Pham, Chuan and Nguyen, Duong Tuan and Tran, Nguyen H and Nguyen,
              Kim Khoa and Cheriet, Mohamed",
  abstract = "Internet of Things (IoT) services have been implemented for
              several network applications from smart cities to rural areas.
              However, there are many barriers to provide an efficient solution
              for the IoT service deployment underlying innovation
              SDN/NFV-based technologies. First, though an IoT service can
              flexibly deploy via virtual network functions (VNFs), a
              deployment scheme needs to solve the joint routing and resource
              allocation problem, which becomes more difficult than the
              traditional centralized cloud/datacenter solution due to
              distributed resources in the edge-cloud network. In addition, due
              to uncertain workloads in IoT services, static optimization
              solutions may not deal with uncompleted knowledge of the entire
              input, which is often given by assumptions, but unrealistic in
              current provisioning approaches. Aiming to address these issues,
              we model an online mechanism for the dynamic IoT service chain
              deployment to optimize the operational cost in a finite horizon.
              We propose a JOint Routing and Placement problem for IoT service
              chain (JORP) that can dynamically scale in/out the number of VNF
              instances. We then propose a learning method to efficiently solve
              JORP based on branch-and-bound (BnB). Our proposed learning
              mechanism can intelligently imitate the branching/pruning actions
              of BnB, and remove unlikely solutions in the search space based
              on the deep neural network model to improve the performance. In
              that respect, we take an intensive simulation that illustrates
              the promising result of our proposed deep learning method
              compared to BnB and the greedy baseline in terms of the
              performance of the algorithm and the operational cost reduction.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  18,
  number   =  1,
  pages    = "538--551",
  month    =  mar,
  year     =  2021,
  keywords = "Internet of Things;Resource management;Cloud
              computing;Optimization;Routing;Neural networks;Computational
              modeling;Internet of Things;edge computing;cloud;resource
              allocation;branch-and-bound;deep neural network;MLNetworking",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2021.3049824"
}

@ARTICLE{Subramanya2021-mg,
  title    = "Centralized and Federated Learning for Predictive {VNF}
              Autoscaling in {Multi-Domain} {5G} Networks and Beyond",
  author   = "Subramanya, Tejas and Riggio, Roberto",
  abstract = "Network Function Virtualization (NFV) and Multi-access Edge
              Computing (MEC) are two technologies expected to play a vital
              role in 5G and beyond networks. However, adequate mechanisms are
              required to meet the dynamically changing network service demands
              to utilize the network resources optimally and also to satisfy
              the demanding QoS requirements. Particularly in multi-domain
              scenarios, the additional challenge of isolation and data privacy
              among domains needs to be tackled. To this end, centralized and
              distributed Artificial Intelligence (AI)-driven resource
              orchestration techniques (e.g., virtual network function (VNF)
              autoscaling) are foreseen as the main enabler. In this work, we
              propose deep learning models, both centralized and federated
              approaches, that can perform horizontal and vertical autoscaling
              in multi-domain networks. The problem of autoscaling is modelled
              as a time series forecasting problem that predicts the future
              number of VNF instances based on the expected traffic demand. We
              evaluate the performance of various deep learning models trained
              over a commercial network operator dataset and investigate the
              pros and cons of federated learning over centralized learning
              approaches. Furthermore, we introduce the AI-driven Kubernetes
              orchestration prototype that we implemented by leveraging our MEC
              platform and assess the performance of the proposed deep learning
              models in a practical setup.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  18,
  number   =  1,
  pages    = "63--78",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/S/Subramanya and Riggio 2021 - Centralized and Federated Learning for Predictive VNF Autoscaling in Multi-Domain 5G Networks and Beyond.pdf",
  keywords = "Servers;Deep learning;Collaborative work;Predictive
              models;Computational modeling;Time series
              analysis;Forecasting;5G;deep learning;federated
              learning;autoscaling;multi-domain;multi-operator multi-access
              edge computing;Kubernetes;ToRead;Important;NFV",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2021.3050955"
}

@ARTICLE{Aceto2021-vw,
  title    = "Characterization and Prediction of {Mobile-App} Traffic Using
              Markov Modeling",
  author   = "Aceto, Giuseppe and Bovenzi, Giampaolo and Ciuonzo, Domenico and
              Montieri, Antonio and Persico, Valerio and Pescap{\'e}, Antonio",
  abstract = "Modeling network traffic is an endeavor actively carried on since
              early digital communications, supporting a number of practical
              applications, that range from network planning and provisioning
              to security. Accordingly, many theoretical and empirical
              approaches have been proposed in this long-standing research,
              most notably, Machine Learning (ML) ones. Indeed, recent interest
              from network equipment vendors is sparking around the evaluation
              of solid information-theoretical modeling approaches
              complementary to ML ones, especially applied to new network
              traffic profiles stemming from the massive diffusion of mobile
              apps. To cater to these needs, we analyze mobile-app traffic
              available in the public dataset MIRAGE-2019 adopting two related
              modeling approaches based on the well-known methodological
              toolset of Markov models (namely, Markov Chains and Hidden Markov
              Models). We propose a novel heuristic to reconstruct
              application-layer messages in the common case of encrypted
              traffic. We discuss and experimentally evaluate the suitability
              of the provided modeling approaches for different tasks:
              characterization of network traffic (at different granularities,
              such as application, application category, and application
              version), and prediction of network traffic at both packet and
              message level. We also compare the results with several ML
              approaches, showing performance comparable to a state-of-the-art
              ML predictor (Random Forest Regressor). Also, with this work we
              provide a viable and theoretically sound traffic-analysis toolset
              to help improving ML evaluation (and possibly its design), and a
              sensible and interpretable baseline.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  18,
  number   =  1,
  pages    = "907--925",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/A/Aceto et al. 2021 - Characterization and Prediction of Mobile-App Traffic Using Markov Modeling.pdf",
  keywords = "Hidden Markov models;Predictive models;Analytical models;Markov
              processes;Context modeling;Mobile applications;Data
              models;Android apps;encrypted traffic;Markov models;mobile
              apps;traffic characterization;traffic modeling;traffic
              prediction;NetworkTraffic",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2021.3051381"
}

@ARTICLE{Labriji2021-rg,
  title    = "Mobility Aware and Dynamic Migration of {MEC} Services for the
              Internet of Vehicles",
  author   = "Labriji, Ibtissam and Meneghello, Francesca and Cecchinato,
              Davide and Sesia, Stefania and Perraud, Eric and Strinati, Emilio
              Calvanese and Rossi, Michele",
  abstract = "Vehicles are becoming connected entities, and with the advent of
              online gaming, on demand streaming and assisted driving services,
              are expected to turn into data hubs with abundant computing
              needs. In this article, we show the value of estimating vehicular
              mobility as 5G users move across radio cells, and of using such
              estimates in combination with an online algorithm that assesses
              when and where the computing services (virtual machines, VM) that
              are run on the mobile edge nodes are to be migrated to ensure
              service continuity at the vehicles. This problem is tackled via a
              Lyapunov-based approach, which is here solved in closed form,
              leading to a low-complexity and distributed algorithm, whose
              performance is numerically assessed in a real-life scenario,
              featuring thousands of vehicles and densely deployed 5G base
              stations. Our numerical results demonstrate a reduction of more
              than 50\% in the energy expenditure with respect to previous
              strategies (full migration). Also, our scheme self-adapts to meet
              any given risk target, which is posed as an optimization
              constraint and represents the probability that the computing
              service is interrupted during a handover. Through it, we can
              effectively control the trade-off between seamless computation
              and energy consumption when migrating VMs.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  18,
  number   =  1,
  pages    = "570--584",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/L/Labriji et al. 2021 - Mobility Aware and Dynamic Migration of MEC Services for the Internet of Vehicles.pdf",
  keywords = "Handover;5G mobile communication;Optimization;Virtual
              machining;Task analysis;Roads;Prediction algorithms;5G;Internet
              of Vehicles (IoV);multi-access edge computing (MEC);virtual
              machine (VM);service migration;mobility estimation;Lyapunov
              optimization;recurrent neural network;convolutional neural
              network;Markov chain;ToRead;Important;EdgeFogCloudIoT",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2021.3052808"
}

@ARTICLE{Perdices2021-fl,
  title    = "{Deep-FDA}: Using Functional Data Analysis and Neural Networks to
              Characterize Network Services Time Series",
  author   = "Perdices, Daniel and de Vergara, Jorge E L{\'o}pez and Ramos,
              Javier",
  abstract = "In network management, it is important to model baselines,
              trends, and regular behaviors to adequately deliver network
              services. However, their characterization is complex, so network
              operation and system alarming become a challenge. Several
              problems exist: Gaussian assumptions cannot be made, time series
              have different trends, and it is difficult to reduce their
              dimensionality. To overcome this situation, we propose Deep-FDA,
              a novel approach for network service modeling that combines
              functional data analysis (FDA) and neural networks. Specifically,
              we explore the use of functional clustering and functional depth
              measurements to characterize network services with time series
              generated from enriched flow records, showing how this method can
              detect different separated trends. Moreover, we augment this
              statistical approach with the use of autoencoder neural networks,
              improving the classification results. To evaluate and check the
              applicability of our proposal, we performed experiments with
              synthetic and real-world data, where we show graphically and
              numerically the performance of our method compared to other
              state-of-the-art alternatives. We also exemplify its application
              in different network management use cases. The results show that
              FDA and neural networks are complementary, as they can help each
              other to improve the drawbacks that both analysis methods have
              when are applied separately.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  18,
  number   =  1,
  pages    = "986--999",
  month    =  mar,
  year     =  2021,
  keywords = "Time series analysis;Neural networks;Monitoring;Market
              research;Hidden Markov models;Data analysis;Machine
              learning;Functional data analysis;network monitoring and
              management;autoencoders;service characterization;time
              series;baseline model\%;NetworkTraffic",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2021.3053835"
}

@ARTICLE{Syu2021-au,
  title    = "{QoS} Time Series Modeling and Forecasting for Web Services: A
              Comprehensive Survey",
  author   = "Syu, Yang and Wang, Chien-Min",
  abstract = "In research, time series-based (time-aware) QoS modeling and
              forecasting for Web services (WSs) has been studied for over a
              decade, and a large number of research papers have been
              published; however, this current research lacks a systematic and
              comprehensive survey. Thus, in this article, an insightful and
              detailed review and investigation of current WS QoS time series
              modeling and forecasting research is provided. Based on our
              investigation of the collected literature, we identify and divide
              the entire research subject into four essential research concerns
              (components): the addressed problem, the proposed or employed
              approach, the considered performance measure, and the adopted QoS
              time series dataset. This article reviews, classifies, and
              discusses current studies in terms of the four identified
              research concerns. Moreover, for each research concern, a set of
              criteria and classification are proposed for its detailed
              categorization and comparison. Finally, this survey also
              discusses the insufficiency of current studies in each research
              concern and the potential future directions of this research
              area. Overall, this article can be an informative guide for
              researchers to understand the concerns that they should address
              when studying this research subject and to clearly comprehend
              what has been done in current research for each identified
              research concern.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  18,
  number   =  1,
  pages    = "926--944",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/S/Syu and Wang 2021 - QoS Time Series Modeling and Forecasting for Web Services - A Comprehensive Survey.pdf",
  keywords = "Quality of service;Predictive models;Forecasting;Time series
              analysis;Analytical models;Systematics;Web services;Data
              analysis;Web services;quality of service;time-series modeling and
              forecasting;time-aware dynamic attributes;NetworkTraffic",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2021.3056399"
}

@ARTICLE{Schmidt2021-sx,
  title    = "{RAN} Engine: {Service-Oriented} {RAN} Through Containerized
              {Micro-Services}",
  author   = "Schmidt, Robert and Nikaein, Navid",
  abstract = "Network slicing is considered to be the enabler for a coexistence
              of a multitude of services with heterogeneous requirements on a
              multi-tenant 5G infrastructure. In the core network, it has shown
              its potential in customizing and extending service-specific
              functionality beyond a mere configuration. In the radio access
              network (RAN) however, service customization and functionality
              extension remain a challenge due to the rigid and complex nature
              of the RAN and the fact that all services have to be mapped onto
              the scarce radio resources. In this article, we present the RAN
              service engine that allows services to customize and extend RAN
              functionality using containerized micro-services. This is
              achieved through micro-SDKs that abstract key RAN control
              endpoints, and which can then be used by the services to flexibly
              customize and extend the RAN in order to steer control plane
              behavior. Through these micro-SDKs, the engine can enforce
              isolation between services while multiplexing them efficiently
              onto the infrastructure. We also present a concrete
              implementation of the engine with its key micro-SDKs, and
              demonstrate the feasibility through a prototype for the MAC
              scheduling control endpoint, showing the versatility of the RAN
              engine.",
  journal  = "IEEE Trans. Netw. Serv. Manage.",
  volume   =  18,
  number   =  1,
  pages    = "469--481",
  month    =  mar,
  year     =  2021,
  keywords = "Radio access networks;Base stations;Virtualization;Cloud
              computing;5G mobile communication;Process
              control;Engines;5G;RAN;service;customization;extension;slicing;cloud-native;5G6G",
  issn     = "1932-4537",
  doi      = "10.1109/TNSM.2021.3057642"
}

@ARTICLE{Zhu2023-ps,
  title    = "Transfer Learning in Deep Reinforcement Learning: A Survey",
  author   = "Zhu, Zhuangdi and Lin, Kaixiang and Jain, Anil K and Zhou, Jiayu",
  abstract = "Reinforcement learning is a learning paradigm for solving
              sequential decision-making problems. Recent years have witnessed
              remarkable progress in reinforcement learning upon the fast
              development of deep neural networks. Along with the promising
              prospects of reinforcement learning in numerous domains such as
              robotics and game-playing, transfer learning has arisen to tackle
              various challenges faced by reinforcement learning, by
              transferring knowledge from external expertise to facilitate the
              efficiency and effectiveness of the learning process. In this
              survey, we systematically investigate the recent progress of
              transfer learning approaches in the context of deep reinforcement
              learning. Specifically, we provide a framework for categorizing
              the state-of-the-art transfer learning approaches, under which we
              analyze their goals, methodologies, compatible reinforcement
              learning backbones, and practical applications. We also draw
              connections between transfer learning and other relevant topics
              from the reinforcement learning perspective and explore their
              potential challenges that await future research progress.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  45,
  number   =  11,
  pages    = "13344--13362",
  month    =  nov,
  year     =  2023,
  file     = "All Papers/Z/Zhu et al. 2023 - Transfer Learning in Deep Reinforcement Learning - A Survey.pdf",
  language = "en",
  issn     = "0162-8828",
  pmid     = "37402188",
  doi      = "10.1109/TPAMI.2023.3292075"
}

@ARTICLE{Wu2020-al,
  title    = "Doris: An Adaptive Soft {Real-Time} Scheduler in Virtualized
              Environments",
  author   = "Wu, S and Zhou, L and Wang, X and Chen, F and Jin, H",
  abstract = "With the development of cloud computing and virtualization
              technologies, more and more soft real-time applications, such as
              Voice over Internet Protocol (VoIP) server and cloud gaming, are
              running in virtualized data centers. Though previous studies
              optimize CPU schedulers of hypervisors to support these
              applications in virtualized environments, there are some
              important challenges in designing an efficient CPU scheduler
              which is suitable for real-world clouds. On one hand, hypervisors
              do not know whether an application in a virtual machine (VM) has
              real-time requirements, so manually setting the scheduling
              parameters is a common case for CPU schedulers, which probably
              increases users' burden, lacks flexibility, and causes
              misconfigurations. On the other hand, it has been reported that
              most of existing CPU schedulers designed for soft real-time
              applications have an obvious propensity to such applications
              which prevents them from being applied in practical multi-tenant
              cloud environments. In this paper, we design and implement an
              adaptive soft real-time scheduler based on Xen, named Doris, to
              address these challenges. It identifies the VMs running soft
              real-time applications (RT-VMs) and infers their scheduling
              parameters according to the communication behaviors of VMs
              adaptively. Then, it promotes the priorities of VCPUs of the
              RT-VMs temporarily according to I/O events and the inferred
              scheduling parameters of RT-VMs to support soft real-time
              applications adaptively while minimizing the impacts on
              non-real-time applications. Finally, considering the importance
              of privileged entities (such as Domain0 in Xen) in I/O
              processing, Doris sets their types and scheduling parameters
              dynamically, which enables the adaptive scheduling of them to
              guarantee the performance of soft real-time applications. Our
              evaluation shows Doris can support soft real-time applications
              adaptively and efficiently, and only introduces very slight
              overhead.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  13,
  number   =  5,
  pages    = "815--828",
  month    =  sep,
  year     =  2020,
  keywords = "cloud computing;computer centres;Internet;resource
              allocation;scheduling;virtual machines;virtualisation;adaptive
              scheduling;nonreal-time applications;inferred scheduling
              parameters;practical multitenant cloud environments;existing CPU
              schedulers;efficient CPU scheduler;virtualized data centers;cloud
              gaming;real-time applications;virtualization technologies;cloud
              computing;virtualized environments;adaptive soft real-time
              scheduler;Real-time systems;Virtual machine monitors;Cloud
              computing;Schedules;Ports (Computers);Servers;Processor
              scheduling;Cloud computing;virtualization;soft real-time;CPU
              scheduling;Datacentre",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2017.2720732"
}

@ARTICLE{Ren2020-tu,
  title    = "A Reinforcement Learning Method for {Constraint-Satisfied}
              Services Composition",
  author   = "Ren, L and Wang, W and Xu, H",
  abstract = "With increasing adoption and presence of Web services, service
              composition becomes an effective way to construct software
              applications. Composite services need to satisfy both the
              functional and the non-functional requirements. Traditional
              methods usually assume that the quality of service (QoS) and the
              behaviors of services are deterministic, and they execute the
              composite service after all the component services are selected.
              It is difficult to guarantee the satisfaction of user constraints
              and the successful execution of the composite service. This paper
              models the constraint-satisfied service composition (CSSC)
              problem as a Markov decision process (MDP), namely CSSC-MDP, and
              designs a Q-learning algorithm to solve the model. CSSC-MDP takes
              the uncertainty of QoS and service behavior into account, and
              selects a component service after the execution of previous
              services. Thus, CSSC-MDP can select the globally optimal service
              based on the constraints which need the following services to
              satisfy. In the case of selected service failure, CSSC-MDP can
              timely provide the optimal alternative service. Simulation
              experiments show that the proposed method can successfully solve
              the CSSC problem of different sizes. Comparing with three
              representative methods, CSSC-MDP has obvious advantages,
              especially in terms of the success rate of service composition.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  13,
  number   =  5,
  pages    = "786--800",
  month    =  sep,
  year     =  2020,
  file     = "All Papers/R/Ren et al. 2020 - A Reinforcement Learning Method for Constraint-Satisfied Services Composition.pdf",
  keywords = "constraint handling;learning (artificial intelligence);Markov
              processes;quality of service;Web services;globally optimal
              service;selected service failure;optimal alternative
              service;Markov decision process;Q-learning algorithm;QoS;previous
              services;service behavior;CSSC-MDP;constraint-satisfied service
              composition problem;component service;composite
              service;constraint-satisfied services composition;reinforcement
              learning method;Quality of service;Internet;Time
              factors;Optimization;Markov processes;Uncertainty;Learning
              (artificial intelligence);Web service
              composition;constraint-satisfied;uncertainty of service
              behaviors;undetermined QoS;Markov decision process
              (MDP);Q-learning algorithm;MLNetworking;EdgeFogCloudIoT;NFV",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2017.2727050"
}

@ARTICLE{Wang2020-zc,
  title    = "{ENORM}: A Framework For Edge {NOde} Resource Management",
  author   = "Wang, N and Varghese, B and Matthaiou, M and Nikolopoulos, D S",
  abstract = "Current computing techniques using the cloud as a centralised
              server will become untenable as billions of devices get connected
              to the Internet. This raises the need for fog computing, which
              leverages computing at the edge of the network on nodes, such as
              routers, base stations and switches, along with the cloud.
              However, to realise fog computing the challenge of managing edge
              nodes will need to be addressed. This paper is motivated to
              address the resource management challenge. We develop the first
              framework to manage edge nodes, namely the Edge NOde Resource
              Management (ENORM) framework. Mechanisms for provisioning and
              auto-scaling edge node resources are proposed. The feasibility of
              the framework is demonstrated on a Pok{\'e}Mon Go-like online
              game use-case. The benefits of using ENORM are observed by
              reduced application latency between 20-80 percent and reduced
              data transfer and communication frequency between the edge node
              and the cloud by up to 95 percent. These results highlight the
              potential of fog computing for improving the quality of service
              and experience.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  13,
  number   =  6,
  pages    = "1086--1099",
  month    =  nov,
  year     =  2020,
  annote   = "Use case might be relevant.",
  file     = "All Papers/W/Wang et al. 2020 - ENORM - A Framework For Edge NOde Resource Management.pdf",
  keywords = "Servers;Cloud computing;Edge computing;Resource
              management;Computational modeling;Quality of
              service;Scalability;Fog computing;edge nodes;resource
              management;provisioning;scaling resources;EdgeFogCloudIoT",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2017.2753775"
}

@ARTICLE{Wang2020-ph,
  title    = "Efficient {QoS-Aware} Service Recommendation for {Multi-Tenant}
              {Service-Based} Systems in Cloud",
  author   = "Wang, Y and He, Q and Zhang, X and Ye, D and Yang, Y",
  abstract = "The popularity of cloud computing has fueled the growth in
              multi-tenant service-based systems (SBSs) that are composed of
              selected cloud services. In the cloud environment, a multi-tenant
              SBS simultaneously serves multiple tenants that usually have
              differentiated QoS requirements. This unique characteristic
              further complicates the problems of QoS-aware service selection
              at build-time and system adaptation at runtime, and renders
              conventional approaches obsolete and inefficient. In the dynamic
              and volatile cloud environment, the efficiency of building and
              adapting a multi-tenant SBS is of paramount importance. In this
              paper, we present two service recommendation approaches for
              multi-tenant SBSs, one for build-time and one for runtime, based
              on K-Means clustering and Locality-Sensitive Hashing (LSH)
              techniques respectively, aiming at finding appropriate services
              efficiently. Extensive experimental results demonstrate that our
              approaches can facilitate fast multi-tenant SBS construction and
              rapid system adaptation.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  13,
  number   =  6,
  pages    = "1045--1058",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/W/Wang et al. 2020 - Efficient QoS-Aware Service Recommendation for Multi-Tenant Service-Based Systems in Cloud.pdf",
  keywords = "Quality of service;Cloud computing;Runtime;Streaming
              media;Quality of service;Clustering methods;Cloud
              computing;clustering;quality of service;service-based
              system;service recommendation;system
              adaptation;NFV;EdgeFogCloudIoT",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2017.2761346"
}

@ARTICLE{Wang2020-av,
  title    = "Consistent State Updates for Virtualized Network Function
              Migration",
  author   = "Wang, W and Liu, Y and Liu, J and Li, Y and Song, H and Wang, Y
              and Yuan, J",
  abstract = "Combining Network Functions Virtualization (NFV) with
              Software-Defined Networking (SDN) is an emerging and promising
              solution to provide scalable and elastic network control and
              service. In such a system, virtualized Network Functions (NFs)
              need to be consistently migrated from one instance to another for
              various purposes, such as resource optimization, fault tolerance,
              load balancing, etc. These migrations involve simultaneously
              coordinating updates to the NF state and SDN forwarding state. To
              solve this problem, we design two consistent NF state update
              schemes: a controller-forwarding based scheme and a tagging-based
              scheme. Through analysis of the update process, we demonstrate
              that they both guarantee loss-free and order-preserving
              migrations. We further implement a prototype and carry out
              experiments with diverse traffic settings. Results demonstrate
              that the controller-forwarding based solution achieves 77 percent
              migration time compared with the state-of-the-art solution
              OpenNF, while correcting an error of it. Moreover, the
              tagging-based solution not only achieves 4.4 percent migration
              time, but also reduces up to 75 percent controller overhead
              compared with OpenNF at the cost of adding a tag in the unused
              fields of packet header.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  13,
  number   =  6,
  pages    = "999--1006",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/W/Wang et al. 2020 - Consistent State Updates for Virtualized Network Function Migration.pdf",
  keywords = "Noise measurement;Process control;Control systems;Network
              function virtualization;Computer architecture;Software defined
              networking;Software-defined networks;virtualized network
              function;state consistency;network function migration;NFV",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2017.2765636"
}

@ARTICLE{Rampersaud2021-et,
  title    = "An Approximation Algorithm for {Sharing-Aware} Virtual Machine
              Revenue Maximization",
  author   = "Rampersaud, Safraz and Grosu, Daniel",
  abstract = "Cloud providers face the challenge of efficiently managing their
              infrastructure through minimizing resource consumption while
              allocating service requests such that their revenue is maximized.
              Solutions addressing this challenge should consider the sharing
              of memory pages among virtual machines (VMs) and the available
              capacity of each type of requested resources. We provide such
              solution by designing a greedy approximation algorithm for
              solving the sharing-aware virtual machine revenue maximization
              (SAVMRM) problem. The SAVMRM problem requires determining the set
              of VMs that can be instantiated on a given server such that the
              revenue derived from hosting the VMs is maximized. In addition,
              we model the SAVMRM problem as a multilinear binary program and
              optimally solve it, while accounting for page sharing and
              multiple resource constraints. We determine and analyze the
              approximability properties of our proposed greedy algorithm and
              evaluate it by performing extensive experiments using Google
              cluster workload traces. The experimental results show that under
              various scenarios, our proposed algorithm generates higher
              revenue than other VM allocation algorithms while achieving
              significant reduction of allocated memory.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  14,
  number   =  1,
  pages    = "1--15",
  month    =  jan,
  year     =  2021,
  keywords = "Resource management;Servers;Virtual machine monitors;Algorithm
              design and analysis;Virtual machining;Approximation
              algorithms;Clustering algorithms;Approximation
              algorithm;multilinear programming;multidimensional
              knapsack;sharing-aware;virtual machine;Datacentre",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2017.2786728"
}

@ARTICLE{Sebastio2021-ed,
  title    = "An Availability Analysis Approach for Deployment Configurations
              of Containers",
  author   = "Sebastio, Stefano and Ghosh, Rahul and Mukherjee, Tridib",
  abstract = "Operating system (OS) containers enabling the
              microservice-oriented architecture are becoming popular in the
              context of Cloud services. Containers provide the ability to
              create lightweight and portable runtime environments that
              decouple the application requirements from the characteristics of
              the underlying system. Services built on containers have a small
              resource footprint in terms of processing, storage, memory and
              network, allowing a more dense deployment environment. While the
              performance of such containers is addressed in few previous
              studies, understanding the failure-repair behavior of the
              containers remains unexplored. In this paper, from an
              availability point of view, we propose and compare different
              configuration models for deploying a containerized software
              system. Inspired by Google Kubernetes, a container management
              system, these configurations are characterized with a failure
              response and migration service. We develop novel non-state-space
              (i.e., fault tree) and state-space (i.e., stochastic reward net)
              analytic models for container availability analysis. Analytical
              as well as simulative solutions are obtained for the developed
              models. Our analysis provides insights on k out-of N availability
              and sensitivity of system availability for key system parameters.
              Finally, we build an open-source software tool powered by these
              models. The tool helps a Cloud administrator to assess the
              availability of a containerized system and to conduct a what-if
              analysis based on user-provided parameters and configurations.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  14,
  number   =  1,
  pages    = "16--29",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/S/Sebastio et al. 2021 - An Availability Analysis Approach for Deployment Configurations of Containers.pdf",
  keywords = "Containers;Analytical models;Cloud computing;Stochastic
              processes;Tools;Computer architecture;Google;Container;system
              availability;virtual machine;cloud computing;analytic
              model;stochastic reward net;Datacentre",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2017.2788442"
}

@ARTICLE{Liu2021-jb,
  title    = "{RepNet}: Cutting Latency with Flow Replication in Data Center
              Networks",
  author   = "Liu, Shuhao and Xu, Hong and Liu, Libin and Bai, Wei and Chen,
              Kai and Cai, Zhiping",
  abstract = "Data center networks need to provide low latency, especially at
              the tail, as demanded by many interactive applications. To
              improve tail latency, existing approaches require modifications
              to switch hardware and/or end-host operating systems, making them
              difficult to be deployed. We present the design, implementation,
              and evaluation of RepNet, an application layer transport that can
              be deployed today. RepNet exploits the fact that only a few paths
              among many are congested at any moment in the network, and
              applies simple flow replication to mice flows to
              opportunistically use the less congested path. RepNet has two
              designs for flow replication: (1) RepSYN, which only replicates
              SYN packets and uses the first connection that finishes TCP
              handshaking for data transmission, and (2) RepFlow which
              replicates the entire mice flow. We implement RepNet on node.js,
              one of the most commonly used platforms for networked interactive
              applications. node's single threaded event-loop and non-blocking
              I/O make flow replication highly efficient. Performance
              evaluation on a real network testbed and in Mininet reveals that
              RepNet is able to reduce the tail latency of mice flows, as well
              as application completion times, by more than 50 percent.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  14,
  number   =  1,
  pages    = "248--261",
  month    =  jan,
  year     =  2021,
  keywords = "Mice;Switches;Electronic mail;Topology;Performance
              evaluation;Production;Data center networks;latency;flow
              replication",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2018.2793250"
}

@ARTICLE{Xu2021-ct,
  title    = "Enabling Cloud Applications to Negotiate Multiple Resources in a
              {Cost-Efficient} Manner",
  author   = "Xu, Yu and Yao, Jianguo and Jacobsen, Hans-Arno and Guan, Haibing",
  abstract = "Cloud applications can achieve similar performance with diverse
              multi-resource configurations, allowing cloud service providers
              to benefit from optimal resource allocation for reducing their
              operation cost. This paper aims to solve the problem of
              multi-resource negotiation with considerations of both the
              service-level agreement (SLA) and the cost efficiency so that the
              performance requirement for cloud services is satisfied and the
              cost of resource usage is also minimized. The performance and
              resource demand are usually application-dependent, making the
              optimization problem complicated, especially when the dimension
              of multi-resource configurations is large. To this end, we use
              reinforcement learning to solve the optimal problem of
              multi-resource configuration with simultaneous optimization of
              the learning efficiency and performance guarantee. The developed
              prototype named SmartYARN is an extended Apache YARN equipped
              with our learning algorithm which can enable cloud applications
              to negotiate multiple resources cost-effectively. The extensive
              evaluations with four typical benchmarks show that SmartYARN
              performs well in reducing the cost of resource usage while
              maintaining compliance with the SLA constraints of cloud service
              simultaneously.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  14,
  number   =  2,
  pages    = "413--425",
  month    =  mar,
  year     =  2021,
  keywords = "Cloud computing;Pricing;Optimization;Resource
              management;Yarn;Learning (artificial intelligence);Jacobian
              matrices;Cloud scheduler;multi-resource;cost
              efficiency;reinforcement learning;Datacentre",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2018.2814045"
}

@ARTICLE{Yang2019-rh,
  title    = "{Delay-Sensitive} and {Availability-Aware} Virtual Network
              Function Scheduling for {NFV}",
  author   = "Yang, S and Li, F and Yahyapour, R and Fu, X",
  abstract = "Network Function Virtualization (NFV) has been emerging as an
              appealing solution that transforms from dedicated hardware
              implementations to software instances running in a virtualized
              environment. In NFV, the requested service is implemented by a
              sequence of Virtual Network Functions (VNF) that can run on
              generic servers by leveraging the virtualization technology.
              These VNFs are pitched with a predefined order, and it is also
              known as the Service Function Chaining (SFC). Considering that
              the delay and resiliency are two important Service Level
              Agreements (SLA) in a NFV service, in this paper, we first
              investigate how to quantitatively model the traversing delay of a
              flow in both totally ordered and partially ordered SFCs.
              Subsequently, we study how to calculate the VNF placement
              availability mathematically for both unprotected and protected
              SFCs. After that, we study the delay-sensitive Virtual Network
              Function (VNF) placement and routing problem with and without
              resiliency concerns. We prove that this problem is NP-hard under
              two cases. We subsequently propose an exact Integer Nonlinear
              Programming formulation and an efficient heuristic for this
              problem in each case. Finally, we evaluate the proposed
              algorithms in terms of acceptance ratio, average number of used
              nodes and total running time via extensive simulations.",
  journal  = "IEEE Trans. Serv. Comput.",
  pages    = "1--1",
  year     =  2019,
  keywords = "Delays;Routing;Resilience;Approximation algorithms;Hidden Markov
              models;Computational modeling;Virtualization;Network Function
              Virtualization;Service Function
              Chaining;Delay;Resiliency;Availability;Placement;Routing;NFV",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2019.2927339"
}

@ARTICLE{Zhou2020-zd,
  title    = "Towards Service Composition Aware Virtual Machine Migration
              Approach in the Cloud",
  author   = "Zhou, A and Wang, S and Ma, X and Yau, S S",
  abstract = "There is a growing trend for service providers to migrate their
              services from local clusters to the cloud data center. When there
              is no single service can satisfy the functionality requirement of
              the end user, existing services are combined together to fulfill
              the requirements. The data communication between component
              service hosting servers imposes a heavy burden on the data center
              network. In this article, we seek to reduce the data center
              network resource consumption by designing a novel service
              composition aware virtual machine migration approach. First, we
              formulate the problem as a multi-object integer non-linear(INLP)
              programming problem. The problem, which can be reduced into a
              well-known multi-object quadratic assignment problem, is proved
              to be NP-hard. Second, we simplify the multiple-objects INLP
              formulation into an equivalent, but much simplified single object
              ILP formulation. Then, we prove that the simplified formulation
              can also lead to the optimal solutions. Finally, optimization
              problem solvers, such as LPSolver, are employed to solve the
              problem. Experimental results in a large scale cloud data center
              demonstrate that our method significantly reduce the network
              resource consumption than other approaches.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  13,
  number   =  4,
  pages    = "735--744",
  month    =  jul,
  year     =  2020,
  file     = "All Papers/Z/Zhou et al. 2020 - Towards Service Composition Aware Virtual Machine Migration Approach in the Cloud.pdf",
  keywords = "cloud computing;computer centres;integer programming;linear
              programming;nonlinear programming;virtual
              machines;multiple-objects;simplified single object ILP
              formulation;simplified formulation;optimization problem
              solvers;scale cloud data;service providers;local clusters;cloud
              data center;single service;functionality requirement;data
              communication;component service hosting servers;data center
              network resource consumption;multiobject quadratic assignment
              problem;service composition aware virtual machine migration
              approach;Virtual machining;Data centers;Servers;Cloud
              computing;Optimization;Data communication;Topology;Cloud
              computing;service composition;data center;virtual machine
              migration;EdgeFogCloudIoT",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2019.2962128"
}

@ARTICLE{Ma2020-iw,
  title    = "A Cyclic Game for {Service-Oriented} Resource Allocation in Edge
              Computing",
  author   = "Ma, S and Guo, S and Wang, K and Jia, W and Guo, M",
  abstract = "Existing works adopt the Edge-Oriented Resource Allocation (EORA)
              scheme, in which edge nodes cache services and schedule user
              requests to distribute workloads over cloud and edge nodes, so as
              to achieve high-quality services and low latency. Unfortunately,
              EORA does not fully take into account the fact that service
              providers are sometimes independent from the edge operators with
              their own objectives. To deal with the conflict and cooperation
              between service providers and edge nodes, we devise a
              service-oriented resource allocation (SORA) scheme, where edge
              nodes and service providers adjust their resource allocations to
              provide requested services. We first prove that such resource
              allocation problem is NP-hard. We then propose a three-sided
              cyclic game (3CG) involving users, edge nodes, and service
              providers who make their individual decisions by choosing
              respectively high-quality services, high-value users, and
              cost-effective edge nodes for service deployment. Based on 3CG,
              we prove the existence and approximation ratio of pure-strategy
              Nash equilibriums (NEs). We also develop both centralized and
              distributed approximate algorithms for resource allocation.
              Finally, extensive experimental results validate the
              effectiveness and convergence of the proposed algorithms.",
  journal  = "IEEE Trans. Serv. Comput.",
  volume   =  13,
  number   =  4,
  pages    = "723--734",
  month    =  jul,
  year     =  2020,
  keywords = "cloud computing;computational complexity;game theory;resource
              allocation;scheduling;distributed approximate
              algorithm;centralized approximate algorithm;Nash
              equilibriums;NP-hard problem;edge nodes cache
              services;edge-oriented resource allocation scheme;service
              deployment;three-sided cyclic game;cost-effective edge
              nodes;service-oriented resource allocation scheme;edge
              operators;service providers;high-quality services;edge
              computing;Resource management;Games;Edge computing;Quality of
              experience;Computational modeling;Numerical models;Approximation
              algorithms;Edge computing;resource allocation;game theory",
  issn     = "1939-1374",
  doi      = "10.1109/TSC.2020.2966196"
}

@ARTICLE{Tan2018-op,
  title    = "{Mobility-Aware} Edge Caching and Computing in Vehicle Networks:
              A Deep Reinforcement Learning",
  author   = "Tan, L T and Hu, R Q",
  abstract = "This paper studies the joint communication, caching and computing
              design problem for achieving the operational excellence and the
              cost efficiency of the vehicular networks. Moreover, the resource
              allocation policy is designed by considering the vehicle's
              mobility and the hard service deadline constraint. These critical
              challenges have often been either neglected or addressed
              inadequately in the existing work on the vehicular networks
              because of their high complexity. We develop a deep reinforcement
              learning with the multi-timescale framework to tackle these grand
              challenges in this paper. Furthermore, we propose the
              mobility-aware reward estimation for the large timescale model to
              mitigate the complexity due to the large action space. Numerical
              results are presented to illustrate the theoretical findings
              developed in the paper and to quantify the performance gains
              attained.",
  journal  = "IEEE Trans. Veh. Technol.",
  volume   =  67,
  number   =  11,
  pages    = "10190--10203",
  month    =  nov,
  year     =  2018,
  file     = "All Papers/T/Tan and Hu 2018 - Mobility-Aware Edge Caching and Computing in Vehicle Networks - A Deep Reinforcement Learning.pdf",
  keywords = "communication complexity;learning (artificial
              intelligence);mobile computing;resource
              allocation;telecommunication computing;vehicular ad hoc
              networks;mobility-aware edge caching;mobility-aware edge
              computing;multitimescale framework;hard service deadline
              constraint;resource allocation policy;vehicular
              networks;computing design problem;deep reinforcement
              learning;mobility-aware reward estimation;Computational
              modeling;Device-to-device communication;Task analysis;Machine
              learning;Resource management;Edge computing;Quality of
              service;Vehicular networks;mobility;edge caching;edge
              computing;deep reinforcement
              learning;EdgeFogCloudIoT;MLNetworking",
  issn     = "0018-9545, 1939-9359",
  doi      = "10.1109/TVT.2018.2867191"
}

@ARTICLE{Dai2018-ud,
  title    = "Joint Computation Offloading and User Association in {Multi-Task}
              Mobile Edge Computing",
  author   = "Dai, Y and Xu, D and Maharjan, S and Zhang, Y",
  abstract = "Computation intensive and delay-sensitive applications impose
              severe requirements on mobile devices of providing required
              computation capacity and ensuring latency. Mobile edge computing
              (MEC) is a promising technology that can alleviate computation
              limitation of mobile users and prolong their lifetime through
              computation offloading. However, computation offloading in an MEC
              environment faces severe issues due to dense deployment of MEC
              servers. Moreover, a mobile user has multiple mutually dependent
              tasks, which make offloading policy design even more challenging.
              To address the above-mentioned problems in this paper, we first
              propose a novel two-tier computation offloading framework in
              heterogeneous networks. Then, we formulate joint computation
              offloading and user association problem for multi-task mobile
              edge computing system to minimize overall energy consumption. To
              solve the optimization problem, we develop an efficient
              computation offloading algorithm by jointly optimizing user
              association and computation offloading where computation resource
              allocation and transmission power allocation are also considered.
              Numerical results illustrate fast convergence of the proposed
              algorithm, and demonstrate the superior performance of our
              proposed algorithm compared to state of the art solutions.",
  journal  = "IEEE Trans. Veh. Technol.",
  volume   =  67,
  number   =  12,
  pages    = "12313--12325",
  month    =  dec,
  year     =  2018,
  file     = "All Papers/D/Dai et al. 2018 - Joint Computation Offloading and User Association in Multi-Task Mobile Edge Computing.pdf",
  keywords = "cellular radio;cloud computing;mobile
              computing;optimisation;resource allocation;efficient computation
              offloading algorithm;computation resource allocation;transmission
              power allocation;multitask mobile edge computing;computation
              intensive;mobile devices;computation limitation;mobile
              user;offloading policy design;two-tier computation offloading
              framework;computation capacity;joint computation offloading and
              user association;Task analysis;Servers;Base
              stations;Computational modeling;Edge computing;Resource
              management;Energy consumption;Mobile edge computing
              (MEC);computation off-loading;user association;resource
              allocation;optimization;EdgeFogCloudIoT",
  issn     = "0018-9545, 1939-9359",
  doi      = "10.1109/TVT.2018.2876804"
}

@ARTICLE{Tran2019-rv,
  title    = "Joint Task Offloading and Resource Allocation for {Multi-Server}
              {Mobile-Edge} Computing Networks",
  author   = "Tran, T X and Pompili, D",
  abstract = "Mobile-edge computing (MEC) is an emerging paradigm that provides
              a capillary distribution of cloud computing capabilities to the
              edge of the wireless access network, enabling rich services and
              applications in close proximity to the end users. In this paper,
              an MEC enabled multi-cell wireless network is considered where
              each base station (BS) is equipped with a MEC server that assists
              mobile users in executing computation-intensive tasks via task
              offloading. The problem of joint task offloading and resource
              allocation is studied in order to maximize the users' task
              offloading gains, which is measured by a weighted sum of
              reductions in task completion time and energy consumption. The
              considered problem is formulated as a mixed integer nonlinear
              program (MINLP) that involves jointly optimizing the task
              offloading decision, uplink transmission power of mobile users,
              and computing resource allocation at the MEC servers. Due to the
              combinatorial nature of this problem, solving for optimal
              solution is difficult and impractical for a large-scale network.
              To overcome this drawback, we propose to decompose the original
              problem into a resource allocation (RA) problem with fixed task
              offloading decision and a task offloading (TO) problem that
              optimizes the optimal-value function corresponding to the RA
              problem. We address the RA problem using convex and quasi-convex
              optimization techniques, and propose a novel heuristic algorithm
              to the TO problem that achieves a suboptimal solution in
              polynomial time. Simulation results show that our algorithm
              performs closely to the optimal solution and that it
              significantly improves the users' offloading utility over
              traditional approaches.",
  journal  = "IEEE Trans. Veh. Technol.",
  volume   =  68,
  number   =  1,
  pages    = "856--868",
  month    =  jan,
  year     =  2019,
  file     = "All Papers/T/Tran and Pompili 2019 - Joint Task Offloading and Resource Allocation for Multi-Server Mobile-Edge Computing Networks.pdf",
  keywords = "cellular radio;cloud computing;computational complexity;convex
              programming;integer programming;mobile computing;nonlinear
              programming;optimisation;radio access networks;resource
              allocation;telecommunication computing;multiserver mobile-edge
              computing networks;wireless access network;rich services;end
              users;multicell wireless network;MEC server;assists mobile
              users;computation-intensive tasks;joint task offloading;task
              completion time;optimal solution;large-scale network;resource
              allocation problem;fixed task offloading decision;RA problem;TO
              problem;quasiconvex optimization techniques;Task
              analysis;Servers;Resource management;Cloud computing;Wireless
              communication;Energy consumption;Uplink;Mobile edge
              computing;computation offloading;multi-server resource
              allocation;distributed systems;EdgeFogCloudIoT",
  issn     = "0018-9545, 1939-9359",
  doi      = "10.1109/TVT.2018.2881191"
}

@ARTICLE{Liu2018-qk,
  title    = "{Prediction-Based} Mobile Data Offloading in Mobile Cloud
              Computing",
  author   = "Liu, D and Khoukhi, L and Hafid, A",
  abstract = "Cellular network is facing a severe traffic overload problem
              caused by the phenomenal growth of mobile data. Offloading part
              of the mobile data traffic from the cellular network to
              alternative networks is a promising solution. In this paper, we
              study the mobile data offloading problem under the architecture
              of mobile cloud computing, where mobile data can be delivered by
              WiFi network and device-to-device communication. In order to
              minimize the overall cost for the data delivery task, it is
              crucial to reduce cellular network usage while satisfying delay
              requirements. In our proposed model, we formulate the data
              offloading task as a finite horizon Markov decision process. We
              first propose a hybrid offloading algorithm for mobile data with
              different delay requirements. Moreover, we establish sufficient
              conditions for the existence of threshold policy. Then, we
              propose a monotone offloading algorithm based on threshold policy
              in order to reduce the computational complexity. The simulation
              results show that the proposed offloading approach can achieve
              minimal communication cost compared with the other three
              offloading schemes.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  17,
  number   =  7,
  pages    = "4660--4673",
  month    =  jul,
  year     =  2018,
  file     = "All Papers/L/Liu et al. 2018 - Prediction-Based Mobile Data Offloading in Mobile Cloud Computing.pdf",
  keywords = "cellular radio;cloud computing;decision theory;Markov
              processes;mobile computing;telecommunication traffic;wireless
              LAN;alternative networks;mobile data offloading problem;mobile
              cloud computing;data delivery task;data offloading task;hybrid
              offloading algorithm;monotone offloading
              algorithm;prediction-based mobile data offloading;severe traffic
              overload problem;mobile data traffic;cellular network usage;delay
              requirements;finite horizon Markov decision process;threshold
              policy;WiFi network;Wireless fidelity;Device-to-device
              communication;Cellular networks;Delays;Data models;Wireless
              communication;Cloud computing;Mobile data
              offloading;device-to-device communication;mobile cloud
              computing;Markov decision process;MLNetworking",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2018.2829513"
}

@ARTICLE{Li2020-tc,
  title    = "Edge {AI}: {On-Demand} Accelerating Deep Neural Network Inference
              via Edge Computing",
  author   = "Li, En and Zeng, Liekang and Zhou, Zhi and Chen, Xu",
  abstract = "As a key technology of enabling Artificial Intelligence (AI)
              applications in 5G era, Deep Neural Networks (DNNs) have quickly
              attracted widespread attention. However, it is challenging to run
              computation-intensive DNN-based tasks on mobile devices due to
              the limited computation resources. What's worse, traditional
              cloud-assisted DNN inference is heavily hindered by the
              significant wide-area network latency, leading to poor real-time
              performance as well as low quality of user experience. To address
              these challenges, in this paper, we propose Edgent, a framework
              that leverages edge computing for DNN collaborative inference
              through device-edge synergy. Edgent exploits two design knobs:
              (1) DNN partitioning that adaptively partitions computation
              between device and edge for purpose of coordinating the powerful
              cloud resource and the proximal edge resource for real-time DNN
              inference; (2) DNN right-sizing that further reduces computing
              latency via early exiting inference at an appropriate
              intermediate DNN layer. In addition, considering the potential
              network fluctuation in real-world deployment, Edgent is properly
              design to specialize for both static and dynamic network
              environment. Specifically, in a static environment where the
              bandwidth changes slowly, Edgent derives the best configurations
              with the assist of regression-based prediction models, while in a
              dynamic environment where the bandwidth varies dramatically,
              Edgent generates the best execution plan through the online
              change point detection algorithm that maps the current bandwidth
              state to the optimal configuration. We implement Edgent prototype
              based on the Raspberry Pi and the desktop PC and the extensive
              experimental evaluations demonstrate Edgent's effectiveness in
              enabling on-demand low-latency edge intelligence.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  19,
  number   =  1,
  pages    = "447--457",
  month    =  jan,
  year     =  2020,
  file     = "All Papers/L/Li et al. 2020 - Edge AI - On-Demand Accelerating Deep Neural Network Inference via Edge Computing.pdf",
  keywords = "Computational modeling;Mobile handsets;Bandwidth;Image edge
              detection;Performance evaluation;Edge computing;Wireless
              communication;Edge intelligence;edge computing;deep
              learning;computation offloading;MLAspects",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2019.2946140"
}

@ARTICLE{Ren2020-ai,
  title    = "Scheduling for Cellular Federated Edge Learning With Importance
              and Channel Awareness",
  author   = "Ren, Jinke and He, Yinghui and Wen, Dingzhu and Yu, Guanding and
              Huang, Kaibin and Guo, Dongning",
  abstract = "In cellular federated edge learning (FEEL), multiple edge devices
              holding local data jointly train a neural network by
              communicating learning updates with an access point without
              exchanging their data samples. With very limited communication
              resources, it is beneficial to schedule the most informative
              local learning updates. This paper focuses on FEEL with gradient
              averaging over participating devices in each round of
              communication. A novel scheduling policy is proposed to exploit
              both diversity in multiuser channels and diversity in the
              ``importance'' of the edge devices' learning updates. First, a
              new probabilistic scheduling framework is developed to yield
              unbiased update aggregation in FEEL. The importance of a local
              learning update is measured by its gradient divergence. If one
              edge device is scheduled in each communication round, the
              scheduling policy is derived in closed form to achieve the
              optimal trade-off between channel quality and update importance.
              The probabilistic scheduling framework is then extended to allow
              scheduling multiple edge devices in each communication round.
              Numerical results obtained using popular models and learning
              datasets demonstrate that the proposed scheduling policy can
              achieve faster model convergence and higher learning accuracy
              than conventional scheduling policies that only exploit a single
              type of diversity.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  19,
  number   =  11,
  pages    = "7690--7703",
  month    =  nov,
  year     =  2020,
  file     = "All Papers/R/Ren et al. 2020 - Scheduling for Cellular Federated Edge Learning With Importance and Channel Awareness.pdf",
  keywords = "Scheduling;Convergence;Servers;Wireless communication;Processor
              scheduling;Probabilistic logic;Resource management;Federated edge
              learning;scheduling;multiuser diversity;resource
              management;convergence analysis;MLNetworking",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3015671"
}

@ARTICLE{Wen2020-el,
  title    = "Joint {Parameter-and-Bandwidth} Allocation for Improving the
              Efficiency of Partitioned Edge Learning",
  author   = "Wen, Dingzhu and Bennis, Mehdi and Huang, Kaibin",
  abstract = "To leverage data and computation capabilities of mobile devices,
              machine learning algorithms are deployed at the network edge for
              training artificial intelligence (AI) models, resulting in the
              new paradigm of edge learning. In this paper, we consider the
              framework of partitioned edge learning for iteratively training a
              large-scale model using many resource-constrained devices (called
              workers). To this end, in each iteration, the model is
              dynamically partitioned into parametric blocks, which are
              downloaded to worker groups for updating using data subsets.
              Then, the local updates are uploaded to and cascaded by the
              server for updating a global model. To reduce resource usage by
              minimizing the total learning-and-communication latency, this
              work focuses on the novel joint design of parameter (computation
              load) allocation and bandwidth allocation (for downloading and
              uploading). Two design approaches are adopted. First, a practical
              sequential approach, called partially integrated
              parameter-and-bandwidth allocation (PABA), yields two schemes,
              namely bandwidth aware parameter allocation and parameter aware
              bandwidth allocation. The former minimizes the load for the
              slowest (in computing) of worker groups, each training a same
              parametric block. The latter allocates the largest bandwidth to
              the worker being the latency bottleneck. Second, PABA are jointly
              optimized. Despite it being a nonconvex problem, an efficient and
              optimal solution algorithm is derived by intelligently nesting a
              bisection search and solving a convex problem. Experimental
              results using real data demonstrate that integrating PABA can
              substantially improve the performance of partitioned edge
              learning in terms of latency (by e.g., 46\%) and accuracy (by
              e.g., 4\% given the latency of 100 seconds).",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  19,
  number   =  12,
  pages    = "8272--8286",
  month    =  dec,
  year     =  2020,
  file     = "All Papers/W/Wen et al. 2020 - Joint Parameter-and-Bandwidth Allocation for Improving the Efficiency of Partitioned Edge Learning.pdf",
  keywords = "Computational modeling;Resource management;Servers;Channel
              allocation;Load modeling;Wireless communication;Training
              data;Partitioned edge learning;parameter allocation;bandwidth
              allocation;learning latency;Net4ML;MLAspects",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3021177"
}

@ARTICLE{Chen2021-tk,
  title    = "A Joint Learning and Communications Framework for Federated
              Learning Over Wireless Networks",
  author   = "Chen, M and Yang, Z and Saad, W and Yin, C and Poor, H V and Cui,
              S",
  abstract = "In this article, the problem of training federated learning (FL)
              algorithms over a realistic wireless network is studied. In the
              considered model, wireless users execute an FL algorithm while
              training their local FL models using their own data and
              transmitting the trained local FL models to a base station (BS)
              that generates a global FL model and sends the model back to the
              users. Since all training parameters are transmitted over
              wireless links, the quality of training is affected by wireless
              factors such as packet errors and the availability of wireless
              resources. Meanwhile, due to the limited wireless bandwidth, the
              BS needs to select an appropriate subset of users to execute the
              FL algorithm so as to build a global FL model accurately. This
              joint learning, wireless resource allocation, and user selection
              problem is formulated as an optimization problem whose goal is to
              minimize an FL loss function that captures the performance of the
              FL algorithm. To seek the solution, a closed-form expression for
              the expected convergence rate of the FL algorithm is first
              derived to quantify the impact of wireless factors on FL. Then,
              based on the expected convergence rate of the FL algorithm, the
              optimal transmit power for each user is derived, under a given
              user selection and uplink resource block (RB) allocation scheme.
              Finally, the user selection and uplink RB allocation is optimized
              so as to minimize the FL loss function. Simulation results show
              that the proposed joint federated learning and communication
              framework can improve the identification accuracy by up to 1.4\%,
              3.5\% and 4.1\%, respectively, compared to: 1) An optimal user
              selection algorithm with random resource allocation, 2) a
              standard FL algorithm with random user selection and resource
              allocation, and 3) a wireless optimization algorithm that
              minimizes the sum packet error rates of all users while being
              agnostic to the FL parameters.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  1,
  pages    = "269--283",
  month    =  jan,
  year     =  2021,
  file     = "All Papers/C/Chen et al. 2021 - A Joint Learning and Communications Framework for Federated Learning Over Wireless Networks.pdf",
  keywords = "Data models;Resource management;Training;Convergence;Wireless
              networks;Optimization;Federated learning (FL);user
              selection;wireless resource management",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3024629"
}

@ARTICLE{Park2021-qx,
  title    = "Mobile Edge {Computing-Enabled} Heterogeneous Networks",
  author   = "Park, Chanwon and Lee, Jemin",
  abstract = "The mobile edge computing (MEC) has been introduced for providing
              computing capabilities at the edge of networks to improve the
              latency performance of wireless networks. In this paper, we
              provide the novel framework for MEC-enabled heterogeneous
              networks (HetNets), composed of the multi-tier networks with
              access points (APs) (i.e., MEC servers), which have different
              transmission power and different computing capabilities. In this
              framework, we also consider multiple-type mobile users with
              different sizes of computation tasks, and they offload the tasks
              to a MEC server, and receive the computation resulting data from
              the server. We derive the successful edge computing probability
              (SECP), defined as the probability that a user offloads and
              finishes its computation task at the MEC server within the target
              latency. We provide a closed-form expression of the approximated
              SECP for general case, and closed-form expressions of the exact
              SECP for special cases. This paper then provides the design
              insights for the optimal configuration of MEC-enabled HetNets by
              analyzing the effects of network parameters and bias factors,
              used in MEC server association, on the SECP. Specifically, it
              shows how the optimal bias factors in terms of SECP can be
              changed according to the numbers of user types and tiers of MEC
              servers, and how they are different to the conventional ones that
              did not consider the computing capabilities and task sizes.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  2,
  pages    = "1038--1051",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/P/Park and Lee 2021 - Mobile Edge Computing-Enabled Heterogeneous Networks.pdf",
  keywords = "Servers;Task analysis;Downlink;Uplink;Wireless
              communication;Heterogeneous networks;Edge computing;Mobile edge
              computing;heterogeneous network;latency;offloading;queueing
              theory;stochastic geometry;EdgeFogCloudIoT",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3030178"
}

@ARTICLE{Sun2021-le,
  title    = "Distributed Task Replication for Vehicular Edge Computing:
              Performance Analysis and {Learning-Based} Algorithm",
  author   = "Sun, Yuxuan and Zhou, Sheng and Niu, Zhisheng",
  abstract = "In a vehicular edge computing (VEC) system, vehicles can share
              their surplus computation resources to provide cloud computing
              services. The highly dynamic environment of the vehicular network
              makes it challenging to guarantee the task offloading delay. To
              this end, we introduce task replication to the VEC system, where
              the replicas of a task are offloaded to multiple vehicles at the
              same time, and the task is completed upon the first response
              among replicas. First, the impact of the number of task replicas
              on the offloading delay is characterized, and the optimal number
              of task replicas is approximated in closed-form. Based on the
              analytical result, we design a learning-based task replication
              algorithm (LTRA) with combinatorial multi-armed bandit theory,
              which works in a distributed manner and can automatically adapt
              itself to the dynamics of the VEC system. A realistic traffic
              scenario is used to evaluate the delay performance of the
              proposed algorithm. Results show that, under our simulation
              settings, LTRA with an optimized number of task replicas can
              reduce the average offloading delay by over 30\% compared to the
              benchmark without task replication, and at the same time can
              improve the task completion ratio from 97\% to 99.6\%.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  2,
  pages    = "1138--1151",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/S/Sun et al. 2021 - Distributed Task Replication for Vehicular Edge Computing - Performance Analysis and Learning-Based Algorithm.pdf",
  keywords = "Task analysis;Delays;Heuristic algorithms;Wireless
              communication;Vehicle dynamics;Edge computing;Cloud
              computing;Vehicular edge computing;computation task
              offloading;task replication;online learning;combinatorial
              multi-armed bandit;5G6G",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3030889"
}

@ARTICLE{Xu2021-xd,
  title    = "Client Selection and Bandwidth Allocation in Wireless Federated
              Learning Networks: A {Long-Term} Perspective",
  author   = "Xu, Jie and Wang, Heqiang",
  abstract = "This paper studies federated learning (FL) in a classic wireless
              network, where learning clients share a common wireless link to a
              coordinating server to perform federated model training using
              their local data. In such wireless federated learning networks
              (WFLNs), optimizing the learning performance depends crucially on
              how clients are selected and how bandwidth is allocated among the
              selected clients in every learning round, as both radio and
              client energy resources are limited. While existing works have
              made some attempts to allocate the limited wireless resources to
              optimize FL, they focus on the problem in individual learning
              rounds, overlooking an inherent yet critical feature of federated
              learning. This paper brings a new long-term perspective to
              resource allocation in WFLNs, realizing that learning rounds are
              not only temporally interdependent but also have varying
              significance towards the final learning outcome. To this end, we
              first design data-driven experiments to show that different
              temporal client selection patterns lead to considerably different
              learning performance. With the obtained insights, we formulate a
              stochastic optimization problem for joint client selection and
              bandwidth allocation under long-term client energy constraints,
              and develop a new algorithm that utilizes only currently
              available wireless channel information but can achieve long-term
              performance guarantee. Experiments show that our algorithm
              results in the desired temporal client selection pattern, is
              adaptive to changing network environments and far outperforms
              benchmarks that ignore the long-term effect of FL.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  2,
  pages    = "1188--1200",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/X/Xu and Wang 2021 - Client Selection and Bandwidth Allocation in Wireless Federated Learning Networks - A Long-Term Perspective.pdf",
  keywords = "Bandwidth;Servers;Wireless networks;Channel allocation;Data
              models;Training;Federated learning (FL);client selection;resource
              allocation;wireless networks;MLNetworking",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3031503"
}

@ARTICLE{Maatouk2021-cv,
  title    = "On the Optimality of the Whittle's Index Policy for Minimizing
              the Age of Information",
  author   = "Maatouk, Ali and Kriouile, Saad and Assad, Mohamad and
              Ephremides, Anthony",
  abstract = "In this article, we consider the average age minimization problem
              where a central entity schedules M users among the N available
              users for transmission over unreliable channels. It is well-known
              that obtaining the optimal policy, in this case, is a difficult
              task. Accordingly, the Whittle's index policy has been suggested
              in earlier works as a heuristic for this problem. However, the
              analysis of its performance remained elusive. In the sequel, we
              overcome these difficulties and provide rigorous results on its
              asymptotic optimality in the many-users regime. Specifically, we
              first establish its optimality in the neighborhood of a specific
              system's state. Next, we extend our proof to the global case
              under a recurrence assumption, which we verify numerically. These
              findings showcase that the Whittle's index policy has
              analytically provable optimality in the many-users regime for the
              AoI minimization problem. Finally, numerical results that
              showcase its performance and corroborate our theoretical findings
              are presented.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  2,
  pages    = "1263--1277",
  month    =  feb,
  year     =  2021,
  file     = "All Papers/M/Maatouk et al. 2021 - On the Optimality of the Whittle's Index Policy for Minimizing the Age of Information.pdf",
  keywords = "Indexes;Measurement;Scheduling;Wireless
              communication;Minimization;Numerical models;Information age;Age
              of Information (AoI);status updates;broadcast
              networks;age-optimal scheduling;Whittle's index
              policy;MLNetworking",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3032237"
}

@ARTICLE{Yang2021-gj,
  title    = "Energy Efficient Federated Learning Over Wireless Communication
              Networks",
  author   = "Yang, Zhaohui and Chen, Mingzhe and Saad, Walid and Hong, Choong
              Seon and Shikh-Bahaei, Mohammad",
  abstract = "In this paper, the problem of energy efficient transmission and
              computation resource allocation for federated learning (FL) over
              wireless communication networks is investigated. In the
              considered model, each user exploits limited local computational
              resources to train a local FL model with its collected data and,
              then, sends the trained FL model to a base station (BS) which
              aggregates the local FL model and broadcasts it back to all of
              the users. Since FL involves an exchange of a learning model
              between users and the BS, both computation and communication
              latencies are determined by the learning accuracy level.
              Meanwhile, due to the limited energy budget of the wireless
              users, both local computation energy and transmission energy must
              be considered during the FL process. This joint learning and
              communication problem is formulated as an optimization problem
              whose goal is to minimize the total energy consumption of the
              system under a latency constraint. To solve this problem, an
              iterative algorithm is proposed where, at every step, closed-form
              solutions for time allocation, bandwidth allocation, power
              control, computation frequency, and learning accuracy are
              derived. Since the iterative algorithm requires an initial
              feasible solution, we construct the completion time minimization
              problem and a bisection-based algorithm is proposed to obtain the
              optimal solution, which is a feasible solution to the original
              energy minimization problem. Numerical results show that the
              proposed algorithms can reduce up to 59.5\% energy consumption
              compared to the conventional FL method.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  3,
  pages    = "1935--1949",
  month    =  mar,
  year     =  2021,
  file     = "All Papers/Y/Yang et al. 2021 - Energy Efficient Federated Learning Over Wireless Communication Networks.pdf",
  keywords = "Wireless communication;Computational
              modeling;Training;Minimization;Wireless sensor networks;Resource
              management;Data models;Federated learning;resource
              allocation;energy efficiency;MLNetworking",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3037554"
}

@ARTICLE{Murti2021-us,
  title    = "An Optimal Deployment Framework for {Multi-Cloud} Virtualized
              Radio Access Networks",
  author   = "Murti, Fahri Wisnu and Ayala-Romero, Jose A and Garcia-Saavedra,
              Andres and Costa-P{\'e}rez, Xavier and Iosifidis, George",
  abstract = "Virtualized radio access networks (vRAN) are emerging as a key
              component of wireless cellular networks, and it is therefore
              imperative to optimize their architecture. vRANs are
              decentralized systems where the Base Station (BS) functions can
              be split between the edge Distributed Units (DUs) and Cloud
              computing Units (CUs); hence they have many degrees of design
              freedom. We propose a framework for optimizing the number and
              location of CUs, the function split for each BS, and the
              association and routing for each DU-CU pair. We combine a
              linearization technique with a cutting-planes method to expedite
              the exact problem solution. The goal is to minimize the network
              costs and balance them with the criterion of centralization,
              i.e., the number of functions placed at CUs. Using data-driven
              simulations we find that multi-CU vRANs achieve cost savings up
              to 28\% and improve centralization by 77\%, compared to single-CU
              vRANs. Interestingly, we see non-trivial trade-offs among
              centralization and cost, which can be aligned or conflicting
              based on the traffic and network parameters. Our work sheds light
              on the vRAN design problem from a new angle, highlights the
              importance of deploying multiple CUs, and offers a rigorous
              optimization tool for balancing costs and performance.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  4,
  pages    = "2251--2265",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/M/Murti et al. 2021 - An Optimal Deployment Framework for Multi-Cloud Virtualized Radio Access Networks.pdf",
  keywords = "Copper;Routing;Computer architecture;Cloud
              computing;Servers;Radio frequency;Topology;Radio access networks
              (RANs);wireless networks;pareto optimization;network
              virtualization;Wireless",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3040791"
}

@ARTICLE{Han2021-nl,
  title    = "Hierarchical Broadcast Coding: Expediting Distributed Learning at
              the Wireless Edge",
  author   = "Han, Dong-Jun and Sohn, Jy-Yong and Moon, Jaekyun",
  abstract = "Distributed learning plays a key role in reducing the training
              time of modern deep neural networks with massive datasets. In
              this article, we consider a distributed learning problem where
              gradient computation is carried out over a number of computing
              devices at the wireless edge. We propose hierarchical broadcast
              coding, a provable coding-theoretic framework to speed up
              distributed learning at the wireless edge. Our contributions are
              threefold. First, motivated by the hierarchical nature of
              real-world edge computing systems, we propose a layered code
              which mitigates the effects of not only packet losses at the
              wireless computing nodes but also straggling access points (APs)
              or small base stations. Second, by strategically allocating data
              partitions to nodes in the overlapping areas between cells, our
              technique achieves the fundamental lower bound on computational
              load to combat stragglers. Finally, we take advantage of the
              broadcast nature of wireless networks by which wireless devices
              in the overlapping cell coverage broadcast to more than one AP.
              This further reduces the overall training time in the presence of
              straggling APs. Experimental results on Amazon EC2 confirm the
              advantage of the proposed methods in speeding up learning. Our
              design targets any gradient descent based learning algorithms,
              including linear/logistic regressions and deep learning.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  4,
  pages    = "2266--2281",
  month    =  apr,
  year     =  2021,
  keywords = "Encoding;Wireless communication;Training;Distributed
              databases;Computational modeling;Servers;Data models;Distributed
              learning;gradient descent;wireless edge;stragglers;MLNetworking",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3040792"
}

@ARTICLE{Buyukates2021-sq,
  title    = "Scaling Laws for Age of Information in Wireless Networks",
  author   = "Buyukates, Baturalp and Soysal, Alkan and Ulukus, Sennur",
  abstract = "We study age of information in a multiple source-multiple
              destination setting with a focus on its scaling in large wireless
              networks. There are $n$ nodes uniformly and independently
              distributed on a fixed area that are randomly paired with each
              other to form $n$ source-destination (S-D) pairs. Each source
              node wants to keep its destination node as up-to-date as
              possible. To accommodate successful communication between all $n$
              S-D pairs, we first propose a three-phase transmission scheme
              which utilizes local cooperation between the nodes along with
              what we call mega update packets to serve multiple S-D pairs at
              once. We show that under the proposed scheme average age of an
              S-D pair scales as $O\left(n^\frac 14\log n\right)$ as the number
              of users, $n$ , in the network grows. Next, we observe that
              communications that take place in Phases I and III of the
              proposed scheme are scaled-down versions of network-level
              communications. With this along with scale-invariance of the
              system, we introduce hierarchy to improve this scaling result and
              show that when hierarchical cooperation between users is
              utilized, an average age scaling of $O(n^\alpha (h)\log n)$
              per-user is achievable, where $h$ denotes the number of hierarchy
              levels and $\alpha (h) = \frac 13\cdot 2^h+1$ . We note that
              $\alpha (h)$ tends to 0 as $h$ increases, and asymptotically, the
              average age scaling of the proposed hierarchical scheme is
              $O(\log n)$ . To the best of our knowledge, this is the best
              average age scaling result in a status update system with
              multiple S-D pairs.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  4,
  pages    = "2413--2427",
  month    =  apr,
  year     =  2021,
  keywords = "Information age;Throughput;Delays;Wireless networks;Random
              variables;Spread spectrum communication;Social networking
              (online);Age of information;scaling laws;large
              networks;hierarchical cooperation;Wireless",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3042169"
}

@ARTICLE{Alizadeh2021-jl,
  title    = "Distributed User Association in {B5G} Networks Using Early
              Acceptance Matching Game",
  author   = "Alizadeh, Alireza and Vu, Mai",
  abstract = "We study distributed user association in 5G and beyond
              millimeter-wave enabled heterogeneous networks using matching
              theory. We propose a novel and efficient distributed matching
              game, called early acceptance (EA), which allows users to apply
              for association with their ranked-preference base station in a
              distributed fashion and get accepted as soon as they are in the
              base station's preference list with available quota. Several
              variants of the EA matching game with preference list updating
              and reapplying are compared with the original and
              stability-optimal deferred acceptance (DA) matching game, which
              implements a waiting list at each base station and delays user
              association until the game finishes. We show that matching
              stability needs not lead to optimal performance in other metrics
              such as throughput. Analysis and simulations show that compared
              to DA, the proposed EA matching games achieve higher network
              throughput while exhibiting a significantly faster association
              process. Furthermore, the EA games either playing once or
              multiple times can reach closely the network utility of a
              centralized user association while having much lower complexity.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  4,
  pages    = "2428--2441",
  month    =  apr,
  year     =  2021,
  keywords = "Games;Channel models;Interference;Array signal
              processing;Throughput;Microwave antenna arrays;Linear antenna
              arrays;Matching theory;user association;early
              acceptance;mmWave-enabled networks",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3042393"
}

@ARTICLE{Chen2021-nm,
  title    = "Convergence Time Optimization for Federated Learning Over
              Wireless Networks",
  author   = "Chen, Mingzhe and Poor, H Vincent and Saad, Walid and Cui,
              Shuguang",
  abstract = "In this paper, the convergence time of federated learning (FL),
              when deployed over a realistic wireless network, is studied. In
              particular, a wireless network is considered in which wireless
              users transmit their local FL models (trained using their locally
              collected data) to a base station (BS). The BS, acting as a
              central controller, generates a global FL model using the
              received local FL models and broadcasts it back to all users. Due
              to the limited number of resource blocks (RBs) in a wireless
              network, only a subset of users can be selected to transmit their
              local FL model parameters to the BS at each learning step.
              Moreover, since each user has unique training data samples, the
              BS prefers to include all local user FL models to generate a
              converged global FL model. Hence, the FL training loss and
              convergence time will be significantly affected by the user
              selection scheme. Therefore, it is necessary to design an
              appropriate user selection scheme that can select the users who
              can contribute toward improving the FL convergence speed more
              frequently. This joint learning, wireless resource allocation,
              and user selection problem is formulated as an optimization
              problem whose goal is to minimize the FL convergence time and the
              FL training loss. To solve this problem, a probabilistic user
              selection scheme is proposed such that the BS is connected to the
              users whose local FL models have significant effects on the
              global FL model with high probabilities. Given the user selection
              policy, the uplink RB allocation can be determined. To further
              reduce the FL convergence time, artificial neural networks (ANNs)
              are used to estimate the local FL models of the users that are
              not allocated any RBs for local FL model transmission at each
              given learning step, which enables the BS to improve the global
              model, the FL convergence speed, and the training loss.
              Simulation results show that the proposed approach can reduce the
              FL convergence time by up to 56\% and improve the accuracy of
              identifying handwritten digits by up to 3\%, compared to a
              standard FL algorithm.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  4,
  pages    = "2457--2471",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/C/Chen et al. 2021 - Convergence Time Optimization for Federated Learning Over Wireless Networks.pdf",
  keywords = "Training;Solid modeling;Wireless networks;Data models;Resource
              management;Optimization;Convergence;Federated learning; wireless
              resource allocation;probabilistic user selection;artificial
              neural networks;Wireless",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3042530"
}

@ARTICLE{Zhang2021-uv,
  title    = "Deep Reinforcement Learning for {Multi-Agent} Power Control in
              Heterogeneous Networks",
  author   = "Zhang, Lin and Liang, Ying-Chang",
  abstract = "We consider a typical heterogeneous network (HetNet), in which
              multiple access points (APs) are deployed to serve users by
              reusing the same spectrum band. Since different APs and users may
              cause severe interference to each other, advanced power control
              techniques are needed to manage the interference and enhance the
              sum-rate of the whole network. Conventional power control
              techniques first collect instantaneous global channel state
              information (CSI) and then calculate sub-optimal solutions.
              Nevertheless, it is challenging to collect instantaneous global
              CSI in the HetNet, in which global CSI typically changes fast. In
              this article, we exploit deep reinforcement learning (DRL) to
              design a multi-agent power control algorithm, which has a
              centralized-training-distributed-execution framework. To be
              specific, each AP acts as an agent with a local deep neural
              network (DNN) and we propose a multiple-actor-shared-critic
              (MASC) method to train the local DNNs separately in an online
              trial-and-error manner. With the proposed algorithm, each AP can
              independently use the local DNN to control the transmit power
              with only local observations. Simulations results show that the
              proposed algorithm outperforms the conventional power control
              algorithms in terms of both the converged average sum-rate and
              the computational complexity.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  4,
  pages    = "2551--2564",
  month    =  apr,
  year     =  2021,
  file     = "All Papers/Z/Zhang and Liang 2021 - Deep Reinforcement Learning for Multi-Agent Power Control in Heterogeneous Networks.pdf",
  keywords = "Power control;Wireless communication;Resource
              management;Interference;Heuristic algorithms;Rayleigh
              channels;Reinforcement learning;DRL;multi-agent;power
              control;MASC;HetNet;Wireless;Mobile\_Wireless",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3043009"
}

@ARTICLE{Zhang2021-xb,
  title    = "Distributed {Multi-Cloud} {Multi-Access} Edge Computing by
              {Multi-Agent} Reinforcement Learning",
  author   = "Zhang, Yutong and Di, Boya and Zheng, Zijie and Lin, Jinlong and
              Song, Lingyang",
  abstract = "In this paper, we consider a three-layer distributed multi-access
              edge computing (MEC) network where multiple clouds, MEC servers,
              and edge devices (EDs) are deployed at the top layer, middle
              layer, and bottom layer, respectively. Each cloud center (CC) is
              associated with an independent service provider and publishes an
              application-driven computing task. To deliver the tasks, CCs rely
              on EDs to generate the raw data and offload part of the computing
              tasks to both EDs and MEC servers such that their computing and
              transmission resources can be fully utilized to reduce the system
              latency. However, in such a three-layer network, the distributed
              deployment of tasks leads to inevitable resource competition
              among CCs. To address this issue, we propose a distributed scheme
              based on multi-agent reinforcement learning, where each CC
              jointly determines the task offloading and resource allocation
              strategy based on its inference of other CCs' decisions.
              Simulation results indicate that a lower system latency is
              achieved via our proposed scheme compared with the existing
              schemes. In addition, the influence of the number of CCs, MEC
              servers, and EDs on latency performance is also discussed.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  4,
  pages    = "2565--2578",
  month    =  apr,
  year     =  2021,
  keywords = "Task analysis;Servers;Wireless communication;Resource
              management;Data processing;Cloud computing;Reinforcement
              learning;Distributed;multi-access edge
              computing;multi-cloud;multi-agent reinforcement
              learning;EdgeFogCloudIoT",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3043038"
}

@ARTICLE{Zhang2021-wk,
  title    = "Optimal {BS} Deployment and User Association for {5G} Millimeter
              Wave Communication Networks",
  author   = "Zhang, Yue and Dai, Lin and Wong, Eric W M",
  abstract = "Although millimeter wave (mmWave) communications can well support
              high-data-rate transmissions, the inherent shortcomings, e.g.,
              high path loss and sensitivity to blockage, may cause severe
              outage problems if the network is not configured properly. This
              paper aims to minimize the long-term outage probability of an
              mmWave communication network by optimizing the base station (BS)
              deployment and user association. For the BS deployment problem,
              existing works usually assumed that the positions of users are
              fixed and formulated it as a deterministic optimization problem.
              With the time-varying nature of positions of user equipments
              (UEs) taken into account, we establish a stochastic optimization
              framework for BS deployment optimization. The objective is to
              maximize the average number of physically accessible BSs of each
              UE under an inaccessible probability constraint, and a
              cooperative stochastic approximation (CSA)-based algorithm is
              developed to effectively search the optimal positions of BSs. For
              user association, our focus is to properly associate UEs with BSs
              to minimize the outage probability with balanced workloads among
              BSs. Combined with the proposed user association scheme, the
              proposed BS deployment scheme can significantly improve the
              network outage probability in the long term, especially when the
              aggregation degree of UEs is large.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  5,
  pages    = "2776--2791",
  month    =  may,
  year     =  2021,
  keywords = "Optimization;Probability;Power system reliability;Millimeter wave
              communication;Wireless communication;Approximation
              algorithms;Shape;Millimeter wave (mmWave) networks;base-station
              (BS) deployment;user association;outage probability;stochastic
              optimization;Wireless",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3044288"
}

@ARTICLE{Elsayed2021-wr,
  title    = "Transfer Reinforcement Learning for {5G} New Radio mmWave
              Networks",
  author   = "Elsayed, Medhat and Erol-Kantarci, Melike and Yanikomeroglu,
              Halim",
  abstract = "In this paper, we aim at interference mitigation in 5G
              millimeter-Wave (mm-Wave) communications by employing beamforming
              and Non-Orthogonal Multiple Access (NOMA) techniques with the aim
              of improving network's aggregate rate. Despite the potential
              capacity gains of mm-Wave and NOMA, many technical challenges
              might hinder that performance gain. In particular, the
              performance of Successive Interference Cancellation (SIC)
              diminishes rapidly as the number of users increases per beam,
              which leads to higher intra-beam interference. Furthermore,
              intersection regions between adjacent cells give rise to
              inter-beam inter-cell interference. To mitigate both interference
              levels, optimal selection of the number of beams in addition to
              best allocation of users to those beams is essential. In this
              paper, we address the problem of joint user-cell association and
              selection of number of beams for the purpose of maximizing the
              aggregate network capacity. We propose three machine
              learning-based algorithms; transfer Q-learning (TQL), Q-learning,
              and Best SINR association with Density-based Spatial Clustering
              of Applications with Noise (BSDC) algorithms and compare their
              performance under different scenarios. Under mobility, TQL and
              Q-learning demonstrate 12\% rate improvement over BSDC at the
              highest offered traffic load. For stationary scenarios,
              Q-learning and BSDC outperform TQL, however TQL achieves about
              29\% convergence speedup compared to Q-learning.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  5,
  pages    = "2838--2849",
  month    =  may,
  year     =  2021,
  keywords = "Reinforcement learning;Task analysis;Resource
              management;Interference;NOMA;Array signal processing;Silicon
              carbide;5G-NR;beamforming;mm-Wave;Q-learning;reinforcement
              learning;transfer learning;Wireless",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3044597"
}

@ARTICLE{Zhao2021-vr,
  title    = "Novel Online Sequential {Learning-Based} Adaptive Routing for
              Edge {Software-Defined} Vehicular Networks",
  author   = "Zhao, Liang and Zhao, Weiliang and Hawbani, Ammar and Al-Dubai,
              Ahmed Y and Min, Geyong and Zomaya, Albert Y and Gong, Changqing",
  abstract = "To provide efficient networking services at the edge of
              Internet-of-Vehicles (IoV), Software-Defined Vehicular Network
              (SDVN) has been a promising technology to enable intelligent data
              exchange without giving additional duties to the resource
              constrained vehicles. Compared with conventional centralized
              SDVNs, hybrid SDVNs combine the centralized control of SDVNs and
              self-organized distributed routing of Vehicular Ad-hoc NETworks
              (VANETs) to mitigate the burden on the central controller caused
              by the frequent uplink and downlink transmissions. Although a
              wide variety of routing protocols have been developed, existing
              protocols are designed for specific scenarios without considering
              flexibility and adaptivity in dynamic vehicular networks. To
              address this problem, we propose an efficient online sequential
              learning-based adaptive routing scheme, namely, Penicillium
              reproduction-based Online Learning Adaptive Routing scheme
              (POLAR) for hybrid SDVNs. By utilizing the computational power of
              edge servers, this scheme can dynamically select a routing
              strategy for a specific traffic scenario by learning the pattern
              from network traffic. Firstly, this paper applies Geohash to
              divide the large geographical area into multiple grids, which
              facilitates the collection and processing of real-time traffic
              data for regional management in controller. Secondly, a new
              Penicillium Reproduction Algorithm (PRA) with outstanding
              optimization capabilities is designed to improve the learning
              effectiveness of Online Sequential Extreme Learning Machine
              (OS-ELM). Finally, POLAR is deployed in control plane to generate
              decision-making model (i.e., routing policy). Based on the
              real-time featured data, this scheme can choose the optimal
              routing strategy for a specific area. Extensive simulation
              results show that POLAR is superior to a single traditional
              routing protocol in terms of packet delivery ratio and latency.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  5,
  pages    = "2991--3004",
  month    =  may,
  year     =  2021,
  keywords = "Routing;Particle swarm optimization;Optimization;Wireless
              communication;Adaptive systems;Decision making;Routing
              protocols;Hybrid SDVNs;VANETs;adaptive routing scheme;penicillium
              reproduction algorithm;OS-ELM",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3046275"
}

@ARTICLE{Yang2021-jn,
  title    = "Multicast {eMBB} and Bursty {URLLC} Service Multiplexing in a
              {CoMP-Enabled} {RAN}",
  author   = "Yang, Peng and Xi, Xing and Fu, Yaru and Quek, Tony Q S and Cao,
              Xianbin and Wu, Dapeng",
  abstract = "This paper is concerned with slicing a radio access network (RAN)
              for simultaneously serving two 5G-and-Beyond typical use cases,
              i.e., enhanced mobile broadband (eMBB) and ultra-reliable and
              low-latency communications (URLLC). Although many researches have
              been conducted to tackle this issue, few of them have considered
              the impact of bursty URLLC. The bursty characteristic of URLLC
              traffic may significantly increase the difficulty of RAN slicing
              in terms of ensuring an ultra-low packet blocking probability. To
              reduce the probability, we re-visit the structure of physical
              resource blocks orchestrated for URLLC traffic based on
              theoretical results. Meanwhile, we formulate the problem of
              slicing a RAN enabling coordinated multi-point (CoMP)
              transmissions for multicast eMBB and bursty URLLC service
              multiplexing as a multi-timescale optimization problem aiming at
              maximizing eMBB and URLLC slice utilities, subject to physical
              resource constraints. To mitigate this problem, we transform it
              into multiple single timescale problems by exploring sample
              average approximations. An iterative algorithm with provable
              performance guarantees is developed to obtain solutions to these
              single timescale problems and aggregate obtained solutions into
              those of the multi-timescale problem. We also design a
              CoMP-enabled RAN slicing system prototype and compare the
              iterative algorithm with the state-of-the-art algorithm to verify
              its effectiveness.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  5,
  pages    = "3061--3077",
  month    =  may,
  year     =  2021,
  file     = "All Papers/Y/Yang et al. 2021 - Multicast eMBB and Bursty URLLC Service Multiplexing in a CoMP-Enabled RAN.pdf",
  keywords = "Ultra reliable low latency communication;Multiplexing;Quality of
              service;Time-frequency analysis;Resource
              management;Reliability;Optimization;RAN slicing;multicast
              eMBB;bursty URLLC;service multiplexing;coordinated multi-point
              transmission",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3047263"
}

@ARTICLE{Bian2021-ce,
  title    = "A General {3D} {Non-Stationary} Wireless Channel Model for {5G}
              and Beyond",
  author   = "Bian, Ji and Wang, Cheng-Xiang and Gao, Xiqi and You, Xiaohu and
              Zhang, Minggao",
  abstract = "In this paper, a novel three-dimensional (3D) non-stationary
              geometry-based stochastic model (GBSM) for the fifth generation
              (5G) and beyond 5G (B5G) systems is proposed. The proposed B5G
              channel model (B5GCM) is designed to capture various channel
              characteristics in (B)5G systems such as space-time-frequency
              (STF) non-stationarity, spherical wavefront (SWF), high delay
              resolution, time-variant velocities and directions of motion of
              the transmitter, receiver, and scatterers, spatial consistency,
              etc. By combining different channel properties into a general
              channel model framework, the proposed B5GCM is able to be applied
              to multiple frequency bands and multiple scenarios, including
              massive multiple-input multiple-output (MIMO), vehicle-to-vehicle
              (V2V), high-speed train (HST), and millimeter wave-terahertz
              (mmWave-THz) communication scenarios. Key statistics of the
              proposed B5GCM are obtained and compared with those of standard
              5G channel models and corresponding measurement data, showing the
              generalization and usefulness of the proposed model.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  5,
  pages    = "3211--3224",
  month    =  may,
  year     =  2021,
  file     = "All Papers/B/Bian et al. 2021 - A General 3D Non-Stationary Wireless Channel Model for 5G and Beyond.pdf",
  keywords = "3D space-time-frequency non-stationary GBSM;massive
              MIMO;mmWave-THz;high-mobility;multi-mobility
              communications;Channels",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3047973"
}

@ARTICLE{Yang2021-xh,
  title    = "Understanding Age of Information in {Large-Scale} Wireless
              Networks",
  author   = "Yang, Howard H and Xu, Chao and Wang, Xijun and Feng, Daquan and
              Quek, Tony Q S",
  abstract = "The notion of age-of-information (AoI) is investigated in the
              context of large-scale wireless networks, in which transmitters
              need to send a sequence of information packets, which are
              generated as independent Bernoulli processes, to their intended
              receivers over a shared spectrum. Due to interference, the rate
              of packet depletion at any given node is entangled with both the
              spatial configurations, which determine the path loss, and
              temporal dynamics, which influence the active states, of the
              other transmitters, resulting in the queues to interact with each
              other in both space and time over the entire network. To that
              end, variants in the packet update frequency affect not just the
              inter-arrival time but also the departure process, and the impact
              of such phenomena on the AoI is not well understood. In this
              paper, we establish a theoretical framework to characterize the
              AoI performance in the aforementioned setting. Particularly,
              tractable expressions are derived for both the peak and average
              AoI under two different transmission protocols, namely the
              first-come-first-serve (FCFS) and the last-come-first-serve with
              preemption (LCFS-PR). Additionally, our analysis also accounts
              for the effects of channel access controls such as ALOHA on the
              AoI. The accuracy of the analysis is verified via simulations,
              and based on the theoretical outcomes, we find that: $i$ )
              networks operating under LCFS-PR are able to attain smaller
              values of peak and average AoI than that under FCFS, whereas the
              gain is more pronounced when the infrastructure is densely
              deployed, $ii$ ) in sparsely deployed networks, ALOHA with a
              universally designed channel access probability is not
              instrumental in reducing the AoI, thus calling for more advanced
              channel access approaches, and $iii$ ) when the infrastructure is
              densely rolled out, there exists a non-trivial ALOHA channel
              access probability that minimizes the peak and average AoI under
              both FCFS and LCFS-PR.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  5,
  pages    = "3196--3210",
  month    =  may,
  year     =  2021,
  file     = "All Papers/Y/Yang et al. 2021 - Understanding Age of Information in Large-Scale Wireless Networks.pdf",
  keywords = "Transmitters;Interference;Receivers;Wireless networks;Queueing
              analysis;Protocols;Analytical models;Poisson bipolar network;age
              of information;transmission protocol;spatially interacting
              queues;stochastic geometry;Wireless",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2020.3048008"
}

@ARTICLE{Naderializadeh2021-ff,
  title    = "Resource Management in Wireless Networks via {Multi-Agent} Deep
              Reinforcement Learning",
  author   = "Naderializadeh, Navid and Sydir, Jaroslaw J and Simsek, Meryem
              and Nikopour, Hosein",
  abstract = "We propose a mechanism for distributed resource management and
              interference mitigation in wireless networks using multi-agent
              deep reinforcement learning (RL). We equip each transmitter in
              the network with a deep RL agent that receives delayed
              observations from its associated users, while also exchanging
              observations with its neighboring agents, and decides on which
              user to serve and what transmit power to use at each scheduling
              interval. Our proposed framework enables agents to make decisions
              simultaneously and in a distributed manner, unaware of the
              concurrent decisions of other agents. Moreover, our design of the
              agents' observation and action spaces is scalable, in the sense
              that an agent trained on a scenario with a specific number of
              transmitters and users can be applied to scenarios with different
              numbers of transmitters and/or users. Simulation results
              demonstrate the superiority of our proposed approach compared to
              decentralized baselines in terms of the tradeoff between average
              and 5th percentile user rates, while achieving performance close
              to, and even in certain cases outperforming, that of a
              centralized information-theoretic baseline. We also show that our
              trained agents are robust and maintain their performance gains
              when experiencing mismatches between train and test deployments.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  6,
  pages    = "3507--3523",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/N/Naderializadeh et al. 2021 - Resource Management in Wireless Networks via Multi-Agent Deep Reinforcement Learning.pdf",
  keywords = "Wireless networks;Resource management;Power control;Wireless
              communication;Radio transmitters;Interference;Downlink;Radio
              resource management;interference mitigation;deep neural
              networks;multi-agent deep reinforcement learning;centralized
              training and distributed execution;MLNetworking;Mobile\_Wireless",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2021.3051163"
}

@ARTICLE{Amiri2021-mj,
  title    = "Convergence of Update Aware Device Scheduling for Federated
              Learning at the Wireless Edge",
  author   = "Amiri, Mohammad Mohammadi and G{\"u}nd{\"u}z, Deniz and Kulkarni,
              Sanjeev R and Poor, H Vincent",
  abstract = "We study federated learning (FL) at the wireless edge, where
              power-limited devices with local datasets collaboratively train a
              joint model with the help of a remote parameter server (PS). We
              assume that the devices are connected to the PS through a
              bandwidth-limited shared wireless channel. At each iteration of
              FL, a subset of the devices are scheduled to transmit their local
              model updates to the PS over orthogonal channel resources, while
              each participating device must compress its model update to
              accommodate to its link capacity. We design novel scheduling and
              resource allocation policies that decide on the subset of the
              devices to transmit at each round, and how the resources should
              be allocated among the participating devices, not only based on
              their channel conditions, but also on the significance of their
              local model updates. We then establish convergence of a wireless
              FL algorithm with device scheduling, where devices have limited
              capacity to convey their messages. The results of numerical
              experiments show that the proposed scheduling policy, based on
              both the channel conditions and the significance of the local
              model updates, provides a better long-term performance than
              scheduling policies based only on either of the two metrics
              individually. Furthermore, we observe that when the data is
              independent and identically distributed (i.i.d.) across devices,
              selecting a single device at each round provides the best
              performance, while when the data distribution is non-i.i.d.,
              scheduling multiple devices at each round improves the
              performance. This observation is verified by the convergence
              result, which shows that the number of scheduled devices should
              increase for a less diverse and more biased data distribution.",
  journal  = "IEEE Trans. Wireless Commun.",
  volume   =  20,
  number   =  6,
  pages    = "3643--3658",
  month    =  jun,
  year     =  2021,
  file     = "All Papers/A/Amiri et al. 2021 - Convergence of Update Aware Device Scheduling for Federated Learning at the Wireless Edge.pdf",
  keywords = "Performance evaluation;Convergence;Bandwidth;Wireless
              networks;Propagation losses;Servers;Wireless sensor
              networks;Federated learning;update aware device
              selection;stochastic gradient descent;MLNetworking",
  issn     = "1558-2248",
  doi      = "10.1109/TWC.2021.3052681"
}

@INPROCEEDINGS{Sener2020-on,
  title     = "Delivering Machine Learning Applications via Cloud Platforms: An
               Experience Report",
  booktitle = "2020 Turkish National Software Engineering Symposium ({UYMS})",
  author    = "Sener, Yigit and Yetim, Hasan Fahri and Bagriyanik, Selami",
  abstract  = "Cloud technologies enable developers and organizations to focus
               on their product, without having to consider issues such as
               local server capacity, infrastructure modifications, data
               security, licensing or human capital. This paper attempts to
               explain a case in which a Machine Learning application is
               deployed via Amazon Web Services (AWS) tools. In doing so, it
               demonstrates the reasoning behind choosing a cloud-based
               environment instead of on-premise sources, by putting forward
               the advantages of the former. On the other hand, it should be
               noted that the application in this experience is generated with
               a hybrid approach: It is developed using on-premise
               infrastructure and then moved to the Cloud environment for the
               deployment phase only. In this regard, it can be read as a PaaS
               experience. This study is considered to be a beneficial guide
               for entrepreneurs and start-ups on a budget who aim at launching
               their products in a swift and scalable manner.",
  pages     = "1--3",
  month     =  oct,
  year      =  2020,
  keywords  = "Cloud computing;Web services;Buildings;Machine
               learning;Tools;Maintenance engineering;Software
               engineering;cloud computing;machine learning;productization;MVP
               (Minimum Viable Product);agility;entrepreneurship;PaaS",
  doi       = "10.1109/UYMS50627.2020.9247050"
}

@INPROCEEDINGS{Gamal2019-mf,
  title     = "Mapping and Scheduling for {Non-Uniform} Arrival of Virtual
               Network Function ({VNF}) Requests",
  booktitle = "2019 {IEEE} 90th Vehicular Technology Conference
               ({VTC2019-Fall})",
  author    = "Gamal, M and Jafarizadeh, S and Abolhasan, M and Lipman, J and
               Ni, W",
  abstract  = "As a new research concept for both academia and industry, there
               are several challenges faced by the Network Function
               Virtualization (NFV). One such challenge is to find the optimal
               mapping and scheduling for the incoming service requests which
               is the focus of this study. This optimization has been done by
               maximizing the number of accepted service requests, minimizing
               the number of bottleneck links and the overall processing time.
               The resultant problem is formulated as a multi- objective
               optimization problem, and two novel algorithms based on genetic
               algorithm have been developed. Through simulations, it has been
               shown that the developed algorithms can converge to the near to
               optimal solutions and they are scalable to large networks.",
  pages     = "1--6",
  month     =  sep,
  year      =  2019,
  keywords  = "genetic algorithms;resource allocation;software defined
               networking;telecommunication
               scheduling;virtualisation;NFV;genetic algorithm;multi objective
               optimization problem;service request scheduling;optimal
               mapping;Network Function Virtualization;VNF;Cost
               function;Genetic algorithms;Job shop scheduling;Network function
               virtualization;Resource management;Low quality;NFV",
  issn      = "2577-2465",
  doi       = "10.1109/VTCFall.2019.8891197"
}

@INPROCEEDINGS{Kubisch2003-jh,
  title     = "Distributed algorithms for transmission power control in
               wireless sensor networks",
  booktitle = "2003 {IEEE} Wireless Communications and Networking, 2003. {WCNC}
               2003.",
  author    = "Kubisch, M and Karl, H and Wolisz, A and Zhong, L C and Rabaey,
               J",
  abstract  = "In a wireless, multi-hop sensor network, choosing transmission
               power levels has an important impact on energy efficiency and
               network lifetime. Two algorithms for dynamically adjusting
               transmission power level on a per-node basis are proposed here.
               Network lifetime, convergence speed as well as resulting network
               connectivity are used as figures of merit for these two
               algorithms. They have been evaluated in an indoor sensor
               environment. The network lifetime metrics of these two local
               algorithms are also benchmarked against power control algorithms
               using global information. We show that these local algorithms
               outperform fixed power level assignment and that the lifetime
               achieved by them is usually within a factor of two of globally
               computed solution while being scalable.",
  publisher = "ieeexplore.ieee.org",
  volume    =  1,
  pages     = "558--563 vol.1",
  month     =  mar,
  year      =  2003,
  keywords  = "distributed algorithms;power control;wireless sensor
               networks;network routing;indoor radio;distributed
               algorithms;transmission power control;wireless multihop sensor
               networks;energy efficiency;network lifetime;per-node
               adjustment;convergence speed;network connectivity;indoor sensor
               environment;benchmarking;power assignment;Distributed
               algorithms;Power control;Intelligent networks;Wireless sensor
               networks;Relays;Energy efficiency;Routing;Computer
               networks;Sensor phenomena and
               characterization;Protocols;MyPapers",
  issn      = "1525-3511",
  doi       = "10.1109/WCNC.2003.1200410"
}

@INPROCEEDINGS{Deng2021-jd,
  title     = "A Digital Twin Approach for Self-optimization of Mobile Networks",
  booktitle = "2021 {IEEE} Wireless Communications and Networking Conference
               Workshops ({WCNCW})",
  author    = "Deng, Juan and Zheng, Qingbi and Liu, Guangyi and Bai, Jielin
               and Tian, Kaicong and Sun, Changhao and Yan, Yujie and Liu,
               Yitong",
  abstract  = "Most of the methods in operators' current 5G networks use expert
               knowledge assisted by machine learning algorithms to generate
               optimization decisions. However, these methods are inadaptive to
               the dynamic changes of high-dimensional network states, thus the
               result is often suboptimal. Reinforcement learning can better
               cope with high-dimensional network state space and parameter
               space. However, when applied to real network, challenges arise
               such as difficult to obtain data samples, time-consuming and
               risky to explore real networks during model training. To solve
               these problems, this paper proposes a combined approach of
               expert knowledge, reinforcement learning and digital twin for
               the self-optimization of mobile networks. By constructing a
               digital twin of the current network, the future network state is
               predicted based on which optimization decisions are generated by
               expert knowledge and reinforcement learning respectively, and
               then input into the digital twin. Digital twin simulates their
               rewards and decides a final action for execution. Simulation
               results have confirmed that the proposed scheme can achieve
               higher rewards than either expert knowledge or reinforcement
               learning, and can avoid negative impact on real network
               performance. This paper also describes several potential
               application scenarios for the proposed approach in 6G networks
               and discusses key issues for future research.",
  pages     = "1--6",
  month     =  mar,
  year      =  2021,
  keywords  = "Knowledge engineering;Training;6G mobile communication;Machine
               learning algorithms;Digital twin;Conferences;Simulation;mobile
               network self-optimization;digital twinning;reinforcement
               learning;5G6G",
  doi       = "10.1109/WCNCW49093.2021.9420037"
}

@INPROCEEDINGS{Pawar2020-wx,
  title     = "Scalable, Reliable and Robust Data Mining Infrastructures",
  booktitle = "2020 Fourth World Conference on Smart Trends in Systems,
               Security and Sustainability ({WorldS4})",
  author    = "Pawar, Shrikant and Stanam, Aditya",
  abstract  = "Mining of data is used to analyze facts to discover formerly
               unknown patterns, classifying and grouping the records. There
               are several crucial scalable statistics mining platforms that
               have been developed in latest years. RapidMiner is a famous open
               source software which can be used for advanced analytics, Weka
               and Orange are important tools of machine learning for
               classifying patterns with techniques of clustering and
               regression, whilst Knime is often used for facts preprocessing
               like information extraction, transformation and loading. This
               article encapsulates the most important and robust platforms.",
  pages     = "123--125",
  month     =  jul,
  year      =  2020,
  keywords  = "Data mining;Data visualization;Task analysis;Robustness;Open
               source software;Machine learning;Tools;Data
               Mining;Techniques;Scalable;Robust;Infrastructure",
  doi       = "10.1109/WorldS450073.2020.9210388"
}

@ARTICLE{Bellavista2020-cn,
  title     = "{HOlistic} pRocessing and {NETworking} ({HORNET)}: An integrated
               solution for {IoT-based} fog computing services",
  author    = "Bellavista, Paolo and Giannelli, Carlo and Montenero, Dmitrij
               David Padalino and Poltronieri, Filippo and Stefanelli, Cesare
               and Tortonesi, Mauro",
  journal   = "IEEE Access",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  8,
  pages     = "66707--66721",
  year      =  2020,
  keywords  = "EdgeFogCloudIoT;EdgeFogCloudIoT",
  issn      = "2169-3536",
  doi       = "10.1109/access.2020.2984930"
}

@ARTICLE{Dayarathna2016-it,
  title     = "Data center energy consumption modeling: A survey",
  author    = "Dayarathna, Miyuru and Wen, Yonggang and Fan, Rui",
  journal   = "IEEE Commun. Surv. Tutor.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  18,
  number    =  1,
  pages     = "732--794",
  year      =  2016,
  file      = "All Papers/D/Dayarathna et al. 2016 - Data center energy consumption modeling - A survey.pdf",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/comst.2015.2481183"
}

@ARTICLE{Agiwal2016-oa,
  title     = "Next generation {5G} wireless networks: A comprehensive survey",
  author    = "Agiwal, Mamta and Roy, Abhishek and Saxena, Navrati",
  journal   = "IEEE Commun. Surv. Tutor.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  18,
  number    =  3,
  pages     = "1617--1655",
  year      =  2016,
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/comst.2016.2532458"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Karmakar2017-qg,
  title     = "Impact of {IEEE} 802.11n/ac {PHY/MAC} high throughput
               enhancements on transport and application protocols---A survey",
  author    = "Karmakar, Raja and Chattopadhyay, Samiran and Chakraborty,
               Sandip",
  journal   = "IEEE Commun. Surv. Tutor.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  19,
  number    =  4,
  pages     = "2050--2091",
  year      =  2017,
  file      = "All Papers/K/Karmakar et al. 2017 - Impact of IEEE 802.11n - ac PHY - MAC high throughput enhancements on transport and application protocols—A survey.pdf",
  keywords  = "Mobile\_Wireless;wifi",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/comst.2017.2745052"
}

@ARTICLE{Khorov2019-xp,
  title     = "A Tutorial on {IEEE} 802.11ax High Efficiency {WLANs}",
  author    = "Khorov, Evgeny and Kiryanov, Anton and Lyakhov, Andrey and
               Bianchi, Giuseppe",
  journal   = "IEEE Commun. Surv. Tutor.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  21,
  number    =  1,
  pages     = "197--216",
  year      =  2019,
  file      = "All Papers/K/Khorov et al. 2019 - A Tutorial on IEEE 802.11ax High Efficiency WLANs.pdf",
  keywords  = "Mobile\_Wireless;wifi",
  copyright = "https://creativecommons.org/licenses/by/3.0/legalcode",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/comst.2018.2871099"
}

@ARTICLE{He2019-ap,
  title     = "Flexibility in softwarized networks: Classifications and
               research challenges",
  author    = "He, Mu and Alba, Alberto Martinez and Basta, Arsany and Blenk,
               Andreas and Kellerer, Wolfgang",
  journal   = "IEEE Commun. Surv. Tutor.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  21,
  number    =  3,
  pages     = "2600--2636",
  year      =  2019,
  file      = "All Papers/H/He et al. 2019 - Flexibility in softwarized networks - Classifications and research challenges.pdf",
  keywords  = "FutureInternet;ProgrammableNetworks",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/comst.2019.2892806"
}

@ARTICLE{Luong2019-nt,
  title     = "Applications of deep reinforcement learning in communications
               and networking: A survey",
  author    = "Luong, Nguyen Cong and Hoang, Dinh Thai and Gong, Shimin and
               Niyato, Dusit and Wang, Ping and Liang, Ying-Chang and Kim, Dong
               In",
  journal   = "IEEE Commun. Surv. Tutor.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  21,
  number    =  4,
  pages     = "3133--3174",
  year      =  2019,
  file      = "All Papers/L/Luong et al. 2019 - Applications of deep reinforcement learning in communications and networking - A survey.pdf",
  keywords  = "MLAspects",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/comst.2019.2916583"
}

@ARTICLE{Li2022-dg,
  title     = "Applications of multi-agent reinforcement learning in future
               internet: A comprehensive survey",
  author    = "Li, Tianxu and Zhu, Kun and Luong, Nguyen Cong and Niyato, Dusit
               and Wu, Qihui and Zhang, Yang and Chen, Bing",
  journal   = "IEEE Commun. Surv. Tutor.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  24,
  number    =  2,
  pages     = "1240--1279",
  year      =  2022,
  file      = "All Papers/L/Li et al. 2022 - Applications of multi-agent reinforcement learning in future internet - A comprehensive survey.pdf",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/comst.2022.3160697"
}

@ARTICLE{Polese2023-xz,
  title     = "Understanding {O-RAN}: Architecture, interfaces, algorithms,
               security, and research challenges",
  author    = "Polese, Michele and Bonati, Leonardo and D'Oro, Salvatore and
               Basagni, Stefano and Melodia, Tommaso",
  journal   = "IEEE Commun. Surv. Tutor.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  25,
  number    =  2,
  pages     = "1376--1411",
  year      =  2023,
  file      = "All Papers/P/Polese et al. 2023 - Understanding O-RAN - Architecture, interfaces, algorithms, security, and research challenges.pdf",
  keywords  = "Mobile\_Wireless",
  copyright = "https://creativecommons.org/licenses/by/4.0/legalcode",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/comst.2023.3239220"
}

@INPROCEEDINGS{Benomar2018-mv,
  title           = "Extending openstack for cloud-based networking at the edge",
  booktitle       = "2018 {IEEE} International Conference on Internet of Things
                     (iThings) and {IEEE} Green Computing and Communications
                     ({GreenCom}) and {IEEE} Cyber, Physical and Social
                     Computing ({CPSCom}) and {IEEE} Smart Data ({SmartData})",
  author          = "Benomar, Zakaria and Bruneo, Dario and Distefano,
                     Salvatore and Elbaamrani, Khalid and Idboufker, Noureddine
                     and Longo, Francesco and Merlino, Giovanni and Puliafito,
                     Antonio",
  publisher       = "IEEE",
  month           =  jul,
  year            =  2018,
  keywords        = "EdgeFogCloudIoT",
  conference      = "2018 IEEE International Conference on Internet of Things
                     (iThings) and IEEE Green Computing and Communications
                     (GreenCom) and IEEE Cyber, Physical and Social Computing
                     (CPSCom) and IEEE Smart Data (SmartData)",
  location        = "Halifax, NS, Canada",
  isbn            = "9781538679753",
  doi             = "10.1109/cybermatics\_2018.2018.00058"
}

@INPROCEEDINGS{Ojo2016-if,
  title           = "A {SDN-IoT} Architecture with {NFV} Implementation",
  booktitle       = "2016 {IEEE} Globecom Workshops ({GC} Wkshps)",
  author          = "Ojo, Mike and Adami, Davide and Giordano, Stefano",
  publisher       = "IEEE",
  month           =  dec,
  year            =  2016,
  keywords        = "NFV\_SDN;EdgeFogCloudIoT",
  conference      = "2016 IEEE Globecom Workshops (GC Wkshps)",
  location        = "Washington, DC, USA",
  isbn            = "9781509024827",
  doi             = "10.1109/glocomw.2016.7848825"
}

@TECHREPORT{noauthor_2014-um,
  title       = "{IEEE} standard for local and metropolitan area networks:
                 Overview and architecture",
  institution = "IEEE",
  year        =  2014,
  file        = "All Papers/Other/IEEE standard for local and metropoli... 2014 - IEEE standard for local and metropolitan area networks - Overview and architecture.pdf",
  address     = "Piscataway, NJ, USA",
  doi         = "10.1109/ieeestd.2014.6847097"
}

@ARTICLE{Chang2021-al,
  title     = "A survey of recent advances in edge-computing-powered artificial
               intelligence of things",
  author    = "Chang, Zhuoqing and Liu, Shubo and Xiong, Xingxing and Cai,
               Zhaohui and Tu, Guoqing",
  journal   = "IEEE Internet Things J.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  8,
  number    =  18,
  pages     = "13849--13875",
  month     =  sep,
  year      =  2021,
  file      = "All Papers/C/Chang et al. 2021 - A survey of recent advances in edge-computing-powered artificial intelligence of things.pdf",
  keywords  = "MLAspects",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "2327-4662, 2372-2541",
  doi       = "10.1109/jiot.2021.3088875"
}

@ARTICLE{Shuvo2023-mx,
  title     = "Efficient acceleration of deep learning inference on
               resource-constrained edge devices: A review",
  author    = "Shuvo, Md Maruf Hossain and Islam, Syed Kamrul and Cheng,
               Jianlin and Morshed, Bashir I",
  journal   = "Proc. IEEE Inst. Electr. Electron. Eng.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  111,
  number    =  1,
  pages     = "42--91",
  month     =  jan,
  year      =  2023,
  file      = "All Papers/S/Shuvo et al. 2023 - Efficient acceleration of deep learning inference on resource-constrained edge devices - A review.pdf",
  keywords  = "MLAspects",
  copyright = "https://creativecommons.org/licenses/by/4.0/legalcode",
  issn      = "0018-9219, 1558-2256",
  doi       = "10.1109/jproc.2022.3226481"
}

@ARTICLE{Peng2021-fq,
  title     = "Multi-agent reinforcement learning based resource management in
               {MEC-} and {UAV-assisted} vehicular networks",
  author    = "Peng, Haixia and Shen, Xuemin",
  journal   = "IEEE J. Sel. Areas Commun.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  39,
  number    =  1,
  pages     = "131--141",
  month     =  jan,
  year      =  2021,
  file      = "All Papers/P/Peng and Shen 2021 - Multi-agent reinforcement learning based resource management in MEC- and UAV-assisted vehicular networks.pdf",
  keywords  = "MLNetworking;Wireless;EdgeFogCloudIoT",
  issn      = "0733-8716, 1558-0008",
  doi       = "10.1109/jsac.2020.3036962"
}

@ARTICLE{Shen2021-on,
  title     = "Graph neural networks for scalable radio resource management:
               Architecture design and theoretical analysis",
  author    = "Shen, Yifei and Shi, Yuanming and Zhang, Jun and Letaief, Khaled
               B",
  journal   = "IEEE J. Sel. Areas Commun.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  39,
  number    =  1,
  pages     = "101--115",
  month     =  jan,
  year      =  2021,
  file      = "All Papers/S/Shen et al. 2021 - Graph neural networks for scalable radio resource management - Architecture design and theoretical analysis.pdf",
  keywords  = "MLNetworking;Wireless",
  issn      = "0733-8716, 1558-0008",
  doi       = "10.1109/jsac.2020.3036965"
}

@ARTICLE{Oh2019-vh,
  title     = "Wireless Backhaul Based on {IEEE} 802.11ac With Smart
               Beamforming",
  author    = "Oh, Youngseok and Cho, Sungmin and Yu, Heejung",
  journal   = "IEEE Syst. J.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  13,
  number    =  3,
  pages     = "2354--2362",
  month     =  sep,
  year      =  2019,
  file      = "All Papers/O/Oh et al. 2019 - Wireless Backhaul Based on IEEE 802.11ac With Smart Beamforming.pdf",
  keywords  = "Mobile\_Wireless;wifi",
  issn      = "1932-8184, 1937-9234",
  doi       = "10.1109/jsyst.2019.2898883"
}

@ARTICLE{Nitsche2014-dp,
  title     = "{IEEE} 802.11ad: directional 60 {GHz} communication for
               multi-Gigabit-per-second {Wi-Fi} [Invited Paper]",
  author    = "Nitsche, Thomas and Cordeiro, Carlos and Flores, Adriana and
               Knightly, Edward and Perahia, Eldad and Widmer, Joerg",
  journal   = "IEEE Commun. Mag.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  52,
  number    =  12,
  pages     = "132--141",
  month     =  dec,
  year      =  2014,
  file      = "All Papers/N/Nitsche et al. 2014 - IEEE 802.11ad - directional 60 GHz communication for multi-Gigabit-per-second Wi-Fi [Invited Paper].pdf",
  keywords  = "Mobile\_Wireless;wifi",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "0163-6804, 1558-1896",
  doi       = "10.1109/mcom.2014.6979964"
}

@ARTICLE{Bjornson2014-pv,
  title     = "Optimal multiuser transmit beamforming: A difficult problem with
               a simple solution structure [lecture notes]",
  author    = "Bjornson, Emil and Bengtsson, Mats and Ottersten, Bjorn",
  journal   = "IEEE Signal Process. Mag.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  31,
  number    =  4,
  pages     = "142--148",
  month     =  jul,
  year      =  2014,
  annote    = "https://github.com/emilbjornson/optimal-beamforming",
  file      = "All Papers/B/Bjornson et al. 2014 - Optimal multiuser transmit beamforming - A difficult problem with a simple solution structure [lecture notes].pdf",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "1053-5888, 1558-0792",
  doi       = "10.1109/msp.2014.2312183"
}

@ARTICLE{Charfi2013-ro,
  title     = "{PHY/MAC} enhancements and {QoS} mechanisms for very high
               throughput {WLANs}: A survey",
  author    = "Charfi, Emna and Chaari, Lamia and Kamoun, Lotfi",
  journal   = "IEEE Commun. Surv. Tutor.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  15,
  number    =  4,
  pages     = "1714--1735",
  year      =  2013,
  file      = "All Papers/C/Charfi et al. 2013 - PHY - MAC enhancements and QoS mechanisms for very high throughput WLANs - A survey.pdf",
  keywords  = "Mobile\_Wireless;wifi",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "1553-877X, 2373-745X",
  doi       = "10.1109/surv.2013.013013.00084"
}

@ARTICLE{Anthony2020-lk,
  title    = "Non-intrusive and Workflow-aware Virtual Network Function
              Scheduling in User-space",
  author   = "{Anthony} and Anthony, Anthony and Chowdhury, Shihabur Rahman and
              Bai, Tim and Boutaba, Raouf and Francois, Jerome",
  abstract = "The simple programming model and very low-overhead I/O
              capabilities of emerging packet processing techniques leveraging
              kernel-bypass I/O and poll-mode processing is gaining significant
              popularity for building high performance Virtual Network
              Functions (VNFs). However, existing OS schedulers fall short in
              rightsizing CPU allocation to poll-mode VNFs due to the
              schedulers' shortcoming in capturing the actual processing cost
              of these VNFs. This issue is further exacerbated by their
              inability to consider VNF processing order when VNFs are chained
              to form Service Function Chains (SFCs). The state-of-the-art VNF
              schedulers proposed as an alternative to OS schedulers are
              intrusive, requiring the VNFs to be built with scheduler specific
              libraries or having carefully selected scheduling checkpoints.
              This highly restricts the VNFs that can properly work with these
              schedulers. In this paper, we present UNiS, a User-space
              Non-intrusive work-flow aware VNF Scheduler. Unlike existing
              approaches, UNiS is non-intrusive, i.e., does not require VNF
              modifications and treats poll-mode VNFs as black boxes. UNiS is
              also workflow-aware, i.e., takes SFC processing order into
              account while scheduling VNFs. Testbed experiments show that UNiS
              is able to achieve a throughput within 90\% and 98\% of that
              achievable using an intrusive co-operative scheduler for
              synthetic and real data center traffic, respectively.",
  journal  = "IEEE Transactions on Cloud Computing",
  pages    = "1--1",
  year     =  2020,
  doi      = "10.1109/tcc.2020.3024232"
}

@ARTICLE{Schneider2023-wy,
  title     = "Multi-agent deep reinforcement learning for coordinated
               multipoint in mobile networks",
  author    = "Schneider, Stefan and Karl, Holger and Khalili, Ramin and
               Hecker, Artur",
  journal   = "IEEE Trans. Netw. Serv. Manage.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  pages     = "1--1",
  year      =  2023,
  file      = "All Papers/S/Schneider et al. 2023 - Multi-agent deep reinforcement learning for coordinated multipoint in mobile networks.pdf",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  issn      = "1932-4537, 2373-7379",
  doi       = "10.1109/tnsm.2023.3300962"
}

@ARTICLE{Clementi2010-ou,
  title     = "Flooding Time of {Edge-Markovian} Evolving Graphs",
  author    = "Clementi, Andrea E F and Macci, Claudio and Monti, Angelo and
               Pasquale, Francesco and Silvestri, Riccardo",
  abstract  = "=1We introduce stochastic time-dependency in evolving graphs:
               starting from an initial graph, at every time step, every edge
               changes its state (existing or not) according to a two-state
               Markovian process with probabilities p (edge birth-rate) and q
               (edge death-rate). If an edge exists at time t, then, at time
               $t+1$, it dies with probability q. If instead the edge does not
               exist at time t, then it will come into existence at time $t+1$
               with probability p. Such an evolving graph model is a wide
               generalization of time-independent dynamic random graphs [A. E.
               F. Clementi, A. Monti, F. Pasquale, and R. Silvestri, J. Comput.
               System Sci., 75 (2009), pp. 213?220] and will be called
               edge-Markovian evolving graphs. We investigate the speed of
               information spreading in such evolving graphs. We provide nearly
               tight bounds (which in fact turn out to be tight for a wide
               range of probabilities p and q) on the completion time of the
               flooding mechanism aiming to broadcast a piece of information
               from a source node to all nodes. In particular, we provide i) a
               tight characterization of the class of edge-Markovian evolving
               graphs where flooding time is constant and, thus, it does not
               asymptotically depend on the initial graph; ii) a tight
               characterization of the class of edge-Markovian evolving graphs
               where flooding time does not asymptotically depend on the edge
               death-rate q. An interesting consequence of our results is that
               information spreading can be fast even if the graph, at every
               time step, is very sparse and disconnected. Furthermore, our
               bounds imply that the flooding time can be exponentially shorter
               than the mixing time of the edge-Markovian graph.",
  journal   = "SIAM J. Discrete Math.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  24,
  number    =  4,
  pages     = "1694--1712",
  month     =  jan,
  year      =  2010,
  file      = "All Papers/C/Clementi et al. 2010 - Flooding Time of Edge-Markovian Evolving Graphs.pdf",
  issn      = "0895-4801",
  doi       = "10.1137/090756053"
}

@ARTICLE{Hennessy1982-dc,
  title     = "{MIPS}: A microprocessor architecture",
  author    = "Hennessy, John and Jouppi, Norman and Przybylski, Steven and
               Rowen, Christopher and Gross, Thomas and Baskett, Forest and
               Gill, John",
  abstract  = "MIPS is a new single chip VLSI microprocessor. It attempts to
               achieve high performance with the use of a simplified
               instruction set, similar to those found in microengines. The
               processor is a fast pipelined engine without pipeline
               interlocks. Software solutions to several traditional hardware
               problems, such as providing pipeline interlocks, are used.",
  journal   = "SIGMICRO Newsl.",
  publisher = "Association for Computing Machinery",
  volume    =  13,
  number    =  4,
  pages     = "17--22",
  month     =  oct,
  year      =  1982,
  file      = "All Papers/H/Hennessy et al. 1982 - MIPS - A microprocessor architecture.pdf",
  address   = "New York, NY, USA",
  keywords  = "GDS",
  issn      = "1050-916X",
  doi       = "10.1145/1014194.800930"
}

@ARTICLE{Appenzeller2004-mw,
  title     = "Sizing router buffers",
  author    = "Appenzeller, Guido and Keslassy, Isaac and McKeown, Nick",
  abstract  = "All Internet routers contain buffers to hold packets during
               times of congestion. Today, the size of the buffers is
               determined by the dynamics of TCP's congestion control
               algorithm. In particular, the goal is to make sure that when a
               link is congested, it is busy 100\% of the time; which is
               equivalent to making sure its buffer never goes empty. A widely
               used rule-of-thumb states that each link needs a buffer of size
               B = overline RTT x C), where overline RTT is the average
               round-trip time of a flow passing across the link, and C is the
               data rate of the link. For example, a 10Gb/s router linecard
               needs approximately 250ms x 10Gb/s = 2.5Gbits of buffers; and
               the amount of buffering grows linearly with the line-rate. Such
               large buffers are challenging for router manufacturers, who must
               use large, slow, off-chip DRAMs. And queueing delays can be
               long, have high variance, and may destabilize the congestion
               control algorithms. In this paper we argue that the
               rule-of-thumb ( B = (overline RTT x C ) is now outdated and
               incorrect for backbone routers. This is because of the large
               number of flows (TCP connections) multiplexed together on a
               single backbone link. Using theory, simulation and experiments
               on a network of real routers, we show that a link with n flows
               requires no more than B = (overline RTT x C ) $\surd$ n , for
               long-lived or short-lived TCP flows. The consequences on router
               design are enormous: A 2.5Gb/s link carrying 10,000 flows could
               reduce its buffers by 99\% with negligible difference in
               throughput; and a 10Gb/s link carrying 50,000 flows requires
               only 10Mbits of buffering, which can easily be implemented using
               fast, on-chip SRAM.",
  journal   = "Comput. Commun. Rev.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  34,
  number    =  4,
  pages     = "281--292",
  month     =  aug,
  year      =  2004,
  file      = "All Papers/A/Appenzeller et al. 2004 - Sizing router buffers.pdf",
  keywords  = "FutureInternet",
  language  = "en",
  issn      = "0146-4833, 1943-5819",
  doi       = "10.1145/1030194.1015499"
}

@ARTICLE{Enachescu2005-si,
  title     = "Part {III}: routers with very small buffers",
  author    = "Enachescu, Mihaela and Ganjali, Yashar and Goel, Ashish and
               McKeown, Nick and Roughgarden, Tim",
  abstract  = "Internet routers require buffers to hold packets during times of
               congestion. The buffers need to be fast, and so ideally they
               should be small enough to use fast memory technologies such as
               SRAM or all-optical buffering. Unfortunately, a widely used
               rule-of-thumb says we need a bandwidth-delay product of
               buffering at each router so as not to lose link utilization.
               This can be prohibitively large. In a recent paper, Appenzeller
               et al. challenged this rule-of-thumb and showed that for a
               backbone network, the buffer size can be divided by pN without
               sacrificing throughput, where N is the number of ows sharing the
               bottleneck. In this paper, we explore how buffers in the
               backbone can be significantly reduced even more, to as little as
               a few dozen packets, if we are willing to sacrifice a small
               amount of link capacity. We argue that if the TCP sources are
               not overly bursty, then fewer than twenty packet buffers are
               sufficient for high throughput. Specifically, we argue that
               O(log W) buffers are sufficient, where W is the window size of
               each ow. We support our claim with analysis and a variety of
               simulations. The change we need to make to TCP is minimal--each
               sender just needs to pace packet injections from its window.
               Moreover, there is some evidence that such small buffers are
               sufficient even if we don't modify the TCP sources so long as
               the access network is much slower than the backbone, which is
               true today and likely to remain true in the future. We conclude
               that buffers can be made small enough for all-optical routers
               with small integrated optical buffers.",
  journal   = "SIGCOMM Comput. Commun. Rev.",
  publisher = "Association for Computing Machinery",
  volume    =  35,
  number    =  3,
  pages     = "83--90",
  month     =  jul,
  year      =  2005,
  file      = "All Papers/E/Enachescu et al. 2005 - Part III - routers with very small buffers.pdf",
  address   = "New York, NY, USA",
  keywords  = "all-optical routers, buffer size, TCP, congestion
               control;FutureInternet",
  issn      = "0146-4833",
  doi       = "10.1145/1070873.1070886"
}

@INPROCEEDINGS{Deng2006-qd,
  title     = "Secure code distribution in dynamically programmable wireless
               sensor networks",
  booktitle = "Proceedings of the 5th international conference on Information
               processing in sensor networks",
  author    = "Deng, Jing and Han, Richard and Mishra, Shivakant",
  abstract  = "Remote reprogramming of in situ wireless sensor networks (WSNs)
               via the wireless link is an important capability. Securing the
               process of reprogramming allows each sensor node to authenticate
               each received code image. Due to the resource constraints of
               WSNs, public key schemes must be used sparingly. This paper
               introduces a mechanism for secure and efficient code
               distribution that employs public key cryptography only to sign
               the root of a combined structure consisting of both hash chains
               and hash trees. The chain based scheme works best when packets
               are received in the order they are sent with very few losses.
               Our hash tree based scheme allows nodes to authenticate packets
               and verify their integrity quickly, even when the packets may
               arrive out of order, but can result in too many public key
               operations. Integrating hash chains and hash trees produces a
               mechanism that is both resilient to losses and lightweight in
               terms of reducing memory consumption and the number of public
               key operations that a node has to perform. Simulation shows that
               the proposed secure reprogramming schemes add only a modest
               amount of overhead to a conventional non-secure reprogramming
               scheme, namely Deluge, and are therefore feasible and practical
               in a wireless sensor network.",
  publisher = "Association for Computing Machinery",
  pages     = "292--300",
  series    = "IPSN '06",
  month     =  apr,
  year      =  2006,
  file      = "All Papers/D/Deng et al. 2006 - Secure code distribution in dynamically programmable wireless sensor networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "secure reprogramming, sensor networks, security",
  location  = "Nashville, Tennessee, USA",
  isbn      = "9781595933348",
  doi       = "10.1145/1127777.1127822"
}

@ARTICLE{Abraham2008-mb,
  title     = "Compact name-independent routing with minimum stretch",
  author    = "Abraham, Ittai and Gavoille, Cyril and Malkhi, Dahlia and Nisan,
               Noam and Thorup, Mikkel",
  abstract  = "Given a weighted undirected network with arbitrary node names,
               we present a compact routing scheme, using a{\~O}($\surd$n,)
               space routing table at each node, and routing along paths of
               stretch 3, that is, at most thrice as long as the minimum cost
               paths. This is optimal in a very strong sense. It is known that
               no compact routing using o(n) space per node can route with
               stretch below 3. Also, it is known that any stretch below 5
               requires $\Omega$($\surd$n,)space per node.",
  journal   = "ACM Trans. Algorithms",
  publisher = "Association for Computing Machinery",
  volume    =  4,
  number    =  3,
  pages     = "1--12",
  month     =  jul,
  year      =  2008,
  file      = "All Papers/A/Abraham et al. 2008 - Compact name-independent routing with minimum stretch.pdf",
  address   = "New York, NY, USA",
  keywords  = "Compact routing;ComputerNetworks",
  issn      = "1549-6325",
  doi       = "10.1145/1367064.1367077"
}

@ARTICLE{Ha2008-hj,
  title     = "{CUBIC}: a new {TCP-friendly} high-speed {TCP} variant",
  author    = "Ha, Sangtae and Rhee, Injong and Xu, Lisong",
  abstract  = "CUBIC is a congestion control protocol for TCP (transmission
               control protocol) and the current default TCP algorithm in
               Linux. The protocol modifies the linear window growth function
               of existing TCP standards to be a cubic function in order to
               improve the scalability of TCP over fast and long distance
               networks. It also achieves more equitable bandwidth allocations
               among flows with different RTTs (round trip times) by making the
               window growth to be independent of RTT -- thus those flows grow
               their congestion window at the same rate. During steady state,
               CUBIC increases the window size aggressively when the window is
               far from the saturation point, and the slowly when it is close
               to the saturation point. This feature allows CUBIC to be very
               scalable when the bandwidth and delay product of the network is
               large, and at the same time, be highly stable and also fair to
               standard TCP flows. The implementation of CUBIC in Linux has
               gone through several upgrades. This paper documents its design,
               implementation, performance and evolution as the default TCP
               algorithm of Linux.",
  journal   = "Oper. Syst. Rev.",
  publisher = "Association for Computing Machinery",
  volume    =  42,
  number    =  5,
  pages     = "64--74",
  month     =  jul,
  year      =  2008,
  file      = "All Papers/H/Ha et al. 2008 - CUBIC - a new TCP-friendly high-speed TCP variant.pdf",
  address   = "New York, NY, USA",
  keywords  = "ComputerNetworks",
  issn      = "0163-5980",
  doi       = "10.1145/1400097.1400105"
}

@INPROCEEDINGS{ODonnell2008-ma,
  title     = "Using {BitTorrent} to distribute virtual machine images for
               classes",
  booktitle = "Proceedings of the 36th annual {ACM} {SIGUCCS} fall conference:
               moving mountains, blazing trails",
  author    = "O'Donnell, Chris M",
  abstract  = "There are a number of benefits to using virtual machines in a
               training environment; however, when the total size of the images
               range from 5-40 GB, getting those images to the physical
               machines in a timely fashion can be a concern.In our
               environment, we have a single machine we can use as a server,
               and classrooms located on different subnets (in different
               cities). With this situation, a multicast solution isn't
               possible, and standard file transfer methods (e.g. over a common
               windows share) take 6-24 hours to complete.Given the existing
               limitations, we needed a faster method of deploying images to
               our client machines. Our solution centers on BitTorrent
               technology, a few additional freely available applications, and
               is glued together with a little scripting. With our solution in
               place, the 6-24 hour completion times have been reduced to 1-4
               hours, and the deployment process takes less than 10 simple
               steps.The implementation is flexible enough that the server
               environment can be run from a Live CD if necessary, and similar
               implementations can easily be made platform independent. With
               the variety of tools that can be used to implement this
               environment, we also avoid the pitfalls of vendor lock-in.",
  publisher = "Association for Computing Machinery",
  pages     = "287--290",
  series    = "SIGUCCS '08",
  month     =  oct,
  year      =  2008,
  file      = "All Papers/O/O'Donnell 2008 - Using BitTorrent to distribute virtual machine images for classes.pdf",
  address   = "New York, NY, USA",
  keywords  = "virtualization, deployment, bittorrent, classroom,
               administration, training",
  location  = "Portland, OR, USA",
  isbn      = "9781605580746",
  doi       = "10.1145/1449956.1450040"
}

@INPROCEEDINGS{Lischka2009-pd,
  title     = "A virtual network mapping algorithm based on subgraph
               isomorphism detection",
  booktitle = "Proceedings of the 1st {ACM} workshop on Virtualized
               infrastructure systems and architectures",
  author    = "Lischka, Jens and Karl, Holger",
  abstract  = "Assigning the resources of a virtual network to the components
               of a physical network, called Virtual Network Mapping, plays a
               central role in network virtualization. Existing approaches use
               classical heuristics like simulated annealing or attempt a two
               stage solution by solving the node mapping in a first stage and
               doing the link mapping in a second stage.The contribution of
               this paper is a Virtual Network Mapping (VNM) algorithm based on
               subgraph isomorphism detection: it maps nodes and links during
               the same stage. Our experimental evaluations show that this
               method results in better mappings and is faster than the two
               stage approach, especially for large virtual networks with high
               resource consumption which are hard to map.",
  publisher = "Association for Computing Machinery",
  pages     = "81--88",
  series    = "VISA '09",
  month     =  aug,
  year      =  2009,
  file      = "All Papers/L/Lischka and Karl 2009 - A virtual network mapping algorithm based on subgraph isomorphism detection.pdf",
  address   = "New York, NY, USA",
  keywords  = "subgraph isomorphism, network embedding, virtual network
               mapping, resource allocation, virtualization;MyPapers",
  location  = "Barcelona, Spain",
  isbn      = "9781605585956",
  doi       = "10.1145/1592648.1592662"
}

@ARTICLE{Niranjan_Mysore2009-zw,
  title     = "{PortLand}: a scalable fault-tolerant layer 2 data center
               network fabric",
  author    = "Niranjan Mysore, Radhika and Pamboris, Andreas and Farrington,
               Nathan and Huang, Nelson and Miri, Pardis and Radhakrishnan,
               Sivasankar and Subramanya, Vikram and Vahdat, Amin",
  abstract  = "This paper considers the requirements for a scalable, easily
               manageable, fault-tolerant, and efficient data center network
               fabric. Trends in multi-core processors, end-host
               virtualization, and commodities of scale are pointing to future
               single-site data centers with millions of virtual end points.
               Existing layer 2 and layer 3 network protocols face some
               combination of limitations in such a setting: lack of
               scalability, difficult management, inflexible communication, or
               limited support for virtual machine migration. To some extent,
               these limitations may be inherent for Ethernet/IP style
               protocols when trying to support arbitrary topologies. We
               observe that data center networks are often managed as a single
               logical network fabric with a known baseline topology and growth
               model. We leverage this observation in the design and
               implementation of PortLand, a scalable, fault tolerant layer 2
               routing and forwarding protocol for data center environments.
               Through our implementation and evaluation, we show that PortLand
               holds promise for supporting a ``plug-and-play`` large-scale,
               data center network.",
  journal   = "SIGCOMM Comput. Commun. Rev.",
  publisher = "Association for Computing Machinery",
  volume    =  39,
  number    =  4,
  pages     = "39--50",
  month     =  aug,
  year      =  2009,
  file      = "All Papers/N/Niranjan Mysore et al. 2009 - PortLand - a scalable fault-tolerant layer 2 data center network fabric.pdf",
  address   = "New York, NY, USA",
  keywords  = "layer 2 routing in data centers, data center network
               fabric;DataCenter",
  issn      = "0146-4833",
  doi       = "10.1145/1594977.1592575"
}

@ARTICLE{Szabo2010-zz,
  title     = "Predicting the popularity of online content",
  author    = "Szabo, Gabor and Huberman, Bernardo A",
  abstract  = "Early patterns of Digg diggs and YouTube views reflect long-term
               user interest.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  8,
  pages     = "80--88",
  month     =  aug,
  year      =  2010,
  file      = "All Papers/S/Szabo and Huberman 2010 - Predicting the popularity of online content.pdf",
  address   = "New York, NY, USA",
  keywords  = "GeneralNetworking",
  issn      = "0001-0782",
  doi       = "10.1145/1787234.1787254"
}

@INPROCEEDINGS{Singla2010-cs,
  title     = "Scalable routing on flat names",
  booktitle = "Proceedings of the 6th International {COnference}",
  author    = "Singla, Ankit and Godfrey, P Brighten and Fall, Kevin and
               Iannaccone, Gianluca and Ratnasamy, Sylvia",
  abstract  = "We introduce a protocol which routes on flat,
               location-independent identifiers with guaranteed scalability and
               low stretch. Our design builds on theoretical advances in the
               area of compact routing, and is the first to realize these
               guarantees in a dynamic distributed setting.",
  publisher = "Association for Computing Machinery",
  number    = "Article 20",
  pages     = "1--12",
  series    = "Co-NEXT '10",
  month     =  nov,
  year      =  2010,
  file      = "All Papers/S/Singla et al. 2010 - Scalable routing on flat names.pdf",
  address   = "New York, NY, USA",
  keywords  = "ComputerNetworks",
  location  = "Philadelphia, Pennsylvania",
  isbn      = "9781450304481",
  doi       = "10.1145/1921168.1921195"
}

@INPROCEEDINGS{Razavi2013-yx,
  title     = "Scalable virtual machine deployment using {VM} image caches",
  booktitle = "{SC} '13: Proceedings of the International Conference on High
               Performance Computing, Networking, Storage and Analysis",
  author    = "Razavi, Kaveh and Kielmann, Thilo",
  abstract  = "In IaaS clouds, VM startup times are frequently perceived as
               slow, negatively impacting both dynamic scaling of web
               applications and the startup of high-performance computing
               applications consisting of many VM nodes. A significant part of
               the startup time is due to the large transfers of VM image
               content from a storage node to the actual compute nodes, even
               when copy-on-write schemes are used. We have observed that only
               a tiny part of the VM image is needed for the VM to be able to
               start up. Based on this observation, we propose using small
               caches for VM images to overcome the VM startup bottlenecks. We
               have implemented such caches as an extension to KVM/QEMU. Our
               evaluation with up to 64 VMs shows that using our caches reduces
               the time needed for simultaneous VM startups to the one of a
               single VM.",
  pages     = "1--12",
  month     =  nov,
  year      =  2013,
  keywords  = "Booting;Linux;Scalability;Virtual
               machining;Servers;Delays;Infrastructure-as-a-Service;Scalability",
  issn      = "2167-4337",
  doi       = "10.1145/2503210.2503274"
}

@ARTICLE{Bosshart2014-ly,
  title     = "P4: programming protocol-independent packet processors",
  author    = "Bosshart, Pat and Daly, Dan and Gibb, Glen and Izzard, Martin
               and McKeown, Nick and Rexford, Jennifer and Schlesinger, Cole
               and Talayco, Dan and Vahdat, Amin and Varghese, George and
               Walker, David",
  abstract  = "P4 is a high-level language for programming protocol-independent
               packet processors. P4 works in conjunction with SDN control
               protocols like OpenFlow. In its current form, OpenFlow
               explicitly specifies protocol headers on which it operates. This
               set has grown from 12 to 41 fields in a few years, increasing
               the complexity of the specification while still not providing
               the flexibility to add new headers. In this paper we propose P4
               as a strawman proposal for how OpenFlow should evolve in the
               future. We have three goals: (1) Reconfigurability in the field:
               Programmers should be able to change the way switches process
               packets once they are deployed. (2) Protocol independence:
               Switches should not be tied to any specific network protocols.
               (3) Target independence: Programmers should be able to describe
               packet-processing functionality independently of the specifics
               of the underlying hardware. As an example, we describe how to
               use P4 to configure a switch to add a new hierarchical label.",
  journal   = "SIGCOMM Comput. Commun. Rev.",
  publisher = "Association for Computing Machinery",
  volume    =  44,
  number    =  3,
  pages     = "87--95",
  month     =  jul,
  year      =  2014,
  file      = "All Papers/B/Bosshart et al. 2014 - P4 - programming protocol-independent packet processors.pdf",
  address   = "New York, NY, USA",
  keywords  = "reconfigurability, p4, sdn,
               protocol-independent;ProgrammableNetworks",
  issn      = "0146-4833",
  doi       = "10.1145/2656877.2656890"
}

@ARTICLE{Pejovic2015-bc,
  title     = "Anticipatory Mobile Computing: A Survey of the State of the Art
               and Research Challenges",
  author    = "Pejovic, Veljko and Musolesi, Mirco",
  abstract  = "Today's mobile phones are far from the mere communication
               devices they were 10 years ago. Equipped with sophisticated
               sensors and advanced computing hardware, phones can be used to
               infer users' location, activity, social setting, and more. As
               devices become increasingly intelligent, their capabilities
               evolve beyond inferring context to predicting it, and then
               reasoning and acting upon the predicted context. This article
               provides an overview of the current state of the art in mobile
               sensing and context prediction paving the way for full-fledged
               anticipatory mobile computing. We present a survey of phenomena
               that mobile phones can infer and predict, and offer a
               description of machine learning techniques used for such
               predictions. We then discuss proactive decision making and
               decision delivery via the user-device feedback loop. Finally, we
               discuss the challenges and opportunities of anticipatory mobile
               computing.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  47,
  number    =  3,
  pages     = "1--29",
  month     =  apr,
  year      =  2015,
  file      = "All Papers/P/Pejovic and Musolesi 2015 - Anticipatory Mobile Computing - A Survey of the State of the Art and Research Challenges.pdf",
  address   = "New York, NY, USA",
  keywords  = "mobile sensing, context-aware systems, Anticipatory
               computing;GeneralNetworking",
  issn      = "0360-0300",
  doi       = "10.1145/2693843"
}

@ARTICLE{Gember-Jacobson2014-ht,
  title     = "{OpenNF}: enabling innovation in network function control",
  author    = "Gember-Jacobson, Aaron and Viswanathan, Raajay and Prakash,
               Chaithan and Grandl, Robert and Khalid, Junaid and Das, Sourav
               and Akella, Aditya",
  abstract  = "Network functions virtualization (NFV) together with
               software-defined networking (SDN) has the potential to help
               operators satisfy tight service level agreements, accurately
               monitor and manipulate network traffic, and minimize operating
               expenses. However, in scenarios that require packet processing
               to be redistributed across a collection of network function (NF)
               instances, simultaneously achieving all three goals requires a
               framework that provides efficient, coordinated control of both
               internal NF state and network forwarding state. To this end, we
               design a control plane called OpenNF. We use carefully designed
               APIs and a clever combination of events and forwarding updates
               to address race conditions, bound overhead, and accommodate a
               variety of NFs. Our evaluation shows that OpenNF offers
               efficient state control without compromising flexibility, and
               requires modest additions to NFs.",
  journal   = "SIGCOMM Comput. Commun. Rev.",
  publisher = "Association for Computing Machinery",
  volume    =  44,
  number    =  4,
  pages     = "163--174",
  month     =  aug,
  year      =  2014,
  address   = "New York, NY, USA",
  keywords  = "network functions, software-defined networking, middleboxes",
  issn      = "0146-4833",
  doi       = "10.1145/2740070.2626313"
}

@INPROCEEDINGS{Barford1998-zs,
  title     = "Generating representative Web workloads for network and server
               performance evaluation",
  booktitle = "Proceedings of the 1998 {ACM} {SIGMETRICS} joint international
               conference on Measurement and modeling of computer systems",
  author    = "Barford, Paul and Crovella, Mark",
  abstract  = "One role for workload generation is as a means for understanding
               how servers and networks respond to variation in load. This
               enables management and capacity planning based on current and
               projected usage. This paper applies a number of observations of
               Web server usage to create a realistic Web workload generation
               tool which mimics a set of real users accessing a server. The
               tool, called Surge (Scalable URL Reference Generator) generates
               references matching empirical measurements of 1) server file
               size distribution; 2) request size distribution; 3) relative
               file popularity; 4) embedded file references; 5) temporal
               locality of reference; and 6) idle periods of individual users.
               This paper reviews the essential elements required in the
               generation of a representative Web workload. It also addresses
               the technical challenges to satisfying this large set of
               simultaneous constraints on the properties of the reference
               stream, the solutions we adopted, and their associated accuracy.
               Finally, we present evidence that Surge exercises servers in a
               manner significantly different from other Web server benchmarks.",
  publisher = "Association for Computing Machinery",
  pages     = "151--160",
  series    = "SIGMETRICS '98/PERFORMANCE '98",
  month     =  jun,
  year      =  1998,
  file      = "All Papers/B/Barford and Crovella 1998 - Generating representative Web workloads for network and server performance evaluation.pdf",
  address   = "New York, NY, USA",
  keywords  = "NetworkTraffic",
  location  = "Madison, Wisconsin, USA",
  isbn      = "9780897919821",
  doi       = "10.1145/277851.277897"
}

@INPROCEEDINGS{Zhang2016-mu,
  title           = "{OpenNetVM}",
  booktitle       = "Proceedings of the 2016 workshop on Hot topics in
                     Middleboxes and Network Function Virtualization -
                     {HotMIddlebox} '16",
  author          = "Zhang, Wei and Liu, Guyue and Zhang, Wenhui and Shah, Neel
                     and Lopreiato, Phillip and Todeschi, Gregoire and
                     Ramakrishnan, K K and Wood, Timothy",
  publisher       = "ACM Press",
  year            =  2016,
  address         = "New York, New York, USA",
  keywords        = "NFV\_SDN",
  copyright       = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  conference      = "the 2016 workshop",
  location        = "Florianopolis, Brazil",
  isbn            = "9781450344241",
  doi             = "10.1145/2940147.2940155"
}

@INPROCEEDINGS{Blenk2017-fs,
  title     = "o'zapft is: Tap Your Network Algorithm's Big Data!",
  booktitle = "Proceedings of the Workshop on Big Data Analytics and Machine
               Learning for Data Communication Networks",
  author    = "Blenk, Andreas and Kalmbach, Patrick and Kellerer, Wolfgang and
               Schmid, Stefan",
  abstract  = "At the heart of many computer network planning, deployment, and
               operational tasks lie hard algorithmic problems. Accordingly,
               over the last decades, we have witnessed a continuous pursuit
               for ever more accurate and faster algorithms. We propose an
               approach to design network algorithms which is radically
               different from most existing algorithms. Our approach is
               motivated by the observation that most existing algorithms to
               solve a given hard computer networking problem overlook a simple
               yet very powerful optimization opportunity in practice: many
               network algorithms are executed repeatedly (e.g., for each
               virtual network request or in reaction to user mobility), and
               hence with each execution, generate interesting data:
               (problem,solution)-pairs. We make the case for leveraging the
               potentially big data of an algorithm's past executions to
               improve and speed up future, similar solutions, by reducing the
               algorithm's search space. We study the applicability of machine
               learning to network algorithm design, identify challenges and
               discuss limitations. We empirically demonstrate the potential of
               machine learning network algorithms in two case studies, namely
               the embedding of virtual networks (a packing optimization
               problem) and k-center facility location (a covering optimization
               problem), using a prototype implementation.",
  publisher = "Association for Computing Machinery",
  pages     = "19--24",
  series    = "Big-DAMA '17",
  month     =  aug,
  year      =  2017,
  file      = "All Papers/B/Blenk et al. 2017 - o'zapft is - Tap Your Network Algorithm's Big Data!.pdf",
  address   = "New York, NY, USA",
  keywords  = "Algorithms, Big Data, Machine Learning, Computer
               Networks;MLNetworking",
  location  = "Los Angeles, CA, USA",
  isbn      = "9781450350549",
  doi       = "10.1145/3098593.3098597"
}

@INPROCEEDINGS{Miao2017-zu,
  title     = "{SilkRoad}: Making Stateful Layer-4 Load Balancing Fast and
               Cheap Using Switching {ASICs}",
  booktitle = "Proceedings of the Conference of the {ACM} Special Interest
               Group on Data Communication",
  author    = "Miao, Rui and Zeng, Hongyi and Kim, Changhoon and Lee, Jeongkeun
               and Yu, Minlan",
  abstract  = "In this paper, we show that up to hundreds of software load
               balancer (SLB) servers can be replaced by a single modern
               switching ASIC, potentially reducing the cost of load balancing
               by over two orders of magnitude. Today, large data centers
               typically employ hundreds or thousands of servers to
               load-balance incoming traffic over application servers. These
               software load balancers (SLBs) map packets destined to a service
               (with a virtual IP address, or VIP), to a pool of servers tasked
               with providing the service (with multiple direct IP addresses,
               or DIPs). An SLB is stateful, it must always map a connection to
               the same server, even if the pool of servers changes and/or if
               the load is spread differently across the pool. This property is
               called per-connection consistency or PCC. The challenge is that
               the load balancer must keep track of millions of connections
               simultaneously.Until recently, it was not possible to implement
               a load balancer with PCC in a merchant switching ASIC, because
               high-performance switching ASICs typically can not maintain
               per-connection states with PCC. Newer switching ASICs provide
               resources and primitives to enable PCC at a large scale. In this
               paper, we explore how to use switching ASICs to build much
               faster load balancers than have been built before. Our system,
               called SilkRoad, is defined in a 400 line P4 program and when
               compiled to a state-of-the-art switching ASIC, we show it can
               load-balance ten million connections simultaneously at line
               rate.",
  publisher = "Association for Computing Machinery",
  pages     = "15--28",
  series    = "SIGCOMM '17",
  month     =  aug,
  year      =  2017,
  file      = "All Papers/M/Miao et al. 2017 - SilkRoad - Making Stateful Layer-4 Load Balancing Fast and Cheap Using Switching ASICs.pdf",
  address   = "New York, NY, USA",
  keywords  = "Load balancing, Programmable switches;ProgrammableNetworks",
  location  = "Los Angeles, CA, USA",
  isbn      = "9781450346535",
  doi       = "10.1145/3098822.3098824"
}

@INPROCEEDINGS{Mao2017-aq,
  title     = "Neural Adaptive Video Streaming with Pensieve",
  booktitle = "Proceedings of the Conference of the {ACM} Special Interest
               Group on Data Communication",
  author    = "Mao, Hongzi and Netravali, Ravi and Alizadeh, Mohammad",
  abstract  = "Client-side video players employ adaptive bitrate (ABR)
               algorithms to optimize user quality of experience (QoE). Despite
               the abundance of recently proposed schemes, state-of-the-art ABR
               algorithms suffer from a key limitation: they use fixed control
               rules based on simplified or inaccurate models of the deployment
               environment. As a result, existing schemes inevitably fail to
               achieve optimal performance across a broad set of network
               conditions and QoE objectives.We propose Pensieve, a system that
               generates ABR algorithms using reinforcement learning (RL).
               Pensieve trains a neural network model that selects bitrates for
               future video chunks based on observations collected by client
               video players. Pensieve does not rely on pre-programmed models
               or assumptions about the environment. Instead, it learns to make
               ABR decisions solely through observations of the resulting
               performance of past decisions. As a result, Pensieve
               automatically learns ABR algorithms that adapt to a wide range
               of environments and QoE metrics. We compare Pensieve to
               state-of-the-art ABR algorithms using trace-driven and real
               world experiments spanning a wide variety of network conditions,
               QoE metrics, and video properties. In all considered scenarios,
               Pensieve outperforms the best state-of-the-art scheme, with
               improvements in average QoE of 12\%--25\%. Pensieve also
               generalizes well, outperforming existing schemes even on
               networks for which it was not explicitly trained.",
  publisher = "Association for Computing Machinery",
  pages     = "197--210",
  series    = "SIGCOMM '17",
  month     =  aug,
  year      =  2017,
  file      = "All Papers/M/Mao et al. 2017 - Neural Adaptive Video Streaming with Pensieve.pdf",
  address   = "New York, NY, USA",
  keywords  = "bitrate adaptation, video streaming, reinforcement
               learning;MLAspects",
  location  = "Los Angeles, CA, USA",
  isbn      = "9781450346535",
  doi       = "10.1145/3098822.3098843"
}

@INPROCEEDINGS{Jin2017-ph,
  title     = "{NetCache}: Balancing {Key-Value} Stores with Fast {In-Network}
               Caching",
  booktitle = "Proceedings of the 26th Symposium on Operating Systems
               Principles",
  author    = "Jin, Xin and Li, Xiaozhou and Zhang, Haoyu and Soul{\'e}, Robert
               and Lee, Jeongkeun and Foster, Nate and Kim, Changhoon and
               Stoica, Ion",
  abstract  = "We present NetCache, a new key-value store architecture that
               leverages the power and flexibility of new-generation
               programmable switches to handle queries on hot items and balance
               the load across storage nodes. NetCache provides high aggregate
               throughput and low latency even under highly-skewed and
               rapidly-changing workloads. The core of NetCache is a
               packet-processing pipeline that exploits the capabilities of
               modern programmable switch ASICs to efficiently detect, index,
               cache and serve hot key-value items in the switch data plane.
               Additionally, our solution guarantees cache coherence with
               minimal overhead. We implement a NetCache prototype on Barefoot
               Tofino switches and commodity servers and demonstrate that a
               single switch can process 2+ billion queries per second for 64K
               items with 16-byte keys and 128-byte values, while only
               consuming a small portion of its hardware resources. To the best
               of our knowledge, this is the first time that a sophisticated
               application-level functionality, such as in-network caching, has
               been shown to run at line rate on programmable switches.
               Furthermore, we show that NetCache improves the throughput by
               3-10x and reduces the latency of up to 40\% of queries by 50\%,
               for high-performance, in-memory key-value stores.",
  publisher = "Association for Computing Machinery",
  pages     = "121--136",
  series    = "SOSP '17",
  month     =  oct,
  year      =  2017,
  file      = "All Papers/J/Jin et al. 2017 - NetCache - Balancing Key-Value Stores with Fast In-Network Caching.pdf",
  address   = "New York, NY, USA",
  keywords  = "Caching, Key-value stores, Programmable
               switches;ProgrammableNetworks",
  location  = "Shanghai, China",
  isbn      = "9781450350853",
  doi       = "10.1145/3132747.3132764"
}

@INPROCEEDINGS{Peng2018-ad,
  title     = "Optimus: an efficient dynamic resource scheduler for deep
               learning clusters",
  booktitle = "Proceedings of the Thirteenth {EuroSys} Conference",
  author    = "Peng, Yanghua and Bao, Yixin and Chen, Yangrui and Wu, Chuan and
               Guo, Chuanxiong",
  abstract  = "Deep learning workloads are common in today's production
               clusters due to the proliferation of deep learning driven AI
               services (e.g., speech recognition, machine translation). A deep
               learning training job is resource-intensive and time-consuming.
               Efficient resource scheduling is the key to the maximal
               performance of a deep learning cluster. Existing cluster
               schedulers are largely not tailored to deep learning jobs, and
               typically specifying a fixed amount of resources for each job,
               prohibiting high resource efficiency and job performance. This
               paper proposes Optimus, a customized job scheduler for deep
               learning clusters, which minimizes job training time based on
               online resource-performance models. Optimus uses online fitting
               to predict model convergence during training, and sets up
               performance models to accurately estimate training speed as a
               function of allocated resources in each job. Based on the
               models, a simple yet effective method is designed and used for
               dynamically allocating resources and placing deep learning tasks
               to minimize job completion time. We implement Optimus on top of
               Kubernetes, a cluster manager for container orchestration, and
               experiment on a deep learning cluster with 7 CPU servers and 6
               GPU servers, running 9 training jobs using the MXNet framework.
               Results show that Optimus outperforms representative cluster
               schedulers by about 139\% and 63\% in terms of job completion
               time and makespan, respectively.",
  publisher = "Association for Computing Machinery",
  number    = "Article 3",
  pages     = "1--14",
  series    = "EuroSys '18",
  month     =  apr,
  year      =  2018,
  file      = "All Papers/P/Peng et al. 2018 - Optimus - an efficient dynamic resource scheduler for deep learning clusters.pdf",
  address   = "New York, NY, USA",
  keywords  = "deep learning, resource management;DataCenter",
  location  = "Porto, Portugal",
  isbn      = "9781450355841",
  doi       = "10.1145/3190508.3190517"
}

@INPROCEEDINGS{Zhang2018-oz,
  title     = "{Long-Term} Mobile Traffic Forecasting Using Deep
               {Spatio-Temporal} Neural Networks",
  booktitle = "Proceedings of the Eighteenth {ACM} International Symposium on
               Mobile Ad Hoc Networking and Computing",
  author    = "Zhang, Chaoyun and Patras, Paul",
  abstract  = "Forecasting with high accuracy the volume of data traffic that
               mobile users will consume is becoming increasingly important for
               precision traffic engineering, demand-aware network resource
               allocation, as well as public transportation. Measurements
               collection in dense urban deployments is however complex and
               expensive, and the post-processing required to make predictions
               is highly non-trivial, given the intricate spatio-temporal
               variability of mobile traffic due to user mobility. To overcome
               these challenges, in this paper we harness the exceptional
               feature extraction abilities of deep learning and propose a
               Spatio-Temporal neural Network (STN) architecture purposely
               designed for precise network-wide mobile traffic forecasting. We
               present a mechanism that fine tunes the STN and enables its
               operation with only limited ground truth observations. We then
               introduce a Double STN technique (D-STN), which uniquely
               combines the STN predictions with historical statistics, thereby
               making faithful long-term mobile traffic projections.
               Experiments we conduct with real-world mobile traffic data sets,
               collected over 60 days in both urban and rural areas,
               demonstrate that the proposed (D-)STN schemes perform up to
               10-hour long predictions with remarkable accuracy, irrespective
               of the time of day when they are triggered. Specifically, our
               solutions achieve up to 61\% smaller prediction errors as
               compared to widely used forecasting approaches, while operating
               with up to 600 times shorter measurement intervals.",
  publisher = "Association for Computing Machinery",
  pages     = "231--240",
  series    = "Mobihoc '18",
  month     =  jun,
  year      =  2018,
  file      = "All Papers/Z/Zhang and Patras 2018 - Long-Term Mobile Traffic Forecasting Using Deep Spatio-Temporal Neural Networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "mobile traffic forecasting, spatio-temporal modelling, deep
               learning;GeneralNetworking",
  location  = "Los Angeles, CA, USA",
  isbn      = "9781450357708",
  doi       = "10.1145/3209582.3209606"
}

@INPROCEEDINGS{Sanvito2018-ov,
  title     = "Can the Network be the {AI} Accelerator?",
  booktitle = "Proceedings of the 2018 Morning Workshop on {In-Network}
               Computing",
  author    = "Sanvito, Davide and Siracusano, Giuseppe and Bifulco, Roberto",
  abstract  = "Artificial Neural Networks (NNs) play an increasingly important
               role in many services and applications, contributing
               significantly to compute infrastructures' workloads. When used
               in latency sensitive services, NNs are usually processed by CPUs
               since using an external dedicated hardware accelerator would be
               inefficient. However, with growing workloads size and
               complexity, CPUs are hitting their computation limits, requiring
               the introduction of new specialized hardware accelerators
               tailored to the task. In this paper we analyze the option to use
               programmable network devices, such as Network Cards and
               Switches, as NN accelerators in place of purpose built dedicated
               hardware. To this end, in this preliminary work we analyze in
               depth the properties of NN processing on CPUs, derive options to
               efficiently split such processing, and show that programmable
               network devices may be a suitable engine for implementing a
               CPU's NN co-processor.",
  publisher = "Association for Computing Machinery",
  pages     = "20--25",
  series    = "NetCompute '18",
  month     =  aug,
  year      =  2018,
  file      = "All Papers/S/Sanvito et al. 2018 - Can the Network be the AI Accelerator.pdf",
  address   = "New York, NY, USA",
  keywords  = "Computation offloading, SmartNIC, Programmable
               switches;ProgrammableNetworks",
  location  = "Budapest, Hungary",
  isbn      = "9781450359085",
  doi       = "10.1145/3229591.3229594"
}

@INPROCEEDINGS{Gupta2018-sn,
  title     = "Sonata: query-driven streaming network telemetry",
  booktitle = "Proceedings of the 2018 Conference of the {ACM} Special Interest
               Group on Data Communication",
  author    = "Gupta, Arpit and Harrison, Rob and Canini, Marco and Feamster,
               Nick and Rexford, Jennifer and Willinger, Walter",
  abstract  = "Managing and securing networks requires collecting and analyzing
               network traffic data in real time. Existing telemetry systems do
               not allow operators to express the range of queries needed to
               perform management or scale to large traffic volumes and rates.
               We present Sonata, an expressive and scalable telemetry system
               that coordinates joint collection and analysis of network
               traffic. Sonata provides a declarative interface to express
               queries for a wide range of common telemetry tasks; to enable
               real-time execution, Sonata partitions each query across the
               stream processor and the data plane, running as much of the
               query as it can on the network switch, at line rate. To optimize
               the use of limited switch memory, Sonata dynamically refines
               each query to ensure that available resources focus only on
               traffic that satisfies the query. Our evaluation shows that
               Sonata can support a wide range of telemetry tasks while
               reducing the workload for the stream processor by as much as
               seven orders of magnitude compared to existing telemetry
               systems.",
  publisher = "Association for Computing Machinery",
  pages     = "357--371",
  series    = "SIGCOMM '18",
  month     =  aug,
  year      =  2018,
  file      = "All Papers/G/Gupta et al. 2018 - Sonata - query-driven streaming network telemetry.pdf",
  address   = "New York, NY, USA",
  keywords  = "analytics, programmable switches, stream processing",
  location  = "Budapest, Hungary",
  isbn      = "9781450355674",
  doi       = "10.1145/3230543.3230555"
}

@INPROCEEDINGS{Agarwal2018-he,
  title     = "Sincronia: near-optimal network design for coflows",
  booktitle = "Proceedings of the 2018 Conference of the {ACM} Special Interest
               Group on Data Communication",
  author    = "Agarwal, Saksham and Rajakrishnan, Shijin and Narayan, Akshay
               and Agarwal, Rachit and Shmoys, David and Vahdat, Amin",
  abstract  = "We present Sincronia, a near-optimal network design for coflows
               that can be implemented on top on any transport layer (for
               flows) that supports priority scheduling. Sincronia achieves
               this using a key technical result --- we show that given a
               ``right'' ordering of coflows, any per-flow rate allocation
               mechanism achieves average coflow completion time within 4X of
               the optimal as long as (co)flows are prioritized with respect to
               the ordering.Sincronia uses a simple greedy mechanism to
               periodically order all unfinished coflows; each host sets
               priorities for its flows using corresponding coflow order and
               offloads the flow scheduling and rate allocation to the
               underlying priority-enabled transport layer. We evaluate
               Sincronia over a real testbed comprising 16-servers and
               commodity switches, and using simulations across a variety of
               workloads. Evaluation results suggest that Sincronia not only
               admits a practical, near-optimal design but also improves upon
               state-of-the-art network designs for coflows (sometimes by as
               much as 8X).",
  publisher = "Association for Computing Machinery",
  pages     = "16--29",
  series    = "SIGCOMM '18",
  month     =  aug,
  year      =  2018,
  file      = "All Papers/A/Agarwal et al. 2018 - Sincronia - near-optimal network design for coflows.pdf",
  address   = "New York, NY, USA",
  keywords  = "datacenter networks, coflow, approximation
               algorithms;NetworkTraffic",
  location  = "Budapest, Hungary",
  isbn      = "9781450355674",
  doi       = "10.1145/3230543.3230569"
}

@ARTICLE{Yang2019-xn,
  title     = "Federated Machine Learning: Concept and Applications",
  author    = "Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin",
  abstract  = "Today's artificial intelligence still faces two major
               challenges. One is that, in most industries, data exists in the
               form of isolated islands. The other is the strengthening of data
               privacy and security. We propose a possible solution to these
               challenges: secure federated learning. Beyond the
               federated-learning framework first proposed by Google in 2016,
               we introduce a comprehensive secure federated-learning
               framework, which includes horizontal federated learning,
               vertical federated learning, and federated transfer learning. We
               provide definitions, architectures, and applications for the
               federated-learning framework, and provide a comprehensive survey
               of existing works on this subject. In addition, we propose
               building data networks among organizations based on federated
               mechanisms as an effective solution to allowing knowledge to be
               shared without compromising user privacy.",
  journal   = "ACM Trans. Intell. Syst. Technol.",
  publisher = "Association for Computing Machinery",
  volume    =  10,
  number    =  2,
  pages     = "1--19",
  month     =  jan,
  year      =  2019,
  file      = "All Papers/Y/Yang et al. 2019 - Federated Machine Learning - Concept and Applications.pdf",
  address   = "New York, NY, USA",
  keywords  = "GDPR, Federated learning, transfer learning",
  issn      = "2157-6904",
  doi       = "10.1145/3298981"
}

@ARTICLE{Ma2019-ck,
  title     = "{WiFi} Sensing with Channel State Information: A Survey",
  author    = "Ma, Yongsen and Zhou, Gang and Wang, Shuangquan",
  abstract  = "With the high demand for wireless data traffic, WiFi networks
               have experienced very rapid growth, because they provide high
               throughput and are easy to deploy. Recently, Channel State
               Information (CSI) measured by WiFi networks is widely used for
               different sensing purposes. To get a better understanding of
               existing WiFi sensing technologies and future WiFi sensing
               trends, this survey gives a comprehensive review of the signal
               processing techniques, algorithms, applications, and performance
               results of WiFi sensing with CSI. Different WiFi sensing
               algorithms and signal processing techniques have their own
               advantages and limitations and are suitable for different WiFi
               sensing applications. The survey groups CSI-based WiFi sensing
               applications into three categories, detection, recognition, and
               estimation, depending on whether the outputs are
               binary/multi-class classifications or numerical values. With the
               development and deployment of new WiFi technologies, there will
               be more WiFi sensing opportunities wherein the targets may go
               beyond from humans to environments, animals, and objects. The
               survey highlights three challenges for WiFi sensing: robustness
               and generalization, privacy and security, and coexistence of
               WiFi sensing and networking. Finally, the survey presents three
               future WiFi sensing trends, i.e., integrating cross-layer
               network information, multi-device cooperation, and fusion of
               different sensors, for enhancing existing WiFi sensing
               capabilities and enabling new WiFi sensing opportunities.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  52,
  number    =  3,
  pages     = "1--36",
  month     =  jun,
  year      =  2019,
  file      = "All Papers/M/Ma et al. 2019 - WiFi Sensing with Channel State Information - A Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "activity recognition, human identification, human counting, WiFi
               imaging, WiFi sensing, gesture recognition, respiration
               monitoring, channel state information,
               localization;Mobile\_Wireless",
  issn      = "0360-0300",
  doi       = "10.1145/3310194"
}

@INPROCEEDINGS{Jepsen2019-fv,
  title     = "Fast String Searching on {PISA}",
  booktitle = "Proceedings of the 2019 {ACM} Symposium on {SDN} Research",
  author    = "Jepsen, Theo and Alvarez, Daniel and Foster, Nate and Kim,
               Changhoon and Lee, Jeongkeun and Moshref, Masoud and Soul{\'e},
               Robert",
  abstract  = "This paper presents PPS, a system for locating occurrences of
               string keywords stored in the payload of packets using a
               programmable network ASIC. The PPS compiler first converts
               keywords into Deterministic Finite Automata (DFA)
               representations, and then maps the DFA into a sequence of
               forwarding tables in the switch pipeline. Our design leverages
               several hardware primitives (e.g., TCAM, hashing, parallel
               tables) to achieve high throughput. Our evaluation shows that
               PPS demonstrates significantly higher throughput and lower
               latency than string searches running on CPUs, GPUs, or FPGAs.",
  publisher = "Association for Computing Machinery",
  pages     = "21--28",
  series    = "SOSR '19",
  month     =  apr,
  year      =  2019,
  file      = "All Papers/J/Jepsen et al. 2019 - Fast String Searching on PISA.pdf",
  address   = "New York, NY, USA",
  keywords  = "Programmable switches, String searching;ProgrammableNetworks",
  location  = "San Jose, CA, USA",
  isbn      = "9781450367103",
  doi       = "10.1145/3314148.3314356"
}

@INPROCEEDINGS{Ports2019-qs,
  title     = "When Should The Network Be The Computer?",
  booktitle = "Proceedings of the Workshop on Hot Topics in Operating Systems",
  author    = "Ports, Dan R K and Nelson, Jacob",
  abstract  = "Researchers have repurposed programmable network devices to
               place small amounts of application computation in the network,
               sometimes yielding orders-of-magnitude performance gains. At the
               same time, effectively using these devices requires careful use
               of limited resources and managing deployment challenges.This
               paper provides a framework for principled use of in-network
               processing. We provide a set of guidelines for building robust
               and deployable in-network primives, along with a taxonomy to
               help identify which applications can benefit from in-network
               processing and what types of devices they should use.",
  publisher = "Association for Computing Machinery",
  pages     = "209--215",
  series    = "HotOS '19",
  month     =  may,
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "reconfigurable devices, in-network computation, programmable
               switches, smart NICs",
  location  = "Bertinoro, Italy",
  isbn      = "9781450367271",
  doi       = "10.1145/3317550.3321439"
}

@INPROCEEDINGS{Ports2019-jb,
  title     = "When Should The Network Be The Computer?",
  booktitle = "Proceedings of the Workshop on Hot Topics in Operating Systems",
  author    = "Ports, Dan R K and Nelson, Jacob",
  abstract  = "Researchers have repurposed programmable network devices to
               place small amounts of application computation in the network,
               sometimes yielding orders-of-magnitude performance gains. At the
               same time, effectively using these devices requires careful use
               of limited resources and managing deployment challenges.This
               paper provides a framework for principled use of in-network
               processing. We provide a set of guidelines for building robust
               and deployable in-network primives, along with a taxonomy to
               help identify which applications can benefit from in-network
               processing and what types of devices they should use.",
  publisher = "Association for Computing Machinery",
  pages     = "209--215",
  series    = "HotOS '19",
  month     =  may,
  year      =  2019,
  file      = "All Papers/P/Ports and Nelson 2019 - When Should The Network Be The Computer.pdf",
  address   = "New York, NY, USA",
  keywords  = "reconfigurable devices, in-network computation, programmable
               switches, smart NICs;ProgrammableNetworks",
  location  = "Bertinoro, Italy",
  isbn      = "9781450367271",
  doi       = "10.1145/3317550.3321439"
}

@ARTICLE{Ben-Nun2019-cq,
  title     = "Demystifying Parallel and Distributed Deep Learning: An In-depth
               Concurrency Analysis",
  author    = "Ben-Nun, Tal and Hoefler, Torsten",
  abstract  = "Deep Neural Networks (DNNs) are becoming an important tool in
               modern computing applications. Accelerating their training is a
               major challenge and techniques range from distributed algorithms
               to low-level circuit design. In this survey, we describe the
               problem from a theoretical perspective, followed by approaches
               for its parallelization. We present trends in DNN architectures
               and the resulting implications on parallelization strategies. We
               then review and model the different types of concurrency in
               DNNs: from the single operator, through parallelism in network
               inference and training, to distributed deep learning. We discuss
               asynchronous stochastic optimization, distributed system
               architectures, communication schemes, and neural architecture
               search. Based on those approaches, we extrapolate potential
               directions for parallelism in deep learning.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  52,
  number    =  4,
  pages     = "1--43",
  month     =  aug,
  year      =  2019,
  file      = "All Papers/B/Ben-Nun and Hoefler 2019 - Demystifying Parallel and Distributed Deep Learning - An In-depth Concurrency Analysis.pdf",
  address   = "New York, NY, USA",
  keywords  = "distributed computing, parallel algorithms, Deep
               learning;MLNetworking;ML",
  issn      = "0360-0300",
  doi       = "10.1145/3320060"
}

@ARTICLE{Adhikari2019-lv,
  title     = "A Survey on Scheduling Strategies for Workflows in Cloud
               Environment and Emerging Trends",
  author    = "Adhikari, Mainak and Amgoth, Tarachand and Srirama, Satish
               Narayana",
  abstract  = "Workflow scheduling is one of the challenging issues in emerging
               trends of the distributed environment that focuses on satisfying
               various quality of service (QoS) constraints. The cloud receives
               the applications as a form of a workflow, consisting of a set of
               interdependent tasks, to solve the large-scale scientific or
               enterprise problems. Workflow scheduling in the cloud
               environment has been studied extensively over the years, and
               this article provides a comprehensive review of the approaches.
               This article analyses the characteristics of various workflow
               scheduling techniques and classifies them based on their
               objectives and execution model. In addition, the recent
               technological developments and paradigms such as serverless
               computing and Fog computing are creating new
               requirements/opportunities for workflow scheduling in a
               distributed environment. The serverless infrastructures are
               mainly designed for processing background tasks such as
               Internet-of-Things (IoT), web applications, or event-driven
               applications. To address the ever-increasing demands of
               resources and to overcome the drawbacks of the cloud-centric
               IoT, the Fog computing paradigm has been developed. This article
               also discusses workflow scheduling in the context of these
               emerging trends of cloud computing.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  52,
  number    =  4,
  pages     = "1--36",
  month     =  aug,
  year      =  2019,
  file      = "All Papers/A/Adhikari et al. 2019 - A Survey on Scheduling Strategies for Workflows in Cloud Environment and Emerging Trends.pdf",
  address   = "New York, NY, USA",
  keywords  = "Cloud computing, scientific problems, serverless computing, QoS
               constraint, Fog computing, workflow scheduling;EdgeFogCloudIoT",
  issn      = "0360-0300",
  doi       = "10.1145/3325097"
}

@ARTICLE{Hong2019-so,
  title     = "Resource Management in {Fog/Edge} Computing: A Survey on
               Architectures, Infrastructure, and Algorithms",
  author    = "Hong, Cheol-Ho and Varghese, Blesson",
  abstract  = "Contrary to using distant and centralized cloud data center
               resources, employing decentralized resources at the edge of a
               network for processing data closer to user devices, such as
               smartphones and tablets, is an upcoming computing paradigm,
               referred to as fog/edge computing. Fog/edge resources are
               typically resource-constrained, heterogeneous, and dynamic
               compared to the cloud, thereby making resource management an
               important challenge that needs to be addressed. This article
               reviews publications as early as 1991, with 85\% of the
               publications between 2013 and 2018, to identify and classify the
               architectures, infrastructure, and underlying algorithms for
               managing resources in fog/edge computing.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  52,
  number    =  5,
  pages     = "1--37",
  month     =  sep,
  year      =  2019,
  file      = "All Papers/H/Hong and Varghese 2019 - Resource Management in Fog - Edge Computing - A Survey on Architectures, Infrastructure, and Algorithms.pdf",
  address   = "New York, NY, USA",
  keywords  = "Fog/edge computing, resource management, architectures,
               infrastructure, algorithms;EdgeFogCloudIoT",
  issn      = "0360-0300",
  doi       = "10.1145/3326066"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Van_Glabbeek2019-or,
  title     = "Progress, Justness, and Fairness",
  author    = "Van Glabbeek, Rob and H{\"o}fner, Peter",
  abstract  = "Fairness assumptions are a valuable tool when reasoning about
               systems. In this article, we classify several fairness
               properties found in the literature and argue that most of them
               are too restrictive for many applications. As an alternative, we
               introduce the concept of justness.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  52,
  number    =  4,
  pages     = "1--38",
  month     =  aug,
  year      =  2019,
  file      = "All Papers/V/Van Glabbeek and Höfner 2019 - Progress, Justness, and Fairness.pdf",
  address   = "New York, NY, USA",
  keywords  = "labelled transition systems, Fairness, liveness,
               justness;GenericInterest",
  issn      = "0360-0300",
  doi       = "10.1145/3329125"
}

@ARTICLE{Duc2019-tb,
  title     = "Machine Learning Methods for Reliable Resource Provisioning in
               {Edge-Cloud} Computing: A Survey",
  author    = "Duc, Thang Le and Leiva, Rafael Garc{\'\i}a and Casari, Paolo
               and {\"O}stberg, Per-Olov",
  abstract  = "Large-scale software systems are currently designed as
               distributed entities and deployed in cloud data centers. To
               overcome the limitations inherent to this type of deployment,
               applications are increasingly being supplemented with components
               instantiated closer to the edges of networks---a paradigm known
               as edge computing. The problem of how to efficiently orchestrate
               combined edge-cloud applications is, however, incompletely
               understood, and a wide range of techniques for resource and
               application management are currently in use.This article
               investigates the problem of reliable resource provisioning in
               joint edge-cloud environments, and surveys technologies,
               mechanisms, and methods that can be used to improve the
               reliability of distributed applications in diverse and
               heterogeneous network environments. Due to the complexity of the
               problem, special emphasis is placed on solutions to the
               characterization, management, and control of complex distributed
               applications using machine learning approaches. The survey is
               structured around a decomposition of the reliable resource
               provisioning problem into three categories of techniques:
               workload characterization and prediction, component placement
               and system consolidation, and application elasticity and
               remediation. Survey results are presented along with a
               problem-oriented discussion of the state-of-the-art. A summary
               of identified challenges and an outline of future research
               directions are presented to conclude the article.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  52,
  number    =  5,
  pages     = "1--39",
  month     =  sep,
  year      =  2019,
  file      = "All Papers/D/Duc et al. 2019 - Machine Learning Methods for Reliable Resource Provisioning in Edge-Cloud Computing - A Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "optimization, machine learning, edge computing, cloud computing,
               Reliability, remediation, autoscaling, distributed systems,
               placement, consolidation;EdgeFogCloudIoT;ML4Net;MLAspects",
  issn      = "0360-0300",
  doi       = "10.1145/3341145"
}

@ARTICLE{Liu2020-jm,
  title     = "Resource Management and Scheduling in Distributed Stream
               Processing Systems: A Taxonomy, Review, and Future Directions",
  author    = "Liu, Xunyun and Buyya, Rajkumar",
  abstract  = "Stream processing is an emerging paradigm to handle data streams
               upon arrival, powering latency-critical application such as
               fraud detection, algorithmic trading, and health surveillance.
               Though there are a variety of Distributed Stream Processing
               Systems (DSPSs) that facilitate the development of streaming
               applications, resource management and task scheduling is not
               automatically handled by the DSPS middleware and requires a
               laborious process to tune toward specific deployment targets. As
               the advent of cloud computing has supported renting resources
               on-demand, it is of great interest to review the research
               progress of hosting streaming systems in clouds under certain
               Service Level Agreements (SLA) and cost constraints. In this
               article, we introduce the hierarchical structure of streaming
               systems, define the scope of the resource management problem,
               and present a comprehensive taxonomy in this context covering
               critical research topics such as resource provisioning, operator
               parallelisation, and task scheduling. The literature is then
               reviewed following the taxonomy structure, facilitating a deeper
               understanding of the research landscape through classification
               and comparison of existing works. Finally, we discuss the open
               issues and future research directions toward realising an
               automatic, SLA-aware resource management framework.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  3,
  pages     = "1--41",
  month     =  may,
  year      =  2020,
  file      = "All Papers/L/Liu and Buyya 2020 - Resource Management and Scheduling in Distributed Stream Processing Systems - A Taxonomy, Review, and Future Directions.pdf",
  address   = "New York, NY, USA",
  keywords  = "distributed stream processing systems, stream processing, task
               scheduling, Resource management;Teaching",
  issn      = "0360-0300",
  doi       = "10.1145/3355399"
}

@INPROCEEDINGS{Wang2019-cx,
  title     = "Pigeon: an Effective Distributed, Hierarchical Datacenter Job
               Scheduler",
  booktitle = "Proceedings of the {ACM} Symposium on Cloud Computing",
  author    = "Wang, Zhijun and Li, Huiyang and Li, Zhongwei and Sun, Xiaocui
               and Rao, Jia and Che, Hao and Jiang, Hong",
  abstract  = "In today's datacenters, job heterogeneity makes it difficult for
               schedulers to simultaneously meet latency requirements and
               maintain high resource utilization. The state-of-the-art
               datacenter schedulers, including centralized, distributed, and
               hybrid schedulers, fail to ensure low latency for short jobs in
               large-scale and highly loaded systems. The key issues are the
               scalability in centralized schedulers, ineffective and
               inefficient probing and resource sharing in both distributed and
               hybrid schedulers.In this paper, we propose Pigeon, a
               distributed, hierarchical job scheduler based on a two-layer
               design. Pigeon divides workers into groups, each managed by a
               separate master. In Pigeon, upon a job arrival, a distributed
               scheduler directly distribute tasks evenly among masters with
               minimum job processing overhead, hence, preserving highest
               possible scalability. Meanwhile, each master manages and
               distributes all the received tasks centrally, oblivious of the
               job context, allowing for full sharing of the worker pool at the
               group level to maximize multiplexing gain. To minimize the
               chance of head-of-line blocking for short jobs and avoid
               starvation for long jobs, two weighted fair queues are employed
               in each master to accommodate tasks from short and long jobs,
               separately, and a small portion of the workers are reserved for
               short jobs. Evaluation via theoretical analysis, trace-driven
               simulations, and a prototype implementation shows that Pigeon
               significantly outperforms Sparrow, a representative distributed
               scheduler, and Eagle, a hybrid scheduler.",
  publisher = "Association for Computing Machinery",
  pages     = "246--258",
  series    = "SoCC '19",
  month     =  nov,
  year      =  2019,
  file      = "All Papers/W/Wang et al. 2019 - Pigeon - an Effective Distributed, Hierarchical Datacenter Job Scheduler.pdf",
  address   = "New York, NY, USA",
  keywords  = "resource management, datacenter, Job scheduling",
  location  = "Santa Cruz, CA, USA",
  isbn      = "9781450369732",
  doi       = "10.1145/3357223.3362728"
}

@ARTICLE{Xie2019-mp,
  title     = "Pay as Your Service Needs: An {Application-Driven} Pricing
               Approach for the Internet Economics",
  author    = "Xie, Hong and Wu, Weijie and Ma, Richard T B and Lui, John C S",
  abstract  = "Various differentiated pricing schemes have been proposed for
               the Internet market. Aiming at replacing the traditional
               single-class pricing for better welfare, yet, researchers have
               shown that existing schemes can bring only marginal profit gain
               for the ISPs. In this article, we point out that a proper form
               of differentiated pricing for the Internet should not only
               consider congestion, but more importantly, it should provide
               application specific treatment to data delivery. Formally, we
               propose an ``application-driven pricing'' approach, where an ISP
               offers a number of service classes in terms of a guaranteed
               quality of service and announces a unit usage price for each
               class, and content providers are free to choose which class to
               use depending on the requirement of their applications. Unlike
               previous studies, we point out that the revenue gain of
               multi-class pricing under our scheme can be significant. This is
               because we capture important aspects of application
               heterogeneity and take the quality of service and price as
               control knobs. We identify key factors that impact the revenue
               gain and reveal fundamental understandings on when and why an
               application-driven multi-class pricing can significantly
               increase the revenue of ISPs.",
  journal   = "ACM Trans. Internet Technol.",
  publisher = "Association for Computing Machinery",
  volume    =  19,
  number    =  4,
  pages     = "1--28",
  month     =  nov,
  year      =  2019,
  file      = "All Papers/X/Xie et al. 2019 - Pay as Your Service Needs - An Application-Driven Pricing Approach for the Internet Economics.pdf",
  address   = "New York, NY, USA",
  keywords  = "quality of service, revenue maximization, Application driven
               pricing;ComputerNetworks",
  issn      = "1533-5399",
  doi       = "10.1145/3361148"
}

@ARTICLE{Mayer2020-fs,
  title     = "Scalable Deep Learning on Distributed Infrastructures:
               Challenges, Techniques, and Tools",
  author    = "Mayer, Ruben and Jacobsen, Hans-Arno",
  abstract  = "Deep Learning (DL) has had an immense success in the recent
               past, leading to state-of-the-art results in various domains,
               such as image recognition and natural language processing. One
               of the reasons for this success is the increasing size of DL
               models and the proliferation of vast amounts of training data
               being available. To keep on improving the performance of DL,
               increasing the scalability of DL systems is necessary. In this
               survey, we perform a broad and thorough investigation on
               challenges, techniques and tools for scalable DL on distributed
               infrastructures. This incorporates infrastructures for DL,
               methods for parallel DL training, multi-tenant resource
               scheduling, and the management of training and model data.
               Further, we analyze and compare 11 current open-source DL
               frameworks and tools and investigate which of the techniques are
               commonly implemented in practice. Finally, we highlight future
               research trends in DL systems that deserve further research.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  1,
  pages     = "1--37",
  month     =  feb,
  year      =  2020,
  file      = "All Papers/M/Mayer and Jacobsen 2020 - Scalable Deep Learning on Distributed Infrastructures - Challenges, Techniques, and Tools.pdf",
  address   = "New York, NY, USA",
  keywords  = "Deep-learning systems;ML",
  issn      = "0360-0300",
  doi       = "10.1145/3363554"
}

@ARTICLE{Randal2020-qi,
  title     = "The Ideal Versus the Real: Revisiting the History of Virtual
               Machines and Containers",
  author    = "Randal, Allison",
  abstract  = "The common perception in both academic literature and industry
               today is that virtual machines offer better security, whereas
               containers offer better performance. However, a detailed review
               of the history of these technologies and the current threats
               they face reveals a different story. This survey covers key
               developments in the evolution of virtual machines and containers
               from the 1950s to today, with an emphasis on countering modern
               misperceptions with accurate historical details and providing a
               solid foundation for ongoing research into the future of secure
               isolation for multitenant infrastructures, such as cloud and
               container deployments.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  1,
  pages     = "1--31",
  month     =  feb,
  year      =  2020,
  file      = "All Papers/R/Randal 2020 - The Ideal Versus the Real - Revisiting the History of Virtual Machines and Containers.pdf",
  address   = "New York, NY, USA",
  keywords  = "virtualization, virtual machines, Containers;Teaching",
  issn      = "0360-0300",
  doi       = "10.1145/3365199"
}

@ARTICLE{Hilman2020-kh,
  title     = "Multiple Workflows Scheduling in Multi-tenant Distributed
               Systems: A Taxonomy and Future Directions",
  author    = "Hilman, Muhammad H and Rodriguez, Maria A and Buyya, Rajkumar",
  abstract  = "Workflows are an application model that enables the automated
               execution of multiple interdependent and interconnected tasks.
               They are widely used by the scientific community to manage the
               distributed execution and dataflow of complex simulations and
               experiments. As the popularity of scientific workflows continue
               to rise, and their computational requirements continue to
               increase, the emergence and adoption of multi-tenant computing
               platforms that offer the execution of these workflows as a
               service becomes widespread. This article discusses the
               scheduling and resource provisioning problems particular to this
               type of platform. It presents a detailed taxonomy and a
               comprehensive survey of the current literature and identifies
               future directions to foster research in the field of multiple
               workflow scheduling in multi-tenant distributed computing
               systems.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  1,
  pages     = "1--39",
  month     =  feb,
  year      =  2020,
  annote    = "This is a note on this paper.",
  file      = "All Papers/H/Hilman et al. 2020 - Multiple Workflows Scheduling in Multi-tenant Distributed Systems - A Taxonomy and Future Directions.pdf",
  address   = "New York, NY, USA",
  keywords  = "Scientific workflows, multiple workflows scheduling,
               multi-tenant platforms;Teaching",
  issn      = "0360-0300",
  doi       = "10.1145/3368036"
}

@ARTICLE{Vieira2020-tn,
  title     = "Fast Packet Processing with {eBPF} and {XDP}: Concepts, Code,
               Challenges, and Applications",
  author    = "Vieira, Marcos A M and Castanho, Matheus S and Pac{\'\i}fico,
               Racyus D G and Santos, Elerson R S and J{\'u}nior, Eduardo P M
               C{\^a}mara and Vieira, Luiz F M",
  abstract  = "Extended Berkeley Packet Filter (eBPF) is an instruction set and
               an execution environment inside the Linux kernel. It enables
               modification, interaction, and kernel programmability at
               runtime. eBPF can be used to program the eXpress Data Path
               (XDP), a kernel network layer that processes packets closer to
               the NIC for fast packet processing. Developers can write
               programs in C or P4 languages and then compile to eBPF
               instructions, which can be processed by the kernel or by
               programmable devices (e.g., SmartNICs). Since its introduction
               in 2014, eBPF has been rapidly adopted by major companies such
               as Facebook, Cloudflare, and Netronome. Use cases include
               network monitoring, network traffic manipulation, load
               balancing, and system profiling. This work aims to present eBPF
               to an inexpert audience, covering the main theoretical and
               fundamental aspects of eBPF and XDP, as well as introducing the
               reader to simple examples to give insight into the general
               operation and use of both technologies.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  1,
  pages     = "1--36",
  month     =  feb,
  year      =  2020,
  file      = "All Papers/V/Vieira et al. 2020 - Fast Packet Processing with eBPF and XDP - Concepts, Code, Challenges, and Applications.pdf",
  address   = "New York, NY, USA",
  keywords  = "packet processing, Computer networking, network
               functions;Teaching;NFV\_SDN;ProgrammableNetworks;NFV\_Microservices",
  issn      = "0360-0300",
  doi       = "10.1145/3371038"
}

@ARTICLE{Willinger2019-oa,
  title     = "Lessons from ``on the self-similar nature of ethernet traffic''",
  author    = "Willinger, Walter and Taqqu, Murad S and Wilson, Daniel V",
  journal   = "Comput. Commun. Rev.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  49,
  number    =  5,
  pages     = "56--62",
  month     =  nov,
  year      =  2019,
  file      = "All Papers/W/Willinger et al. 2019 - Lessons from 'on the self-similar nature of ethernet traffic'.pdf",
  keywords  = "NetworkTraffic",
  copyright = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  language  = "en",
  issn      = "0146-4833, 1943-5819",
  doi       = "10.1145/3371934.3371955"
}

@ARTICLE{Feamster2020-pa,
  title     = "Measuring internet speed: current challenges and future
               recommendations",
  author    = "Feamster, Nick and Livingood, Jason",
  abstract  = "Speed testing methods have flourished over the last decade, but
               none without at least some limitations.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  63,
  number    =  12,
  pages     = "72--80",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/F/Feamster and Livingood 2020 - Measuring internet speed - current challenges and future recommendations.pdf",
  address   = "New York, NY, USA",
  keywords  = "ComputerNetworks",
  issn      = "0001-0782",
  doi       = "10.1145/3372135"
}

@ARTICLE{Emek2020-kc,
  title     = "Approximating Generalized Network Design under (Dis)economies of
               Scale with Applications to Energy Efficiency",
  author    = "Emek, Yuval and Kutten, Shay and Lavi, Ron and Shi, Yangguang",
  abstract  = "In a generalized network design (GND) problem, a set of
               resources are assigned (non-exclusively) to multiple requests.
               Each request contributes its weight to the resources it uses and
               the total load on a resource is then translated to the cost it
               incurs via a resource-specific cost function. Motivated by
               energy efficiency applications, recently, there is a growing
               interest in GND using cost functions that exhibit (dis)economies
               of scale ((D)oS), namely, cost functions that appear subadditive
               for small loads and superadditive for larger loads.The current
               article advances the existing literature on approximation
               algorithms for GND problems with (D)oS cost functions in various
               aspects: (1) while the existing results are restricted to
               routing requests in undirected graphs, identifying the resources
               with the graph's edges, the current article presents a generic
               approximation framework that yields approximation results for a
               much wider family of requests (including various types of
               Steiner tree and Steiner forest requests) in both directed and
               undirected graphs, where the resources can be identified with
               either the edges or the vertices; (2) while the existing results
               assume that a request contributes the same weight to each
               resource it uses, our approximation framework allows for
               unrelated weights, thus providing the first non-trivial
               approximation for the problem of scheduling unrelated parallel
               machines with (D)oS cost functions; (3) while most of the
               existing approximation algorithms are based on convex
               programming, our approximation framework is fully combinatorial
               and runs in strongly polynomial time; (4) the family of (D)oS
               cost functions considered in the current article is more general
               than the one considered in the existing literature, providing a
               more accurate abstraction for practical energy conservation
               scenarios; and (5) we obtain the first approximation ratio for
               GND with (D)oS cost functions that depends only on the
               parameters of the resources' technology and does not grow with
               the number of resources, the number of requests, or their
               weights. The design of our approximation framework relies
               heavily on Roughgarden's smoothness toolbox [43], thus
               demonstrating the possible usefulness of this toolbox in the
               area of approximation algorithms.",
  journal   = "J. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  67,
  number    =  1,
  pages     = "1--33",
  month     =  feb,
  year      =  2020,
  file      = "All Papers/E/Emek et al. 2020 - Approximating Generalized Network Design under (Dis)economies of Scale with Applications to Energy Efficiency.pdf",
  address   = "New York, NY, USA",
  keywords  = "(dis)economies of scale, generalized network design, best
               response dynamics, Approximation algorithms, smoothness, energy
               consumption, real exponent polynomial cost
               functions;ComputerNetworks",
  issn      = "0004-5411",
  doi       = "10.1145/3377387"
}

@ARTICLE{Verbraeken2020-ys,
  title     = "A Survey on Distributed Machine Learning",
  author    = "Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and
               Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S",
  abstract  = "The demand for artificial intelligence has grown significantly
               over the past decade, and this growth has been fueled by
               advances in machine learning techniques and the ability to
               leverage hardware acceleration. However, to increase the quality
               of predictions and render machine learning solutions feasible
               for more complex applications, a substantial amount of training
               data is required. Although small machine learning models can be
               trained with modest amounts of data, the input for training
               larger models such as neural networks grows exponentially with
               the number of parameters. Since the demand for processing
               training data has outpaced the increase in computation power of
               computing machinery, there is a need for distributing the
               machine learning workload across multiple machines, and turning
               the centralized into a distributed system. These distributed
               systems present new challenges: first and foremost, the
               efficient parallelization of the training process and the
               creation of a coherent model. This article provides an extensive
               overview of the current state-of-the-art in the field by
               outlining the challenges and opportunities of distributed
               machine learning over conventional (centralized) machine
               learning, discussing the techniques used for distributed machine
               learning, and providing an overview of the systems that are
               available.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  2,
  pages     = "1--33",
  month     =  mar,
  year      =  2020,
  file      = "All Papers/V/Verbraeken et al. 2020 - A Survey on Distributed Machine Learning.pdf",
  address   = "New York, NY, USA",
  keywords  = "distributed systems, Distributed machine learning;ML",
  issn      = "0360-0300",
  doi       = "10.1145/3377454"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Goren2020-mj,
  title     = "Silence",
  author    = "Goren, Guy and Moses, Yoram",
  abstract  = "The cost of communication is a substantial factor affecting the
               scalability of many distributed applications. Every message sent
               can incur a cost in storage, computation, energy, and bandwidth.
               Consequently, reducing the communication costs of distributed
               applications is highly desirable. The best way to reduce message
               costs is by communicating without sending any messages
               whatsoever. This article initiates a rigorous investigation into
               the use of silence in synchronous settings, in which processes
               can fail. We formalize sufficient conditions for information
               transfer using silence, as well as necessary conditions for
               particular cases of interest. This allows us to identify message
               patterns that enable communication through silence. In
               particular, a pattern called a silent choir is identified, and
               shown to be central to information transfer via silence in
               failure-prone systems. The power of the new framework is
               demonstrated on the atomic commitment problem (AC). A complete
               characterization of the tradeoff between message complexity and
               round complexity in the synchronous model with crash failures is
               provided, in terms of lower bounds and matching protocols. In
               particular, a new message-optimal AC protocol is designed using
               silence, in which processes decide in three rounds in the common
               case. This significantly improves on the best previously known
               message-optimal AC protocol, in which decisions were performed
               in $\Theta$(n) rounds.And in the naked light I sawTen thousand
               people, maybe morePeople talking without speaking…People writing
               songs that voices never shareAnd no one daredDisturb the sound
               of silencePaul Simon, 1964",
  journal   = "J. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  67,
  number    =  1,
  pages     = "1--26",
  month     =  jan,
  year      =  2020,
  file      = "All Papers/G/Goren and Moses 2020 - Silence.pdf",
  address   = "New York, NY, USA",
  keywords  = "fault-tolerance, consensus, atomic commitment, silent choir,
               null messages, Silent information exchange, knowledge,
               optimality;Distributed Systems",
  issn      = "0004-5411",
  doi       = "10.1145/3377883"
}

@ARTICLE{Bozan2020-fx,
  title     = "How to transition incrementally to microservice architecture",
  author    = "Bozan, Karoly and Lyytinen, Kalle and Rose, Gregory M",
  abstract  = "A field study examines technological advances that have created
               versatile software ecosystems to develop and deploy
               microservices.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  64,
  number    =  1,
  pages     = "79--85",
  month     =  dec,
  year      =  2020,
  file      = "All Papers/B/Bozan et al. 2020 - How to transition incrementally to microservice architecture.pdf",
  address   = "New York, NY, USA",
  keywords  = "NFV",
  issn      = "0001-0782",
  doi       = "10.1145/3378064"
}

@ARTICLE{Alam2020-hd,
  title     = "A Survey of Network Virtualization Techniques for Internet of
               Things Using {SDN} and {NFV}",
  author    = "Alam, Iqbal and Sharif, Kashif and Li, Fan and Latif, Zohaib and
               Karim, M M and Biswas, Sujit and Nour, Boubakr and Wang, Yu",
  abstract  = "Internet of Things (IoT) and Network Softwarization are fast
               becoming core technologies of information systems and network
               management for the next-generation Internet. The deployment and
               applications of IoT range from smart cities to urban computing
               and from ubiquitous healthcare to tactile Internet. For this
               reason, the physical infrastructure of heterogeneous network
               systems has become more complicated and thus requires efficient
               and dynamic solutions for management, configuration, and flow
               scheduling. Network softwarization in the form of Software
               Defined Networks and Network Function Virtualization has been
               extensively researched for IoT in the recent past. In this
               article, we present a systematic and comprehensive review of
               virtualization techniques explicitly designed for IoT networks.
               We have classified the literature into software-defined networks
               designed for IoT, function virtualization for IoT networks, and
               software-defined IoT networks. These categories are further
               divided into works that present architectural, security, and
               management solutions. Besides, the article highlights several
               short-term and long-term research challenges and open issues
               related to the adoption of software-defined Internet of Things.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  2,
  pages     = "1--40",
  month     =  apr,
  year      =  2020,
  file      = "All Papers/A/Alam et al. 2020 - A Survey of Network Virtualization Techniques for Internet of Things Using SDN and NFV.pdf",
  address   = "New York, NY, USA",
  keywords  = "software-defined network, network function virtualization,
               Internet of Things, software-defined IoT, network
               softwarization;Done;Boring;NFV\_SDN;EdgeFogCloudIoT",
  issn      = "0360-0300",
  doi       = "10.1145/3379444"
}

@ARTICLE{Zolfaghari2020-vt,
  title     = "Content Delivery Networks: State of the Art, Trends, and Future
               Roadmap",
  author    = "Zolfaghari, Behrouz and Srivastava, Gautam and Roy, Swapnoneel
               and Nemati, Hamid R and Afghah, Fatemeh and Koshiba, Takeshi and
               Razi, Abolfazl and Bibak, Khodakhast and Mitra, Pinaki and Rai,
               Brijesh Kumar",
  abstract  = "Recently, Content Delivery Networks (CDN) have become more and
               more popular. The technology itself is ahead of academic
               research in this area. Several dimensions of the technology have
               not been adequately investigated by academia. These dimensions
               include outline management, security, and standardization.
               Discovering and highlighting aspects of this technology that may
               have or have not been covered by academic research is the first
               step toward helping academia bridge the gap with industry or
               even go one step further to lead industry in the right
               direction. This suggests a comprehensive survey on research
               works in this regard. The literature in this area has already
               come up with some surveys and taxonomies, but some of them are
               outdated or do not cover every aspect of CDN while others fail
               to detect existing trends or to develop a holistic roadmap for
               research on the technology. Furthermore, none of the existing
               surveys aim at enlightening the dark aspects of the technology
               that have not been subject to academic research. In this survey,
               we first extract the lifecycle of a CDN as suggested by the
               existing research. Then, we investigate previous relevant works
               on each phase of the lifecycle to clarify where the research is
               currently located and headed. We show how CDN technology tends
               to converge with emerging paradigms such as cloud computing,
               edge computing, and machine learning, which are more mature in
               terms of academic research. This helps us determine the right
               direction for further research by revealing the deficiencies in
               existing works.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  2,
  pages     = "1--34",
  month     =  apr,
  year      =  2020,
  file      = "All Papers/Z/Zolfaghari et al. 2020 - Content Delivery Networks - State of the Art, Trends, and Future Roadmap.pdf",
  address   = "New York, NY, USA",
  keywords  = "trend, survey, Content delivery network;Teaching",
  issn      = "0360-0300",
  doi       = "10.1145/3380613"
}

@INPROCEEDINGS{Paillisse2020-vo,
  title     = "{SD-access}: practical experiences in designing and deploying
               software defined enterprise networks",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Paillisse, Jordi and Portoles, Marc and Lopez, Albert and
               Rodriguez-Natal, Alberto and Iacobacci, David and Leong, Johnson
               and Moreno, Victor and Cabellos, Albert and Maino, Fabio and
               Hooda, Sanjay",
  abstract  = "Enterprise networks, over the years, have become more and more
               complex trying to keep up with new requirements that challenge
               traditional solutions. Just to mention one out of many possible
               examples, technologies such as Virtual LANs (VLANs) struggle to
               address the scalability and operational requirements introduced
               by Internet of Things (IoT) use cases. To keep up with these
               challenges we have identified four main requirements that are
               common across modern enterprise networks: (i) scalable mobility,
               (ii) endpoint segmentation, (iii) simplified administration, and
               (iv) resource optimization. To address these challenges we
               designed SDA (Software Defined Access), a solution for modern
               enterprise networks that leverages Software-Defined Networking
               (SDN) and other state of the art techniques. In this paper we
               present the design, implementation and evaluation of SDA.
               Specifically, SDA: (i) leverages a combination of an overlay
               approach with an event-driven protocol (LISP) to dynamically
               adapt to traffic and mobility patterns while preserving
               resources, and (ii) enforces policies to groups of endpoints for
               scalable segmentation with low operational burden. We present
               our experience with deploying SDA in two real-life scenarios: an
               enterprise campus, and a large warehouse with mobile robots. Our
               evaluation shows that SDA, when compared with traditional
               enterprise networks, can (i) reduce overall data plane
               forwarding state up to 70\% thanks to a reactive protocol using
               a centralized routing server, and (ii) reduce by an order of
               magnitude the handover delays in scenarios of massive mobility
               with respect to other approaches. Finally, we discuss lessons
               learned while deploying and operating SDA, and possible
               optimizations regarding the use of an event-driven protocol and
               group-based segmentation.",
  publisher = "Association for Computing Machinery",
  pages     = "496--508",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/P/Paillisse et al. 2020 - SD-access - practical experiences in designing and deploying software defined enterprise networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "locator-id separation protocol, massive mobility, campus
               networks, enterprise networks, software-defined networks,
               reactive protocols;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431288"
}

@INPROCEEDINGS{Yen2020-mx,
  title     = "Meeting {SLOs} in cross-platform {NFV}",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Yen, Jane and Wang, Jianfeng and Supittayapornpong, Sucha and
               Vieira, Marcos A M and Govindan, Ramesh and Raghavan, Barath",
  abstract  = "Network Functions (NFs) perform on-path processing of network
               traffic. ISPs are deploying NF Virtualization (NFV) with
               software NFs run on commodity servers. ISPs aim to ensure that
               NF chains, directed acyclic graphs of NFs, do not violate
               Service Level Objectives (SLOs) promised by the ISP to its
               customers. To meet SLOs, NFV systems sometimes leverage on-path
               hardware (such as programmable switches and smart NICs) to
               accelerate NF execution.Lemur places and executes NF chains
               across heterogeneous hardware while meeting SLOs. Lemur's novel
               placement algorithm yields an SLO-satisfying NF placement while
               weighing many constraints: hardware memory and processing
               stages, server cores, link capacity, NF profiles, and NF chain
               interactions. Lemur's metacompiler automatically generates code
               and rules (in P4, Python, eBPF, C++, and OpenFlow) to stitch
               cross-platform NF chain execution while also optimizing resource
               usage. Our experiments show that Lemur is alone among competing
               strategies in meeting SLOs for canonical NF chains while
               maximizing marginal throughput (the traffic rate in excess of
               the service-level objective).",
  publisher = "Association for Computing Machinery",
  pages     = "509--523",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/Y/Yen et al. 2020 - Meeting SLOs in cross-platform NFV.pdf",
  address   = "New York, NY, USA",
  keywords  = "PISA switch, service chain, NFV;NFV;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431292"
}

@INPROCEEDINGS{Kozlowski2020-sk,
  title     = "Designing a quantum network protocol",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Kozlowski, Wojciech and Dahlberg, Axel and Wehner, Stephanie",
  abstract  = "The second quantum revolution brings with it the promise of a
               quantum internet. As the first quantum network hardware
               prototypes near completion new challenges emerge. A functional
               network is more than just the physical hardware, yet work on
               scalable quantum network systems is in its infancy. In this
               paper we present a quantum network protocol designed to enable
               end-to-end quantum communication in the face of the new
               fundamental and technical challenges brought by quantum
               mechanics. We develop a quantum data plane protocol that enables
               end-to-end quantum communication and can serve as a building
               block for more complex services. One of the key challenges in
               near-term quantum technology is decoherence --- the gradual
               decay of quantum information --- which imposes extremely
               stringent limits on storage times. Our protocol is designed to
               be efficient in the face of short quantum memory lifetimes. We
               demonstrate this using a simulator for quantum networks and show
               that the protocol is able to deliver its service even in the
               face of significant losses due to decoherence. Finally, we
               conclude by showing that the protocol remains functional on the
               extremely resource limited hardware that is being developed
               today underlining the timeliness of this work.",
  publisher = "Association for Computing Machinery",
  pages     = "1--16",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/K/Kozlowski et al. 2020 - Designing a quantum network protocol.pdf",
  address   = "New York, NY, USA",
  keywords  = "quantum networks, quantum internet, quantum
               communication;GeneralInterest",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431293"
}

@INPROCEEDINGS{Mangla2020-al,
  title     = "Drop the packets: using coarse-grained data to detect video
               performance issues",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Mangla, Tarun and Halepovic, Emir and Zegura, Ellen and Ammar,
               Mostafa",
  abstract  = "Understanding end-user video Quality of Experience (QoE) is
               important for Internet Service Providers (ISPs). Existing work
               presents mechanisms that use network measurement data to
               estimate video QoE. Most of these mechanisms assume access to
               packet-level traces, the most-detailed data available from the
               network. However, collecting packet-level traces can be
               challenging at a network-wide scale. Therefore, we ask:``Is it
               feasible to estimate video QoE with lightweight,
               readily-available, but coarse-grained network data?'' We
               specifically consider data in the form of Transport Layer
               Security (TLS) transactions that can be collected using a
               standard proxy and present a machine learning-based methodology
               to estimate QoE. Our evaluation with three popular streaming
               services shows that the estimation accuracy using TLS
               transactions is high (up to 72\%) with up to 85\% recall in
               detecting low QoE (low video quality or high re-buffering)
               instances. Compared to packet traces, the estimation accuracy
               (recall) is 7\% (9\%) lower but has up to 60 times lower
               computation overhead.",
  publisher = "Association for Computing Machinery",
  pages     = "71--77",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/M/Mangla et al. 2020 - Drop the packets - using coarse-grained data to detect video performance issues.pdf",
  address   = "New York, NY, USA",
  keywords  = "passive measurements, QoE, video streaming;ComputerNetworks",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431294"
}

@INPROCEEDINGS{Goswami2020-vc,
  title     = "Parking packet payload with {P4}",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Goswami, Swati and Kodirov, Nodir and Mustard, Craig and
               Beschastnikh, Ivan and Seltzer, Margo",
  abstract  = "Network Function (NF) deployments suffer from poor link goodput,
               because popular NFs such as firewalls process only packet
               headers while receiving and transmitting complete packets. As a
               result, unnecessary packet payloads needlessly consume link
               bandwidth. We introduce PayloadPark, which improves goodput by
               temporarily parking packet payloads in the stateful memory of
               dataplane programmable switches. PayloadPark forwards only
               packet headers to NF servers, thereby saving bandwidth between
               the switch and the NF server. PayloadPark is a transparent
               in-network optimization that complements existing approaches for
               optimizing NF performance on end-hosts.We prototyped PayloadPark
               on a Barefoot Tofino ASIC using the P4 language. Our prototype,
               when deployed on a top-of-rack switch, can service up to 8 NF
               servers using less than 40\% of the on-chip memory resources.
               The prototype improves goodput by 10-26\% for a Firewall
               $\rightarrow$ NAT NF chain and reduces PCIe bandwidth load by
               2-58\%. With workloads that have datacenter network traffic
               characteristics, PayloadPark provides a 13\% goodput gain with a
               Firewall $\rightarrow$ NAT $\rightarrow$ LB NF chain, without
               latency penalty. In this scenario, we can further increase the
               goodput gain to 28\% by using packet recirculation.",
  publisher = "Association for Computing Machinery",
  pages     = "274--281",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/G/Goswami et al. 2020 - Parking packet payload with P4.pdf",
  address   = "New York, NY, USA",
  keywords  = "network function virtualization, programmable switches,
               P4;NFV;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431295"
}

@INPROCEEDINGS{Ding2020-zy,
  title     = "Agora: Real-time massive {MIMO} baseband processing in software",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Ding, Jian and Doost-Mohammady, Rahman and Kalia, Anuj and
               Zhong, Lin",
  abstract  = "Massive multiple-input multiple-output (MIMO) is a key
               technology in 5G New Radio (NR) to improve spectral efficiency.
               A major challenge in its realization is the huge amount of
               real-time computation required. All existing massive MIMO
               baseband processing solutions use dedicated and specialized
               hardware like FPGAs, which can efficiently process baseband data
               but are expensive, inflexible and difficult to program. In this
               paper, we show that a software-only system called Agora can
               handle the high computational demand of real-time massive MIMO
               baseband processing on a single many-core server. To achieve
               this goal, we identify the rich dimensions of parallelism in
               massive MIMO baseband processing, and exploit them across
               multiple CPU cores. We optimize Agora to best use CPU hardware
               and software features, including SIMD extensions to accelerate
               computation, cache optimizations to accelerate data movement,
               and kernel-bypass packet I/O. We evaluate Agora with up to 64
               antennas and show that it meets the data rate and latency
               requirements of 5G NR.",
  publisher = "Association for Computing Machinery",
  pages     = "232--244",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/D/Ding et al. 2020 - Agora - Real-time massive MIMO baseband processing in software.pdf",
  address   = "New York, NY, USA",
  keywords  = "Wireless;Mobile\_Wireless",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431296"
}

@INPROCEEDINGS{Agarwal2020-bb,
  title     = "Mind the delay: the adverse effects of delay-based {TCP} on
               {HTTP}",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Agarwal, Neil and Varvello, Matteo and Aucinas, Andrius and
               Bustamante, Fabi{\'a}n and Netravali, Ravi",
  abstract  = "The last three decades have seen much evolution in web and
               network protocols: amongst them, a transition from HTTP/1.1 to
               HTTP/2 and a shift from loss-based to delay-based TCP congestion
               control algorithms. This paper argues that these two trends come
               at odds with one another, ultimately hurting web performance.
               Using a controlled synthetic study, we show how delay-based
               congestion control protocols (e.g., BBR and CUBIC + Hybrid Slow
               Start) result in the underestimation of the available congestion
               window in mobile networks, and how that dramatically hampers the
               effectiveness of HTTP/2. To quantify the impact of such finding
               in the current web, we evolve the web performance toolbox in two
               ways. First, we develop Igor, a client-side TCP congestion
               control detection tool that can differentiate between loss-based
               and delay-based algorithms by focusing on their behavior during
               slow start. Second, we develop a Chromium patch which allows
               fine-grained control on the HTTP version to be used per domain.
               Using these new web performance tools, we analyze over 300 real
               websites and find that 67\% of sites relying solely on
               delay-based congestion control algorithms have better
               performance with HTTP/1.1.",
  publisher = "Association for Computing Machinery",
  pages     = "364--370",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/A/Agarwal et al. 2020 - Mind the delay - the adverse effects of delay-based TCP on HTTP.pdf",
  address   = "New York, NY, USA",
  keywords  = "web performance, TCP, congestion control algorithm, protocol
               design, HTTP;ComputerNetworks",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431299"
}

@INPROCEEDINGS{Prehn2020-ub,
  title     = "When wells run dry: the 2020 {IPv4} address market",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Prehn, Lars and Lichtblau, Franziska and Feldmann, Anja",
  abstract  = "With the recent IPv4 address exhaustion, many networks can no
               longer rely on requesting additional IPv4 addresses space. They
               resort to new ways to obtain addresses: buying and leasing. In
               this paper, we first shed light on the recent economic trends of
               the IPv4 buying market by augmenting transfer statistics with
               public and private pricing information from four large IPv4
               brokers. We infer the size of the IPv4 leasing market through
               two different data sources: routing information observed from
               BGP collectors and RDAP databases operated by the Regional
               Internet Registries. We find that neither of those sources alone
               is capable of estimating the full market size. We relate our
               findings to discussions with 13 IPv4 brokers and summarize how
               networks handle their demand for obtaining IPv4 addresses in
               2020.",
  publisher = "Association for Computing Machinery",
  pages     = "46--54",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/P/Prehn et al. 2020 - When wells run dry - the 2020 IPv4 address market.pdf",
  address   = "New York, NY, USA",
  keywords  = "network management, address shortage, IPv4 shortage, IPv4
               address market, IP address distribution;ComputerNetworks",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431301"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Kucera2020-rv,
  title     = "Detecting routing loops in the data plane",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Ku{\v c}era, Jan and Basat, Ran Ben and Kuka, M{\'a}rio and
               Antichi, Gianni and Yu, Minlan and Mitzenmacher, Michael",
  abstract  = "Routing loops can harm network operation. Existing loop
               detection mechanisms, including mirroring packets, storing state
               on switches, or encoding the path onto packets, impose
               significant overheads on either the switches or the network.We
               present Unroller, a solution that enables real-time
               identification of routing loops in the data plane with minimal
               overheads. Our algorithms encode a varying fixed-size subset of
               the traversed path on each packet. That way, our overhead is
               independent of the path length, while we can detect the loop
               once the packet returns to some encoded switch. We implemented
               Unroller in P4 and compiled into three different FPGA targets.
               We then compared it against state-of-the-art solutions on real
               WAN and data center topologies and show that it requires from 6x
               to 100x fewer bits added to packets than existing methods.",
  publisher = "Association for Computing Machinery",
  pages     = "466--473",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/K/Kučera et al. 2020 - Detecting routing loops in the data plane.pdf",
  address   = "New York, NY, USA",
  keywords  = "network algorithms, routing loops, programmable data
               planes;ComputerNetworks",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431303"
}

@INPROCEEDINGS{Wang2020-hv,
  title     = "{CEFS}: compute-efficient flow scheduling for iterative
               synchronous applications",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Wang, Shuai and Li, Dan and Zhang, Jiansong and Lin, Wei",
  abstract  = "Iterative Synchronous Applications (ISApps) are popular in
               today's data centers, represented by distributed deep learning
               (DL) training. In ISApps, multiple nodes carry out the computing
               task iteratively, with globally synchronizing the results in
               each iteration. To increase the scaling efficiency of ISApps, in
               this paper we propose a new flow scheduling approach, called
               CEFS. CEFS saves the waiting time of computing nodes from two
               aspects. For a single node, flows with data which can trigger
               earlier computation at the node are assigned with higher
               priority; among nodes, flows towards slower nodes are assigned
               with higher priority.To address the challenges of realizing CEFS
               in real systems, e.g., the limited number of priority queues on
               commodity switches, the combination of two types of priorities,
               and the adaption to different applications and hardware
               environments, we design an online Bayesian optimization based
               priority assignment algorithm which meets a two-dimension
               order-preserving rule.We implement a CEFS prototype and evaluate
               CEFS through both a 16-node GPU/RoCEv2 testbed by training
               typical DL models and NS-3 simulations. Compared with TensorFlow
               and two representative scheduling solutions: TicTac and
               ByteScheduler, CEFS improves the training throughput by up to
               253\%, 252\% and 47\%, respectively. Besides, the scaling
               efficiency of the 16-node system under TensorFlow, TicTac,
               ByteScheduler and CEFS is 26.6\%~46.9\%, 26.7\%~47.0\%,
               63.9\%~80.3\%, and 92.9\%~94.7\%, respectively. The NS-3
               simulation results show that CEFS can even achieve similar
               scaling efficiency at a larger scale.",
  publisher = "Association for Computing Machinery",
  pages     = "136--148",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/W/Wang et al. 2020 - CEFS - compute-efficient flow scheduling for iterative synchronous applications.pdf",
  address   = "New York, NY, USA",
  keywords  = "colud computing, iterative synchronization applications, flow
               scheduling, datacenter networks;Distributed
               Systems;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431307"
}

@INPROCEEDINGS{Padmanabhan2020-il,
  title     = "{DynamIPs}: analyzing address assignment practices in {IPv4} and
               {IPv6}",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Padmanabhan, Ramakrishna and Rula, John P and Richter, Philipp
               and Strowes, Stephen D and Dainotti, Alberto",
  abstract  = "IP addresses are commonly used to identify hosts or properties
               of hosts. The address assigned to a host may change, however,
               and the extent to which these changes occur in time as well as
               in the address space is currently unknown, especially in IPv6.In
               this work, we take a first step towards understanding the
               dynamics of IPv6 address assignments in various networks around
               the world and how they relate to IPv4 dynamics. We present
               fine-grained observations of dynamics using data collected from
               over 3,000 RIPE Atlas probes in dual-stack networks. RIPE Atlas
               probes in these networks report both their IPv4 and their IPv6
               address, allowing us to track changes over time and in the
               address space. To corroborate and extend our findings, we also
               use a dataset containing 32.7 billion IPv4 and IPv6 address
               associations observed by a major CDN. Our investigation of
               temporal dynamics with these datasets shows that IPv6
               assignments have longer durations than IPv4 assignments---often
               remaining stable for months---thereby allowing the possibility
               of long-term fingerprinting of IPv6 subscribers. Our analysis of
               spatial dynamics reveals IPv6 address-assignment patterns that
               shed light on the size of the address pools network operators
               use in domestic networks, and provides preliminary results on
               the size of the prefixes delegated to home networks. Our
               observations can benefit many applications, including host
               reputation systems, active probing methods, and mechanisms for
               privacy preservation.",
  publisher = "Association for Computing Machinery",
  pages     = "55--70",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/P/Padmanabhan et al. 2020 - DynamIPs - analyzing address assignment practices in IPv4 and IPv6.pdf",
  address   = "New York, NY, USA",
  keywords  = "ComputerNetworks",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431314"
}

@INPROCEEDINGS{Jepsen2020-nc,
  title     = "Forwarding and routing with packet subscriptions",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Jepsen, Theo and Fattaholmanan, Ali and Moshref, Masoud and
               Foster, Nate and Carzaniga, Antonio and Soul{\'e}, Robert",
  abstract  = "In this paper, we explore how programmable data planes can
               naturally provide a higher-level of service to user applications
               via a new abstraction called packet subscriptions. Packet
               subscriptions generalize forwarding rules, and can be used to
               express both traditional routing and more esoteric,
               content-based approaches. We present strategies for routing with
               packet subscriptions in which a centralized controller has a
               global view of the network, and the network topology is
               organized as a hierarchical structure. We also describe a
               compiler for packet subscriptions that uses a novel BDD-based
               algorithm to efficiently translate predicates into P4 tables
               that can support O(100K) expressions. Using our system, we have
               built three diverse applications. We show that these
               applications can be deployed in brownfield networks while
               performing line-rate message processing, using the full switch
               bandwidth of 6.5Tbps.",
  publisher = "Association for Computing Machinery",
  pages     = "282--294",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/J/Jepsen et al. 2020 - Forwarding and routing with packet subscriptions.pdf",
  address   = "New York, NY, USA",
  keywords  = "GeneralNetworking;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431315"
}

@INPROCEEDINGS{Taheri2020-ft,
  title     = "{RoCC}: robust congestion control for {RDMA}",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Taheri, Parvin and Menikkumbura, Danushka and Vanini, Erico and
               Fahmy, Sonia and Eugster, Patrick and Edsall, Tom",
  abstract  = "In this paper, we present RoCC, a robust congestion control
               approach for datacenter networks based on RDMA. RoCC leverages
               switch queue size as an input to a PI controller, which computes
               the fair data rate of flows in the queue, signaling it to the
               flow sources. The PI parameters are self-tuning to guarantee
               stability, rapid convergence, and fair and near-optimal
               throughput in a wide range of congestion scenarios. Our
               simulation and DPDK implementation results show that RoCC can
               achieve up to 7$\times$ reduction in PFC frames generated under
               high average load levels, compared to DCQCN. At the same time,
               RoCC can achieve up to 8$\times$ lower tail latency, compared to
               DCQCN and HPCC. We also find that RoCC does not require PFC. The
               functional components of RoCC are implementable in P4-based and
               fixed-function switch ASICs.",
  publisher = "Association for Computing Machinery",
  pages     = "17--30",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/T/Taheri et al. 2020 - RoCC - robust congestion control for RDMA.pdf",
  address   = "New York, NY, USA",
  keywords  = "network programmability, congestion control, datacenter,
               RDMA;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431316"
}

@INPROCEEDINGS{Aggarwal2020-tv,
  title     = "{LiBRA}: learning-based link adaptation leveraging {PHY} layer
               information in 60 {GHz} {WLANs}",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Aggarwal, Shivang and Sardesai, Urjit Satish and Sinha, Viral
               and Mohan, Deen Dayal and Ghoshal, Moinak and Koutsonikolas,
               Dimitrios",
  abstract  = "We conduct one of the first extensive experimental studies of
               the two main link adaptation mechanisms in 60 GHz WLANs, namely
               rate adaptation and beam adaptation. We first show, using a
               variety of commodity 60 GHz devices, that simple heuristics,
               used by these devices to determine which the two mechanisms
               should be triggered, can lead to wrong decisions even in
               seemingly simple scenarios. We then explore for first time the
               feasibility of leveraging PHY layer information and ML to guide
               link adaptation, using a large dataset collected with a 60 GHz
               software-define radio testbed in a variety of indoor
               environments and scenarios. Finally, we design LiBRA, a
               practical, standard-compliant link adaptation framework that
               leverages ML and PHY layer information to determine when to
               trigger link adaptation and which adaptation mechanism to use.
               LiBRA strikes a balance between throughput and link recovery
               delay, performing close to an oracle solution, and outperforming
               significantly two simple heuristics used by off-the-shelf
               devices.",
  publisher = "Association for Computing Machinery",
  pages     = "245--260",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/A/Aggarwal et al. 2020 - LiBRA - learning-based link adaptation leveraging PHY layer information in 60 GHz WLANs.pdf",
  address   = "New York, NY, USA",
  keywords  = "Wireless",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431319"
}

@INPROCEEDINGS{Gunarathne2020-lz,
  title     = "Distributing deep learning inference on edge devices",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Gunarathne, Buddhi and Prabhath, Chiranthana and Perera, Vinura
               and Gunasekara, Kutila",
  abstract  = "Deep Neural Networks (DNNs) and Convolutional Neural Networks
               (CNNs) are widely used in IoT related applications. However,
               inferencing pre-trained large DNNs and CNNs consumes a
               significant amount of time, memory and computational resources.
               This makes it infeasible to use such DNNs/CNNs on resource
               constrained edge devices, In this research we are trying to
               implement a distributed inference schema for processing large
               DNNs and CNNs in such resource constrained edge devices. Our
               approach of solving this issue is based on partitioning
               DNNs/CNNs model and processing the inference tasks using two or
               more edge devices. Through this poster we introduce our novel
               approach of distributing the inference process among multiple
               edge devices while minimising the network overheads due to
               communication among devices. In addition to that, we present a
               task sharing mechanism among working devices and idle devices in
               the network and a way to convert pre-trained models to a
               separately executable model format.",
  publisher = "Association for Computing Machinery",
  pages     = "556--557",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/G/Gunarathne et al. 2020 - Distributing deep learning inference on edge devices.pdf",
  address   = "New York, NY, USA",
  keywords  = "distributed computing, internet of things, edge computing,
               distributed inference;MLNetworking",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431666"
}

@INPROCEEDINGS{Sacco2020-wb,
  title     = "A distributed reinforcement learning approach for energy and
               congestion-aware edge networks",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Sacco, Alessio and Esposito, Flavio and Marchetto, Guido",
  abstract  = "The abiding attempt of automation has also pervaded computer
               networks, with the ability to measure, analyze, and control
               themselves in an automated manner, by reacting to changes in the
               environment (e.g., demand) while exploiting existing
               flexibilities. When provided with these features, networks are
               often referred to as ``self-driving''. Network virtualization
               and machine learning are the drivers. In this regard, the
               provision and orchestration of physical or virtual resources are
               crucial for both Quality of Service guarantees and cost
               management in the edge/cloud computing ecosystem. Auto-scaling
               mechanisms are hence essential to effectively manage the
               lifecycle of network resources. In this poster, we propose
               Relevant, a distributed reinforcement learning approach to
               enable distributed automation for network orchestrators. Our
               solution aims at solving the congestion control problem within
               Software-Defined Network infrastructures, while being mindful of
               the energy consumption, helping resources to scale up and down
               as traffic demands fluctuate and energy optimization
               opportunities arise.",
  publisher = "Association for Computing Machinery",
  pages     = "546--547",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/S/Sacco et al. 2020 - A distributed reinforcement learning approach for energy and congestion-aware edge networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "reinforcement learning, auto-scaling, self-driving
               networks;FutureInternet;MLNetworking",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431670"
}

@INPROCEEDINGS{Sharma2020-vz,
  title     = "Towards explainable artificial intelligence for network function
               virtualization",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Sharma, Sachin and Nag, Avishek and Cordeiro, Luis and Ayoub,
               Omran and Tornatore, Massimo and Nekovee, Maziar",
  abstract  = "Network Function Virtualization (NFV) refers to the process of
               running network functions in virtualized IT infrastructures as
               softwarized Virtual Network Functions (VNFs). Several telecom
               service providers are currently benefiting from this concept, as
               it enables a faster introduction of new network services,
               thereby meeting changing requirements. Following a trend
               initially adopted by cloud service providers, telecom service
               providers are also adopting de-aggregation of the VNFs into
               microservices ($\mu$services). However, a $\mu$service-based
               architecture that can manage a large set of diverse and
               sensitive network functions requires new Artificial Intelligence
               (AI)-based methodologies to cope with the complexity of the
               $\mu$service-based NFV paradigm. This paper focuses on the use
               of explainable AI (XAI) for gradually migrating towards a
               $\mu$services-based architecture in NFV. The paper first
               establishes the need for XAI to transform the NFV architecture
               to a $\mu$service-based architecture and then describes some of
               our research objectives. Afterwards, our preliminary approach
               and long-term visions are provided.",
  publisher = "Association for Computing Machinery",
  pages     = "558--559",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/S/Sharma et al. 2020 - Towards explainable artificial intelligence for network function virtualization.pdf",
  address   = "New York, NY, USA",
  keywords  = "network function virtualization, explainable AI,
               microservices;NFV;MLNetworking",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431673"
}

@INPROCEEDINGS{Kim2020-kj,
  title     = "Multi-directional {CPU} resource control in edge computing",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Kim, Young Ki and Zomaya, Albert Y",
  abstract  = "Edge computing promises a considerable reduction in latency and
               data volume by placing edge frameworks near the data source.
               Stream processing framework is a critical use case for edge
               computing, presenting in-situ data processing and low latency.
               However, careful CPU resource control is more important than
               cloud computing to provide effective multi-tenancy in
               resource-constrained edge computing. In this work, we
               investigate practical ways of controlling CPU resources of
               stream processing frameworks. Evaluation results from the
               different stream processor parallelism and cgroup parameters
               give a clear direction of designing an essential controller for
               stream processing frameworks in edge computing.",
  publisher = "Association for Computing Machinery",
  pages     = "550--551",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/K/Kim and Zomaya 2020 - Multi-directional CPU resource control in edge computing.pdf",
  address   = "New York, NY, USA",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431675"
}

@INPROCEEDINGS{Amoroso2020-uh,
  title     = "Estimation of traffic matrices via super-resolution and
               federated learning",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Amoroso, Roberto and Esposito, Flavio and Merani, Maria Luisa",
  abstract  = "Network measurement and telemetry techniques are central to the
               management of today's computer networks. One popular technique
               with several applications is the estimation of traffic matrices.
               Existing traffic matrix inference approaches that use
               statistical methods, often make assumptions on the structure of
               the matrix that may be invalid. Data-driven methods, instead,
               often use detailed information about the network topology that
               may be unavailable or impractical to collect.Inspired by the
               field of image processing, we propose a superresolution
               technique for traffic matrix inference that does not require any
               knowledge on the structural properties of the matrix elements to
               infer, nor a large data collection. Our experiments with
               anonymized Internet traces demonstrate that the proposed
               approach can infer fine-grained network traffic with high
               precision outperforming existing data interpolation techniques,
               such as bicubic interpolation.",
  publisher = "Association for Computing Machinery",
  pages     = "560--561",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/A/Amoroso et al. 2020 - Estimation of traffic matrices via super-resolution and federated learning.pdf",
  address   = "New York, NY, USA",
  keywords  = "traffic engineering, machine
               learning;ComputerNetworks;MLNetworking",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431677"
}

@INPROCEEDINGS{Turina2020-lq,
  title     = "Combining split and federated architectures for efficiency and
               privacy in deep learning",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Turina, Valeria and Zhang, Zongshun and Esposito, Flavio and
               Matta, Ibrahim",
  abstract  = "Distributed learning systems are increasingly being adopted for
               a variety of applications as centralized training becomes
               unfeasible. A few architectures have emerged to divide and
               conquer the computational load, or to run privacy-aware deep
               learning models, using split or federated learning. Each
               architecture has benefits and drawbacks. In this work, we
               compare the efficiency and privacy performance of two
               distributed learning architectures that combine the principles
               of split and federated learning, trying to get the best of both.
               In particular, our design goal is to reduce the computational
               power required by each client in Federated Learning and to
               parallelize Split Learning. We share some initial lessons
               learned from our implementation that leverages the PySyft and
               PyGrid libraries.",
  publisher = "Association for Computing Machinery",
  pages     = "562--563",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/T/Turina et al. 2020 - Combining split and federated architectures for efficiency and privacy in deep learning.pdf",
  address   = "New York, NY, USA",
  keywords  = "split learning, privacy, federated
               learning;FutureInternet;MLNetworking",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431678"
}

@INPROCEEDINGS{Nagda2020-mr,
  title     = "{FDP}: a teaching and demo platform for {SDN}",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Nagda, Heena and Nagda, Rakesh and Pedisich, Isaac and Sultana,
               Nik and Loo, Boon Thau",
  abstract  = "There are a wealth of good-quality open-source tools for
               teaching, learning about, and experimenting with P4-based SDN.
               But this tooling and its mode of distribution is geared towards
               usage at the level of ``nuts and bolts'', requiring effort to
               setup even for casual or inexperienced users, and does not give
               ``big picture'' insight into the network as the system executes.
               Our poster describes FDP, a portable platform that builds on
               existing tooling to enable end-to-end experimentation and
               zero-effort in-browser interactive visualization, which we
               envision can benefit teaching of P4-based SDN and research
               demonstration.",
  publisher = "Association for Computing Machinery",
  pages     = "524--525",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/N/Nagda et al. 2020 - FDP - a teaching and demo platform for SDN.pdf",
  address   = "New York, NY, USA",
  keywords  = "visualization, FDP, demonstration, P4, teaching,
               SDN;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3431886"
}

@INPROCEEDINGS{Wang2020-kz,
  title     = "Job scheduling for large-scale machine learning clusters",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Wang, Haoyu and Liu, Zetian and Shen, Haiying",
  abstract  = "With the rapid proliferation of Machine Learning (ML) and Deep
               learning (DL) applications running on modern platforms, it is
               crucial to satisfy application performance requirements such as
               meeting deadline and ensuring accuracy. To this end, researchers
               have proposed several job schedulers for ML clusters. However,
               none of the previously proposed schedulers consider ML model
               parallelism, though it has been proposed as an approach to
               increase the efficiency of running large-scale ML and DL jobs.
               Thus, in this paper, we propose an ML job Feature based job
               Scheduling system (MLFS) for ML clusters running both data
               parallelism and model parallelism ML jobs. MLFS first uses a
               heuristic scheduling method that considers an ML job's spatial
               and temporal features to determine task priority for job queue
               ordering in order to improve job completion time (JCT) and
               accuracy performance. It uses the data from the heuristic
               scheduling method for training a deep reinforcement learning
               (RL) model. After the RL model is well trained, it then switches
               to the RL method to automatically make decisions on job
               scheduling. Furthermore, MLFS has a system load control method
               that selects tasks from overloaded servers to move to
               underloaded servers based on task priority, and also
               intelligently removes the tasks that generate little or no
               improvement on the desired accuracy performance when the system
               is overloaded to improve JCT and accuracy by job deadline. Real
               experiments and large-scale simulation based on real trace show
               that MLFS reduces JCT by up to 53\% and makespan by up to 52\%,
               and improves accuracy by up to 64\% when compared with existing
               ML job schedulers. We also open sourced our code.",
  publisher = "Association for Computing Machinery",
  pages     = "108--120",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/W/Wang et al. 2020 - Job scheduling for large-scale machine learning clusters.pdf",
  address   = "New York, NY, USA",
  keywords  = "resource management, job scheduling, machine
               learning;Distributed Systems",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3432588"
}

@INPROCEEDINGS{Gupta2020-eb,
  title     = "Near-optimal multihop scheduling in general circuit-switched
               networks",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Gupta, Himanshu and Curran, Max and Zhan, Caitao",
  abstract  = "Circuit switched networks with high-bandwidth links are
               essential to handling ever increasing traffic demands in today's
               data centers. As these networks incur a non-trivial
               reconfiguration delay, they are mainly suited for bursty traffic
               or large flows. To address the reconfiguration delay vs.
               high-bandwidth tradeoff in circuit networks, an essential
               traffic scheduling problem is to determine a sequence of network
               configurations to optimally serve a given traffic. Recent works
               have addressed this scheduling problem for one-hop traffic in
               fully-connected circuit networks.In this work, we consider the
               traffic scheduling problem in general circuit networks with
               multi-hop traffic load. Such a general model is essential for
               networks with indirect routes between some nodes, e.g., for
               recently proposed wireless optical (FSO-based) networks, or to
               allow multi-hop routes for load balancing. In this context, we
               develop an efficient algorithm that empirically delivers high
               network throughput, while also guaranteeing a constant-factor
               approximation with respect to an objective closely related to
               network throughput. We generalize our technique and
               approximation result to more general settings, including to the
               joint optimization problem of determining flow routes as well as
               a sequence of network configurations. We demonstrate the
               effectiveness of our techniques via extensive simulations on
               synthetic traffic loads based on published traffic
               characteristics as well as publicly available real traffic
               loads; we observe significant performance gains in terms of
               network throughput when compared to approaches based on prior
               work, and very similar performance to an appropriate upper
               bound.",
  publisher = "Association for Computing Machinery",
  pages     = "31--45",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/G/Gupta et al. 2020 - Near-optimal multihop scheduling in general circuit-switched networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "reconfiguration networks, matching, approximation
               algorithms;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3432589"
}

@INPROCEEDINGS{Faltelli2020-ly,
  title     = "Metronome: adaptive and precise intermittent packet retrieval in
               {DPDK}",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Faltelli, Marco and Belocchi, Giacomo and Quaglia, Francesco and
               Pontarelli, Salvatore and Bianchi, Giuseppe",
  abstract  = "DPDK (Data Plane Development Kit) is arguably today's most
               employed framework for software packet processing. Its
               impressive performance however comes at the cost of precious CPU
               resources, dedicated to continuously poll the NICs. To face this
               issue, this paper presents Metronome, an approach devised to
               replace the continuous DPDK polling with a sleep\&wake
               intermittent mode. Metronome revolves around two main
               innovations. First, we design a microseconds time-scale sleep
               function, named hr\_sleep(), which outperforms Linux'
               nanosleep() of more than one order of magnitude in terms of
               precision when running threads with common time-sharing
               priorities. Then, we design, model, and assess an efficient
               multi-thread operation which guarantees service continuity and
               improved robustness against preemptive thread executions, like
               in common CPU-sharing scenarios, meanwhile providing controlled
               latency and high polling efficiency by dynamically adapting to
               the measured traffic load.",
  publisher = "Association for Computing Machinery",
  pages     = "406--420",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/F/Faltelli et al. 2020 - Metronome - adaptive and precise intermittent packet retrieval in DPDK.pdf",
  address   = "New York, NY, USA",
  keywords  = "FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3432730"
}

@INPROCEEDINGS{Van_Bemten2020-lm,
  title     = "Chameleon: predictable latency and high utilization with
               queue-aware and adaptive source routing",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Van Bemten, Amaury and {\DH}eri{\'c}, Nemanja and Varasteh, Amir
               and Schmid, Stefan and Mas-Machuca, Carmen and Blenk, Andreas
               and Kellerer, Wolfgang",
  abstract  = "This paper presents Chameleon, a cloud network providing both
               predictable latency and high utilization, typically two
               conflicting goals, especially in multi-tenant datacenters.
               Chameleon exploits routing flexibilities available in modern
               communication networks to dynamically adapt toward the demand,
               and uses network calculus principles along individual paths.
               More specifically, Chameleon employs source routing on the
               ``queue-level topology'', a network abstraction that accounts
               for the current states of the network queues and, hence, the
               different delays of different paths. Chameleon is based on a
               simple greedy algorithm and can be deployed at the edge; it does
               not require any modifications of network devices. We implement
               and evaluate Chameleon in simulations and a real testbed.
               Compared to state-of-the-art, we find that Chameleon can admit
               and embed significantly, i.e., up to 15 times more flows,
               improving network utilization while meeting strict latency
               guarantees.",
  publisher = "Association for Computing Machinery",
  pages     = "451--465",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/V/Van Bemten et al. 2020 - Chameleon - predictable latency and high utilization with queue-aware and adaptive source routing.pdf",
  address   = "New York, NY, USA",
  keywords  = "predictability, latency, reconfigurations, network
               calculus;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3432879"
}

@INPROCEEDINGS{Gilad2020-qa,
  title     = "{MPCC}: online learning multipath transport",
  booktitle = "Proceedings of the 16th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Gilad, Tomer and Rozen-Schiff, Neta and Godfrey, P Brighten and
               Raiciu, Costin and Schapira, Michael",
  abstract  = "Multipath transport, as embodied in MPTCP, is deployed to
               improve throughput and reliability in mobile and residential
               access networks, with additional use-cases including spreading
               load in data centers and WANs. However, MPTCP is fundamentally
               tied to TCP Reno's legacy AIMD algorithm, and significantly lags
               behind the performance of modern single-path designs.
               Consequently, MPTCP fails to achieve high performance in many
               real-world environments.We present MPCC, a high-performance
               multipath congestion control architecture. To achieve our
               combined goals of fairness and high performance in challenging
               environments, MPCC employs online convex optimization (a.k.a.
               online learning). In experiments with a kernel implementation on
               emulated and live networks, MPCC significantly outperforms
               MPTCP.",
  publisher = "Association for Computing Machinery",
  pages     = "121--135",
  series    = "CoNEXT '20",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/G/Gilad et al. 2020 - MPCC - online learning multipath transport.pdf",
  address   = "New York, NY, USA",
  keywords  = "congestion control, multipath;MLNetworking;FutureInternet",
  location  = "Barcelona, Spain",
  isbn      = "9781450379489",
  doi       = "10.1145/3386367.3433030"
}

@ARTICLE{Beaumont2020-qn,
  title     = "Scheduling on Two Types of Resources: A Survey",
  author    = "Beaumont, Olivier and Canon, Louis-Claude and Eyraud-Dubois,
               Lionel and Lucarelli, Giorgio and Marchal, Loris and Mommessin,
               Cl{\'e}ment and Simon, Bertrand and Trystram, Denis",
  abstract  = "The evolution in the design of modern parallel platforms leads
               to revisit the scheduling jobs on distributed heterogeneous
               resources. The goal of this survey is to present the main
               existing algorithms, to classify them based on their underlying
               principles, and to propose unified implementations to enable
               their fair comparison, in terms of running time and quality of
               schedules, on a large set of common benchmarks that we made
               available for the community. Beyond this comparison, our goal is
               also to understand the main difficulties that heterogeneity
               conveys and the shared principles that guide the design of
               efficient algorithms.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  3,
  pages     = "1--36",
  month     =  may,
  year      =  2020,
  file      = "All Papers/B/Beaumont et al. 2020 - Scheduling on Two Types of Resources - A Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "resource allocation, makespan minimization, Scheduling,
               performance evaluation, online scheduling,
               heterogeneity;Teaching",
  issn      = "0360-0300",
  doi       = "10.1145/3387110"
}

@INPROCEEDINGS{Manousis2020-ty,
  title     = "{Contention-Aware} Performance Prediction For Virtualized
               Network Functions",
  booktitle = "Proceedings of the Annual conference of the {ACM} Special
               Interest Group on Data Communication on the applications,
               technologies, architectures, and protocols for computer
               communication",
  author    = "Manousis, Antonis and Sharma, Rahul Anand and Sekar, Vyas and
               Sherry, Justine",
  abstract  = "At the core of Network Functions Virtualization lie Network
               Functions (NFs) that run co-resident on the same server, contend
               over its hardware resources and, thus, might suffer from reduced
               performance relative to running alone on the same hardware.
               Therefore, to efficiently manage resources and meet performance
               SLAs, NFV orchestrators need mechanisms to predict
               contention-induced performance degradation. In this work, we
               find that prior performance prediction frameworks suffer from
               poor accuracy on modern architectures and NFs because they treat
               memory as a monolithic whole. In addition, we show that, in
               practice, there exist multiple components of the memory
               subsystem that can separately induce contention. By precisely
               characterizing (1) the pressure each NF applies on the server's
               shared hardware resources (contentiousness) and (2) how
               susceptible each NF is to performance drop due to competing
               contentiousness (sensitivity), we develop SLOMO, a multivariable
               performance prediction framework for Network Functions. We show
               that relative to prior work SLOMO reduces prediction error by
               2-5x and enables 6-14\% more efficient cluster utilization.
               SLOMO's codebase can be found at
               https://github.com/cmu-snap/SLOMO.",
  publisher = "Association for Computing Machinery",
  pages     = "270--282",
  series    = "SIGCOMM '20",
  month     =  jul,
  year      =  2020,
  file      = "All Papers/M/Manousis et al. 2020 - Contention-Aware Performance Prediction For Virtualized Network Functions.pdf",
  address   = "New York, NY, USA",
  keywords  = "Packet Processing Software, Network Functions Performance;NFV",
  location  = "Virtual Event, USA",
  isbn      = "9781450379557",
  doi       = "10.1145/3387514.3405868"
}

@ARTICLE{Wang2020-rw,
  title     = "An Efficient Service Function Chaining Placement Algorithm in
               Mobile Edge Computing",
  author    = "Wang, Meng and Cheng, Bo and Chen, Junliang",
  abstract  = "Mobile Edge Computing (MEC) is a promising network architecture
               that pushes network control and mobile computing to the network
               edge. Recent studies propose to deploy MEC applications in the
               Network Function Virtualization (NFV) environment. The mobile
               network service in NFV is deployed as a Service Function
               Chaining (SFC). In the dynamic and resource-limited mobile
               network, SFC placement aiming at optimizing resource utilization
               is a challenging problem. In this article, we solve the SFC
               placement problem in the MEC-NFV environment. We formulate the
               SFC placement problem as a weighted graph matching problem,
               including two sub-problems: a graph matching problem and an SFC
               mapping problem. To efficiently solve the graph matching
               problem, we propose a Linear Programming--(LP) based approach to
               calculate the similarity between VNFs and physical nodes. Based
               on the similarity, we design a Hungarian-based algorithm to
               solve the SFC mapping problem. Evaluation results show that our
               proposed LP-based solutions outperform the heuristic algorithms
               in terms of execution time and resource utilization.",
  journal   = "ACM Trans. Internet Technol.",
  publisher = "Association for Computing Machinery",
  volume    =  20,
  number    =  4,
  pages     = "1--21",
  month     =  oct,
  year      =  2020,
  file      = "All Papers/W/Wang et al. 2020 - An Efficient Service Function Chaining Placement Algorithm in Mobile Edge Computing.pdf",
  address   = "New York, NY, USA",
  keywords  = "Mobile edge computing, network function virtualization, linear
               programming, service function chaining,
               placement;NFV;EdgeFogCloudIoT",
  issn      = "1533-5399",
  doi       = "10.1145/3388241"
}

@ARTICLE{Welsh2020-hu,
  title     = "On Resilience in Cloud Computing: A Survey of Techniques across
               the Cloud Domain",
  author    = "Welsh, Thomas and Benkhelifa, Elhadj",
  abstract  = "Cloud infrastructures are highly favoured as a computing
               delivery model worldwide, creating a strong societal dependence.
               It is therefore vital to enhance their resilience, providing
               persistent service delivery under a variety of conditions. Cloud
               environments are highly complex and continuously evolving.
               Additionally, the plethora of use-cases ensures requirements for
               persistent service delivery vary. As a contribution to
               knowledge, this work surveys resilience techniques for cloud
               environments. We apply a novel perspective using a layered model
               of traditional and emerging cloud paradigms. Works are then
               classified according to the Resilinets model. For each layer,
               the most common techniques with limitations are derived
               including an actor's strength in influencing resilience in the
               cloud with each technique. We conclude with some future
               challenges to the field of resilient cloud computing.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  3,
  pages     = "1--36",
  month     =  may,
  year      =  2020,
  file      = "All Papers/W/Welsh and Benkhelifa 2020 - On Resilience in Cloud Computing - A Survey of Techniques across the Cloud Domain.pdf",
  address   = "New York, NY, USA",
  keywords  = "cloud, edge, Resilience, fog, survey;Distributed Systems",
  issn      = "0360-0300",
  doi       = "10.1145/3388922"
}

@ARTICLE{Ismail2020-uh,
  title     = "Computing Server Power Modeling in a Data Center: Survey,
               Taxonomy, and Performance Evaluation",
  author    = "Ismail, Leila and Materwala, Huned",
  abstract  = "Data centers are large-scale, energy-hungry infrastructure
               serving the increasing computational demands as the world is
               becoming more connected in smart cities. The emergence of
               advanced technologies such as cloud-based services, internet of
               things (IoT), and big data analytics has augmented the growth of
               global data centers, leading to high energy consumption. This
               upsurge in energy consumption of the data centers not only
               incurs the issue of surging high cost (operational and
               maintenance) but also has an adverse effect on the environment.
               Dynamic power management in a data center environment requires
               the cognizance of the correlation between the system and
               hardware-level performance counters and the power consumption.
               Power consumption modeling exhibits this correlation and is
               crucial in designing energy-efficient optimization strategies
               based on resource utilization. Several works in power modeling
               are proposed and used in the literature. However, these power
               models have been evaluated using different benchmarking
               applications, power-measurement techniques, and
               error-calculation formulas on different machines. In this work,
               we present a taxonomy and evaluation of 24 software-based power
               models using a unified environment, benchmarking applications,
               power-measurement techniques, and error formulas, with the aim
               of achieving an objective comparison. We use different server
               architectures to assess the impact of heterogeneity on the
               models' comparison. The performance analysis of these models is
               elaborated in the article.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  3,
  pages     = "1--34",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/I/Ismail and Materwala 2020 - Computing Server Power Modeling in a Data Center - Survey, Taxonomy, and Performance Evaluation.pdf",
  address   = "New York, NY, USA",
  keywords  = "resource utilization, energy-efficiency, machine learning, Data
               center, green computing, server power consumption modeling",
  issn      = "0360-0300",
  doi       = "10.1145/3390605"
}

@ARTICLE{Salaht2020-oo,
  title     = "An Overview of Service Placement Problem in Fog and Edge
               Computing",
  author    = "Salaht, Farah A{\"\i}t and Desprez, Fr{\'e}d{\'e}ric and Lebre,
               Adrien",
  abstract  = "To support the large and various applications generated by the
               Internet of Things (IoT), Fog Computing was introduced to
               complement the Cloud Computing and offer Cloud-like services at
               the edge of the network with low latency and real-time
               responses. Large-scale, geographical distribution, and
               heterogeneity of edge computational nodes make service placement
               in such infrastructure a challenging issue. Diversity of user
               expectations and IoT devices characteristics also complicate the
               deployment problem. This article presents a survey of current
               research conducted on Service Placement Problem (SPP) in the
               Fog/Edge Computing. Based on a new classification scheme, a
               categorization of current proposals is given and identified
               issues and challenges are discussed.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  3,
  pages     = "1--35",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/S/Salaht et al. 2020 - An Overview of Service Placement Problem in Fog and Edge Computing.pdf",
  address   = "New York, NY, USA",
  keywords  = "optimization, Fog computing, classification, edge computing,
               deployment taxonomy, service
               placement;Good;Done;NFV\_SDN;EdgeFogCloudIoT;FutureInternet;Cloud",
  issn      = "0360-0300",
  doi       = "10.1145/3391196"
}

@ARTICLE{Boukerche2020-bw,
  title     = "Computation Offloading and Retrieval for Vehicular Edge
               Computing: Algorithms, Models, and Classification",
  author    = "Boukerche, Azzedine and Soto, Victor",
  abstract  = "The rapid evolution of mobile devices, their applications, and
               the amount of data generated by them causes a significant
               increase in bandwidth consumption and congestions in the network
               core. Edge Computing offers a solution to these performance
               drawbacks by extending the cloud paradigm to the edge of the
               network using capable nodes of processing compute-intensive
               tasks. In the recent years, vehicular edge computing has emerged
               for supporting mobile applications. Such paradigm relies on
               vehicles as edge node devices for providing storage,
               computation, and bandwidth resources for resource-constrained
               mobile applications. In this article, we study the challenges of
               computation offloading for vehicular edge computing. We propose
               a new classification for the better understanding of the
               literature designing vehicular edge computing. We propose a
               taxonomy to classify partitioning solutions in filter-based and
               automatic techniques; scheduling is separated in adaptive,
               social-based, and deadline-sensitive methods, and finally data
               retrieval is organized in secure, distance, mobility prediction,
               and social-based procedures. By reviewing and analyzing
               literature, we found that vehicular edge computing is feasible
               and a viable option to address the increasing volume of data
               traffic. Moreover, we discuss the open challenges and future
               directions that must be addressed towards efficient and
               effective computation offloading and retrieval from mobile users
               to vehicular edge computing.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  4,
  pages     = "1--35",
  month     =  aug,
  year      =  2020,
  file      = "All Papers/B/Boukerche and Soto 2020 - Computation Offloading and Retrieval for Vehicular Edge Computing - Algorithms, Models, and Classification.pdf",
  address   = "New York, NY, USA",
  keywords  = "data retrieval, vehicular cloud, Computation offloading;NFV\_SDN",
  issn      = "0360-0300",
  doi       = "10.1145/3392064"
}

@ARTICLE{Guo2020-pc,
  title     = "The Future of False Information Detection on Social Media: New
               Perspectives and Trends",
  author    = "Guo, Bin and Ding, Yasan and Yao, Lina and Liang, Yunji and Yu,
               Zhiwen",
  abstract  = "The massive spread of false information on social media has
               become a global risk, implicitly influencing public opinion and
               threatening social/political development. False information
               detection (FID) has thus become a surging research topic in
               recent years. As a promising and rapidly developing research
               field, we find that much effort has been paid to new research
               problems and approaches of FID. Therefore, it is necessary to
               give a comprehensive review of the new research trends of FID.
               We first give a brief review of the literature history of FID,
               based on which we present several new research challenges and
               techniques of it, including early detection, detection by
               multimodal data fusion, and explanatory detection. We further
               investigate the extraction and usage of various crowd
               intelligence in FID, which paves a promising way to tackle FID
               challenges. Finally, we give our views on the open issues and
               future research directions of FID, such as model
               adaptivity/generality to new events, embracing of novel machine
               learning models, aggregation of crowd wisdom, adversarial attack
               and defense in detection models, and so on.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  4,
  pages     = "1--36",
  month     =  jul,
  year      =  2020,
  file      = "All Papers/G/Guo et al. 2020 - The Future of False Information Detection on Social Media - New Perspectives and Trends.pdf",
  address   = "New York, NY, USA",
  keywords  = "crowd intelligence, social media, fake news, explanatory
               detection, False information detection;Done;GenericInterest",
  issn      = "0360-0300",
  doi       = "10.1145/3393880"
}

@ARTICLE{Zhou2020-gq,
  title     = "A Survey of Fake News: Fundamental Theories, Detection Methods,
               and Opportunities",
  author    = "Zhou, Xinyi and Zafarani, Reza",
  abstract  = "The explosive growth in fake news and its erosion to democracy,
               justice, and public trust has increased the demand for fake news
               detection and intervention. This survey reviews and evaluates
               methods that can detect fake news from four perspectives: the
               false knowledge it carries, its writing style, its propagation
               patterns, and the credibility of its source. The survey also
               highlights some potential research tasks based on the review. In
               particular, we identify and detail related fundamental theories
               across various disciplines to encourage interdisciplinary
               research on fake news. It is our hope that this survey can
               facilitate collaborative efforts among experts in computer and
               information sciences, social sciences, political science, and
               journalism to research fake news, where such efforts can lead to
               fake news detection that is not only efficient but, more
               importantly, explainable.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  5,
  pages     = "1--40",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/Z/Zhou and Zafarani 2020 - A Survey of Fake News - Fundamental Theories, Detection Methods, and Opportunities.pdf",
  address   = "New York, NY, USA",
  keywords  = "fact-checking, deception detection, information credibility,
               news verification, disinformation, social media, Fake news,
               misinformation, knowledge graph;GenericInterest",
  issn      = "0360-0300",
  doi       = "10.1145/3395046"
}

@ARTICLE{Fei2020-xb,
  title     = "Paving the Way for {NFV} Acceleration: A Taxonomy, Survey and
               Future Directions",
  author    = "Fei, Xincai and Liu, Fangming and Zhang, Qixia and Jin, Hai and
               Hu, Hongxin",
  abstract  = "As a recent innovation, network functions virtualization
               (NFV)---with its core concept of replacing hardware middleboxes
               with software network functions (NFs) implemented in commodity
               servers---promises cost savings and flexibility benefits.
               However, transitioning NFs from special-purpose hardware to
               commodity servers has turned out to be more challenging than
               expected, as it inevitably incurs performance penalties due to
               bottlenecks in both software and hardware. To achieve
               performance comparable to hardware middleboxes, there is a
               strong demand for a speedup in NF processing, which plays a
               crucial role in the success of NFV. In this article, we study
               the performance challenges that exist in general-purpose servers
               and simultaneously summarize the typical performance bottlenecks
               in NFV. Through reviewing the progress in the field of NFV
               acceleration, we present a new taxonomy of the state-of-the-art
               efforts according to various acceleration approaches. We discuss
               the surveyed works and identify the respective advantages and
               disadvantages in each category. We then discuss the products,
               solutions, and projects emerged in industry. We also present a
               gap analysis to improve current solutions and highlight
               promising research trends that can be explored in the future.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  4,
  pages     = "1--42",
  month     =  aug,
  year      =  2020,
  file      = "All Papers/F/Fei et al. 2020 - Paving the Way for NFV Acceleration - A Taxonomy, Survey and Future Directions.pdf",
  address   = "New York, NY, USA",
  keywords  = "Network functions virtualization, NFV acceleration, high
               performance;NFV\_SDN;FutureInternet",
  issn      = "0360-0300",
  doi       = "10.1145/3397022"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Zhang2020-ow,
  title     = "Online dispatching and scheduling of jobs with heterogeneous
               utilities in edge computing",
  booktitle = "Proceedings of the {Twenty-First} International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Zhang, Chi and Tan, Haisheng and Huang, Haoqiang and Han,
               Zhenhua and Jiang, Shaofeng H-C and Freris, Nikolaos and Li,
               Xiang-Yang",
  abstract  = "Edge computing systems typically handle a wide variety of
               applications that exhibit diverse degrees of sensitivity to job
               latency. Therefore, a multitude of utility functions of the job
               response time need to be considered by the underlying job
               dispatching and scheduling mechanism. Nonetheless, previous
               works in edge computing mainly focused on either one kind of
               utility function (e.g., linear, sigmoid, or the hard deadline)
               or different kinds of utilities separately. In this paper, we
               investigate online job dispatching and scheduling strategies
               under the setting of coexistence of heterogeneous utilities,
               i.e., various coexisting jobs can employ different
               non-increasing utility functions. The goal is to maximize the
               total utility over all jobs in an edge system. Besides
               heterogeneous utilities, we here adopt a practical online model
               where the unrelated machine model and the upload and download
               delay are considered. We proceed to propose an online algorithm,
               O4A, to dispatch and schedule jobs with heterogeneous utilities.
               Our theoretical analysis shows that O4A is O(1/ɛ2)-competitive
               under the (1 + ɛ)-speed augmentation model, where ɛ is a small
               positive constant. We implement O4A on an edge computing testbed
               running deep learning inference jobs. With the production trace
               from Google Cluster, our experimental and large-scale simulation
               results indicate that O4A can increase the total utility by up
               to 39.42\% compared with state-of-the-art utility-agnostic
               methods. Moreover, O4A is robust to estimation errors in job
               processing time and transmission delay.",
  publisher = "Association for Computing Machinery",
  pages     = "101--110",
  series    = "Mobihoc '20",
  month     =  oct,
  year      =  2020,
  file      = "All Papers/Z/Zhang et al. 2020 - Online dispatching and scheduling of jobs with heterogeneous utilities in edge computing.pdf",
  address   = "New York, NY, USA",
  keywords  = "edge computing, job scheduling, online approximation,
               heterogeneous utility function;EdgeFogCloudIoT",
  location  = "Virtual Event, USA",
  isbn      = "9781450380157",
  doi       = "10.1145/3397166.3409122"
}

@INPROCEEDINGS{Son2020-jg,
  title     = "{REFRAIN}: promoting valid transmission in high-density modern
               wi-fi networks",
  booktitle = "Proceedings of the {Twenty-First} International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Son, Youngwook and Lee, Kanghyun and Kim, Seongwon and Lee,
               Jinmyeong and Choi, Sunghyun and Bahk, Saewoong",
  abstract  = "For emerging high-density Wi-Fi networks, there have been plenty
               of studies that claim the need for more aggressive channel
               access to enhance spatial reuse. Against those previous ideas,
               this paper presents a different perspective that existing Wi-Fi
               devices already have excessive transmission opportunities, even
               without protecting each other in certain scenarios. We shed
               light on an anomaly within actual carrier sensing (CS)
               behaviors, which makes some neighboring devices become blind to
               each other and transmit simultaneously, due to undetected
               preambles. Through experimental study and analysis, we reveal
               both sides of the anomaly heavily affecting the overall network
               performance. Based on the observations, we design REFRAIN, a
               standard-compliant PHY/MAC framework, which copes with and
               further exploits the anomaly for better spatial reuse. Our
               prototype using NI USRP and commercial Wi-Fi devices shows the
               feasibility and effectiveness of our approach, while extensive
               simulation results demonstrate that REFRAIN achieves up to
               1.57$\times$ higher average throughput by promoting valid
               transmissions, without modifying the 802.11 CS specification at
               all.",
  publisher = "Association for Computing Machinery",
  pages     = "221--230",
  series    = "Mobihoc '20",
  month     =  oct,
  year      =  2020,
  file      = "All Papers/S/Son et al. 2020 - REFRAIN - promoting valid transmission in high-density modern wi-fi networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "carrier sensing, 802.11, spatial reuse, dense networks,
               wi-fi;Mobile\_Wireless",
  location  = "Virtual Event, USA",
  isbn      = "9781450380157",
  doi       = "10.1145/3397166.3409124"
}

@INPROCEEDINGS{Zhang2020-iw,
  title     = "Online scheduling of heterogeneous distributed machine learning
               jobs",
  booktitle = "Proceedings of the {Twenty-First} International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Zhang, Qin and Zhou, Ruiting and Wu, Chuan and Jiao, Lei and Li,
               Zongpeng",
  abstract  = "Distributed machine learning (ML) has played a key role in
               today's proliferation of AI services. A typical model of
               distributed ML is to partition training datasets over multiple
               worker nodes to update model parameters in parallel, adopting a
               parameter server architecture. ML training jobs are typically
               resource elastic, completed using various time lengths with
               different resource configurations. A fundamental problem in a
               distributed ML cluster is how to explore the demand elasticity
               of ML jobs and schedule them with different resource
               configurations, such that the utilization of resources is
               maximized and average job completion time is minimized. To
               address it, we propose an online scheduling algorithm to decide
               the execution time window, the number and the type of concurrent
               workers and parameter servers for each job upon its arrival,
               with a goal of minimizing the weighted average completion time.
               Our online algorithm consists of (i) an online scheduling
               framework that groups unprocessed ML training jobs into a batch
               iteratively, and (ii) a batch scheduling algorithm that
               configures each ML job to maximize the total weight of scheduled
               jobs in the current iteration. Our online algorithm guarantees a
               good parameterized competitive ratio with polynomial time
               complexity. Extensive evaluations using real-world data
               demonstrate that it outperforms state-of-the-art schedulers in
               today's AI cloud systems.",
  publisher = "Association for Computing Machinery",
  pages     = "111--120",
  series    = "Mobihoc '20",
  month     =  oct,
  year      =  2020,
  file      = "All Papers/Z/Zhang et al. 2020 - Online scheduling of heterogeneous distributed machine learning jobs.pdf",
  address   = "New York, NY, USA",
  keywords  = "MLNetworking",
  location  = "Virtual Event, USA",
  isbn      = "9781450380157",
  doi       = "10.1145/3397166.3409128"
}

@INPROCEEDINGS{Li2020-fx,
  title     = "Emulating round-robin for serving dynamic flows over wireless
               fading channels",
  booktitle = "Proceedings of the {Twenty-First} International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Li, Bin and Eryilmaz, Atilla and Srikant, R",
  abstract  = "Motivated by the Internet of Things (IoT) and Cyber-Physical
               Systems (CPS), we consider dynamic wireless fading networks,
               where each incoming flow has a random service demand and leaves
               the system once its service request is completed. In such
               networks, one of the primary goals of network algorithm design
               is to achieve short-term fairness that characterizes how often
               each flow is served, in addition to the more traditional goals
               such as throughput-optimality and delay-insensitivity to the
               flow size distribution. In wireline networks, all of these
               desired properties can be achieved by the round-robin scheduling
               algorithm. In the context of wireless networks, a natural
               extension of round-robin scheduling has been developed in the
               last few years through the use of a counter called the
               Time-Since-Last-Service (TSLS) that keeps track of the time that
               passed since the last service time of each flow. However, the
               performance of this round-robin-like algorithm has been
               primarily studied in the context of persistent flows that
               continuously inject packets into the network and do not ever
               leave the network. The analysis of dynamic flow arrivals and
               departures is challenging since each individual flow experiences
               independent wireless fading and thus, flows cannot be served in
               a strict round-robin manner. In this paper, we overcome this
               difficulty by exploring the intricate dynamics of TSLS-based
               algorithm and show that flows are provided round-robin-like
               service with a very high probability. Consequently, we then show
               that our algorithm can achieve throughput-optimality. Moreover,
               through simulations, we demonstrate that the proposed TSLS-based
               algorithm also exhibits desired properties such as
               delay-insensitivity and excellent short-term fairness
               performance in the presence of dynamic flows over wireless
               fading channels.",
  publisher = "Association for Computing Machinery",
  pages     = "231--240",
  series    = "Mobihoc '20",
  month     =  oct,
  year      =  2020,
  file      = "All Papers/L/Li et al. 2020 - Emulating round-robin for serving dynamic flows over wireless fading channels.pdf",
  address   = "New York, NY, USA",
  keywords  = "short-term fairness, throughout-optimality, delay-insensitivity,
               wireless scheduling, flow-level dynamics;Wireless",
  location  = "Virtual Event, USA",
  isbn      = "9781450380157",
  doi       = "10.1145/3397166.3409131"
}

@INPROCEEDINGS{Ma2020-rv,
  title     = "Internet transport economics: model and analysis",
  booktitle = "Proceedings of the {Twenty-First} International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Ma, Richard T B",
  abstract  = "With the rise of video streaming and cloud services, the
               Internet has evolved into a content-centric service network;
               however, quality of service (QoS) is still a major concern for
               the content providers. Because quality degradation is influenced
               by 1) the capacities of links along the routes used for content
               delivery and 2) the amount of competing traffic across these
               links, it is very difficult to diagnose.In this paper, we
               establish a novel model to study how business decisions such as
               capacity planning, routing strategies and peering agreements
               affect QoS in terms of the end-to-end delays and drop rates of
               Internet routes. In particular, we take an economics perspective
               of the Internet transport service and model its supply of
               network capacities and demands of throughput driven by network
               protocols. We show that a macroscopic network equilibrium always
               exists and its uniqueness can be guaranteed under various
               scenarios. We analyze the impacts of user demands and resource
               capacities on the network equilibrium and provide implications
               of Netflix-Comcast type of peering on the QoS of users. We
               demonstrate that our framework can be used as a building block
               to understand the routing strategies under a Wardrop equilibrium
               and to enable further studies such as Internet peering and
               in-network caching.",
  publisher = "Association for Computing Machinery",
  pages     = "291--300",
  series    = "Mobihoc '20",
  month     =  oct,
  year      =  2020,
  file      = "All Papers/M/Ma 2020 - Internet transport economics - model and analysis.pdf",
  address   = "New York, NY, USA",
  keywords  = "GeneralInterest",
  location  = "Virtual Event, USA",
  isbn      = "9781450380157",
  doi       = "10.1145/3397166.3409137"
}

@INPROCEEDINGS{Meskar2020-co,
  title     = "Fair multi-resource allocation in mobile edge computing with
               multiple access points",
  booktitle = "Proceedings of the {Twenty-First} International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Meskar, Erfan and Liang, Ben",
  abstract  = "We consider the problem of fair multi-resource allocation for
               mobile edge computing (MEC) with multiple access points. In MEC,
               user tasks are uploaded over wireless communication channels to
               the access points, where they are then processed with multiple
               types of computing resources. What distinguishes fair
               multi-resource allocation in the MEC environment from more
               general cloud computing is that a user may experience different
               levels of wireless channel quality on different access points,
               so that the user's channel bandwidth demand is not fixed.
               Existing resource allocation studies for cloud computing
               generally consider Pareto Optimality (PO), Envy-Freeness (EF),
               Sharing Incentive (SI), and Strategy-Proofness (SP) as the most
               desirable fairness properties. In this work, we show these
               properties are no longer compatible in MEC, since there exists
               no resource allocation rule that can satisfy PO+EF+SP or
               PO+SI+SP. Hence, we propose a resource allocation rule, called
               Maximum Task Product (MTP), that retains PO, EF, and SI.
               Extensive simulation driven by Google cluster traces further
               shows that MTP improves resource utilization while achieving
               these fairness properties.",
  publisher = "Association for Computing Machinery",
  pages     = "11--20",
  series    = "Mobihoc '20",
  month     =  oct,
  year      =  2020,
  file      = "All Papers/M/Meskar and Liang 2020 - Fair multi-resource allocation in mobile edge computing with multiple access points.pdf",
  address   = "New York, NY, USA",
  location  = "Virtual Event, USA",
  isbn      = "9781450380157",
  doi       = "10.1145/3397166.3409144"
}

@INPROCEEDINGS{Yu2020-ho,
  title     = "Robust resource provisioning in time-varying edge networks",
  booktitle = "Proceedings of the {Twenty-First} International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Yu, Ruozhou and Xue, Guoliang and Wan, Yinxin and Tang, Jian and
               Yang, Dejun and Ji, Yusheng",
  abstract  = "Edge computing is one of the revolutionary technologies that
               enable high-performance and low-latency modern applications,
               such as smart cities, connected vehicles, etc. Yet its adoption
               has been limited by factors including high cost of edge
               resources, heterogeneous and fluctuating demands, and lack of
               reliability. In this paper, we study resource provisioning in
               edge computing, taking into account these different factors.
               First, based on observations from real demand traces, we propose
               a time-varying stochastic model to capture the time-dependent
               and uncertain demand and network dynamics in an edge network. We
               then apply a novel robustness model that accounts for both
               expected and worst-case performance of a service. Based on these
               models, we formulate edge provisioning as a multi-stage
               stochastic optimization problem. The problem is NP-hard even in
               the deterministic case. Leveraging the multi-stage structure, we
               apply nested Benders decomposition to solve the problem. We also
               describe several efficiency enhancement techniques, including a
               novel technique for quickly solving the large number of
               decomposed subproblems. Finally, we present results from real
               dataset-based simulations, which demonstrate the advantages of
               the proposed models, algorithm and techniques.",
  publisher = "Association for Computing Machinery",
  pages     = "21--30",
  series    = "Mobihoc '20",
  month     =  oct,
  year      =  2020,
  file      = "All Papers/Y/Yu et al. 2020 - Robust resource provisioning in time-varying edge networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "time-varying, robustness, edge computing, multi-stage stochastic
               optimization, resource allocation;EdgeFogCloudIoT",
  location  = "Virtual Event, USA",
  isbn      = "9781450380157",
  doi       = "10.1145/3397166.3409146"
}

@INPROCEEDINGS{Poularakis2020-pm,
  title     = "Approximation algorithms for data-intensive service chain
               embedding",
  booktitle = "Proceedings of the {Twenty-First} International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Poularakis, Konstantinos and Llorca, Jaime and Tulino, Antonia M
               and Tassiulas, Leandros",
  abstract  = "Recent advances in network virtualization and programmability
               enable innovative service models such as Service Chaining (SC),
               where flows can be steered through a pre-defined sequence of
               service functions deployed at different cloud locations. A key
               aspect dictating the performance and efficiency of a SC is its
               instantiation onto the physical infrastructure. While existing
               SC Embedding (SCE) algorithms can effectively address the
               instantiation of SCs consuming computation and communication
               resources, they lack efficient mechanisms to handle the
               increasing data-intensive nature of next-generation services.
               Differently from computation and communication resources, which
               are allocated in a dedicated per request manner, storage
               resources can be shared to satisfy multiple requests for the
               same data. To fill this gap, in this paper, we formulate the
               data-intensive SCE problem with the goal of minimizing storage,
               computation, and communication resource costs subject to
               resource capacity, service chaining, and data sharing
               constraints. Using a randomized rounding technique that exploits
               a novel data-aware linear programming decomposition procedure,
               we develop a multi-criteria approximation algorithm with
               provable performance guarantees. Evaluation results show that
               the proposed algorithm achieves near-optimal resource costs with
               up to 27.8\% of the cost savings owed to the sharing of the
               data.",
  publisher = "Association for Computing Machinery",
  pages     = "131--140",
  series    = "Mobihoc '20",
  month     =  oct,
  year      =  2020,
  file      = "All Papers/P/Poularakis et al. 2020 - Approximation algorithms for data-intensive service chain embedding.pdf",
  address   = "New York, NY, USA",
  keywords  = "data sharing, randomized rounding, service chain embedding;NFV",
  location  = "Virtual Event, USA",
  isbn      = "9781450380157",
  doi       = "10.1145/3397166.3409149"
}

@ARTICLE{Feldmann2020-ax,
  title     = "Survey on Algorithms for Self-stabilizing Overlay Networks",
  author    = "Feldmann, Michael and Scheideler, Christian and Schmid, Stefan",
  abstract  = "The maintenance of efficient and robust overlay networks is one
               of the most fundamental and reoccurring themes in networking.
               This article presents a survey of state-of-the-art algorithms to
               design and repair overlay networks in a distributed manner. In
               particular, we discuss basic algorithmic primitives to preserve
               connectivity, review algorithms for the fundamental problem of
               graph linearization, and then survey self-stabilizing algorithms
               for metric and scalable topologies. We also identify open
               problems and avenues for future research.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  4,
  pages     = "1--24",
  month     =  jul,
  year      =  2020,
  file      = "All Papers/F/Feldmann et al. 2020 - Survey on Algorithms for Self-stabilizing Overlay Networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "Self-stabilization, overlay networks, resilience, dependability,
               distributed algorithms, topological self-stabilization;Teaching",
  issn      = "0360-0300",
  doi       = "10.1145/3397190"
}

@ARTICLE{Chan2020-ay,
  title     = "Debugging incidents in Google's distributed systems",
  author    = "Chan, Charisma and Cooper, Beth",
  abstract  = "How experts debug production issues in complex distributed
               systems.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  63,
  number    =  10,
  pages     = "40--46",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/C/Chan and Cooper 2020 - Debugging incidents in Google's distributed systems.pdf",
  address   = "New York, NY, USA",
  keywords  = "6GLab",
  issn      = "0001-0782",
  doi       = "10.1145/3397880"
}

@ARTICLE{Qian2020-sp,
  title     = "Orchestrating the Development Lifecycle of Machine
               Learning-based {IoT} Applications: A Taxonomy and Survey",
  author    = "Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and
               Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and
               Yang, Renyu and Zomaya, Albert Y and Rana, Omer and Wang, Lizhe
               and Koutny, Maciej and Ranjan, Rajiv",
  abstract  = "Machine Learning (ML) and Internet of Things (IoT) are
               complementary advances: ML techniques unlock the potential of
               IoT with intelligence, and IoT applications increasingly feed
               data collected by sensors into ML models, thereby employing
               results to improve their business processes and services. Hence,
               orchestrating ML pipelines that encompass model training and
               implication involved in the holistic development lifecycle of an
               IoT application often leads to complex system integration. This
               article provides a comprehensive and systematic survey of the
               development lifecycle of ML-based IoT applications. We outline
               the core roadmap and taxonomy and subsequently assess and
               compare existing standard techniques used at individual stages.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  4,
  pages     = "1--47",
  month     =  aug,
  year      =  2020,
  file      = "All Papers/Q/Qian et al. 2020 - Orchestrating the Development Lifecycle of Machine Learning-based IoT Applications - A Taxonomy and Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "machine learning, deep learning, IoT,
               orchestration;MLNetworking;EdgeFogCloudIoT",
  issn      = "0360-0300",
  doi       = "10.1145/3398020"
}

@ARTICLE{Chen2020-gw,
  title     = "Deep Learning on Mobile and Embedded Devices: State-of-the-art,
               Challenges, and Future Directions",
  author    = "Chen, Yanjiao and Zheng, Baolin and Zhang, Zihan and Wang, Qian
               and Shen, Chao and Zhang, Qian",
  abstract  = "Recent years have witnessed an exponential increase in the use
               of mobile and embedded devices. With the great success of deep
               learning in many fields, there is an emerging trend to deploy
               deep learning on mobile and embedded devices to better meet the
               requirement of real-time applications and user privacy
               protection. However, the limited resources of mobile and
               embedded devices make it challenging to fulfill the intensive
               computation and storage demand of deep learning models. In this
               survey, we conduct a comprehensive review on the related issues
               for deep learning on mobile and embedded devices. We start with
               a brief introduction of deep learning and discuss major
               challenges of implementing deep learning models on mobile and
               embedded devices. We then conduct an in-depth survey on
               important compression and acceleration techniques that help
               adapt deep learning models to mobile and embedded devices, which
               we specifically classify as pruning, quantization, model
               distillation, network design strategies, and low-rank
               factorization. We elaborate on the hardware-based solutions,
               including mobile GPU, FPGA, and ASIC, and describe software
               frameworks for mobile deep learning models, especially the
               development of frameworks based on OpenCL and RenderScript.
               After that, we present the application of mobile deep learning
               in a variety of areas, such as navigation, health, speech
               recognition, and information security. Finally, we discuss some
               future directions for deep learning on mobile and embedded
               devices to inspire further research in this area.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  4,
  pages     = "1--37",
  month     =  aug,
  year      =  2020,
  file      = "All Papers/C/Chen et al. 2020 - Deep Learning on Mobile and Embedded Devices - State-of-the-art, Challenges, and Future Directions.pdf",
  address   = "New York, NY, USA",
  keywords  = "software frameworks, mobile devices, network compression and
               acceleration, Deep learning, hardware solutions;ML",
  issn      = "0360-0300",
  doi       = "10.1145/3398209"
}

@ARTICLE{Khader2020-jt,
  title     = "Density-based Algorithms for Big Data Clustering Using
               {MapReduce} Framework: A Comprehensive Study",
  author    = "Khader, Mariam and Al-Naymat, Ghazi",
  abstract  = "Clustering is used to extract hidden patterns and similar groups
               from data. Therefore, clustering as a method of unsupervised
               learning is a crucial technique for big data analysis owing to
               the massive number of unlabeled objects involved. Density-based
               algorithms have attracted research interest, because they help
               to better understand complex patterns in spatial datasets that
               contain information about data related to co-located objects.
               Big data clustering is a challenging task, because the volume of
               data increases exponentially. However, clustering using
               MapReduce can help answer this challenge. In this context,
               density-based algorithms in MapReduce have been largely
               investigated in the past decade to eliminate the problem of big
               data clustering. Despite the diversity of the algorithms
               proposed, the field lacks a structured review of the available
               algorithms and techniques for desirable partitioning, local
               clustering, and merging. This study formalizes the problem of
               density-based clustering using MapReduce, proposes a taxonomy to
               categorize the proposed algorithms, and provides a systematic
               and comprehensive comparison of these algorithms according to
               the partitioning technique, type of local clustering, merging
               technique, and exactness of their implementations. Finally, the
               study highlights outstanding challenges and opportunities to
               contribute to the field of density-based clustering using
               MapReduce.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  5,
  pages     = "1--38",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/K/Khader and Al-Naymat 2020 - Density-based Algorithms for Big Data Clustering Using MapReduce Framework - A Comprehensive Study.pdf",
  address   = "New York, NY, USA",
  keywords  = "mapreduce framework, density clustering, clustering, Big
               data;Teaching",
  issn      = "0360-0300",
  doi       = "10.1145/3403951"
}

@ARTICLE{Aljeri2020-ft,
  title     = "Mobility Management in 5G-enabled Vehicular Networks: Models,
               Protocols, and Classification",
  author    = "Aljeri, Noura and Boukerche, Azzedine",
  abstract  = "Over the past few years, the next generation of vehicular
               networks is envisioned to play an essential part in autonomous
               driving, traffic management, and infotainment applications. The
               next generation of intelligent vehicular networks enabled by 5G
               systems will integrate various heterogeneous wireless techniques
               to enable time-sensitive services with guaranteed quality of
               service and ultimate bandwidth usage. However, to allow the
               dense diversity of wireless technologies, seamless and reliable
               wireless communication protocols need to be thoroughly
               investigated in vehicular networks environment. Henceforth,
               efficient mobility management protocols that mitigate the
               challenges of vehicles' mobility is essential to support massive
               data loads throughout various applications. In this article, we
               review different mobility management protocols and their ability
               to address issues related to 5G-enabled vehicular networks
               within the related works. First, we provide a broad view of
               existing models of vehicular networks and their applicability to
               the next generation of wireless networks. Next, we propose a
               classification of several vehicular network models that suit the
               5G wireless network, followed by a thorough discussion of the
               mobility management challenges in each of these network models
               that need to be addressed and then discuss each of their
               benefits and drawbacks accordingly.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  5,
  pages     = "1--35",
  month     =  sep,
  year      =  2020,
  annote    = "Bad language quality, makes it difficult to understand.
               Simplistic arguments. Not convinced this is a useful paper.",
  file      = "All Papers/A/Aljeri and Boukerche 2020 - Mobility Management in 5G-enabled Vehicular Networks - Models, Protocols, and Classification.pdf",
  address   = "New York, NY, USA",
  keywords  = "Mobility management, machine learning, handoff management, 5G
               networks, VANETs;Done;Boring;Low quality;5G6G",
  issn      = "0360-0300",
  doi       = "10.1145/3403953"
}

@ARTICLE{Mahmud2020-hv,
  title     = "Application Management in Fog Computing Environments: A
               Taxonomy, Review and Future Directions",
  author    = "Mahmud, Redowan and Ramamohanarao, Kotagiri and Buyya, Rajkumar",
  abstract  = "The Internet of Things (IoT) paradigm is being rapidly adopted
               for the creation of smart environments in various domains. The
               IoT-enabled cyber-physical systems associated with smart city,
               healthcare, Industry 4.0 and Agtech handle a huge volume of data
               and require data processing services from different types of
               applications in real time. The Cloud-centric execution of IoT
               applications barely meets such requirements as the Cloud
               datacentres reside at a multi-hop distance from the IoT devices.
               Fog computing, an extension of Cloud at the edge network, can
               execute these applications closer to data sources. Thus, Fog
               computing can improve application service delivery time and
               resist network congestion. However, the Fog nodes are highly
               distributed and heterogeneous, and most of them are constrained
               in resources and spatial sharing. Therefore, efficient
               management of applications is necessary to fully exploit the
               capabilities of Fog nodes. In this work, we investigate the
               existing application management strategies in Fog computing and
               review them in terms of architecture, placement and maintenance.
               Additionally, we propose a comprehensive taxonomy and highlight
               the research gaps in Fog-based application management. We also
               discuss a perspective model and provide future research
               directions for further improvement of application management in
               Fog computing.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  4,
  pages     = "1--43",
  month     =  jul,
  year      =  2020,
  file      = "All Papers/M/Mahmud et al. 2020 - Application Management in Fog Computing Environments - A Taxonomy, Review and Future Directions.pdf",
  address   = "New York, NY, USA",
  keywords  = "application architecture, Fog computing, application placement,
               application maintenance, Internet of Things;FutureInternet",
  issn      = "0360-0300",
  doi       = "10.1145/3403955"
}

@ARTICLE{Schleier-Smith2021-ix,
  title     = "What serverless computing is and should become: the next phase
               of cloud computing",
  author    = "Schleier-Smith, Johann and Sreekanti, Vikram and Khandelwal,
               Anurag and Carreira, Joao and Yadwadkar, Neeraja J and Popa,
               Raluca Ada and Gonzalez, Joseph E and Stoica, Ion and Patterson,
               David A",
  abstract  = "The evolution that serverless computing represents, the economic
               forces that shape it, why it could fail, and how it might
               fulfill its potential.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  64,
  number    =  5,
  pages     = "76--84",
  month     =  apr,
  year      =  2021,
  file      = "All Papers/S/Schleier-Smith et al. 2021 - What serverless computing is and should become - the next phase of cloud computing.pdf",
  address   = "New York, NY, USA",
  keywords  = "GeneralInterest",
  issn      = "0001-0782",
  doi       = "10.1145/3406011"
}

@ARTICLE{Davoudian2020-aa,
  title     = "Big Data Systems: A Software Engineering Perspective",
  author    = "Davoudian, Ali and Liu, Mengchi",
  abstract  = "Big Data Systems (BDSs) are an emerging class of scalable
               software technologies whereby massive amounts of heterogeneous
               data are gathered from multiple sources, managed, analyzed (in
               batch, stream or hybrid fashion), and served to end-users and
               external applications. Such systems pose specific challenges in
               all phases of software development lifecycle and might become
               very complex by evolving data, technologies, and target value
               over time. Consequently, many organizations and enterprises have
               found it difficult to adopt BDSs. In this article, we provide
               insight into three major activities of software engineering in
               the context of BDSs as well as the choices made to tackle them
               regarding state-of-the-art research and industry efforts. These
               activities include the engineering of requirements, designing
               and constructing software to meet the specified requirements,
               and software/data quality assurance. We also disclose some open
               challenges of developing effective BDSs, which need attention
               from both researchers and practitioners.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  5,
  pages     = "1--39",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/D/Davoudian and Liu 2020 - Big Data Systems - A Software Engineering Perspective.pdf",
  address   = "New York, NY, USA",
  keywords  = "Big Data, software engineering, Big Data systems, quality
               assurance, software reference architecture, requirements
               engineering;Teaching",
  issn      = "0360-0300",
  doi       = "10.1145/3408314"
}

@ARTICLE{Chen2021-nw,
  title     = "{Time-Efficient} Ensemble Learning with Sample Exchange for Edge
               Computing",
  author    = "Chen, Wu and Yu, Yong and Gai, Keke and Liu, Jiamou and Choo,
               Kim-Kwang Raymond",
  abstract  = "In existing ensemble learning algorithms (e.g., random forest),
               each base learner's model needs the entire dataset for sampling
               and training. However, this may not be practical in many
               real-world applications, and it incurs additional computational
               costs. To achieve better efficiency, we propose a decentralized
               framework: Multi-Agent Ensemble. The framework leverages edge
               computing to facilitate ensemble learning techniques by focusing
               on the balancing of access restrictions (small sub-dataset) and
               accuracy enhancement. Specifically, network edge nodes
               (learners) are utilized to model classifications and predictions
               in our framework. Data is then distributed to multiple base
               learners who exchange data via an interaction mechanism to
               achieve improved prediction. The proposed approach relies on a
               training model rather than conventional centralized learning.
               Findings from the experimental evaluations using 20 real-world
               datasets suggest that Multi-Agent Ensemble outperforms other
               ensemble approaches in terms of accuracy even though the base
               learners require fewer samples (i.e., significant reduction in
               computation costs).",
  journal   = "ACM Trans. Internet Technol.",
  publisher = "Association for Computing Machinery",
  volume    =  21,
  number    =  3,
  pages     = "1--17",
  month     =  jun,
  year      =  2021,
  file      = "All Papers/C/Chen et al. 2021 - Time-Efficient Ensemble Learning with Sample Exchange for Edge Computing.pdf",
  address   = "New York, NY, USA",
  keywords  = "Edge computing, Multi-Agent Ensemble, ensemble learning,
               decentralized ensemble learning;MLNetworking",
  issn      = "1533-5399",
  doi       = "10.1145/3409265"
}

@ARTICLE{Bu2020-iy,
  title     = "Unveiling the Mystery of Internet Packet Forwarding: A Survey of
               Network Path Validation",
  author    = "Bu, Kai and Laird, Avery and Yang, Yutian and Cheng, Linfeng and
               Luo, Jiaqing and Li, Yingjiu and Ren, Kui",
  abstract  = "Validating the network paths taken by packets is critical in
               constructing a secure Internet architecture. Any feasible
               solution must both enforce packet forwarding along end-host
               specified paths and verify whether packets have taken those
               paths. However, the current Internet supports neither
               enforcement nor verification. Likely due to the radical changes
               to the Internet architecture and a long-standing confusion
               between routing and forwarding, only limited solutions for path
               validation exist in the literature. This survey article aims to
               reinvigorate research on the essential topic of path validation
               by crystallizing not only how path validation works but also
               where seemingly qualified solutions fall short. The analyses
               explore future research directions in path validation aimed at
               improving security, privacy, and efficiency.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  5,
  pages     = "1--34",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/B/Bu et al. 2020 - Unveiling the Mystery of Internet Packet Forwarding - A Survey of Network Path Validation.pdf",
  address   = "New York, NY, USA",
  keywords  = "path validation, authentication, Future Internet architecture,
               packet forwarding;Teaching",
  issn      = "0360-0300",
  doi       = "10.1145/3409796"
}

@ARTICLE{Chatterjee2020-zt,
  title     = "Computational Sustainability: A Socio-technical Perspective",
  author    = "Chatterjee, Deya and Rao, Shrisha",
  abstract  = "This is a consolidated look at computational techniques for
               sustainability, and their limits and possibilities.
               Sustainability is already well established as a concern and a
               topic of study and practice, given the alarming increase of
               environmental degradation, pollution, and other adverse effects
               of industrialization and urbanization. Computational
               sustainability, which focuses on the use of effective
               computational models and computational approaches to help
               achieve the goal of sustainability, has attracted interest from
               computer science researchers worldwide. We review recent work on
               computational techniques applied to a range of domains related
               to sustainability, from bio-surveillance to poverty mapping,
               from renewable energy production forecasting to crop disease
               monitoring, and from agent-based modeling to stochastic network
               design. In sustainable computing, we discuss some directions
               that have recently been explored. Finally, we analyze research
               directions that could be explored in the future to achieve the
               goal of long-term environmental sustainability.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  5,
  pages     = "1--29",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/C/Chatterjee and Rao 2020 - Computational Sustainability - A Socio-technical Perspective.pdf",
  address   = "New York, NY, USA",
  keywords  = "smart systems, renewable resources, Conservation;GenericInterest",
  issn      = "0360-0300",
  doi       = "10.1145/3409797"
}

@ARTICLE{Zimmerling2020-bz,
  title     = "Synchronous Transmissions in {Low-Power} Wireless: A Survey of
               Communication Protocols and Network Services",
  author    = "Zimmerling, Marco and Mottola, Luca and Santini, Silvia",
  abstract  = "Low-power wireless communication is a central building block of
               cyber-physical systems and the Internet of Things. Conventional
               low-power wireless protocols make avoiding packet collisions a
               cornerstone design choice. The concept of synchronous
               transmissions challenges this view. As collisions are not
               necessarily destructive, under specific circumstances, commodity
               low-power wireless radios are often able to receive useful
               information even in the presence of superimposed signals from
               different transmitters. We survey the growing number of
               protocols that exploit synchronous transmissions for higher
               robustness and efficiency as well as unprecedented functionality
               and versatility compared to conventional designs. The
               illustration of protocols based on synchronous transmissions is
               cast in a conceptional framework we establish, with the goal of
               highlighting differences and similarities among the proposed
               solutions. We conclude this article with a discussion on open
               questions and challenges in this research field.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  6,
  pages     = "1--39",
  month     =  dec,
  year      =  2020,
  file      = "All Papers/Z/Zimmerling et al. 2020 - Synchronous Transmissions in Low-Power Wireless - A Survey of Communication Protocols and Network Services.pdf",
  address   = "New York, NY, USA",
  keywords  = "Low-power wireless networks, constructive interference,
               synchronous transmissions, sender diversity, message-in-message
               effect, capture effect, simplicity, multi-hop
               communication;Wireless",
  issn      = "0360-0300",
  doi       = "10.1145/3410159"
}

@INPROCEEDINGS{Chen2020-xv,
  title     = "A deep learning approach to efficient drone mobility support",
  booktitle = "Proceedings of the 2nd {ACM} {MobiCom} Workshop on Drone
               Assisted Wireless Communications for {5G} and Beyond",
  author    = "Chen, Yun and Lin, Xingqin and Khan, Talha and Mozaffari,
               Mohammad",
  abstract  = "The growing deployment of drones in a myriad of applications
               relies on seamless and reliable wireless connectivity for safe
               control and operation of drones. Cellular technology is a key
               enabler for providing essential wireless services to drones
               flying in the sky. Existing cellular networks targeting
               terrestrial usage can support the initial deployment of
               low-altitude drone users, but there are challenges such as
               mobility support. In this paper, we propose a novel handover
               framework for providing efficient mobility support and reliable
               wireless connectivity to drones served by a terrestrial cellular
               network. Using tools from deep reinforcement learning, we
               develop a deep Q-learning algorithm to dynamically optimize
               handover decisions to ensure robust connectivity for drone
               users. Simulation results show that the proposed framework
               significantly reduces the number of handovers at the expense of
               a small loss in signal strength relative to the baseline case
               where a drone always connect to a base station that provides the
               strongest received signal strength.",
  publisher = "Association for Computing Machinery",
  pages     = "67--72",
  series    = "DroneCom '20",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/C/Chen et al. 2020 - A deep learning approach to efficient drone mobility support.pdf",
  address   = "New York, NY, USA",
  keywords  = "mobility management, drone, UAV, non-terrestrial networks,
               handover, reinforcement learning, 5G, deep learning",
  location  = "London, United Kingdom",
  isbn      = "9781450381055",
  doi       = "10.1145/3414045.3415948"
}

@ARTICLE{Kirkpatrick2020-au,
  title     = "Who has access to your smartphone data?",
  author    = "Kirkpatrick, Keith",
  abstract  = "ISPs, app developers, and even the government may know more
               about you than you think.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  63,
  number    =  10,
  pages     = "15--17",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/K/Kirkpatrick 2020 - Who has access to your smartphone data.pdf",
  address   = "New York, NY, USA",
  keywords  = "GeneralInterest",
  issn      = "0001-0782",
  doi       = "10.1145/3416078"
}

@ARTICLE{Cusumano2020-zo,
  title     = "Self-driving vehicle technology: progress and promises",
  author    = "Cusumano, Michael A",
  abstract  = "Seeking the answer to the elusive question, 'Are we there yet'?",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  63,
  number    =  10,
  pages     = "20--22",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/C/Cusumano 2020 - Self-driving vehicle technology - progress and promises.pdf",
  address   = "New York, NY, USA",
  keywords  = "GeneralInterest",
  issn      = "0001-0782",
  doi       = "10.1145/3417074"
}

@ARTICLE{Jin2021-qg,
  title     = "On Nonconvex Optimization for Machine Learning: Gradients,
               Stochasticity, and Saddle Points",
  author    = "Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham
               M and Jordan, Michael I",
  abstract  = "Gradient descent (GD) and stochastic gradient descent (SGD) are
               the workhorses of large-scale machine learning. While classical
               theory focused on analyzing the performance of these methods in
               convex optimization problems, the most notable successes in
               machine learning have involved nonconvex optimization, and a gap
               has arisen between theory and practice. Indeed, traditional
               analyses of GD and SGD show that both algorithms converge to
               stationary points efficiently. But these analyses do not take
               into account the possibility of converging to saddle points.
               More recent theory has shown that GD and SGD can avoid saddle
               points, but the dependence on dimension in these analyses is
               polynomial. For modern machine learning, where the dimension can
               be in the millions, such dependence would be catastrophic. We
               analyze perturbed versions of GD and SGD and show that they are
               truly efficient---their dimension dependence is only
               polylogarithmic. Indeed, these algorithms converge to
               second-order stationary points in essentially the same time as
               they take to converge to classical first-order stationary
               points.",
  journal   = "J. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  68,
  number    =  2,
  pages     = "1--29",
  month     =  feb,
  year      =  2021,
  file      = "All Papers/J/Jin et al. 2021 - On Nonconvex Optimization for Machine Learning - Gradients, Stochasticity, and Saddle Points.pdf",
  address   = "New York, NY, USA",
  keywords  = "Saddle points, efficiency, (stochastic) gradient descent,
               perturbations;ML",
  issn      = "0004-5411",
  doi       = "10.1145/3418526"
}

@ARTICLE{Zhu2020-jk,
  title     = "{SDN} Controllers: A Comprehensive Analysis and Performance
               Evaluation Study",
  author    = "Zhu, Liehuang and Karim, Md M and Sharif, Kashif and Xu, Chang
               and Li, Fan and Du, Xiaojiang and Guizani, Mohsen",
  abstract  = "Software-defined networks offer flexible and intelligent network
               operations by splitting a traditional network into a centralized
               control plane and a programmable data plane. The controller in
               the control plane is the fundamental element used to manage all
               operations of the data plane. Hence, the performance and
               capabilities of the controller itself are essential in achieving
               optimal performance. Furthermore, the tools used to benchmark
               their performance must be accurate and useful in measuring
               different evaluation parameters. There are dozens of controller
               proposals for general and specialized networks in the
               literature. However, there is a very limited comprehensive
               quantitative analysis for them. In this article, we present a
               comprehensive qualitative comparison of different SDN
               controllers, along with a quantitative analysis of their
               performance in different network scenarios. We categorize and
               classify 34 controllers and present a qualitative comparison. We
               also present a comparative analysis of controllers for
               specialized networks such as the Internet of Things, blockchain
               networks, vehicular networks, and wireless sensor networks. We
               also discuss in-depth capabilities of benchmarking tools and
               provide a comparative analysis of their capabilities. This work
               uses three benchmarking tools to compare 9 controllers and
               presents a detailed analysis of their performance, along with
               discussion on performance of specialized network controllers.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  53,
  number    =  6,
  pages     = "1--40",
  month     =  dec,
  year      =  2020,
  file      = "All Papers/Z/Zhu et al. 2020 - SDN Controllers - A Comprehensive Analysis and Performance Evaluation Study.pdf",
  address   = "New York, NY, USA",
  keywords  = "Internet of Things, benchmarking tools, vehicular networks,
               blockchain, Software-defined networks, SDN controller, wireless
               sensor networks;NFV\_SDN;FutureInternet",
  issn      = "0360-0300",
  doi       = "10.1145/3421764"
}

@ARTICLE{Gu2021-sb,
  title     = "From {Server-Based} to {Client-Based} Machine Learning: A
               Comprehensive Survey",
  author    = "Gu, Renjie and Niu, Chaoyue and Wu, Fan and Chen, Guihai and Hu,
               Chun and Lyu, Chengfei and Wu, Zhihua",
  abstract  = "In recent years, mobile devices have gained increasing
               development with stronger computation capability and larger
               storage space. Some of the computation-intensive machine
               learning tasks can now be run on mobile devices. To exploit the
               resources available on mobile devices and preserve personal
               privacy, the concept of client-based machine learning has been
               proposed. It leverages the users' local hardware and local data
               to solve machine learning sub-problems on mobile devices and
               only uploads computation results rather than the original data
               for the optimization of the global model. Such an architecture
               can not only relieve computation and storage burdens on servers
               but also protect the users' sensitive information. Another
               benefit is the bandwidth reduction because various kinds of
               local data can be involved in the training process without being
               uploaded. In this article, we provide a literature review on the
               progressive development of machine learning from server based to
               client based. We revisit a number of widely used server-based
               and client-based machine learning methods and applications. We
               also extensively discuss the challenges and future directions in
               this area. We believe that this survey will give a clear
               overview of client-based machine learning and provide guidelines
               on applying client-based machine learning to practice.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  1,
  pages     = "1--36",
  month     =  jan,
  year      =  2021,
  file      = "All Papers/G/Gu et al. 2021 - From Server-Based to Client-Based Machine Learning - A Comprehensive Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "federated learning, Mobile intelligence, machine learning,
               decentralized training, distributed system;ML;MLNetworking",
  issn      = "0360-0300",
  doi       = "10.1145/3424660"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Mirza2021-ve,
  title     = "Potential Deep Learning Solutions to Persistent and Emerging Big
               Data {Challenges---A} Practitioners' Cookbook",
  author    = "Mirza, Behroz and Syed, Tahir Q and Khan, Behraj and Malik,
               Yameen",
  abstract  = "The phenomenon of Big Data continues to present moving targets
               for the scientific and technological state-of-the-art. This work
               demonstrates that the solution space of these challenges has
               expanded with deep learning now moving beyond traditional
               applications in computer vision and natural language processing
               to diverse and core machine learning tasks such as learning with
               streaming and non-iid-data, partial supervision, and large
               volumes of distributed data while preserving privacy. We present
               a framework coalescing multiple deep methods and corresponding
               models as responses to specific Big Data challenges. First, we
               perform a detailed per-challenge review of existing techniques,
               with benchmarks and usage advice, and subsequently synthesize
               them together into one organic construct that we discover
               principally uses extensions of one underlying model, the
               autoencoder. This work therefore provides a synthesis where
               challenges at scale across the Vs of Big Data could be addressed
               by new algorithms and architectures being proposed in the deep
               learning community. The value being proposed to the reader from
               either community in terms of nomenclature, concepts, and
               techniques of the other would advance the cause of
               multi-disciplinary, transversal research and accelerate the
               advance of technology in both domains.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  1,
  pages     = "1--39",
  month     =  jan,
  year      =  2021,
  file      = "All Papers/M/Mirza et al. 2021 - Potential Deep Learning Solutions to Persistent and Emerging Big Data Challenges—A Practitioners' Cookbook.pdf",
  address   = "New York, NY, USA",
  keywords  = "ladder networks, zero shot learning, adaptive deep belief
               networks, reinforcement learning, self-supervision, Distributed
               deep learning, representation learning, federated learning,
               extreme transfer learning, semi-supervised learning,
               regenerative chaining, extreme learning machine",
  issn      = "0360-0300",
  doi       = "10.1145/3427476"
}

@ARTICLE{Bellavista2021-sg,
  title     = "Decentralised Learning in Federated Deployment Environments: A
               {System-Level} Survey",
  author    = "Bellavista, Paolo and Foschini, Luca and Mora, Alessio",
  abstract  = "Decentralised learning is attracting more and more interest
               because it embodies the principles of data minimisation and
               focused data collection, while favouring the transparency of
               purpose specification (i.e., the objective for which a model is
               built). Cloud-centric-only processing and deep learning are no
               longer strict necessities to train high-fidelity models; edge
               devices can actively participate in the decentralised learning
               process by exchanging meta-level information in place of raw
               data, thus paving the way for better privacy guarantees. In
               addition, these new possibilities can relieve the network
               backbone from unnecessary data transfer and allow it to meet
               strict low-latency requirements by leveraging on-device model
               inference. This survey provides a detailed and up-to-date
               overview of the most recent contributions available in the
               state-of-the-art decentralised learning literature. In
               particular, it originally provides the reader audience with a
               clear presentation of the peculiarities of federated settings,
               with a novel taxonomy of decentralised learning approaches, and
               with a detailed description of the most relevant and specific
               system-level contributions of the surveyed solutions for
               privacy, communication efficiency, non-IIDness, device
               heterogeneity, and poisoning defense.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  1,
  pages     = "1--38",
  month     =  feb,
  year      =  2021,
  file      = "All Papers/B/Bellavista et al. 2021 - Decentralised Learning in Federated Deployment Environments - A System-Level Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "communication efficiency, federated deployment, poisoning
               defense, privacy, Decentralised learning;ML",
  issn      = "0360-0300",
  doi       = "10.1145/3429252"
}

@ARTICLE{Canca2020-we,
  title     = "Operationalizing {AI} ethics principles",
  author    = "Canca, Cansu",
  abstract  = "A better ethics analysis guide for developers.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  63,
  number    =  12,
  pages     = "18--21",
  month     =  nov,
  year      =  2020,
  file      = "All Papers/C/Canca 2020 - Operationalizing AI ethics principles.pdf",
  address   = "New York, NY, USA",
  keywords  = "GeneralInterest",
  issn      = "0001-0782",
  doi       = "10.1145/3430368"
}

@ARTICLE{Lv2021-bd,
  title     = "Transfer Learning-powered Resource Optimization for Green
               Computing in {5G-Aided} Industrial Internet of Things",
  author    = "Lv, Zhihan and Lou, Ranran and Kumar Singh, Amit and Wang,
               Qingjun",
  abstract  = "Objective: Green computing meets the needs of a low-carbon
               society and it is an important aspect of promoting social
               sustainable development and technological progress. In the
               investigation, green computing for resource management and
               allocation issues is only discussed. Therefore, in the context
               of the 5G communication network, the investigation of the data
               classification and resource optimization of the Internet of
               Things are conducted. Method: The virtualization architecture of
               the heterogeneous wireless network resource based on 5G
               technology is designed. The related investigation is conducted
               based on 5G network and Internet of Things technology. Under the
               traditional method, the transfer learning is introduced to
               improve the AdaBoost (Adaptive Boosting) algorithm to classify
               the data. The investigated complete resource reuse method is
               used to optimize resources. A method that a sub-channel can be
               reused by a cellular link and any number of D2D links at the
               same time is proposed to conduct resource optimization
               investigation. Results: The investigation indicates that the
               classification accuracy of the algorithm is excellent for the
               data classification of the Internet of Things and has different
               advantages in various aspects compared with other algorithms.
               The designed algorithm can find a larger set of resource reuse
               and have a significant increase in spectrum utilization
               efficiency. Conclusion: The investigation can contribute to the
               boom in the Internet of Things in terms of data classification
               and resource optimization based on 5G.",
  journal   = "ACM Trans. Internet Technol.",
  publisher = "Association for Computing Machinery",
  volume    =  22,
  number    =  2,
  pages     = "1--16",
  month     =  oct,
  year      =  2021,
  file      = "All Papers/L/Lv et al. 2021 - Transfer Learning-powered Resource Optimization for Green Computing in 5G-Aided Industrial Internet of Things.pdf",
  address   = "New York, NY, USA",
  keywords  = "transfer learning, Internet of Things, 5G, AdaBoost, data
               classification, resource optimization;MLNetworking",
  issn      = "1533-5399",
  doi       = "10.1145/3434774"
}

@ARTICLE{Yao2021-ur,
  title     = "Depth and persistence: what researchers need to know about
               impostor syndrome",
  author    = "Yao, Danfeng (daphne)",
  abstract  = "Understanding impostor syndrome's complexity and its effect on
               research persistence.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  64,
  number    =  6,
  pages     = "39--42",
  month     =  may,
  year      =  2021,
  file      = "All Papers/Y/Yao 2021 - Depth and persistence - what researchers need to know about impostor syndrome.pdf",
  address   = "New York, NY, USA",
  issn      = "0001-0782",
  doi       = "10.1145/3437255"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gonczarowski2021-mk,
  title     = "The Sample Complexity of {Up-to-$\epsilon$} Multi-dimensional
               Revenue Maximization",
  author    = "Gonczarowski, Yannai A and Weinberg, S Matthew",
  abstract  = "We consider the sample complexity of revenue maximization for
               multiple bidders in unrestricted multi-dimensional settings.
               Specifically, we study the standard model of additive bidders
               whose values for heterogeneous items are drawn independently.
               For any such instance and any , we show that it is possible to
               learn an -Bayesian Incentive Compatible auction whose expected
               revenue is within of the optimal -BIC auction from only
               polynomially many samples. Our fully nonparametric approach is
               based on ideas that hold quite generally and completely sidestep
               the difficulty of characterizing optimal (or near-optimal)
               auctions for these settings. Therefore, our results easily
               extend to general multi-dimensional settings, including
               valuations that are not necessarily even subadditive, and
               arbitrary allocation constraints. For the cases of a single
               bidder and many goods, or a single parameter (good) and many
               bidders, our analysis yields exact incentive compatibility (and
               for the latter also computational efficiency). Although the
               single-parameter case is already well understood, our corollary
               for this case extends slightly the state of the art.",
  journal   = "J. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  68,
  number    =  3,
  pages     = "1--28",
  month     =  mar,
  year      =  2021,
  file      = "All Papers/G/Gonczarowski and Weinberg 2021 - The Sample Complexity of Up-to-ε Multi-dimensional Revenue Maximization.pdf",
  address   = "New York, NY, USA",
  keywords  = "Algorithmic game theory, algorithmic mechanism design,
               multi-dimensional auctions, auctions, sample complexity,
               generalization bounds, PAC learning, approximate revenue
               maximization",
  issn      = "0004-5411",
  doi       = "10.1145/3439722"
}

@ARTICLE{Fletcher2021-ry,
  title     = "{CAPE}: a framework for assessing equity throughout the computer
               science education ecosystem",
  author    = "Fletcher, Carol L and Warner, Jayce R",
  abstract  = "Examining both the leading indicators of equity in CS and the
               lagging indicators of student outcomes.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  64,
  number    =  2,
  pages     = "23--25",
  month     =  jan,
  year      =  2021,
  file      = "All Papers/F/Fletcher and Warner 2021 - CAPE - a framework for assessing equity throughout the computer science education ecosystem.pdf",
  address   = "New York, NY, USA",
  keywords  = "Teaching",
  issn      = "0001-0782",
  doi       = "10.1145/3442373"
}

@ARTICLE{Varghese2021-pf,
  title     = "A Survey on Edge Performance Benchmarking",
  author    = "Varghese, Blesson and Wang, Nan and Bermbach, David and Hong,
               Cheol-Ho and Lara, Eyal De and Shi, Weisong and Stewart,
               Christopher",
  abstract  = "Edge computing is the next Internet frontier that will leverage
               computing resources located near users, sensors, and data stores
               to provide more responsive services. Therefore, it is envisioned
               that a large-scale, geographically dispersed, and resource-rich
               distributed system will emerge and play a key role in the future
               Internet. However, given the loosely coupled nature of such
               complex systems, their operational conditions are expected to
               change significantly over time. In this context, the performance
               characteristics of such systems will need to be captured
               rapidly, which is referred to as performance benchmarking, for
               application deployment, resource orchestration, and adaptive
               decision-making. Edge performance benchmarking is a nascent
               research avenue that has started gaining momentum over the past
               five years. This article first reviews articles published over
               the past three decades to trace the history of performance
               benchmarking from tightly coupled to loosely coupled systems. It
               then systematically classifies previous research to identify the
               system under test, techniques analyzed, and benchmark runtime in
               edge performance benchmarking.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  3,
  pages     = "1--33",
  month     =  apr,
  year      =  2021,
  file      = "All Papers/V/Varghese et al. 2021 - A Survey on Edge Performance Benchmarking.pdf",
  address   = "New York, NY, USA",
  keywords  = "benchmark runtime, Edge computing, system under test, edge
               performance benchmarking, techniques analyzed",
  issn      = "0360-0300",
  doi       = "10.1145/3444692"
}

@ARTICLE{Czumaj2021-ko,
  title     = "Exploiting Spontaneous Transmissions for Broadcasting and Leader
               Election in Radio Networks",
  author    = "Czumaj, Artur and Davies, Peter",
  abstract  = "We study two fundamental communication primitives: broadcasting
               and leader election in the classical model of multi-hop radio
               networks with unknown topology and without collision detection
               mechanisms. It has been known for almost 20 years that in
               undirected networks with n nodes and diameter D, randomized
               broadcasting requires $\Omega$(D log n/D + log2 n) rounds,
               assuming that uninformed nodes are not allowed to communicate
               (until they are informed). Only very recently, Haeupler and Wajc
               (PODC'2016) showed that this bound can be improved for the model
               with spontaneous transmissions, providing an O(D log n log log
               n/log D + logO(1) n)-time broadcasting algorithm. In this
               article, we give a new and faster algorithm that completes
               broadcasting in O(D log n/log D + logO(1) n) time, succeeding
               with high probability. This yields the first optimal O(D)-time
               broadcasting algorithm whenever n is polynomial in
               D.Furthermore, our approach can be applied to design a new
               leader election algorithm that matches the performance of our
               broadcasting algorithm. Previously, all fast randomized leader
               election algorithms have used broadcasting as a subroutine and
               their complexity has been asymptotically strictly larger than
               the complexity of broadcasting. In particular, the fastest
               previously known randomized leader election algorithm of
               Ghaffari and Haeupler (SODA'2013) requires O(D log n/D min \{log
               log n, log n/D\} + logO(1) n)-time, succeeding with high
               probability. Our new algorithm again requires O(D log n/log D +
               logO(1) n) time, also succeeding with high probability.",
  journal   = "J. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  68,
  number    =  2,
  pages     = "1--22",
  month     =  jan,
  year      =  2021,
  file      = "All Papers/C/Czumaj and Davies 2021 - Exploiting Spontaneous Transmissions for Broadcasting and Leader Election in Radio Networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "leader election, radio networks,
               Broadcasting;Mobile\_Wireless;Distributed Systems",
  issn      = "0004-5411",
  doi       = "10.1145/3446383"
}

@ARTICLE{Celes2021-cp,
  title     = "Mobility Trace Analysis for Intelligent Vehicular Networks:
               Methods, Models, and Applications",
  author    = "Celes, Clayson and Boukerche, Azzedine and Loureiro, Antonio A F",
  abstract  = "Intelligent vehicular networks emerge as a promising technology
               to provide efficient data communication in transportation
               systems and smart cities. At the same time, the popularization
               of devices with attached sensors has allowed the obtaining of a
               large volume of data with spatiotemporal information from
               different entities. In this sense, we are faced with a large
               volume of vehicular mobility traces being recorded. Those traces
               provide unprecedented opportunities to understand the dynamics
               of vehicular mobility and provide data-driven solutions. In this
               article, we give an overview of the main publicly available
               vehicular mobility traces; then, we present the main issues for
               preprocessing these traces. Also, we present the methods used to
               characterize and model mobility data. Finally, we review
               existing proposals that apply the hidden knowledge extracted
               from the mobility trace for vehicular networks. This article
               provides a survey on studies that use vehicular mobility traces
               and provides a guideline for the proposition of data-driven
               solutions in the domain of vehicular networks. Moreover, we
               discuss open research problems and give some directions to
               undertake them.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  3,
  pages     = "1--38",
  month     =  apr,
  year      =  2021,
  file      = "All Papers/C/Celes et al. 2021 - Mobility Trace Analysis for Intelligent Vehicular Networks - Methods, Models, and Applications.pdf",
  address   = "New York, NY, USA",
  keywords  = "data mining, routing, survey, topology, mobility, vanet,
               Vehicular networks, data analysis;ActualTraffic",
  issn      = "0360-0300",
  doi       = "10.1145/3446679"
}

@ARTICLE{Kabir2021-bf,
  title     = "Uncertainty-aware Decisions in Cloud Computing: Foundations and
               Future Directions",
  author    = "Kabir, H M Dipu and Khosravi, Abbas and Mondal, Subrota K and
               Rahman, Mustaneer and Nahavandi, Saeid and Buyya, Rajkumar",
  abstract  = "The rapid growth of the cloud industry has increased challenges
               in the proper governance of the cloud infrastructure. Many
               intelligent systems have been developing, considering
               uncertainties in the cloud. Intelligent approaches with the
               consideration of uncertainties bring optimal management with
               higher profitability. Uncertainties of different levels and
               different types exist in various domains of cloud computing.
               This survey aims to discuss all types of uncertainties and their
               effect on different components of cloud computing. The article
               first presents the concept of uncertainty and its
               quantification. A vast number of uncertain events influence the
               cloud, as it is connected with the entire world through the
               internet. Five major uncertain parameters are identified, which
               are directly affected by numerous uncertain events and affect
               the performance of the cloud. Notable events affecting major
               uncertain parameters are also described. Besides, we present
               notable uncertainty-aware research works in cloud computing. A
               hype curve on uncertainty-aware approaches in the cloud is also
               presented to visualize current conditions and future
               possibilities. We expect the inauguration of numerous
               uncertainty-aware intelligent systems in cloud management over
               time. This article may provide a deeper understanding of
               managing cloud resources with uncertainties efficiently to
               future cloud researchers.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  4,
  pages     = "1--30",
  month     =  may,
  year      =  2021,
  file      = "All Papers/K/Kabir et al. 2021 - Uncertainty-aware Decisions in Cloud Computing - Foundations and Future Directions.pdf",
  address   = "New York, NY, USA",
  keywords  = "cloud traffic, cloud reliability, QoS, Cloud computing,
               uncertainty;ATOS;EdgeFogCloudIoT",
  issn      = "0360-0300",
  doi       = "10.1145/3447583"
}

@ARTICLE{Gewers2021-fs,
  title     = "Principal Component Analysis: A Natural Approach to Data
               Exploration",
  author    = "Gewers, Felipe L and Ferreira, Gustavo R and Arruda, Henrique F
               De and Silva, Filipi N and Comin, Cesar H and Amancio, Diego R
               and Costa, Luciano Da F",
  abstract  = "Principal component analysis (PCA) is often applied for
               analyzing data in the most diverse areas. This work reports, in
               an accessible and integrated manner, several theoretical and
               practical aspects of PCA. The basic principles underlying PCA,
               data standardization, possible visualizations of the PCA
               results, and outlier detection are subsequently addressed. Next,
               the potential of using PCA for dimensionality reduction is
               illustrated on several real-world datasets. Finally, we
               summarize PCA-related approaches and other dimensionality
               reduction techniques. All in all, the objective of this work is
               to assist researchers from the most diverse areas in using and
               interpreting PCA.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  4,
  pages     = "1--34",
  month     =  may,
  year      =  2021,
  file      = "All Papers/G/Gewers et al. 2021 - Principal Component Analysis - A Natural Approach to Data Exploration.pdf",
  address   = "New York, NY, USA",
  keywords  = "Statistical methods, data visualization, covariance and
               correlation, principal component analysis, dimensionality
               reduction;GenericInterest",
  issn      = "0360-0300",
  doi       = "10.1145/3447755"
}

@ARTICLE{Michel2021-af,
  title     = "The Programmable Data Plane: Abstractions, Architectures,
               Algorithms, and Applications",
  author    = "Michel, Oliver and Bifulco, Roberto and R{\'e}tv{\'a}ri,
               G{\'a}bor and Schmid, Stefan",
  abstract  = "Programmable data plane technologies enable the systematic
               reconfiguration of the low-level processing steps applied to
               network packets and are key drivers toward realizing the next
               generation of network services and applications. This survey
               presents recent trends and issues in the design and
               implementation of programmable network devices, focusing on
               prominent abstractions, architectures, algorithms, and
               applications proposed, debated, and realized over the past
               years. We elaborate on the trends that led to the emergence of
               this technology and highlight the most important pointers from
               the literature, casting different taxonomies for the field, and
               identifying avenues for future research.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  4,
  pages     = "1--36",
  month     =  may,
  year      =  2021,
  file      = "All Papers/M/Michel et al. 2021 - The Programmable Data Plane - Abstractions, Architectures, Algorithms, and Applications.pdf",
  address   = "New York, NY, USA",
  keywords  = "in-network computation, packet processing, Programmable data
               planes, programmable switches, network
               programmability;Important;Good;NFV\_SDN;FutureInternet;ProgrammableNetworks",
  issn      = "0360-0300",
  doi       = "10.1145/3447868"
}

@ARTICLE{Zeng2021-ox,
  title     = "Fork and Join Queueing Networks with Heavy Tails: Scaling
               Dimension and Throughput Limit",
  author    = "Zeng, Yun and Tan, Jian and Xia, Cathy H",
  abstract  = "Parallel and distributed computing systems are foundational to
               the success of cloud computing and big data analytics. These
               systems process computational workflows in a way that can be
               mathematically modeled by a fork-and-join queueing network with
               blocking (FJQN/B). While engineering solutions have long been
               made to build and scale such systems, it is challenging to
               rigorously characterize their throughput performance at scale
               theoretically. What further complicates the study is the
               presence of heavy-tailed delays that have been widely documented
               therein. In this article, we utilize an infinite sequence of
               FJQN/Bs to study the throughput limit and focus on an important
               class of heavy-tailed service times that are regularly varying
               with index . The throughput is said to be scalable if the
               throughput limit infimum of the sequence is strictly positive as
               the network size grows to infinity. We introduce two novel
               geometric concepts---scaling dimension and extended metric
               dimension---and show that an infinite sequence of FJQN/Bs is
               throughput scalable if the extended metric dimension and only if
               the scaling dimension . We also show that for the cases where
               buffer sizes are scaling in an order of , the scalability
               conditions are relaxed by a factor of . The results provide new
               insights on the scalability of a rich class of FJQN/Bs with
               various structures, including tandem, lattice, hexagon, pyramid,
               tree, and fractals.",
  journal   = "J. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  68,
  number    =  3,
  pages     = "1--30",
  month     =  may,
  year      =  2021,
  file      = "All Papers/Z/Zeng et al. 2021 - Fork and Join Queueing Networks with Heavy Tails - Scaling Dimension and Throughput Limit.pdf",
  address   = "New York, NY, USA",
  keywords  = "queueing network, heavy tails, Fork/join, scalability,
               throughput limit, network dimension;Datacentre",
  issn      = "0004-5411",
  doi       = "10.1145/3448213"
}

@ARTICLE{Afsar2021-fk,
  title     = "Assessing the Performance of Interactive Multiobjective
               Optimization Methods: A Survey",
  author    = "Afsar, Bekir and Miettinen, Kaisa and Ruiz, Francisco",
  abstract  = "Interactive methods are useful decision-making tools for
               multiobjective optimization problems, because they allow a
               decision-maker to provide her/his preference information
               iteratively in a comfortable way at the same time as (s)he
               learns about all different aspects of the problem. A wide
               variety of interactive methods is nowadays available, and they
               differ from each other in both technical aspects and type of
               preference information employed. Therefore, assessing the
               performance of interactive methods can help users to choose the
               most appropriate one for a given problem. This is a challenging
               task, which has been tackled from different perspectives in the
               published literature. We present a bibliographic survey of
               papers where interactive multiobjective optimization methods
               have been assessed (either individually or compared to other
               methods). Besides other features, we collect information about
               the type of decision-maker involved (utility or value functions,
               artificial or human decision-maker), the type of preference
               information provided, and aspects of interactive methods that
               were somehow measured. Based on the survey and on our own
               experiences, we identify a series of desirable properties of
               interactive methods that we believe should be assessed.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  4,
  pages     = "1--27",
  month     =  may,
  year      =  2021,
  file      = "All Papers/A/Afsar et al. 2021 - Assessing the Performance of Interactive Multiobjective Optimization Methods - A Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "decision-makers, preference information, Interactive methods,
               performance assessment, multiobjective optimization
               problems;GenericInterest",
  issn      = "0360-0300",
  doi       = "10.1145/3448301"
}

@ARTICLE{Ale2021-bc,
  title     = "Spatio-temporal Bayesian Learning for Mobile Edge Computing
               Resource Planning in Smart Cities",
  author    = "Ale, Laha and Zhang, Ning and King, Scott A and Guardiola, Jose",
  abstract  = "A smart city improves operational efficiency and comfort of
               living by harnessing techniques such as the Internet of Things
               (IoT) to collect and process data for decision-making. To better
               support smart cities, data collected by IoT should be stored and
               processed appropriately. However, IoT devices are often
               task-specialized and resource-constrained, and thus, they
               heavily rely on online resources in terms of computing and
               storage to accomplish various tasks. Moreover, these cloud-based
               solutions often centralize the resources and are far away from
               the end IoTs and cannot respond to users in time due to network
               congestion when massive numbers of tasks offload through the
               core network. Therefore, by decentralizing resources spatially
               close to IoT devices, mobile edge computing (MEC) can reduce
               latency and improve service quality for a smart city, where
               service requests can be fulfilled in proximity. As the service
               demands exhibit spatial-temporal features, deploying MEC servers
               at optimal locations and allocating MEC resources play an
               essential role in efficiently meeting service requirements in a
               smart city. In this regard, it is essential to learn the
               distribution of resource demands in time and space. In this
               work, we first propose a spatio-temporal Bayesian hierarchical
               learning approach to learn and predict the distribution of MEC
               resource demand over space and time to facilitate MEC deployment
               and resource management. Second, the proposed model is trained
               and tested on real-world data, and the results demonstrate that
               the proposed method can achieve very high accuracy. Third, we
               demonstrate an application of the proposed method by simulating
               task offloading. Finally, the simulated results show that
               resources allocated based upon our models' predictions are
               exploited more efficiently than the resources are equally
               divided into all servers in unobserved areas.",
  journal   = "ACM Trans. Internet Technol.",
  publisher = "Association for Computing Machinery",
  volume    =  21,
  number    =  3,
  pages     = "1--21",
  month     =  jun,
  year      =  2021,
  file      = "All Papers/A/Ale et al. 2021 - Spatio-temporal Bayesian Learning for Mobile Edge Computing Resource Planning in Smart Cities.pdf",
  address   = "New York, NY, USA",
  keywords  = "geospatial intelligence, Spatial-temporal
               analysis;EdgeFogCloudIoT",
  issn      = "1533-5399",
  doi       = "10.1145/3448613"
}

@ARTICLE{Lalapura2021-nj,
  title     = "Recurrent Neural Networks for Edge Intelligence: A Survey",
  author    = "Lalapura, Varsha S and Amudha, J and Satheesh, Hariramn
               Selvamuruga",
  abstract  = "Recurrent Neural Networks are ubiquitous and pervasive in many
               artificial intelligence applications such as speech recognition,
               predictive healthcare, creative art, and so on. Although they
               provide accurate superior solutions, they pose a massive
               challenge ``training havoc.'' Current expansion of IoT demands
               intelligent models to be deployed at the edge. This is precisely
               to handle increasing model sizes and complex network
               architectures. Design efforts to meet these for greater
               performance have had inverse effects on portability on edge
               devices with real-time constraints of memory, latency, and
               energy. This article provides a detailed insight into various
               compression techniques widely disseminated in the deep learning
               regime. They have become key in mapping powerful RNNs onto
               resource-constrained devices. While compression of RNNs is the
               main focus of the survey, it also highlights challenges
               encountered while training. The training procedure directly
               influences model performance and compression alongside. Recent
               advancements to overcome the training challenges with their
               strengths and drawbacks are discussed. In short, the survey
               covers the three-step process, namely, architecture selection,
               efficient training process, and suitable compression technique
               applicable to a resource-constrained environment. It is thus one
               of the comprehensive survey guides a developer can adapt for a
               time-series problem context and an RNN solution for the edge.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  4,
  pages     = "1--38",
  month     =  may,
  year      =  2021,
  file      = "All Papers/L/Lalapura et al. 2021 - Recurrent Neural Networks for Edge Intelligence - A Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "low-rank, Recurrent neural networks (RNNs), edge intelligence
               (EI), resource constrained modeling, sequence modeling,
               training, sparsity, artificial intelligence (AI),
               compression;MLNetworking;FutureInternet",
  issn      = "0360-0300",
  doi       = "10.1145/3448974"
}

@ARTICLE{Lo2021-ma,
  title     = "A Systematic Literature Review on Federated Machine Learning:
               From a Software Engineering Perspective",
  author    = "Lo, Sin Kit and Lu, Qinghua and Wang, Chen and Paik, Hye-Young
               and Zhu, Liming",
  abstract  = "Federated learning is an emerging machine learning paradigm
               where clients train models locally and formulate a global model
               based on the local model updates. To identify the
               state-of-the-art in federated learning and explore how to
               develop federated learning systems, we perform a systematic
               literature review from a software engineering perspective, based
               on 231 primary studies. Our data synthesis covers the lifecycle
               of federated learning system development that includes
               background understanding, requirement analysis, architecture
               design, implementation, and evaluation. We highlight and
               summarise the findings from the results and identify future
               trends to encourage researchers to advance their current work.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  5,
  pages     = "1--39",
  month     =  may,
  year      =  2021,
  file      = "All Papers/L/Lo et al. 2021 - A Systematic Literature Review on Federated Machine Learning - From a Software Engineering Perspective.pdf",
  address   = "New York, NY, USA",
  keywords  = "systematic literature review, distributed learning, edge
               learning, privacy, software engineering, Federated learning;ML",
  issn      = "0360-0300",
  doi       = "10.1145/3450288"
}

@ARTICLE{Alashaikh2021-rx,
  title     = "A Survey on the Use of Preferences for Virtual Machine Placement
               in Cloud Data Centers",
  author    = "Alashaikh, Abdulaziz and Alanazi, Eisa and Al-Fuqaha, Ala",
  abstract  = "With the rapid development of virtualization techniques, cloud
               data centers allow for cost-effective, flexible, and
               customizable deployments of applications on virtualized
               infrastructure. Virtual machine (VM) placement aims to assign
               each virtual machine to a server in the cloud environment. VM
               Placement is of paramount importance to the design of cloud data
               centers. Typically, VM placement involves complex relations and
               multiple design factors as well as local policies that govern
               the assignment decisions. It also involves different
               constituents including cloud administrators and customers that
               might have disparate preferences while opting for a placement
               solution. Thus, it is often valuable to return not only an
               optimized solution to the VM placement problem but also a
               solution that reflects the given preferences of the
               constituents. In this article, we provide a detailed review on
               the role of preferences in the recent literature on VM
               placement. We examine different preference representations found
               in the literature, explain their existing usage, and explain the
               adopted solving approaches. We further discuss key challenges
               and identify possible research opportunities to better
               incorporate preferences within the context of VM placement.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  5,
  pages     = "1--39",
  month     =  may,
  year      =  2021,
  file      = "All Papers/A/Alashaikh et al. 2021 - A Survey on the Use of Preferences for Virtual Machine Placement in Cloud Data Centers.pdf",
  address   = "New York, NY, USA",
  keywords  = "decision making, preferences, Virtual machines;NFV\_SDN",
  issn      = "0360-0300",
  doi       = "10.1145/3450517"
}

@ARTICLE{Pateria2021-nh,
  title     = "Hierarchical Reinforcement Learning: A Comprehensive Survey",
  author    = "Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-Hwee and
               Quek, Chai",
  abstract  = "Hierarchical Reinforcement Learning (HRL) enables autonomous
               decomposition of challenging long-horizon decision-making tasks
               into simpler subtasks. During the past years, the landscape of
               HRL research has grown profoundly, resulting in copious
               approaches. A comprehensive overview of this vast landscape is
               necessary to study HRL in an organized manner. We provide a
               survey of the diverse HRL approaches concerning the challenges
               of learning hierarchical policies, subtask discovery, transfer
               learning, and multi-agent learning using HRL. The survey is
               presented according to a novel taxonomy of the approaches. Based
               on the survey, a set of important open problems is proposed to
               motivate the future research in HRL. Furthermore, we outline a
               few suitable task domains for evaluating the HRL approaches and
               a few interesting examples of the practical applications of HRL
               in the Supplementary Material.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  5,
  pages     = "1--35",
  month     =  jun,
  year      =  2021,
  file      = "All Papers/P/Pateria et al. 2021 - Hierarchical Reinforcement Learning - A Comprehensive Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "skill discovery, hierarchical reinforcement learning survey,
               subtask discovery, hierarchical reinforcement learning taxonomy,
               Hierarchical reinforcement learning;MLNetworking",
  issn      = "0360-0300",
  doi       = "10.1145/3453160"
}

@ARTICLE{Liu2021-hd,
  title     = "Towards {Communication-Efficient} and {Attack-Resistant}
               Federated Edge Learning for Industrial Internet of Things",
  author    = "Liu, Yi and Zhao, Ruihui and Kang, Jiawen and Yassine,
               Abdulsalam and Niyato, Dusit and Peng, Jialiang",
  abstract  = "Federated Edge Learning (FEL) allows edge nodes to train a
               global deep learning model collaboratively for edge computing in
               the Industrial Internet of Things (IIoT), which significantly
               promotes the development of Industrial 4.0. However, FEL faces
               two critical challenges: communication overhead and data
               privacy. FEL suffers from expensive communication overhead when
               training large-scale multi-node models. Furthermore, due to the
               vulnerability of FEL to gradient leakage and label-flipping
               attacks, the training process of the global model is easily
               compromised by adversaries. To address these challenges, we
               propose a communication-efficient and privacy-enhanced
               asynchronous FEL framework for edge computing in IIoT. First, we
               introduce an asynchronous model update scheme to reduce the
               computation time that edge nodes wait for global model
               aggregation. Second, we propose an asynchronous local
               differential privacy mechanism, which improves communication
               efficiency and mitigates gradient leakage attacks by adding
               well-designed noise to the gradients of edge nodes. Third, we
               design a cloud-side malicious node detection mechanism to detect
               malicious nodes by testing the local model quality. Such a
               mechanism can avoid malicious nodes participating in training to
               mitigate label-flipping attacks. Extensive experimental studies
               on two real-world datasets demonstrate that the proposed
               framework can not only improve communication efficiency but also
               mitigate malicious attacks while its accuracy is comparable to
               traditional FEL frameworks.",
  journal   = "ACM Trans. Internet Technol.",
  publisher = "Association for Computing Machinery",
  volume    =  22,
  number    =  3,
  pages     = "1--22",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/L/Liu et al. 2021 - Towards Communication-Efficient and Attack-Resistant Federated Edge Learning for Industrial Internet of Things.pdf",
  address   = "New York, NY, USA",
  keywords  = "local differential privacy, edge intelligence, Federated edge
               learning, gradient leakage attack, poisoning
               attack;EdgeFogCloudIoT;MLNetworking",
  issn      = "1533-5399",
  doi       = "10.1145/3453169"
}

@ARTICLE{Ashmore2021-bq,
  title     = "Assuring the Machine Learning Lifecycle: Desiderata, Methods,
               and Challenges",
  author    = "Ashmore, Rob and Calinescu, Radu and Paterson, Colin",
  abstract  = "Machine learning has evolved into an enabling technology for a
               wide range of highly successful applications. The potential for
               this success to continue and accelerate has placed machine
               learning (ML) at the top of research, economic, and political
               agendas. Such unprecedented interest is fuelled by a vision of
               ML applicability extending to healthcare, transportation,
               defence, and other domains of great societal importance.
               Achieving this vision requires the use of ML in safety-critical
               applications that demand levels of assurance beyond those needed
               for current ML applications. Our article provides a
               comprehensive survey of the state of the art in the assurance of
               ML, i.e., in the generation of evidence that ML is sufficiently
               safe for its intended use. The survey covers the methods capable
               of providing such evidence at different stages of the machine
               learning lifecycle, i.e., of the complex, iterative process that
               starts with the collection of the data used to train an ML
               component for a system, and ends with the deployment of that
               component within the system. The article begins with a
               systematic presentation of the ML lifecycle and its stages. We
               then define assurance desiderata for each stage, review existing
               methods that contribute to achieving these desiderata, and
               identify open challenges that require further research.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  5,
  pages     = "1--39",
  month     =  may,
  year      =  2021,
  file      = "All Papers/A/Ashmore et al. 2021 - Assuring the Machine Learning Lifecycle - Desiderata, Methods, and Challenges.pdf",
  address   = "New York, NY, USA",
  keywords  = "machine learning workflow, assurance evidence, Machine learning
               lifecycle, assurance, safety-critical systems",
  issn      = "0360-0300",
  doi       = "10.1145/3453444"
}

@ARTICLE{Rauf2021-fi,
  title     = "Application Threats to Exploit Northbound Interface
               Vulnerabilities in Software Defined Networks",
  author    = "Rauf, Bilal and Abbas, Haider and Usman, Muhammad and Zia,
               Tanveer A and Iqbal, Waseem and Abbas, Yawar and Afzal, Hammad",
  abstract  = "Software Defined Networking (SDN) is an evolving technology that
               decouples the control functionality from the underlying hardware
               managed by the control plane. The application plane supports
               programmers to develop numerous applications (such as
               networking, management, security, etc.) that can even be
               executed from remote locations. Northbound interface (NBI)
               bridges the control and application planes to execute the
               third-party applications business logic. Due to the software
               bugs in applications and existing vulnerabilities such as
               illegal function calling, resource exhaustion, lack of trust,
               and so on, NBIs are susceptible to different attacks. Based on
               the extensive literature review, we have identified that the
               researchers and academia have mainly focused on the security of
               the control plane, data plane, and southbound interface (SBI).
               NBI, in comparison, has received far less attention. In this
               article, the security of the least explored, but a critical
               component of the SDN architecture, i.e., NBI, is analyzed. The
               article provides a brief overview of SDN, followed by a detailed
               discussion on the categories of NBI, vulnerabilities of NBI, and
               threats posed by malicious applications to NBI. Efforts of the
               researchers to counter malicious applications and NBI issues are
               then discussed in detail. The standardization efforts for the
               single acceptable NBI and security requirements of SDN by Open
               Networking Foundation (ONF) are also presented. The article
               concludes with the future research directions for the security
               of a single acceptable NBI.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  6,
  pages     = "1--36",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/R/Rauf et al. 2021 - Application Threats to Exploit Northbound Interface Vulnerabilities in Software Defined Networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "SDN security, malicious applications, SDN, application threats,
               Application plane, northbound interface, northbound interface
               vulnerabilities;SDN;FutureInternet",
  issn      = "0360-0300",
  doi       = "10.1145/3453648"
}

@ARTICLE{Padakandla2021-gd,
  title     = "A Survey of Reinforcement Learning Algorithms for Dynamically
               Varying Environments",
  author    = "Padakandla, Sindhu",
  abstract  = "Reinforcement learning (RL) algorithms find applications in
               inventory control, recommender systems, vehicular traffic
               management, cloud computing, and robotics. The real-world
               complications arising in these domains makes them difficult to
               solve with the basic assumptions underlying classical RL
               algorithms. RL agents in these applications often need to react
               and adapt to changing operating conditions. A significant part
               of research on single-agent RL techniques focuses on developing
               algorithms when the underlying assumption of stationary
               environment model is relaxed. This article provides a survey of
               RL methods developed for handling dynamically varying
               environment models. The goal of methods not limited by the
               stationarity assumption is to help autonomous agents adapt to
               varying operating conditions. This is possible either by
               minimizing the rewards lost during learning by RL agent or by
               finding a suitable policy for the RL agent that leads to
               efficient operation of the underlying system. A representative
               collection of these algorithms is discussed in detail in this
               work along with their categorization and their relative merits
               and demerits. Additionally, we also review works that are
               tailored to application domains. Finally, we discuss future
               enhancements for this field.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  6,
  pages     = "1--25",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/P/Padakandla 2021 - 3459991.pdf;All Papers/P/Padakandla 2021 - A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments.pdf",
  address   = "New York, NY, USA",
  keywords  = "Markov decision processes, sequential decision-making, regret
               computation, non-stationary environments, context detection,
               meta-learning, Reinforcement learning;ATOS;MLNetworking;Cloud",
  issn      = "0360-0300",
  doi       = "10.1145/3459991"
}

@ARTICLE{Zhang2021-dx,
  title     = "Edge Learning: The Enabling Technology for Distributed Big Data
               Analytics in the Edge",
  author    = "Zhang, Jie and Qu, Zhihao and Chen, Chenxi and Wang, Haozhao and
               Zhan, Yufeng and Ye, Baoliu and Guo, Song",
  abstract  = "Machine Learning (ML) has demonstrated great promise in various
               fields, e.g., self-driving, smart city, which are fundamentally
               altering the way individuals and organizations live, work, and
               interact. Traditional centralized learning frameworks require
               uploading all training data from different sources to a remote
               data server, which incurs significant communication overhead,
               service latency, and privacy issues.To further extend the
               frontiers of the learning paradigm, a new learning concept,
               namely, Edge Learning (EL) is emerging. It is complementary to
               the cloud-based methods for big data analytics by enabling
               distributed edge nodes to cooperatively training models and
               conduct inferences with their locally cached data. To explore
               the new characteristics and potential prospects of EL, we
               conduct a comprehensive survey of the recent research efforts on
               EL. Specifically, we first introduce the background and
               motivation. We then discuss the challenging issues in EL from
               the aspects of data, computation, and communication.
               Furthermore, we provide an overview of the enabling technologies
               for EL, including model training, inference, security guarantee,
               privacy protection, and incentive mechanism. Finally, we discuss
               future research opportunities on EL. We believe that this survey
               will provide a comprehensive overview of EL and stimulate
               fruitful future research in this field.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  7,
  pages     = "1--36",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/Z/Zhang et al. 2021 - Edge Learning - The Enabling Technology for Distributed Big Data Analytics in the Edge.pdf",
  address   = "New York, NY, USA",
  keywords  = "Edge learning, federated learning, security and privacy, edge
               computing, machine learning;MLNetworking;FutureInternet",
  issn      = "0360-0300",
  doi       = "10.1145/3464419"
}

@INPROCEEDINGS{Zou2021-yy,
  title     = "Minimizing {Age-of-Information} in Heterogeneous {Multi-Channel}
               Systems: A New {Partial-Index} Approach",
  booktitle = "Proceedings of the Twenty-second International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Zou, Yihan and Kim, Kwang Taik and Lin, Xiaojun and Chiang, Mung",
  abstract  = "We study how to schedule data sources in a wireless
               time-sensitive information system with multiple heterogeneous
               and unreliable channels to minimize the total expected
               Age-of-Information (AoI). Although one could formulate this
               problem as a discrete-time Markov Decision Process (MDP), such
               an approach suffers from the curse of dimensionality and lack of
               insights. For single-channel systems, prior studies have
               developed lower-complexity solutions based on the Whittle index.
               However, Whittle index has not been studied for systems with
               multiple heterogeneous channels, mainly because indexability is
               not well defined when there are multiple dual cost values, one
               for each channel. To overcome this difficulty, we introduce new
               notions of partial indexability and partial index, which are
               defined with respect to one channel's cost, given all other
               channels' costs. We then combine the ideas of partial indices
               and max-weight matching to develop a Sum Weighted Index Matching
               (SWIM) policy, which iteratively updates the dual costs and
               partial indices. The proposed policy is shown to be
               asymptotically optimal in minimizing the total expected AoI,
               under a technical condition on a global attractor property.
               Extensive performance simulations demonstrate that the proposed
               policy offers significant gains over conventional approaches by
               achieving a near-optimal AoI. Further, the notion of partial
               index is of independent interest and could be useful for other
               problems with multiple heterogeneous resources.",
  publisher = "Association for Computing Machinery",
  pages     = "11--20",
  series    = "MobiHoc '21",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/Z/Zou et al. 2021 - Minimizing Age-of-Information in Heterogeneous Multi-Channel Systems - A New Partial-Index Approach.pdf",
  address   = "New York, NY, USA",
  keywords  = "Markov decision processes, heterogeneous channels,
               Age-of-Information, Whittle index, restless bandits;Wireless",
  location  = "Shanghai, China",
  isbn      = "9781450385589",
  doi       = "10.1145/3466772.3467030"
}

@INPROCEEDINGS{Fu2021-vg,
  title     = "{Learning-NUM}: Network Utility Maximization with Unknown
               Utility Functions and Queueing Delay",
  booktitle = "Proceedings of the Twenty-second International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Fu, Xinzhe and Modiano, Eytan",
  abstract  = "Network Utility Maximization (NUM) studies the problems of
               allocating traffic rates to network users in order to maximize
               the users' total utility subject to network resource
               constraints. In this paper, we propose a new NUM framework,
               Learning-NUM, where the users' utility functions are unknown
               apriori and the utility function values of the traffic rates can
               be observed only after the corresponding traffic is delivered to
               the destination, which means that the utility feedback
               experiences queueing delay. The goal is to design a policy that
               gradually learns the utility functions and makes rate allocation
               and network scheduling/routing decisions so as to maximize the
               total utility obtained over a finite time horizon T. In addition
               to unknown utility functions and stochastic constraints, a
               central challenge of our problem lies in the queueing delay of
               the observations, which may be unbounded and depends on the
               decisions of the policy. We first show that the expected total
               utility obtained by the best dynamic policy is upper bounded by
               the solution to a static optimization problem. Without the
               presence of feedback delay, we design an algorithm based on the
               ideas of gradient estimation and Max-Weight scheduling. To
               handle the feedback delay, we embed the algorithm in a
               parallel-instance paradigm to form a policy that achieves
               {\~O}(T3/4)-regret, i.e., the difference between the expected
               utility obtained by the best dynamic policy and our policy is in
               {\~O}(T3/4). Finally, to demonstrate the practical applicability
               of the Learning-NUM framework, we apply it to three application
               scenarios including database query, job scheduling and video
               streaming. We further conduct simulations on the job scheduling
               application to evaluate the empirical performance of our policy.",
  publisher = "Association for Computing Machinery",
  pages     = "21--30",
  series    = "MobiHoc '21",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/F/Fu and Modiano 2021 - Learning-NUM - Network Utility Maximization with Unknown Utility Functions and Queueing Delay.pdf",
  address   = "New York, NY, USA",
  keywords  = "FutureInternet;MLNetworking",
  location  = "Shanghai, China",
  isbn      = "9781450385589",
  doi       = "10.1145/3466772.3467031"
}

@INPROCEEDINGS{Yue2021-zi,
  title     = "{Inexact-ADMM} Based Federated {Meta-Learning} for Fast and
               Continual Edge Learning",
  booktitle = "Proceedings of the Twenty-second International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Yue, Sheng and Ren, Ju and Xin, Jiang and Lin, Sen and Zhang,
               Junshan",
  abstract  = "In order to meet the requirements for performance, safety, and
               latency in many IoT applications, intelligent decisions must be
               made right here right now at the network edge. However, the
               constrained resources and limited local data amount pose
               significant challenges to the development of edge AI. To
               overcome these challenges, we explore continual edge learning
               capable of leveraging the knowledge transfer from previous
               tasks. Aiming to achieve fast and continual edge learning, we
               propose a platform-aided federated meta-learning architecture
               where edge nodes collaboratively learn a meta-model, aided by
               the knowledge transfer from prior tasks. The edge learning
               problem is cast as a regularized optimization problem, where the
               valuable knowledge learned from previous tasks is extracted as
               regularization. Then, we devise an ADMM based federated
               meta-learning algorithm, namely ADMM-FedMeta, where ADMM offers
               a natural mechanism to decompose the original problem into many
               subproblems which can be solved in parallel across edge nodes
               and the platform. Further, a variant of inexact-ADMM method is
               employed where the subproblems are 'solved' via linear
               approximation as well as Hessian estimation to reduce the
               computational cost per round to O(n). We provide a comprehensive
               analysis of ADMM-FedMeta, in terms of the convergence
               properties, the rapid adaptation performance, and the forgetting
               effect of prior knowledge transfer, for the general non-convex
               case. Extensive experimental studies demonstrate the
               effectiveness and efficiency of ADMM-FedMeta, and showcase that
               it substantially outperforms the existing baselines.",
  publisher = "Association for Computing Machinery",
  pages     = "91--100",
  series    = "MobiHoc '21",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/Y/Yue et al. 2021 - Inexact-ADMM Based Federated Meta-Learning for Fast and Continual Edge Learning.pdf",
  address   = "New York, NY, USA",
  keywords  = "edge intelligence, ADMM, continual learning, regularization,
               federated meta-learning;MLNetworking;EdgeFogCloudIoT",
  location  = "Shanghai, China",
  isbn      = "9781450385589",
  doi       = "10.1145/3466772.3467038"
}

@INPROCEEDINGS{Pan2021-ne,
  title     = "Minimizing Age of Information via Scheduling over Heterogeneous
               Channels",
  booktitle = "Proceedings of the Twenty-second International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Pan, Jiayu and Bedewy, Ahmed M and Sun, Yin and Shroff, Ness B",
  abstract  = "In this paper, we study the problem of minimizing the age of
               information when a source can transmit status updates over two
               heterogeneous channels. Our work is motivated by recent
               developments in 5G mmWave technology, where transmissions may
               occur over an unreliable but fast (e.g., mmWave) channel or a
               slow reliable (e.g., sub-6GHz) channel. The unreliable channel
               is modeled as a time-correlated Gilbert-Elliot channel, where
               information can be transmitted at a high rate when the channel
               is in the ``ON'' state. The reliable channel provides a
               deterministic but lower data rate. The scheduling strategy
               determines the channel to be used for transmission with the aim
               to minimize the time-average age of information (AoI). The
               optimal scheduling problem is formulated as a Markov Decision
               Process (MDP), which in our setting poses some significant
               challenges because e.g., supermodularity does not hold for part
               of the state space. We show that there exists a
               multi-dimensional threshold-based scheduling policy that is
               optimal for minimizing the age. A low-complexity bisection
               algorithm is further devised to compute the optimal thresholds.
               Numerical simulations are provided to compare different
               scheduling policies.",
  publisher = "Association for Computing Machinery",
  pages     = "111--120",
  series    = "MobiHoc '21",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/P/Pan et al. 2021 - Minimizing Age of Information via Scheduling over Heterogeneous Channels.pdf",
  address   = "New York, NY, USA",
  keywords  = "Heterogeneous channels, Age of information, Scheduling
               policy;Wireless",
  location  = "Shanghai, China",
  isbn      = "9781450385589",
  doi       = "10.1145/3466772.3467040"
}

@INPROCEEDINGS{Yao2021-pe,
  title     = "Battle between Rate and Error in Minimizing Age of Information",
  booktitle = "Proceedings of the Twenty-second International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Yao, Guidan and Bedewy, Ahmed M and Shroff, Ness B",
  abstract  = "In this paper, we consider a status update system, in which
               update packets are sent to the destination via a wireless medium
               that allows for multiple rates, where a higher rate also
               naturally corresponds to a higher error probability. The data
               freshness is measured using age of information, which is defined
               as the age of the recent update at the destination. A packet
               that is transmitted with a higher rate, will encounter a shorter
               delay and a higher error probability. Thus, the choice of the
               transmission rate affects the age at the destination. In this
               paper, we design a low-complexity scheduler that selects between
               two different transmission rate and error probability pairs to
               be used at each transmission epoch. This problem can be cast as
               a Markov Decision Process. We show that there exists a
               threshold-type policy that is age-optimal. More importantly, we
               show that the objective function is quasi-convex or
               non-decreasing in the threshold, based on the system parameters
               values. This enables us to devise a low-complexity algorithm to
               minimize the age. These results reveal an interesting
               phenomenon: While choosing the rate with minimum mean delay is
               delay-optimal, this does not necessarily minimize the age.",
  publisher = "Association for Computing Machinery",
  pages     = "121--130",
  series    = "MobiHoc '21",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/Y/Yao et al. 2021 - Battle between Rate and Error in Minimizing Age of Information.pdf",
  address   = "New York, NY, USA",
  keywords  = "Heterogenous transmission, Threshold-type policy, Markov
               decision process, Age of information;Wireless",
  location  = "Shanghai, China",
  isbn      = "9781450385589",
  doi       = "10.1145/3466772.3467041"
}

@INPROCEEDINGS{Wang2021-ud,
  title     = "An Online Mean Field Approach for Hybrid Edge Server Provision",
  booktitle = "Proceedings of the Twenty-second International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Wang, Zhiyuan and Ye, Jiancheng and Lui, John C S",
  abstract  = "The performance of an edge computing system primarily depends on
               the edge server provision mode, the task migration scheme, and
               the computing resource configuration. This paper studies how to
               perform dynamic resource configuration for hybrid edge server
               provision under two decentralized task migration schemes. We
               formulate the dynamic resource configuration as a multi-period
               online cost minimization problem, aiming to jointly minimize the
               performance degradation (i.e., execution latency) and the
               operation expenditure. Due to the stochastic nature, one can
               only observe the system performance for the currently installed
               configuration, which is also known as the partial feedback. To
               overcome this challenge, we derive a deterministic mean field
               model to approximate the large-scale stochastic edge computing
               system. We then propose an online mean field aided resource
               configuration policy, and show that the proposed policy performs
               asymptotically as good as the offline optimal configuration.
               Numerical results show that the mean field model can
               significantly improve the convergence speed in the online
               resource configuration problem. Moreover, our proposed policy
               under the two decentralized task migration schemes considerably
               reduces the operating cost (by 23\%) and incurs little
               communication overhead.",
  publisher = "Association for Computing Machinery",
  pages     = "131--140",
  series    = "MobiHoc '21",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/W/Wang et al. 2021 - An Online Mean Field Approach for Hybrid Edge Server Provision.pdf",
  address   = "New York, NY, USA",
  keywords  = "load balancing, online learning, Edge computing, resource
               configuration, mean field model;EdgeFogCloudIoT",
  location  = "Shanghai, China",
  isbn      = "9781450385589",
  doi       = "10.1145/3466772.3467042"
}

@INPROCEEDINGS{Moon2021-vs,
  title     = "{Neuro-DCF}: Design of Wireless {MAC} via {Multi-Agent}
               Reinforcement Learning Approach",
  booktitle = "Proceedings of the Twenty-second International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Moon, Sangwoo and Ahn, Sumyeong and Son, Kyunghwan and Park,
               Jinwoo and Yi, Yung",
  abstract  = "The carrier sense multiple access (CSMA) algorithm has been used
               in the wireless medium access control (MAC) under standard
               802.11 implementation due to its simplicity and generality. An
               extensive body of research on CSMA has long been made not only
               in the context of practical protocols, but also in a distributed
               way of optimal MAC scheduling. However, the current
               state-of-the-art CSMA (or its extensions) still suffers from
               poor performance, especially in multi-hop scenarios, and often
               requires patch-based solutions rather than a universal solution.
               In this paper, we propose an algorithm which adopts an
               experience-driven approach and train CSMA-based wireless MAC by
               using deep reinforcement learning. We name our protocol,
               Neuro-DCF. Two key challenges are: (i) a stable training method
               for distributed execution and (ii) a unified training method for
               embracing various interference patterns and configurations. For
               (i), we adopt a multi-agent reinforcement learning framework,
               and for (ii) we introduce a novel graph neural network (GNN)
               based training structure. We provide extensive simulation
               results which demonstrate that our protocol, Neuro-DCF,
               significantly outperforms 802.11 DCF and O-DCF, a recent
               theory-based MAC protocol, especially in terms of improving
               delay performance while preserving optimal utility. We believe
               our multi-agent reinforcement learning based approach would get
               broad interest from other learning-based network controllers in
               different layers that require distributed operation.",
  publisher = "Association for Computing Machinery",
  pages     = "141--150",
  series    = "MobiHoc '21",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/M/Moon et al. 2021 - Neuro-DCF - Design of Wireless MAC via Multi-Agent Reinforcement Learning Approach.pdf",
  address   = "New York, NY, USA",
  keywords  = "Multi-agent RL, Optimal CSMA, Wireless MAC;MLNetworking;Wireless",
  location  = "Shanghai, China",
  isbn      = "9781450385589",
  doi       = "10.1145/3466772.3467043"
}

@INPROCEEDINGS{Tripathi2021-bg,
  title     = "An Online Learning Approach to Optimizing {Time-Varying} Costs
               of {AoI}",
  booktitle = "Proceedings of the Twenty-second International Symposium on
               Theory, Algorithmic Foundations, and Protocol Design for Mobile
               Networks and Mobile Computing",
  author    = "Tripathi, Vishrant and Modiano, Eytan",
  abstract  = "We consider systems that require timely monitoring of sources
               over a communication network, where the cost of delayed
               information is unknown, time-varying and possibly adversarial.
               For the single source monitoring problem, we design algorithms
               that achieve sublinear regret compared to the best fixed policy
               in hindsight. For the multiple source scheduling problem, we
               design a new online learning algorithm called Follow the
               Perturbed Whittle Leader and show that it has low regret
               compared to the best fixed scheduling policy in hindsight, while
               remaining computationally feasible. The algorithm and its regret
               analysis are novel and of independent interest to the study of
               online restless multi-armed bandit problems. We further design
               algorithms that achieve sublinear regret compared to the best
               dynamic policy when the environment is slowly varying. Finally,
               we apply our algorithms to a mobility tracking problem. We
               consider non-stationary and adversarial mobility models and
               illustrate the performance benefit of using our online learning
               algorithms compared to an oblivious scheduling policy.",
  publisher = "Association for Computing Machinery",
  pages     = "241--250",
  series    = "MobiHoc '21",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/T/Tripathi and Modiano 2021 - An Online Learning Approach to Optimizing Time-Varying Costs of AoI.pdf",
  address   = "New York, NY, USA",
  keywords  = "wireless networks, scheduling, online learning, Age of
               Information;MLNetworking",
  location  = "Shanghai, China",
  isbn      = "9781450385589",
  doi       = "10.1145/3466772.3467053"
}

@ARTICLE{Murshed2021-ad,
  title     = "Machine Learning at the Network Edge: A Survey",
  author    = "Murshed, M G Sarwar and Murphy, Christopher and Hou, Daqing and
               Khan, Nazar and Ananthanarayanan, Ganesh and Hussain, Faraz",
  abstract  = "Resource-constrained IoT devices, such as sensors and actuators,
               have become ubiquitous in recent years. This has led to the
               generation of large quantities of data in real-time, which is an
               appealing target for AI systems. However, deploying machine
               learning models on such end-devices is nearly impossible. A
               typical solution involves offloading data to external computing
               systems (such as cloud servers) for further processing but this
               worsens latency, leads to increased communication costs, and
               adds to privacy concerns. To address this issue, efforts have
               been made to place additional computing devices at the edge of
               the network, i.e., close to the IoT devices where the data is
               generated. Deploying machine learning systems on such edge
               computing devices alleviates the above issues by allowing
               computations to be performed close to the data sources. This
               survey describes major research efforts where machine learning
               systems have been deployed at the edge of computer networks,
               focusing on the operational aspects including compression
               techniques, tools, frameworks, and hardware used in successful
               applications of intelligent edge systems.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  8,
  pages     = "1--37",
  month     =  oct,
  year      =  2021,
  file      = "All Papers/M/Murshed et al. 2021 - Machine Learning at the Network Edge - A Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "Edge intelligence, deep learning, machine learning, distributed
               computing, low-power, mobile edge computing, embedded,
               resource-constrained, IoT;FutureInternet;MLNetworking;MLAspects",
  issn      = "0360-0300",
  doi       = "10.1145/3469029"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Dutting2021-cd,
  title     = "Optimal auctions through deep learning",
  author    = "D{\"u}tting, Paul and Feng, Zhe and Narasimhan, Harikrishna and
               Parkes, David C and Ravindranath, Sai S",
  abstract  = "Designing an incentive compatible auction that maximizes
               expected revenue is an intricate task. The single-item case was
               resolved in a seminal piece of work by Myerson in 1981. Even
               after 30--40 years of intense research, the problem remains
               unsolved for settings with two or more items. We overview recent
               research results that show how tools from deep learning are
               shaping up to become a powerful tool for the automated design of
               near-optimal auctions auctions. In this approach, an auction is
               modeled as a multilayer neural network, with optimal auction
               design framed as a constrained learning problem that can be
               addressed with standard machine learning pipelines. Through this
               approach, it is possible to recover to a high degree of accuracy
               essentially all known analytically derived solutions for
               multi-item settings and obtain novel mechanisms for settings in
               which the optimal mechanism is unknown.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  64,
  number    =  8,
  pages     = "109--116",
  month     =  jul,
  year      =  2021,
  file      = "All Papers/D/Dütting et al. 2021 - Optimal auctions through deep learning.pdf",
  address   = "New York, NY, USA",
  issn      = "0001-0782",
  doi       = "10.1145/3470442"
}

@ARTICLE{Arakadakis2021-ig,
  title     = "Firmware Over-the-air Programming Techniques for {IoT} Networks
               - A Survey",
  author    = "Arakadakis, Konstantinos and Charalampidis, Pavlos and
               Makrogiannakis, Antonis and Fragkiadakis, Alexandros",
  abstract  = "The devices forming Internet of Things (IoT) networks need to be
               re-programmed over the air, so that new features are added,
               software bugs or security vulnerabilities are resolved, and
               their applications can be re-purposed. The limitations of IoT
               devices, such as installation in locations with limited physical
               access, resource-constrained nature, large scale, and high
               heterogeneity, should be taken into consideration for designing
               an efficient and reliable pipeline for over-the-air programming
               (OTAP). In this work, we present a survey of OTAP techniques,
               which can be applied to IoT networks. We highlight the main
               challenges and limitations of OTAP for IoT devices and analyze
               the essential steps of the firmware update process, along with
               different approaches and techniques that implement them. In
               addition, we discuss schemes that focus on securing the OTAP
               process. Finally, we present a collection of state-of-the-art
               open-source and commercial platforms that integrate secure and
               reliable OTAP.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  9,
  pages     = "1--36",
  month     =  oct,
  year      =  2021,
  file      = "All Papers/A/Arakadakis et al. 2021 - Firmware Over-the-air Programming Techniques for IoT Networks - A Survey.pdf",
  address   = "New York, NY, USA",
  keywords  = "delta scripts, firmware update, over-the-air-programming,
               firmware image similarity, code dissemination,
               Internet-of-Things (IoT);EdgeFogCloudIoT",
  issn      = "0360-0300",
  doi       = "10.1145/3472292"
}

@INPROCEEDINGS{Li2021-qz,
  title     = "Profit Maximization for Service Placement and Request Assignment
               in Edge Computing via Deep Reinforcement Learning",
  booktitle = "Proceedings of the 24th International {ACM} Conference on
               Modeling, Analysis and Simulation of Wireless and Mobile Systems",
  author    = "Li, Yuchen and Liang, Weifa and Li, Jing",
  abstract  = "With the integration of Mobile Edge Computing (MEC) and Network
               Function Virtualization (NFV), service providers are able to
               provide low-latency services to mobile users for profit. In this
               paper, we study the problem of service instance placement and
               request assignment in an MEC network for a given monitoring
               period, where service requests arrive into the system without
               the knowledge of future arrivals. Each incoming request requires
               a specific service with a maximum tolerable service delay
               requirement. The problem is to maximize the profit of the
               service provider by admitting service requests for the
               monitoring period, which can be achieved by preinstalling
               service instances into cloudlets to shorten service delays, and
               accommodating new services by removing some idle service
               instances from cloudlets due to limited computing resources. We
               then devise an efficient deep-reinforcement-learning-based
               algorithm for this dynamic online service instance placement
               problem. We finally evaluate the performance of the proposed
               algorithm by conducting experiments through simulations.
               Simulation results demonstrate that the proposed algorithm is
               promising.",
  publisher = "Association for Computing Machinery",
  pages     = "51--55",
  series    = "MSWiM '21",
  month     =  nov,
  year      =  2021,
  file      = "All Papers/L/Li et al. 2021 - Profit Maximization for Service Placement and Request Assignment in Edge Computing via Deep Reinforcement Learning.pdf",
  address   = "New York, NY, USA",
  keywords  = "service request provisioning, service instance placement, mobile
               edge-cloud networks, profit
               maximization;FutureInternet;EdgeFogCloudIoT;MLNetworking",
  location  = "Alicante, Spain",
  isbn      = "9781450390774",
  doi       = "10.1145/3479239.3485673"
}

@INPROCEEDINGS{Ayimba2021-az,
  title     = "Closer than Close: {MEC-Assisted} Platooning with Intelligent
               Controller Migration",
  booktitle = "Proceedings of the 24th International {ACM} Conference on
               Modeling, Analysis and Simulation of Wireless and Mobile Systems",
  author    = "Ayimba, Constantine and Segata, Michele and Casari, Paolo and
               Mancuso, Vincenzo",
  abstract  = "The advent of multi access edge computing~(MEC) will enable
               latency-critical applications such as cooperative adaptive
               cruise control (also known as platooning) to be hosted at the
               edge of the network. MEC-based platooning will leverage the
               coverage of the cellular infrastructure to enable
               inter-vehicular communications, potentially overcoming crucial
               problems of vehicular ad-hoc networks~(VANETs) such as
               non-trivial packet loss rates. However, MEC-based platooning
               will require the controller to be migrated to the most suitable
               positions at the network edge, in order to maintain low-latency
               connections as the platoon moves. In this paper, we propose a
               context-awareQ -Learning algorithm that carries out such
               migrations only as often as is necessary, and thereby reduces
               the additional delays implicit in application migration across
               MEC hosts. When compared to the state-of-the-art approach named
               FollowME, our scheme exhibits better compliance of vehicle speed
               and spacing values to preset targets, as well as a reduced
               statistical dispersion.",
  publisher = "Association for Computing Machinery",
  pages     = "23--32",
  series    = "MSWiM '21",
  month     =  nov,
  year      =  2021,
  file      = "All Papers/A/Ayimba et al. 2021 - Closer than Close - MEC-Assisted Platooning with Intelligent Controller Migration.pdf",
  address   = "New York, NY, USA",
  keywords  = "mec, platooning, q-learning",
  location  = "Alicante, Spain",
  isbn      = "9781450390774",
  doi       = "10.1145/3479239.3485681"
}

@INPROCEEDINGS{Sahoo2021-by,
  title     = "Admission Control and Scheduling of Isochronous Traffic in
               {IEEE} 802.11ad {MAC}",
  booktitle = "Proceedings of the 24th International {ACM} Conference on
               Modeling, Analysis and Simulation of Wireless and Mobile Systems",
  author    = "Sahoo, Anirudha and Gao, Weichao and Ropitault, Tanguy and
               Golmie, Nada",
  abstract  = "An upsurge of low latency and bandwidth hungry applications such
               as virtual reality, augmented reality and availability of
               unlicensed spectrum in the mmWave band at 60 GHz have led to
               standardization of the next generation WiFi such as IEEE
               802.11ad and 802.11ay. Due to the stringent Quality of Service
               (QoS) requirement of those applications, 802.11ad/ay have
               introduced contention free channel access called Service Period,
               which provides dedicated channel access exclusively reserved for
               communication between a pair of nodes. One type of user traffic
               supported by IEEE 802.11ad is isochronous traffic, which is
               essentially periodic traffic that requires certain channel time
               to be allocated before its period ends. So, isochronous traffic
               needs guaranteed channel time allocation with stringent
               deadlines. In this paper, we present three Admission Control
               Algorithms (ACAs) which admit isochronous requests to achieve
               the above goals while being fair. We also present an Earliest
               Deadline First (EDF) based scheduling algorithm for isochronous
               traffic. We evaluate the performance of the three ACAs in terms
               of different performance metrics. Our simulation results show
               that, out of the three ACAs, the proportional fair allocation
               based algorithm offers the best tradeoff across different
               performance metrics.",
  publisher = "Association for Computing Machinery",
  pages     = "125--134",
  series    = "MSWiM '21",
  month     =  nov,
  year      =  2021,
  file      = "All Papers/S/Sahoo et al. 2021 - Admission Control and Scheduling of Isochronous Traffic in IEEE 802.11ad MAC.pdf",
  address   = "New York, NY, USA",
  keywords  = "MAC, admission control, IEEE 802.11ad,
               scheduling;Mobile\_Wireless;Wireless;wifi",
  location  = "Alicante, Spain",
  isbn      = "9781450390774",
  doi       = "10.1145/3479239.3485698"
}

@INPROCEEDINGS{Bardou2021-uq,
  title     = "Improving the Spatial Reuse in {IEEE} 802.11ax {WLANs}: A
               {Multi-Armed} Bandit Approach",
  booktitle = "Proceedings of the 24th International {ACM} Conference on
               Modeling, Analysis and Simulation of Wireless and Mobile Systems",
  author    = "Bardou, Anthony and Begin, Thomas and Busson, Anthony",
  abstract  = "The latest amendment 802.11ax to the IEEE 802.11 standard,
               better known by its commercial name Wi-Fi 6, includes a feature
               that aims at improving the spatial reuse of a channel: each
               device can adapt its Clear Channel Assessment sensitivity
               threshold and its transmission power. In this paper, we use the
               Multi-Armed Bandit (MAB) framework to propose a centralized
               solution to dynamically adapt these parameters. We propose a new
               approach based on a Gaussian mixture to sample new network
               configurations, a specific reward function that prevents
               starvations when maximized, as well as a method based on
               Thompson Sampling to select the best network configuration. We
               evaluate our solution using the network simulator ns-3 and
               different topologies. Simulation results confirm the large
               benefits that 802.11ax may bring to spatial reuse. They also
               demonstrate the efficiency of our solution in finding
               appropriate parameter configurations that significantly improve
               the quality of service of the networks.",
  publisher = "Association for Computing Machinery",
  pages     = "135--144",
  series    = "MSWiM '21",
  month     =  nov,
  year      =  2021,
  file      = "All Papers/B/Bardou et al. 2021 - Improving the Spatial Reuse in IEEE 802.11ax WLANs - A Multi-Armed Bandit Approach.pdf",
  address   = "New York, NY, USA",
  keywords  = "thompson sampling, wlan, power control, machine learning, clear
               channel assessment, channel reuse;Wireless;Mobile\_Wireless;wifi",
  location  = "Alicante, Spain",
  isbn      = "9781450390774",
  doi       = "10.1145/3479239.3485715"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Hafner2021-rv,
  title     = "Preventing Failures of Cooperative Maneuvers Among Connected and
               Automated Vehicles",
  booktitle = "Proceedings of the 24th International {ACM} Conference on
               Modeling, Analysis and Simulation of Wireless and Mobile Systems",
  author    = "H{\"a}fner, Bernhard and Jiru, Josef and Schepker, Henning and
               Schmitt, Georg Albrecht and Ott, J{\"o}rg",
  abstract  = "Automated vehicles will be able to drive autonomously in various
               environments. An essential part of that is to predict other
               vehicles' intents and to coordinate maneuvers jointly. Such
               cooperative maneuvers have the ability to make driving safer and
               traffic more efficient. However, among the various communication
               protocols proposed for maneuver coordination, no single one
               satisfies all requirements. This paper assesses failure risks
               and mitigation strategies for cooperative maneuvers, including
               an analysis of popular protocols regarding this aspect. Next, we
               evaluate one particular cooperation protocol, the complex
               vehicular interactions protocol (CVIP), concerning performance
               of mitigation mechanisms and their influence on maneuver success
               rates or times to reach consensus among maneuver participants.
               Via simulation, we show that CVIP is suitable for cooperative
               maneuvers in realistic scenarios and investigate the trade-offs
               individual mitigation mechanisms face. These results are
               well-suited as guidelines and benchmark for other researchers
               developing cooperative maneuver protocols.",
  publisher = "Association for Computing Machinery",
  pages     = "5--12",
  series    = "MSWiM '21",
  month     =  nov,
  year      =  2021,
  file      = "All Papers/H/Häfner et al. 2021 - Preventing Failures of Cooperative Maneuvers Among Connected and Automated Vehicles.pdf",
  address   = "New York, NY, USA",
  keywords  = "risk mitigation, cooperative maneuvers, vehicle-to-everything,
               failure risk, V2X, performance evaluation, connected vehicles",
  location  = "Alicante, Spain",
  isbn      = "9781450390774",
  doi       = "10.1145/3479239.3485721"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Hafner2021-hj,
  title     = "Proposing Cooperative Maneuvers Among Automated Vehicles Using
               Machine Learning",
  booktitle = "Proceedings of the 24th International {ACM} Conference on
               Modeling, Analysis and Simulation of Wireless and Mobile Systems",
  author    = "H{\"a}fner, Bernhard and Jiru, Josef and Schepker, Henning and
               Schmitt, Georg Albrecht and Ott, J{\"o}rg",
  abstract  = "Cooperative maneuvers will enable automated vehicles to optimize
               traffic flow and increase safety via vehicle-to-vehicle
               communication. Different approaches and protocols exist, but no
               study has investigated how to generate intelligent suggestions
               for cooperative maneuvers. We use machine learning to propose
               safe and suitable overtake maneuvers. To this end, we train a
               classifier for maneuver success as well as regression models on
               an extensive data set of randomized initial situations. In
               addition, we show that changing objective functions allows
               optimizing for different goals like smoothness or driven
               distance. Our evaluation shows that machine learning is
               well-suited to suggest cooperative maneuvers while also facing
               some trade-offs. This work may thus provide a benchmark for
               advanced studies on cooperative maneuver proposals.",
  publisher = "Association for Computing Machinery",
  pages     = "173--180",
  series    = "MSWiM '21",
  month     =  nov,
  year      =  2021,
  file      = "All Papers/H/Häfner et al. 2021 - Proposing Cooperative Maneuvers Among Automated Vehicles Using Machine Learning.pdf",
  address   = "New York, NY, USA",
  keywords  = "cooperative maneuvers, vehicle-to-everything, machine learning,
               artificial intelligence, V2X, connected vehicles",
  location  = "Alicante, Spain",
  isbn      = "9781450390774",
  doi       = "10.1145/3479239.3485726"
}

@ARTICLE{Meneguette2021-zh,
  title     = "Vehicular Edge Computing: Architecture, Resource Management,
               Security, and Challenges",
  author    = "Meneguette, Rodolfo and De Grande, Robson and Ueyama, Jo and
               Filho, Geraldo P Rocha and Madeira, Edmundo",
  abstract  = "Vehicular Edge Computing (VEC), based on the Edge Computing
               motivation and fundamentals, is a promising technology
               supporting Intelligent Transport Systems services, smart city
               applications, and urban computing. VEC can provide and manage
               computational resources closer to vehicles and end-users,
               providing access to services at lower latency and meeting the
               minimum execution requirements for each service type. This
               survey describes VEC's concepts and technologies; we also
               present an overview of existing VEC architectures, discussing
               them and exemplifying them through layered designs. Besides, we
               describe the underlying vehicular communication in supporting
               resource allocation mechanisms. With the intent to overview the
               risks, breaches, and measures in VEC, we review related security
               approaches and methods. Finally, we conclude this survey work
               with an overview and study of VEC's main challenges. Unlike
               other surveys in which they are focused on content caching and
               data offloading, this work proposes a taxonomy based on the
               architectures in which VEC serves as the central element. VEC
               supports such architectures in capturing and disseminating data
               and resources to offer services aimed at a smart city through
               their aggregation and the allocation in a secure manner.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  55,
  number    =  1,
  pages     = "1--46",
  month     =  nov,
  year      =  2021,
  file      = "All Papers/M/Meneguette et al. 2021 - Vehicular Edge Computing - Architecture, Resource Management, Security, and Challenges.pdf",
  address   = "New York, NY, USA",
  keywords  = "security, resource management, architecture, Vehicular edge
               computer;EdgeFogCloudIoT",
  issn      = "0360-0300",
  doi       = "10.1145/3485129"
}

@INPROCEEDINGS{De_Ruiter2021-ez,
  title     = "Next-generation internet at terabit speed: {SCION} in {P4}",
  booktitle = "Proceedings of the 17th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "de Ruiter, Joeri and Schutijser, Caspar",
  abstract  = "Regularly, new architectures are proposed to address
               shortcomings in the current internet. It is not always trivial
               to evaluate how these proposals would perform in practice. This
               situation is improved significantly with the introduction of the
               P4 programming language and programmable network equipment. In
               this paper we discuss our implementation of one particular
               future internet architecture, namely SCION. We implemented a
               SCION router in P4 for switches based on the Intel Tofino ASIC.
               Having an open source P4 implementation of SCION that runs on
               high-speed hardware can contribute to its adoption as well as
               support research in this area. Our work lead to several
               recommendations for and subsequent changes to the SCION
               protocol, as well as some generic guidelines when designing
               protocols. A first analysis of our implementation shows it can
               process SCION packets at high speeds.",
  publisher = "Association for Computing Machinery",
  pages     = "119--125",
  series    = "CoNEXT '21",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/D/de Ruiter and Schutijser 2021 - Next-generation internet at terabit speed - SCION in P4.pdf",
  address   = "New York, NY, USA",
  keywords  = "SCION, future internet, programmable networking,
               P4;ProgrammableNetworks",
  location  = "Virtual Event, Germany",
  isbn      = "9781450390989",
  doi       = "10.1145/3485983.3494839"
}

@INPROCEEDINGS{Krude2021-fa,
  title     = "Determination of throughput guarantees for processor-based
               {SmartNICs}",
  booktitle = "Proceedings of the 17th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Krude, Johannes and R{\"u}th, Jan and Schemmel, Daniel and Rath,
               Felix and Folbort, Iohannes-Heorh and Wehrle, Klaus",
  abstract  = "Programmable network devices are on the rise with many
               applications ranging from improved network management to
               accelerating and offloading parts of distributed systems.
               Processor-based SmartNICs, match-action-based switches, and FPGA
               devices offer on-path programmability. Whereas processor-based
               SmartNICs are much easier and more versatile to program, they
               have the huge disadvantage that the resulting throughput may
               vary strongly and is not easily predictable even to the
               programmer. We want to close this gap by presenting a
               methodology which, given a SmartNIC program, determines the
               achievable throughput of this SmartNIC program in terms of
               achievable packet rate and bit rate. Our approach combines
               incremental longest path search with SMT checks to establish a
               lower bound for the slowest satisfiable program path. By
               analyzing only the slowest program paths, our approach estimates
               throughput bounds within a few seconds. The evaluation with our
               prototype on real programs shows that the estimated throughput
               guarantees are correct with an error of at most 1.7\% and
               provide a tight lower bound for processor- and
               memory-bottlenecked programs with only 8.5\% and 18.2\%
               underestimation.",
  publisher = "Association for Computing Machinery",
  pages     = "267--281",
  series    = "CoNEXT '21",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/K/Krude et al. 2021 - Determination of throughput guarantees for processor-based SmartNICs.pdf",
  address   = "New York, NY, USA",
  keywords  = "bit rate, packet rate, longest path search, BPF/XDP,
               SmartNIC;ProgrammableNetworks;ProgrammableNetworks",
  location  = "Virtual Event, Germany",
  isbn      = "9781450390989",
  doi       = "10.1145/3485983.3494842"
}

@INPROCEEDINGS{Zhou2021-lv,
  title     = "{P4Update}: fast and locally verifiable consistent network
               updates in the {P4} data plane",
  booktitle = "Proceedings of the 17th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Zhou, Zikai and He, Mu and Kellerer, Wolfgang and Blenk, Andreas
               and Foerster, Klaus-Tycho",
  abstract  = "Programmable networks come with the promise of logically
               centralized control, in order to optimize the network's routing
               behavior. However, until now, controllers are heavily involved
               in network operations to prevent inconsistencies such as
               blackholes, loops, and congestion. In this paper, we propose the
               P4Update framework, based on the network programming language
               P4, to shift the consistency control and most of the routing
               update logic out of the overloaded and slow control plane. As
               such P4Update avoids high and unnecessary control plane delays
               by mainly scheduling and offloading the update process to the
               data plane.P4Update returns to operating networks in a partially
               centralized and distributed manner --- taking the best of both
               centralized and distributed worlds. The main idea is to flip the
               problem setting and see asynchrony as an opportunity: switches
               inform their local neighborhood on resolved update dependencies.
               What's more, our mechanisms are also provably resilient against
               inconsistent, reordered, or conflicting concurrent updates.
               Unlike prior systems, P4Update enables switches to locally
               verify and reject inconsistent updates, and is also the first
               system to resolve inter-flow update dependencies purely in the
               data plane, significantly reducing control plane preparation
               time and improving its scalability. Beyond verification, we
               implement P4Update in a P4 software-switch-based environment.
               Measurements show that P4Update outperforms existing systems
               with respect to update speed by 28.6\% to 39.1\% in average.",
  publisher = "Association for Computing Machinery",
  pages     = "175--190",
  series    = "CoNEXT '21",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/Z/Zhou et al. 2021 - P4Update - fast and locally verifiable consistent network updates in the P4 data plane.pdf",
  address   = "New York, NY, USA",
  keywords  = "verification, consistent network updates, P4, loop
               freedom;ProgrammableNetworks;ProgrammableNetworks",
  location  = "Virtual Event, Germany",
  isbn      = "9781450390989",
  doi       = "10.1145/3485983.3494845"
}

@INPROCEEDINGS{Ayala-Romero2021-hj,
  title     = "{EdgeBOL}: automating energy-savings for mobile edge {AI}",
  booktitle = "Proceedings of the 17th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Ayala-Romero, Jose A and Garcia-Saavedra, Andres and
               Costa-Perez, Xavier and Iosifidis, George",
  abstract  = "Supporting Edge AI services is one of the most exciting features
               of future mobile networks. These services involve the collection
               and processing of voluminous data streams, right at the network
               edge, so as to offer real-time and accurate inferences to users.
               However, their widespread deployment is hampered by the energy
               cost they induce to the network. To overcome this obstacle, we
               propose a Bayesian learning framework for jointly configuring
               the service and the Radio Access Network (RAN), aiming to
               minimize the total energy consumption while respecting desirable
               accuracy and latency thresholds. Using a fully-fledged prototype
               with a software-defined base station (BS) and a GPU-enabled edge
               server, we profile a state-of-the-art video analytics AI service
               and identify new performance trade-offs. Accordingly, we tailor
               the optimization framework to account for the network context,
               the user needs, and the service metrics. The efficacy of our
               proposal is verified in a series of experiments and comparisons
               with neural network-based benchmarks.",
  publisher = "Association for Computing Machinery",
  pages     = "397--410",
  series    = "CoNEXT '21",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/A/Ayala-Romero et al. 2021 - EdgeBOL - automating energy-savings for mobile edge AI.pdf",
  address   = "New York, NY, USA",
  keywords  = "O-RAN, energy efficiency, QoS, mobile
               networks;EdgeFogCloudIoT;MLNetworking",
  location  = "Virtual Event, Germany",
  isbn      = "9781450390989",
  doi       = "10.1145/3485983.3494849"
}

@INPROCEEDINGS{Liu2021-lp,
  title     = "Floodgate: taming incast in datacenter networks",
  booktitle = "Proceedings of the 17th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Liu, Kexin and Tian, Chen and Wang, Qingyue and Zheng, Hao and
               Yu, Peiwen and Sun, Wenhao and Xu, Yonghui and Meng, Ke and Han,
               Lei and Fu, Jie and Dou, Wanchun and Chen, Guihai",
  abstract  = "Incast occurs frequently in datacenter networks where a large
               number of senders send data to a single receiver simultaneously,
               which makes the last hop the network bottleneck. Incast can hurt
               flows' performance. However, congestion control protocols are
               not effective at handling incast. One key insight is that it is
               too late to handle incast packets after they have already piled
               up at the last hop. Instead, we should avoid incast as early as
               possible. Inspired by flood control in Hydrologic Engineering,
               we propose Floodgate, a novel switch-based per-hop flow control
               to handle incast. Floodgate is compatible with existing
               congestion control protocols. We integrate it with practical
               congestion control approaches such as DCQCN, TIMELY, and HPCC.
               We evaluate Floodgate both in our implementations and
               large-scale simulations. Compared with state of the art,
               Floodgate reduces the buffer occupancy by a factor of 6.6x, as
               well as the queuing delay. Therefore, the average FCT and tail
               latency are greatly reduced.",
  publisher = "Association for Computing Machinery",
  pages     = "30--44",
  series    = "CoNEXT '21",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/L/Liu et al. 2021 - Floodgate - taming incast in datacenter networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "flow control, datacenter, incast;DataCenter",
  location  = "Virtual Event, Germany",
  isbn      = "9781450390989",
  doi       = "10.1145/3485983.3494854"
}

@INPROCEEDINGS{Park2021-lb,
  title     = "{GRAF}: a graph neural network based proactive resource
               allocation framework for {SLO-oriented} microservices",
  booktitle = "Proceedings of the 17th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Park, Jinwoo and Choi, Byungkwon and Lee, Chunghan and Han,
               Dongsu",
  abstract  = "Microservice is an architectural style that has been widely
               adopted in various latency-sensitive applications. Similar to
               the monolith, autoscaling has attracted the attention of
               operators for managing resource utilization of microservices.
               However, it is still challenging to optimize resources in terms
               of latency service-level-objective (SLO) without human
               intervention. In this paper, we present GRAF, a graph neural
               network-based proactive resource allocation framework for
               minimizing total CPU resources while satisfying latency SLO.
               GRAF leverages front-end workload, distributed tracing data, and
               machine learning approaches to (a) observe/estimate impact of
               traffic change (b) find optimal resource combinations (c) make
               proactive resource allocation. Experiments using various
               open-source benchmarks demonstrate that GRAF successfully
               targets latency SLO while saving up to 19\% of total CPU
               resources compared to the fine-tuned autoscaler. Moreover, GRAF
               handles traffic surge with 36\% fewer resources while achieving
               up to 2.6x faster tail latency convergence compared to the
               Kubernetes autoscaler.",
  publisher = "Association for Computing Machinery",
  pages     = "154--167",
  series    = "CoNEXT '21",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/P/Park et al. 2021 - GRAF - a graph neural network based proactive resource allocation framework for SLO-oriented microservices.pdf",
  address   = "New York, NY, USA",
  keywords  = "autoscaler, resources optimization, applied machine learning,
               microservices, graph neural networks, cloud
               computing;NFV;NFV\_Microservices",
  location  = "Virtual Event, Germany",
  isbn      = "9781450390989",
  doi       = "10.1145/3485983.3494866"
}

@INPROCEEDINGS{Schmidt2021-ha,
  title     = "{FlexRIC}: an {SDK} for next-generation {SD-RANs}",
  booktitle = "Proceedings of the 17th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Schmidt, Robert and Irazabal, Mikel and Nikaein, Navid",
  abstract  = "Unlike previous mobile networks, 5G New Radio (5G-NR) provides
               unprecedented flexibility in the radio access network (RAN) to
               support diverse use cases in a multi-tenant environment. In this
               context, the need for programmability and control through
               software-defined radio access networking (SD-RAN) is well
               established. While the underlying RAN is designed to be ultra
               flexible and lean, existing SD-RAN controllers are either not
               flexible to address all use cases or use a one-size-fits-all
               approach.In this paper, we present FlexRIC, a flexible and
               efficient software development kit (SDK) that enables to build
               specialized service-oriented controllers. FlexRIC has a modular
               architecture with minimal footprint and is designed with
               extensibility in mind.We validate the SDK building concrete
               implementations of two specialized controllers for
               state-of-the-art 5G use cases: (1) a recursive RAN controller
               that virtualizes the network to allow multiple tenants to
               concurrently control and operate their services in a shared
               infrastructure over the heterogeneous landscape of 5G networks,
               and (2) an SD-RAN controller providing programmability for
               multi-radio access technology (RAT) RAN slicing, and flow-based
               traffic control targeting low-latency communications. The
               results reveal that FlexRIC reduces the round-trip time by two
               while incurring 83 \% less CPU compared with O-RAN's reference
               implementation, and uses 10x less CPU and one third of the
               memory when compared to FlexRAN. Such performance is required to
               unleash the potential of emerging 5G use cases.",
  publisher = "Association for Computing Machinery",
  pages     = "411--425",
  series    = "CoNEXT '21",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/S/Schmidt et al. 2021 - FlexRIC - an SDK for next-generation SD-RANs.pdf",
  address   = "New York, NY, USA",
  keywords  = "5G, SDK, slicing, RIC, SD-RAN,
               virtualization;ProgrammableNetworks;ProgrammableNetworks",
  location  = "Virtual Event, Germany",
  isbn      = "9781450390989",
  doi       = "10.1145/3485983.3494870"
}

@INPROCEEDINGS{Abdous2021-ml,
  title     = "Burst-tolerant datacenter networks with Vertigo",
  booktitle = "Proceedings of the 17th International Conference on emerging
               Networking {EXperiments} and Technologies",
  author    = "Abdous, Sepehr and Sharafzadeh, Erfan and Ghorbani, Soudeh",
  abstract  = "Microsecond-scale congestion events, known as microbursts, are a
               main cause of packet loss and poor application performance in
               today's datacenters. Given the low network utilization in
               datacenters, one would expect packet deflection, in-situ
               re-routing of packets that arrive at a full buffer to a
               different port, to effectively prevent packet loss. However, if
               deployed naively, deflection leads to excessive packet
               re-ordering, exacerbated congestion, and head-of-the-line
               blocking in switch buffers. In this study, we resolve the above
               challenges by selectively deflecting the packets that cause
               persistent congestion in the network. To enable this, we augment
               the end-host network stacks with a transport-independent
               extension that tracks and marks flows with their remaining
               bytes. Our in-network deflection component uses the flow size
               information to re-route packets from flows with more data to
               send. Finally, an extension to the receive-side of end-host
               stacks retrieves the correct ordering of packets before passing
               them to transport and higherlevel protocols. We evaluate our
               design, Vertigo, under diverse datacenter workloads and show
               that it is effective in managing microbursts under light and
               heavy loads and when combined with various congestion control
               algorithms. For example, in a leaf-spine network under 85\%
               load, Vertigo reduces the mean incast query completion times by
               3.5x, 3.3x, 5x compared to ECMP, DRILL, and DIBS when using TCP,
               3x, 3.5x, 4.5x alongside DCTCP, and 43x, 33x, 16x when using
               Swift, respectively.",
  publisher = "Association for Computing Machinery",
  pages     = "1--15",
  series    = "CoNEXT '21",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/A/Abdous et al. 2021 - Burst-tolerant datacenter networks with Vertigo.pdf",
  address   = "New York, NY, USA",
  keywords  = "packet deflection, datacenter networks, microbursts;DataCenter",
  location  = "Virtual Event, Germany",
  isbn      = "9781450390989",
  doi       = "10.1145/3485983.3494873"
}

@ARTICLE{Adeleke2022-lz,
  title     = "Network Traffic Generation: A Survey and Methodology",
  author    = "Adeleke, Oluwamayowa Ade and Bastin, Nicholas and Gurkan, Deniz",
  abstract  = "Network traffic workloads are widely utilized in applied
               research to verify correctness and to measure the impact of
               novel algorithms, protocols, and network functions. We provide a
               comprehensive survey of traffic generators referenced by
               researchers over the last 13 years, providing in-depth
               classification of the functional behaviors of the most
               frequently cited generators. These classifications are then used
               as a critical component of a methodology presented to aid in the
               selection of generators derived from the workload requirements
               of future research.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  55,
  number    =  2,
  pages     = "1--23",
  month     =  jan,
  year      =  2022,
  file      = "All Papers/A/Adeleke et al. 2022 - Network Traffic Generation - A Survey and Methodology.pdf",
  address   = "New York, NY, USA",
  keywords  = "generator, Network, analysis, experiment, workload, survey,
               packet, traffic;NetworkTraffic",
  issn      = "0360-0300",
  doi       = "10.1145/3488375"
}

@INPROCEEDINGS{Burlachenko2021-bs,
  title     = "{FL\_PyTorch}: optimization research simulator for federated
               learning",
  booktitle = "Proceedings of the 2nd {ACM} International Workshop on
               Distributed Machine Learning",
  author    = "Burlachenko, Konstantin and Horv{\'a}th, Samuel and
               Richt{\'a}rik, Peter",
  abstract  = "Federated Learning (FL) has emerged as a promising technique for
               edge devices to collaboratively learn a shared machine learning
               model while keeping training data locally on the device, thereby
               removing the need to store and access the full data in the
               cloud. However, FL is difficult to implement, test and deploy in
               practice considering heterogeneity in common edge device
               settings, making it fundamentally hard for researchers to
               efficiently prototype and test their optimization algorithms. In
               this work, our aim is to alleviate this problem by introducing
               FL\_PyTorch : a suite of open-source software written in python
               that builds on top of one the most popular research Deep
               Learning (DL) framework PyTorch. We built FL\_PyTorch as a
               research simulator for FL to enable fast development,
               prototyping and experimenting with new and existing FL
               optimization algorithms. Our system supports abstractions that
               provide researchers with a sufficient level of flexibility to
               experiment with existing and novel approaches to advance the
               state-of-the-art. Furthermore, FL\_PyTorch is a simple to use
               console system, allows to run several clients simultaneously
               using local CPUs or GPU(s), and even remote compute devices
               without the need for any distributed implementation provided by
               the user. FL\_PyTorch also offers a Graphical User Interface.
               For new methods, researchers only provide the centralized
               implementation of their algorithm. To showcase the possibilities
               and usefulness of our system, we experiment with several
               well-known state-of-the-art FL algorithms and a few of the most
               common FL datasets.",
  publisher = "Association for Computing Machinery",
  pages     = "1--7",
  series    = "DistributedML '21",
  month     =  dec,
  year      =  2021,
  file      = "All Papers/B/Burlachenko et al. 2021 - FL_PyTorch - optimization research simulator for federated learning.pdf",
  address   = "New York, NY, USA",
  keywords  = "simulator, optimization, federated
               learning;MLAspects;MLNetworking",
  location  = "Virtual Event, Germany",
  isbn      = "9781450391344",
  doi       = "10.1145/3488659.3493775"
}

@INPROCEEDINGS{Arun2022-lv,
  title     = "Starvation in end-to-end congestion control",
  booktitle = "Proceedings of the {ACM} {SIGCOMM} 2022 Conference",
  author    = "Arun, Venkat and Alizadeh, Mohammad and Balakrishnan, Hari",
  abstract  = "To overcome weaknesses in traditional loss-based congestion
               control algorithms (CCAs), researchers have developed and
               deployed several delay-bounding CCAs that achieve high
               utilization without bloating delays (e.g., Vegas, FAST, BBR,
               PCC, Copa, etc.). When run on a path with a fixed bottleneck
               rate, these CCAs converge to a small delay range in equilibrium.
               This paper proves a surprising result: although designed to
               achieve reasonable inter-flow fairness, current methods to
               develop delay-bounding CCAs cannot always avoid starvation, an
               extreme form of unfairness. Starvation may occur when such a CCA
               runs on paths where non-congestive network delay variations due
               to real-world factors such as ACK aggregation and end-host
               scheduling exceed double the delay range that the CCA converges
               to in equilibrium. We provide experimental evidence for this
               result for BBR, PCC Vivace, and Copa with a link emulator. We
               discuss the implications of this result and posit that to
               guarantee no starvation an efficient delay-bounding CCA should
               design for a certain amount of non-congestive jitter and ensure
               that its equilibrium delay oscillations are at least one-half of
               this jitter.",
  publisher = "Association for Computing Machinery",
  pages     = "177--192",
  series    = "SIGCOMM '22",
  month     =  aug,
  year      =  2022,
  file      = "All Papers/A/Arun et al. 2022 - Starvation in end-to-end congestion control.pdf",
  address   = "New York, NY, USA",
  keywords  = "delay-convergence, congestion control, starvation",
  location  = "Amsterdam, Netherlands",
  isbn      = "9781450394208",
  doi       = "10.1145/3544216.3544223"
}

@INPROCEEDINGS{Zhao2022-sw,
  title     = "Multi-resource interleaving for deep learning training",
  booktitle = "Proceedings of the {ACM} {SIGCOMM} 2022 Conference",
  author    = "Zhao, Yihao and Liu, Yuanqiang and Peng, Yanghua and Zhu, Yibo
               and Liu, Xuanzhe and Jin, Xin",
  abstract  = "Training Deep Learning (DL) model requires multiple resource
               types, including CPUs, GPUs, storage IO, and network IO.
               Advancements in DL have produced a wide spectrum of models that
               have diverse usage patterns on different resource types.
               Existing DL schedulers focus on only GPU allocation, while
               missing the opportunity of packing jobs along multiple resource
               types.We present Muri, a multi-resource cluster scheduler for DL
               workloads. Muri exploits multi-resource interleaving of DL
               training jobs to achieve high resource utilization and reduce
               job completion time (JCT). DL jobs have a unique staged,
               iterative computation pattern. In contrast to multi-resource
               schedulers for big data workloads that pack jobs in the space
               dimension, Muri leverages this unique pattern to interleave jobs
               on the same set of resources in the time dimension. Muri adapts
               Blossom algorithm to find the perfect grouping plan for
               single-GPU jobs with two resource types, and generalizes the
               algorithm to handle multi-GPU jobs with more than two types. We
               build a prototype of Muri and integrate it with PyTorch.
               Experiments on a cluster with 64 GPUs demonstrate that Muri
               improves the average JCT by up to 3.6$\times$ and the makespan
               by up to 1.6$\times$ over existing DL schedulers.",
  publisher = "Association for Computing Machinery",
  pages     = "428--440",
  series    = "SIGCOMM '22",
  month     =  aug,
  year      =  2022,
  file      = "All Papers/Z/Zhao et al. 2022 - Multi-resource interleaving for deep learning training.pdf",
  address   = "New York, NY, USA",
  keywords  = "resource sharing, deep learning",
  location  = "Amsterdam, Netherlands",
  isbn      = "9781450394208",
  doi       = "10.1145/3544216.3544224"
}

@INPROCEEDINGS{Uyeda2022-ik,
  title     = "{SDN} in the stratosphere: loon's aerospace mesh network",
  booktitle = "Proceedings of the {ACM} {SIGCOMM} 2022 Conference",
  author    = "Uyeda, Frank and Alvidrez, Marc and Kline, Erik and Petrini,
               Bryce and Barritt, Brian and Mandle, David and Alexander, Aswin
               Chandy",
  abstract  = "The Loon project provided 4G LTE connectivity to under-served
               regions in emergency response and commercial mobile contexts
               using base stations carried by high-altitude balloons. To
               backhaul data, Loon orchestrated a moving mesh network of
               point-to-point radio links that interconnected balloons with
               each other and to ground infrastructure. This paper presents
               insights from 3 years of operational experience with Loon's mesh
               network above 3 continents.The challenging environment,
               comparable to many emerging non-terrestrial networks (NTNs),
               highlighted the design continuum between predictive optimization
               and reactive recovery. By forecasting the physical environment
               as a part of network planning, our novel Temporospatial SDN
               (TS-SDN) successfully moved from reactive to predictive recovery
               in many cases. We present insights on the following NTN
               concerns: connecting meshes of moving nodes using long distance,
               directional point-to-point links; employing a hybrid network
               control plane to balance performance and reliability; and
               understanding the behavior of a complex system spanning physical
               and logical domains in an inaccessible environment. The paper
               validates TS-SDN as a compelling architecture for orchestrating
               networks of moving platforms and steerable beams, and provides
               insights for those building similar networks in the future.",
  publisher = "Association for Computing Machinery",
  pages     = "264--280",
  series    = "SIGCOMM '22",
  month     =  aug,
  year      =  2022,
  file      = "All Papers/U/Uyeda et al. 2022 - SDN in the stratosphere - loon's aerospace mesh network.pdf",
  address   = "New York, NY, USA",
  keywords  = "project loon, minkowski, TS-SDN, stratosphere, balloon,
               non-terrestrial network, temporospatial sdn",
  location  = "Amsterdam, Netherlands",
  isbn      = "9781450394208",
  doi       = "10.1145/3544216.3544231"
}

@INPROCEEDINGS{Markovitch2022-dx,
  title     = "{TIPSY}: predicting where traffic will ingress a {WAN}",
  booktitle = "Proceedings of the {ACM} {SIGCOMM} 2022 Conference",
  author    = "Markovitch, Michael and Agarwal, Sharad and Fonseca, Rodrigo and
               Beckett, Ryan and Zhang, Chuanji and Atov, Irena and
               Chaturmohta, Somesh",
  abstract  = "In addition to consumer workloads, public cloud providers host
               enterprise workloads such as video conferencing and AI+ML
               pipelines. Enterprise workloads can, at times, overwhelm the
               available ingress capacity on individual peering links.
               Traditional techniques to address this problem in the consumer
               setting do not always apply here, such as use of CDN caches in
               eyeball networks.Ingress congestion events necessitate shifting
               traffic to other peering links at short timescales. While
               content providers use such techniques in the egress direction,
               ingress is inherently a different and more challenging problem.
               Once a packet leaves an enterprise network, it is subject to
               opaque routing policies that influence the path to the cloud
               provider.We present TIPSY, a statistical-classification-based
               system for predicting the peering link through which a flow will
               enter a WAN. TIPSY's predictions are used to safely operate a
               congestion mitigation system that injects BGP withdrawal
               messages to redirect traffic away from congested peering links.
               We train TIPSY on traffic data from the Azure WAN, and we
               demonstrate 76\% accuracy in predicting through which 3 peering
               links (out of thousands) a flow will enter the network after BGP
               withdrawals.",
  publisher = "Association for Computing Machinery",
  pages     = "233--249",
  series    = "SIGCOMM '22",
  month     =  aug,
  year      =  2022,
  file      = "All Papers/M/Markovitch et al. 2022 - TIPSY - predicting where traffic will ingress a WAN.pdf",
  address   = "New York, NY, USA",
  keywords  = "BGP, WAN, statistical classification, peering",
  location  = "Amsterdam, Netherlands",
  isbn      = "9781450394208",
  doi       = "10.1145/3544216.3544234"
}

@INPROCEEDINGS{Yang2022-hd,
  title     = "{DeepQueueNet}: towards scalable and generalized network
               performance estimation with packet-level visibility",
  booktitle = "Proceedings of the {ACM} {SIGCOMM} 2022 Conference",
  author    = "Yang, Qingqing and Peng, Xi and Chen, Li and Liu, Libin and
               Zhang, Jingze and Xu, Hong and Li, Baochun and Zhang, Gong",
  abstract  = "Network simulators are an essential tool for network operators,
               and can assist important tasks such as capacity planning,
               topology design, and parameter tuning. Popular simulators are
               all based on discrete event simulation, and their performance
               does not scale with the size of modern networks. Recently,
               deep-learning-based techniques are introduced to solve the
               scalability problem, but, as we show with experiments, they have
               poor visibility in their simulation results, and cannot
               generalize to diverse scenarios. In this work, we combine
               scalable and generalized continuous simulation techniques with
               discrete event simulation to achieve high scalability, while
               providing packet-level visibility. We start from a solid
               queueing-theoretic modeling of modern networks, and carefully
               identify the mathematically-intractable or
               computationally-expensive parts, only which are then modeled
               using deep neural networks (DNN). Dubbed DeepQueueNet, our
               approach combines prior knowledge of networks, and supports
               arbitrary topology and device traffic management mechanisms
               (given sufficient training data). Our extensive experiments show
               that DeepQueueNet achieves near-linear speedup in the number of
               GPUs, and its estimation accuracy for average and 99th
               percentile round-trip time outperforms existing end-to-end
               DNN-based performance estimators in all scenarios.",
  publisher = "Association for Computing Machinery",
  pages     = "441--457",
  series    = "SIGCOMM '22",
  month     =  aug,
  year      =  2022,
  file      = "All Papers/Y/Yang et al. 2022 - DeepQueueNet - towards scalable and generalized network performance estimation with packet-level visibility.pdf",
  address   = "New York, NY, USA",
  keywords  = "network modeling, queueing theory, machine learning, network
               simulation, network performance estimation;ML4Net",
  location  = "Amsterdam, Netherlands",
  isbn      = "9781450394208",
  doi       = "10.1145/3544216.3544248"
}

@INPROCEEDINGS{Addanki2022-yd,
  title     = "{ABM}: active buffer management in datacenters",
  booktitle = "Proceedings of the {ACM} {SIGCOMM} 2022 Conference",
  author    = "Addanki, Vamsi and Apostolaki, Maria and Ghobadi, Manya and
               Schmid, Stefan and Vanbever, Laurent",
  abstract  = "Today's network devices share buffer across queues to avoid
               drops during transient congestion and absorb bursts. As the
               buffer-per-bandwidth-unit in datacenter decreases, the need for
               optimal buffer utilization becomes more pressing. Typical
               devices use a hierarchical packet admission control scheme:
               First, a Buffer Management (BM) scheme decides the maximum
               length per queue at the device level and then an Active Queue
               Management (AQM) scheme decides which packets will be admitted
               at the queue level. Unfortunately, the lack of cooperation
               between the two control schemes leads to (i) harmful
               interference across queues, due to the lack of isolation; (ii)
               increased queueing delay, due to the obliviousness to the
               per-queue drain time; and (iii) thus unpredictable burst
               tolerance. To overcome these limitations, we propose ABM, Active
               Buffer Management which incorporates insights from both BM and
               AQM. Concretely, ABM accounts for both total buffer occupancy
               (typically used by BM) and queue drain time (typically used by
               AQM). We analytically prove that ABM provides isolation, bounded
               buffer drain time and achieves predictable burst tolerance
               without sacrificing throughput. We empirically find that ABM
               improves the 99th percentile FCT for short flows by up to 94\%
               compared to the state-of-the-art buffer management. We further
               show that ABM improves the performance of advanced datacenter
               transport protocols in terms of FCT by up to 76\% compared to
               DCTCP, TIMELY and PowerTCP under bursty workloads even at
               moderate load conditions.",
  publisher = "Association for Computing Machinery",
  pages     = "36--52",
  series    = "SIGCOMM '22",
  month     =  aug,
  year      =  2022,
  file      = "All Papers/A/Addanki et al. 2022 - ABM - active buffer management in datacenters.pdf",
  address   = "New York, NY, USA",
  keywords  = "shared buffer, buffer management, queue management, datacenter",
  location  = "Amsterdam, Netherlands",
  isbn      = "9781450394208",
  doi       = "10.1145/3544216.3544252"
}

@INPROCEEDINGS{Shrivastav2022-so,
  title     = "Programmable multi-dimensional table filters for line rate
               network functions",
  booktitle = "Proceedings of the {ACM} {SIGCOMM} 2022 Conference",
  author    = "Shrivastav, Vishal",
  abstract  = "The ability to filter entries in the data plane from a set or
               table of resources (e.g., network paths, servers, switch ports)
               based on multi-dimensional policies over stateful
               resource-specific metrics (e.g., filter paths with utilization <
               0.6 and latency < 3us) is critical for several key network
               functions, such as performance-aware routing, resource-aware
               load balancing, network diagnosis, security and firewall.
               However, current generation of programmable switches do not
               support table-wide stateful filtering at line rate. We present
               Thanos, which augments the existing programmable switch pipeline
               with support for programmable multi-dimensional filtering over a
               set of resources. Thanos seamlessly integrates with
               multi-terabit programmable switch pipelines at nominal chip area
               overhead. Our evaluation, based on an FPGA prototype and a
               simulator, shows that policies expressed in Thanos can improve
               the performance of key network functions by up to 1.7$\times$
               compared to state-of-the-art.",
  publisher = "Association for Computing Machinery",
  pages     = "649--662",
  series    = "SIGCOMM '22",
  month     =  aug,
  year      =  2022,
  file      = "All Papers/S/Shrivastav 2022 - Programmable multi-dimensional table filters for line rate network functions.pdf",
  address   = "New York, NY, USA",
  keywords  = "switch architecture, programmable networks",
  location  = "Amsterdam, Netherlands",
  isbn      = "9781450394208",
  doi       = "10.1145/3544216.3544266"
}

@ARTICLE{Mccauley2023-rp,
  title     = "Extracting the Essential Simplicity of the Internet",
  author    = "Mccauley, James and Shenker, Scott and Varghese, George",
  abstract  = "Looking past inessential complexities to explain the Internet's
               simple yet daring design.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  66,
  number    =  2,
  pages     = "64--74",
  month     =  jan,
  year      =  2023,
  file      = "All Papers/M/Mccauley et al. 2023 - Extracting the Essential Simplicity of the Internet.pdf",
  address   = "New York, NY, USA",
  keywords  = "ComputerNetworks",
  issn      = "0001-0782",
  doi       = "10.1145/3547137"
}

@ARTICLE{Popek1974-ig,
  title     = "Formal requirements for virtualizable third generation
               architectures",
  author    = "Popek, Gerald J and Goldberg, Robert P",
  abstract  = "Virtual machine systems have been implemented on a limited
               number of third generation computer systems, e.g. CP-67 on the
               IBM 360/67. From previous empirical studies, it is known that
               certain third generation computer systems, e.g. the DEC PDP-10,
               cannot support a virtual machine system. In this paper, model of
               a third-generation-like computer system is developed. Formal
               techniques are used to derive precise sufficient conditions to
               test whether such an architecture can support virtual machines.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  17,
  number    =  7,
  pages     = "412--421",
  month     =  jul,
  year      =  1974,
  file      = "All Papers/P/Popek and Goldberg 1974 - Formal requirements for virtualizable third generation architectures.pdf",
  address   = "New York, NY, USA",
  keywords  = "proof, formal requirements, virtual memory, hypervisor, abstract
               model, virtual machine monitor, virtual machine, operating
               system, sensitive instruction, third generation architecture;GDS",
  issn      = "0001-0782",
  doi       = "10.1145/361011.361073"
}

@ARTICLE{Dijkstra1968-tm,
  title     = "Letters to the editor: go to statement considered harmful",
  author    = "Dijkstra, Edsger W",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  11,
  number    =  3,
  pages     = "147--148",
  month     =  mar,
  year      =  1968,
  file      = "All Papers/D/Dijkstra 1968 - Letters to the editor - go to statement considered harmful.pdf",
  address   = "New York, NY, USA",
  keywords  = "repetitive clause, jump instruction, program sequencing, go to
               statement, branch instruction, program intelligibility,
               alternative clause, conditional clause;GDS",
  issn      = "0001-0782",
  doi       = "10.1145/362929.362947"
}

@INPROCEEDINGS{Thorup2001-vn,
  title     = "Compact routing schemes",
  booktitle = "Proceedings of the thirteenth annual {ACM} symposium on Parallel
               algorithms and architectures",
  author    = "Thorup, Mikkel and Zwick, Uri",
  abstract  = "We describe several compact routing schemes for general weighted
               undirected networks. Our schemes are simple and easy to
               implement. The routing tables stored at the nodes of the network
               are all very small. The headers attached to the routed messages,
               including the name of the destination, are extremely short. The
               routing decision at each node takes constant time. Yet, the
               stretch of these routing schemes, i.e., the worst ratio between
               the cost of the path on which a packet is routed and the cost of
               the cheapest path from source to destination, is a small
               constant. Our schemes achieve a near-optimal tradeoff between
               the size of the routing tables used and the resulting stretch.
               More specifically, we obtain: A routing scheme that uses only O
               (n 1/2) bits of memory at each node of an n-node network that
               has stretch 3. The space is optimal, up to logarithmic factors,
               in the sense that every routing scheme with stretch n-bits long
               and each routing decision takes constant time. A variant of this
               scheme with [log2 n] -bit headers makes routing decisions in
               O(log log n) time. Also, for every integer k > 2, a general
               handshaking based routing scheme that uses O (n1/k) bits of
               memory at each node that has stretch 2k - 1. A conjecture of
               Erd{\"o}s from 1963, settled for k = 3, 5, implies that the
               routing tables are of near-optimal size relative to the stretch.
               The handshaking is similar in spirit to a DNS lookup in TCP/IP.
               Headers are O(log2 n) bits long and each routing decision takes
               constant time. Without handshaking, the stretch of the scheme
               increases to 4k - 5. One ingredient used to obtain the routing
               schemes mentioned above, may be of independent practical and
               theoretical interest: A shortest path routing scheme for trees
               of arbitrary degree and diameter that assigns each vertex of an
               n-node tree a (1 + O(1)) log2 n-bit label. Given the label of a
               source node and the label of a destination it is possible to
               compute, in constant time, the port number of the edge from the
               source that heads in the direction of the destination. The
               general scheme for k > 2 also uses a clustering technique
               introduced recently by the authors. The clusters obtained using
               this technique induce a sparse and low stretch tree cover of the
               network. This essentially reduces routing in general networks
               into routing problems in trees that could be solved using the
               above technique.",
  publisher = "Association for Computing Machinery",
  pages     = "1--10",
  series    = "SPAA '01",
  month     =  jul,
  year      =  2001,
  file      = "All Papers/T/Thorup and Zwick 2001 - Compact routing schemes.pdf",
  address   = "New York, NY, USA",
  keywords  = "ComputerNetworks",
  location  = "Crete Island, Greece",
  isbn      = "9781581134094",
  doi       = "10.1145/378580.378581"
}

@ARTICLE{Bettstetter2001-bh,
  title     = "Mobility modeling in wireless networks: categorization, smooth
               movement, and border effects",
  author    = "Bettstetter, Christian",
  abstract  = "The movement pattern of mobile users plays an important role in
               performance analysis of wireless computer and communication
               networks. In this paper, we first give an overview and
               classification of mobility models used for simulation-based
               studies. Then, we present an enhanced random mobility model,
               which makes the movement trace of mobile stations more realistic
               than common approaches for random mobility. Our movement concept
               is based on random processes for speed and direction control in
               which the new values are correlated to previous ones. Upon a
               speed change event, a new target speed is chosen, and an
               acceleration is set to achieve this target speed. The principles
               for direction changes are similar. Finally, we discuss
               strategies for the stations' border behavior (i.e., what happens
               when nodes move out of the simulation area) and show the effects
               of certain border behaviors and mobility models on the spatial
               user distribution.",
  journal   = "SIGMOBILE Mob. Comput. Commun. Rev.",
  publisher = "Association for Computing Machinery",
  volume    =  5,
  number    =  3,
  pages     = "55--66",
  month     =  jul,
  year      =  2001,
  file      = "All Papers/B/Bettstetter 2001 - Mobility modeling in wireless networks - categorization, smooth movement, and border effects.pdf",
  address   = "New York, NY, USA",
  keywords  = "Mobility",
  issn      = "1559-1662",
  doi       = "10.1145/584051.584056"
}

@INPROCEEDINGS{Reijers2003-mm,
  title     = "Efficient code distribution in wireless sensor networks",
  booktitle = "Proceedings of the 2nd {ACM} international conference on
               Wireless sensor networks and applications",
  author    = "Reijers, Niels and Langendoen, Koen",
  abstract  = "The need to reprogramme a wireless sensor network may arise from
               changing application requirements, bug fixes, or during the
               application development cycle. Once deployed, it will be
               impractical at best to reach each individual node. Thus, a
               scheme is required to wirelessly reprogramme the nodes. We
               present an energy-efficient code distribution scheme to
               wirelessly update the code running in a sensor network. Energy
               is saved by distributing only the changes to the currently
               running code. The new code image is built using an edit script
               of commands that are easy to process by the nodes. A small
               change to the programme code can cause many changes to the
               binary code because the addresses of functions and data change.
               A naive approach to building the edit script string would result
               in a large script. We describe a number of optimisations and
               present experimental results showing that these significantly
               reduce the edit script size.",
  publisher = "Association for Computing Machinery",
  pages     = "60--67",
  series    = "WSNA '03",
  month     =  sep,
  year      =  2003,
  file      = "All Papers/R/Reijers and Langendoen 2003 - Efficient code distribution in wireless sensor networks.pdf",
  address   = "New York, NY, USA",
  keywords  = "wireless, reprogramming, sensor networks, compression, code
               distribution, string distance",
  location  = "San Diego, CA, USA",
  isbn      = "9781581137644",
  doi       = "10.1145/941350.941359"
}

@ARTICLE{Boutaba2018-sq,
  title    = "A comprehensive survey on machine learning for networking:
              evolution, applications and research opportunities",
  author   = "Boutaba, Raouf and Salahuddin, Mohammad A and Limam, Noura and
              Ayoubi, Sara and Shahriar, Nashid and Estrada-Solano, Felipe and
              Caicedo, Oscar M",
  abstract = "Machine Learning (ML) has been enjoying an unprecedented surge in
              applications that solve problems and enable automation in diverse
              domains. Primarily, this is due to the explosion in the
              availability of data, significant improvements in ML techniques,
              and advancement in computing capabilities. Undoubtedly, ML has
              been applied to various mundane and complex problems arising in
              network operation and management. There are various surveys on ML
              for specific areas in networking or for specific network
              technologies. This survey is original, since it jointly presents
              the application of diverse ML techniques in various key areas of
              networking across different network technologies. In this way,
              readers will benefit from a comprehensive discussion on the
              different learning paradigms and ML techniques applied to
              fundamental problems in networking, including traffic prediction,
              routing and classification, congestion control, resource and
              fault management, QoS and QoE management, and network security.
              Furthermore, this survey delineates the limitations, give
              insights, research challenges and future opportunities to advance
              ML in networking. Therefore, this is a timely contribution of the
              implications of ML for networking, that is pushing the barriers
              of autonomic network operation and management.",
  journal  = "Journal of Internet Services and Applications",
  volume   =  9,
  number   =  1,
  pages    = "16",
  month    =  jun,
  year     =  2018,
  file     = "All Papers/B/Boutaba et al. 2018 - A comprehensive survey on machine learning for networking - evolution, applications and research opportunities.pdf",
  keywords = "Good;ML;MLNetworking",
  issn     = "1869-0238",
  doi      = "10.1186/s13174-018-0087-2"
}

@ARTICLE{Cote2018-nj,
  title     = "Using machine learning in communication networks [Invited]",
  author    = "Cote, David",
  abstract  = "In this paper, we first review how the main machine learning
               concepts can apply to communication networks. Then we present
               results from a concrete application using unsupervised machine
               learning in a real network. We show how the application can
               detect anomalies at multiple network layers, including the
               optical layer, how it can be trained to anticipate anomalies
               before they become a problem, and how it can be trained to
               identify the root cause of each anomaly. Finally, we elaborate
               on the importance of this work and speculate about the future of
               intelligent adaptive networks.",
  journal   = "IEEE/OSA J. Opt. Commun. Networking",
  publisher = "IEEE",
  volume    =  10,
  number    =  10,
  pages     = "D100--D109",
  month     =  oct,
  year      =  2018,
  file      = "All Papers/C/Cote 2018 - Using machine learning in communication networks [Invited].pdf",
  keywords  = "MLAspects",
  issn      = "1943-0620, 1943-0639",
  doi       = "10.1364/JOCN.10.00D100"
}

@ARTICLE{Zhang:Ten_Simple_Rules_Writing_Research_Papers:2014,
  title    = "Ten simple rules for writing research papers",
  author   = "Zhang, Weixiong",
  abstract = "The importance of writing well can never be overstated for a
              successful professional career, and the ability to write solid
              papers is an essential trait of a productive researcher. Writing
              and publishing a paper has its own life cycle; properly following
              a course of action and avoiding missteps can be vital to the
              overall success not only of a paper but of the underlying
              research as well. Here, we offer ten simple rules for writing and
              publishing research papers.",
  journal  = "PLoS Comput. Biol.",
  volume   =  10,
  number   =  1,
  pages    = "e1003453",
  month    =  jan,
  year     =  2014,
  file     = "All Papers/Z/Zhang 2014 - Ten simple rules for writing research papers.pdf",
  keywords = "Important;Methods",
  language = "en",
  issn     = "1553-734X, 1553-7358",
  pmid     = "24499936",
  doi      = "10.1371/journal.pcbi.1003453",
  pmc      = "PMC3907284"
}

@ARTICLE{Strubell2020-zt,
  title    = "Energy and Policy Considerations for Modern Deep Learning
              Research",
  author   = "Strubell, Emma and Ganesh, Ananya and McCallum, Andrew",
  abstract = "The field of artificial intelligence has experienced a dramatic
              methodological shift towards large neural networks trained on
              plentiful data. This shift has been fueled by recent advances in
              hardware and techniques enabling remarkable levels of
              computation, resulting in impressive advances in AI across many
              applications. However, the massive computation required to obtain
              these exciting results is costly both financially, due to the
              price of specialized hardware and electricity or cloud compute
              time, and to the environment, as a result of non-renewable energy
              used to fuel modern tensor processing hardware. In a paper
              published this year at ACL, we brought this issue to the
              attention of NLP researchers by quantifying the approximate
              financial and environmental costs of training and tuning neural
              network models for NLP (Strubell, Ganesh, and McCallum 2019). In
              this extended abstract, we briefly summarize our findings in NLP,
              incorporating updated estimates and broader information from
              recent related publications, and provide actionable
              recommendations to reduce costs and improve equity in the machine
              learning and artificial intelligence community.",
  journal  = "AAAI",
  volume   =  34,
  number   =  09,
  pages    = "13693--13696",
  month    =  apr,
  year     =  2020,
  file     = "All Papers/S/Strubell et al. 2020 - Energy and Policy Considerations for Modern Deep Learning Research.pdf",
  language = "en",
  issn     = "2374-3468, 2374-3468",
  doi      = "10.1609/aaai.v34i09.7123"
}

@ARTICLE{Sterz2020-oe,
  title     = "{ReactiFi}: Reactive programming of {WI-fi} firmware on mobile
               devices",
  author    = "Sterz, Artur and Eichholz, Matthias and Mogk, Ragnar and
               Baumg{\"a}rtner, Lars and Graubner, Pablo and Hollick, Matthias
               and Mezini, Mira and Freisleben, Bernd",
  journal   = "Art Sci. Eng. Program.",
  publisher = "Aspect-Oriented Software Association (AOSA)",
  volume    =  5,
  number    =  2,
  month     =  oct,
  year      =  2020,
  file      = "All Papers/S/Sterz et al. 2020 - ReactiFi - Reactive programming of WI-fi firmware on mobile devices.pdf",
  language  = "en",
  issn      = "2473-7321",
  doi       = "10.22152/programming-journal.org/2021/5/4"
}

@INPROCEEDINGS{Yao2017-hb,
  title     = "Deadline-aware and energy-efficient dynamic flow scheduling in
               data center network",
  booktitle = "2017 13th International Conference on Network and Service
               Management ({CNSM})",
  author    = "Yao, Z and Wang, Y and Ba, J and Zong, J and Feng, S and Wu, Z",
  abstract  = "The construction of energy-efficient network and achievement of
               green communication have garnered great attention as a promising
               a way to reduce network operating costs and C emissions.
               Moreover, recently the deadline-aware and energy-efficient
               routing and scheduling algorithms in data center network have
               been attracting a broad attention. However, the dynamic
               scheduling for flows has not been explicitly studied by the
               existing research. In this paper, we investigated the dynamic
               flow scheduling in data center network, and propose a
               deadline-aware and energy-efficient dynamic flow scheduling
               (DEDFS) algorithm, assuming the path of the flow could be
               calculated in advance and pre-stored. In addition, the number of
               mouse flows in data center network accounts for main proportion,
               but consumption is very small. In order to achieve the balance
               of energy-saved and efficiency, mouse flows will be directly
               transferred, while elephant flows will be scheduled by the
               Most-Critical-First static strategy based dynamic scheduling
               algorithm. It selects the interval of largest energy consumption
               density as the critical interval, and all of the flows in this
               critical interval will be preferentially scheduled. Finally, the
               feasibility and validity of the algorithm are verified by
               simulation.",
  pages     = "1--4",
  month     =  nov,
  year      =  2017,
  file      = "All Papers/Y/Yao et al. 2017 - Deadline-aware and energy-efficient dynamic flow scheduling in data center network.pdf",
  keywords  = "computer centres;computer networks;energy conservation;energy
               consumption;resource allocation;telecommunication power
               management;telecommunication scheduling;dynamic scheduling
               algorithm;energy-efficient dynamic flow scheduling;data center
               network;energy-efficient network;energy-efficient
               routing;scheduling algorithms;energy consumption;Dynamic
               scheduling;Heuristic algorithms;Energy efficiency;Computer
               networks;Algorithm design and analysis;Classification
               algorithms;data center
               network;deadline-aware;energy-efficient;dynamic flow
               scheduling;Datacentre",
  issn      = "2165-963X",
  doi       = "10.23919/CNSM.2017.8256053"
}

@INPROCEEDINGS{Beneventi2017-ns,
  title     = "Continuous learning of {HPC} infrastructure models using big
               data analytics and in-memory processing tools",
  booktitle = "Design, Automation Test in Europe Conference Exhibition
               ({DATE)}, 2017",
  author    = "Beneventi, Francesco and Bartolini, Andrea and Cavazzoni, Carlo
               and Benini, Luca",
  abstract  = "Exascale computing represents the next leap in the HPC race.
               Reaching this level of performance is subject to several
               engineering challenges such as energy consumption,
               equipment-cooling, reliability and massive parallelism.
               Model-based optimization is an essential tool in the design
               process and control of energy efficient, reliable and thermally
               constrained systems. However, in the Exascale domain, model
               learning techniques tailored to the specific supercomputer
               require real measurements and must therefore handle and analyze
               a massive amount of data coming from the HPC monitoring
               infrastructure. This becomes rapidly a ``big data'' scale
               problem. The common approach where measurements are first stored
               in large databases and then processed is no more affordable due
               to the increasingly storage costs and lack of real-time support.
               Nowadays instead, cloud-based machine learning techniques aim to
               build on-line models using real-time approaches such as ``stream
               processing'' and ``in-memory'' computing, that avoid storage
               costs and enable fastdata processing. Moreover, the fast
               delivery and adaptation of the models to the quick data
               variations, make the decision stage of the optimization loop
               more effective and reliable. In this paper we leverage scalable,
               lightweight and flexible IoT technologies, such as the MQTT
               protocol, to build a highly scalable HPC monitoring
               infrastructure able to handle the massive sensor data produced
               by next-gen HPC components. We then show how state-of-the art
               tools for big data computing and analysis, such as Apache Spark,
               can be used to manage the huge amount of data delivered by the
               monitoring layer and to build adaptive models in real-time using
               on-line machine learning techniques.",
  pages     = "1038--1043",
  month     =  mar,
  year      =  2017,
  keywords  = "Monitoring;Computational modeling;Data models;Tools;Real-time
               systems;Mathematical model;Protocols",
  issn      = "1558-1101",
  doi       = "10.23919/DATE.2017.7927143"
}

@INPROCEEDINGS{Ben_Yoo2022-db,
  title     = "Photonic Switching Technologies, Architectures, and
               {Integrated-Systems} for Future Disaggregated and Optically
               Reconfigurable Data Centers",
  booktitle = "2022 International Conference on Optical Network Design and
               Modeling ({ONDM})",
  author    = "Ben Yoo, S J",
  abstract  = "This Tutorial covers technologies, architectures, and
               system-integration for future data centers with optical
               reconfigurability. Optical interconnects allow disaggregation of
               computing resources in the data centers thanks to
               distance-independent energy-efficient and high-throughput
               communications of photonics. Photonic switching can provide
               additional benefits of reconfigurability of the interconnection
               topologies without requiring electronic-switches that accompany
               store-and-forward mechanisms. Hence, the primary motivation for
               considering photonic switching in data centers rises from the
               need for energy-efficient and scalable intra-data center
               networks to meet rapid increases in data traffic driven by
               emerging applications, including machine learning. To
               accommodate such traffic, today's large-scale data centers
               employ cascaded stages of many power-hungry electronic packet
               switches interconnected across the data center network in fixed
               hierarchical communication topologies. Numerous research papers
               have predicted significant benefits in scalability, throughput,
               and power efficiency from deploying photonic switches in data
               centers. However, photonic switching is not yet widely deployed
               in commercial warehouse-scale data centers at the time of
               writing this Tutorial due to significant challenges. They are
               related to (1) cross-layer issues involving control and
               management planes together with data integrity during switching,
               (2) scalability to > 5000 racks (> a quarter-million servers),
               (3) performance monitoring required for reliable operation, (4)
               currently existing standards allowing limited power margin (3
               dB), and (5) other practical (technology-dependent) issues
               relating to polarization sensitivity, temperature sensitivity,
               cost, etc. We will discuss possible solutions for future data
               centers involving cross-layer methods, new topologies, and
               innovative photonic switching technologies. Furthermore, the
               Tutorial broadly surveys state-of- the-art photonic switching
               technologies, architectures, and experimental results, and
               further covers the details of arrayed-
               waveguide-grating-router-based switch fabrics offering hybrid
               switching methods with distributed control planes towards
               scalable data center networking.",
  publisher = "IEEE",
  pages     = "1--6",
  month     =  may,
  year      =  2022,
  keywords  = "Optics",
  isbn      = "9783903176447, 9781665479806",
  doi       = "10.23919/ONDM54585.2022.9782842"
}

@INPROCEEDINGS{Pourkiani2020-ow,
  title     = "Machine Learning Based Task Distribution in Heterogeneous
               {Fog-Cloud} Environments",
  booktitle = "2020 International Conference on Software, Telecommunications
               and Computer Networks ({SoftCOM})",
  author    = "Pourkiani, M and Abedi, M",
  abstract  = "In order to improve the quality of service for delay-sensitive
               applications, in this paper, we propose Machine Learning based
               Task Distribution (MLTD) technique, which utilizes Artificial
               Neural Networks to distribute the tasks between the fog and
               cloud resources intelligently. This technique takes the
               diversity of servers (in terms of computing power) in addition
               to their workloads at the time of task distribution into account
               for providing the best possible response time. Evaluating the
               performance of our proposed technique, we utilized it in a
               real-world testbed and investigated its performance in different
               conditions. In comparison with similar methods, the achieved
               results show that MLTD improves the response time when the
               workloads of servers change continuously and reduces the
               internet bandwidth utilization in most cases.",
  pages     = "1--6",
  month     =  sep,
  year      =  2020,
  file      = "All Papers/P/Pourkiani and Abedi 2020 - Machine Learning Based Task Distribution in Heterogeneous Fog-Cloud Environments.pdf",
  keywords  = "Smart Task Distribution;Response Time;Fog Computing;Cloud
               Computing;Neural Networks;EdgeFogCloudIoT",
  issn      = "1847-358X",
  doi       = "10.23919/SoftCOM50211.2020.9238309"
}

@ARTICLE{Yang2018-ba,
  title     = "The (T, {L)-Path} Model and Algorithms for Information
               Dissemination in Dynamic Networks",
  author    = "Yang, Zhiwei and Wu, Weigang",
  abstract  = "A dynamic network is the abstraction of distributed systems with
               frequent network topology changes. With such dynamic network
               models, fundamental distributed computing problems can be
               formally studied with rigorous correctness. Although quite a
               number of models have been proposed and studied for dynamic
               networks, the existing models are usually defined from the point
               of view of connectivity properties. In this paper, instead, we
               examine the dynamicity of network topology according to the
               procedure of changes, i.e., how the topology or links change.
               Following such an approach, we propose the notion of the
               ``instant path'' and define two dynamic network models based on
               the instant path. Based on these two models, we design
               distributed algorithms for the problem of information
               dissemination respectively, one of the fundamental distributing
               computing problems. The correctness of our algorithms is
               formally proved and their performance in time cost and
               communication cost is analyzed. Compared with existing
               connectivity based dynamic network models and algorithms, our
               procedure based ones are definitely easier to be instantiated in
               the practical design and deployment of dynamic networks.",
  journal   = "Information",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  9,
  number    =  9,
  pages     = "212",
  month     =  aug,
  year      =  2018,
  file      = "All Papers/Y/Yang and Wu 2018 - The (T, L)-Path Model and Algorithms for Information Dissemination in Dynamic Networks.pdf",
  language  = "en",
  issn      = "1343-4500",
  doi       = "10.3390/info9090212"
}

@MISC{Creators_Cody_Coleman1_Show_affiliations_1_Stanford_undated-un,
  title  = "{MLSys} 2020 Artifacts for ``{MLPerf} Training Benchmark''",
  author = "{Creators Cody Coleman1 Show affiliations 1. Stanford}",
  file   = "All Papers/C/Creators Cody Coleman1 Show affiliati... - MLSys 2020 Artifacts for 'MLPerf Training Benchmark'.pdf",
  doi    = "10.5281/zenodo.3610717"
}

@MISC{Crovella_undated-si,
  title        = "Explaining world wide web traffic self-similarity",
  author       = "Crovella, Mark E and Bestavros, Azer",
  howpublished = "\url{https://cs-web.bu.edu/faculty/crovella/paper-archive/self-sim/tr-version.pdf}",
  note         = "Accessed: 2021-1-22",
  file         = "All Papers/C/Crovella and Bestavros - Explaining world wide web traffic self-similarity.pdf",
  keywords     = "NetworkTraffic"
}

@ARTICLE{McCanne_undated-wr,
  title    = "The {BSD} Packet Filter: A New Architecture for User-level Packet
              Capture",
  author   = "{McCanne} and {Jacobson}",
  journal  = "USENIX winter",
  file     = "All Papers/M/McCanne and Jacobson - The BSD Packet Filter - A New Architecture for User-level Packet Capture.pdf",
  keywords = "ProgrammableNetworks"
}

@INPROCEEDINGS{Dedhia2020-dk,
  title     = "You Snooze, You Lose: Minimizing {Channel-Aware} Age of
               Information",
  booktitle = "2020 18th International Symposium on Modeling and Optimization
               in Mobile, Ad Hoc, and Wireless Networks ({WiOPT})",
  author    = "Dedhia, Bhishma and Moharir, Sharayu",
  abstract  = "We propose a variant of the Age of Information (AoI) metric
               called Channel-Aware Age of Information (CA-AoI). Unlike AoI,
               CA-AoI takes into account the channel conditions between the
               source and the intended destination to compute the ``age'' of
               the recent most update received by the destination. We design
               scheduling policies for multi-sensor systems in which sensors
               report their measurements to a central monitoring station via a
               shared unreliable communication channel with the goal of
               minimizing the time-average of the weighted sum of CA-AoIs. We
               show that the scheduling problem is indexable and derive low
               complexity Whittle index based scheduling policies. We also
               design randomized scheduling algorithms and give optimization
               procedures to find the optimal parameters of the policy. Via
               simulations, we show that our proposed policies surpass the
               greedy policy in several settings. Moreover the Whittle Index
               based scheduling policies outperform other policies in all the
               settings considered.",
  pages     = "1--8",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/D/Dedhia and Moharir 2020 - You Snooze, You Lose - Minimizing Channel-Aware Age of Information.pdf",
  keywords  = "Indexes;Sensor systems;Monitoring;Measurement;Scheduling
               algorithms;Wireless;NICCI"
}

@INPROCEEDINGS{Xiao2018-rx,
  title     = "Gandiva: Introspective cluster scheduling for deep learning",
  booktitle = "13th {USENIX} Symposium on Operating Systems Design and
               Implementation ({OSDI} 18)",
  author    = "Xiao, Wencong and Bhardwaj, Romil and Ramjee, Ramachandran and
               Sivathanu, Muthian and Kwatra, Nipun and Han, Zhenhua and Patel,
               Pratyush and Peng, Xuan and Zhao, Hanyu and Zhang, Quanlu and
               {Others}",
  pages     = "595--610",
  year      =  2018,
  file      = "All Papers/X/Xiao et al. 2018 - Gandiva - Introspective cluster scheduling for deep learning.pdf",
  keywords  = "DataCenter"
}

@MISC{Natalino_undated-yf,
  title       = "optical-rl-gym: Set of reinforcement learning environments for
                 optical networks",
  author      = "Natalino, Carlos",
  abstract    = "Set of reinforcement learning environments for optical
                 networks - GitHub - carlosnatalino/optical-rl-gym: Set of
                 reinforcement learning environments for optical networks",
  institution = "Github",
  file        = "All Papers/N/Natalino - optical-rl-gym - Set of reinforcement learning environments for optical networks.pdf",
  language    = "en"
}

@ARTICLE{Huang2022-zu,
  title     = "{CleanRL}: high-quality single-file implementations of deep
               reinforcement learning algorithms",
  author    = "Huang, Shengyi and Dossa, Rousslan Fernand Juliendossa and Ye,
               Chang and Braga, Jeff and Chakraborty, Dipam and Mehta, Kinal
               and Ara{\'u}jo, Jo{\~a}o G M",
  abstract  = "CleanRL is an open-source library that provides high-quality
               single-file implementations of Deep Reinforcement Learning (DRL)
               algorithms. These single-file implementations are self-contained
               algorithm variant files such as dqn.py, ppo.py, and
               ppo\_atari.py that individually include all algorithm variant's
               implementation details. Such a paradigm significantly reduces
               the complexity and the lines of code (LOC) in each implemented
               variant, which makes them quicker and easier to understand. This
               paradigm gives the researchers the most fine-grained control
               over all aspects of the algorithm in a single file, allowing
               them to prototype novel features quickly. Despite having
               succinct implementations, CleanRL's codebase is thoroughly
               documented and benchmarked to ensure performance is on par with
               reputable sources. As a result, CleanRL produces a repository
               tailor-fit for two purposes: 1) understanding all implementation
               details of DRL algorithms and 2) quickly prototyping novel
               features.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  23,
  number    =  1,
  pages     = "12585--12602",
  month     =  jan,
  year      =  2022,
  keywords  = "open-source, single-file implementation, deep reinforcement
               learning",
  issn      = "1532-4435"
}

@BOOK{Patterson2017-qi,
  title     = "Computer Organization and Design {RISC-V} Edition: The Hardware
               Software Interface",
  author    = "Patterson, David A and Hennessy, John L",
  abstract  = "The new RISC-V Edition of Computer Organization and Design
               features the RISC-V open source instruction set architecture,
               the first open source architecture designed to be used in modern
               computing environments such as cloud computing, mobile devices,
               and other embedded systems. With the post-PC era now upon us,
               Computer Organization and Design moves forward to explore this
               generational change with examples, exercises, and material
               highlighting the emergence of mobile computing and the Cloud.
               Updated content featuring tablet computers, Cloud
               infrastructure, and the x86 (cloud computing) and ARM (mobile
               computing devices) architectures is included. An online
               companion Web site provides advanced content for further study,
               appendices, glossary, references, and recommended reading.
               Features RISC-V, the first such architecture designed to be used
               in modern computing environments, such as cloud computing,
               mobile devices, and other embedded systemsIncludes relevant
               examples, exercises, and material highlighting the emergence of
               mobile computing and the cloud",
  publisher = "Elsevier Science",
  month     =  apr,
  year      =  2017,
  keywords  = "GDS",
  language  = "en",
  isbn      = "9780128122754"
}

@ARTICLE{Wirjawan2006-oo,
  title     = "Balancing computation and code distribution costs: The case for
               hybrid execution in sensor networks",
  author    = "Wirjawan, I and Koshy, J and Pandey, R and Ramin, Y",
  abstract  = "… a hybrid execution environment that enables co- execution of …
               execution environments. In the interpreted mode,
               platform-independent bytecode is executed by an intepretive
               execution …",
  journal   = "Communications and Networks …",
  publisher = "Citeseer",
  year      =  2006,
  file      = "All Papers/W/Wirjawan et al. 2006 - Balancing computation and code distribution costs - The case for hybrid execution in sensor networks.pdf"
}

@INPROCEEDINGS{Camelo2022-qa,
  title       = "Requirements and Specifications for the Orchestration of
                 Network Intelligence in {6G}",
  booktitle   = "2022 {IEEE} 19th Annual Consumer Communications \& Networking
                 Conference ({CCNC})",
  author      = "Camelo, Miguel and Cominardi, Luca and Gramaglia, Marco and
                 Fiore, Marco and Garcia-Saavedra, Andres and Fuentes, Lidia
                 and De Vleeschauwer, Danny and Soto-Arenas, Paola and
                 Slamnik-Krijestorac, Nina and Ballesteros, Joaqu{\'\i}n and
                 {Others}",
  pages       = "1--9",
  institution = "IEEE",
  year        =  2022,
  keywords    = "MLNetworking;5G6G"
}

@ARTICLE{Shen2017-fp,
  title     = "{RIAL}: Resource intensity aware load balancing in clouds",
  author    = "Shen, Haiying and Chen, Liuhua",
  journal   = "IEEE Transactions on Cloud Computing",
  publisher = "IEEE",
  year      =  2017,
  file      = "All Papers/S/Shen and Chen 2017 - RIAL - Resource intensity aware load balancing in clouds.pdf"
}

@MISC{Munagala_undated-wk,
  title        = "Approximation algorithms for stochastic optimization",
  author       = "Munagala, Kamesh",
  howpublished = "\url{https://simons.berkeley.edu/sites/default/files/docs/5296/simons16-tut-final.pdf}",
  note         = "Accessed: 2021-5-30",
  file         = "All Papers/M/Munagala - Approximation algorithms for stochastic optimization.pdf"
}

@INCOLLECTION{Herzog:CoreConceptStatistics:2019,
  title     = "The Core Concept of Statistics",
  booktitle = "Understanding Statistics and Experimental Design: How not to lie
               with statistics",
  author    = "Herzog, Michael and Francis, Gregory and Clarke, Aaron",
  publisher = "Springer International Publishing",
  pages     = "23--50",
  year      =  2019,
  file      = "All Papers/H/Herzog et al. 2019 - The Core Concept of Statistics.pdf",
  address   = "Cham, Switzerland",
  keywords  = "Statistics;Methods",
  isbn      = "9783030034993"
}

@INPROCEEDINGS{Nga_Nguyen2020-vz,
  title     = "Joint downlink power control and channel allocation based on a
               partial view of future channel conditions",
  booktitle = "2020 18th International Symposium on Modeling and Optimization
               in Mobile, Ad Hoc, and Wireless Networks ({WiOPT})",
  author    = "Nga Nguyen, T T and Brun, Olivier and Prabhu, Balakrishna J",
  abstract  = "We propose two downlink scheduling algorithms that take
               advantage of partial information on future channel conditions
               for improving the sum utility. The scheduling model allows for
               both power control and channel allocation. The objective of the
               scheduler is the long-term utility under an average power
               constraint. The two algorithms incorporate the channel
               predictions in their decisions. The STO1 algorithm computes the
               decision in each slot based on the means of future channel
               gains. Depending on the horizon considered, this can require
               solving a large-dimensional problem in each slot. The STO2
               algorithm reduces the dimensionality by operating on two
               time-scales. On the slower scale it computes an estimation over
               a larger horizon, and in the faster scale of a slot, it computes
               the decision based on a shorter horizon. Numerical experiments
               with both fixed number of users as well as a dynamic number of
               users show that the two algorithms provide gains in utility
               compared to agnostic ones.",
  pages     = "1--8",
  month     =  jun,
  year      =  2020,
  keywords  = "Channel allocation;Optimization;Power control;Base
               stations;Prediction algorithms;Downlink;Heuristic
               algorithms;Scheduling;utility maximization;average power
               constraint;Wireless"
}

@INPROCEEDINGS{Bhandari2020-lm,
  title     = "{Age-of-Information} Bandits",
  booktitle = "2020 18th International Symposium on Modeling and Optimization
               in Mobile, Ad Hoc, and Wireless Networks ({WiOPT})",
  author    = "Bhandari, Kavya and Fatale, Santosh and Narula, Urvidh and
               Moharir, Sharayu and Hanawal, Manjesh Kumar",
  abstract  = "We consider a system with a single source that measures/tracks a
               time-varying quantity and periodically attempts to report these
               measurements to a monitoring station. Each update from the
               source has to be scheduled on one of K available communication
               channels. The probability of success of each attempted
               communication is a function of the channel used. This function
               is unknown to the scheduler. The metric of interest is the
               Age-of-Information (AoI), formally defined as the time elapsed
               since the destination received the recent most update from the
               source. We model our scheduling problem as a variant of the
               multi-arm bandit problem with communication channels as arms. We
               characterize a lower bound on the AoI regret achievable by any
               policy and characterize the performance of UCB, Thompson
               Sampling, and their variants. In addition, we propose novel
               policies which, unlike UCB and Thompson Sampling, use the
               current AoI to make scheduling decisions. Via simulations, we
               show the proposed AoI-aware policies outperform existing
               AoI-agnostic policies.",
  pages     = "1--8",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/B/Bhandari et al. 2020 - Age-of-Information Bandits.pdf",
  keywords  = "Monitoring;Measurement;Schedules;Channel estimation;Time-varying
               systems;Channel state information;NICCI"
}

@INPROCEEDINGS{Bhimaraju2020-ro,
  title     = "Non-clairvoyant Scheduling of Coflows",
  booktitle = "2020 18th International Symposium on Modeling and Optimization
               in Mobile, Ad Hoc, and Wireless Networks ({WiOPT})",
  author    = "Bhimaraju, Akhil and Nayak, Debanuj and Vaze, Rahul",
  abstract  = "The coflow scheduling problem is considered: given an
               input/output switch with each port having a fixed capacity, find
               a scheduling algorithm that minimizes the weighted sum of the
               coflow completion times respecting the port capacities, where
               each flow of a coflow has a demand per input/output port, and
               coflow completion time is the finishing time of the last flow of
               the coflow. The objective of this paper is to present
               theoretical guarantees on approximating the sum of coflow
               completion time in the non-clairvoyant setting, where on a
               coflow arrival, only the number of flows, and their input-output
               port is revealed, while the critical demand volumes for each
               flow on the respective input-output port is unknown. The main
               result of this paper is to show that the proposed BlindFlow
               algorithm is 8p-approximate, where p is the largest number of
               input-output port pairs that a coflow uses. This result holds
               even in the online case, where coflows arrive over time and the
               scheduler has to use only causal information. Simulations reveal
               that the experimental performance of BlindFlow is far better
               than the theoretical guarantee.",
  pages     = "1--8",
  month     =  jun,
  year      =  2020,
  keywords  = "Approximation algorithms;Optimized production
               technology;Switches;Schedules;Processor scheduling;Heuristic
               algorithms;Datacentre"
}

@INPROCEEDINGS{Mittal2018-hg,
  title     = "{Self-Organizing} Infrastructure for Machine (Deep) Learning at
               Scale",
  booktitle = "2018 {IEEE/ACM} 1st International Workshop on Software
               Engineering for Cognitive Services ({SE4COG})",
  author    = "Mittal, Samir",
  abstract  = "Building machine (deep) learning systems is hard. Computation
               requirements grow non-linearly with the complexity of the task
               at hand creating acute challenges relating to data
               dimensionality, complex model development, slow experiments, and
               scalability of production deployments. e bulk of the ML/DL
               effort is consumed in infrastructure and data management.
               Automating such workflows has become the focus of recent
               research activity, so as to make ML/DL systems universally
               accessible. We extend these paradigms by infusing domain
               knowledge for infrastructure self-management. Key elements
               include understanding application design intent, fingerprinting
               the neural network for its computational, data and convergence
               properties, optimizing the implementation to achieving workload
               intent, and accelerating the neural network implementation in
               real-time hardware implementation. Keys to success require
               offline behavioural modelling coupled with online dynamic
               adaptation, made possible by the use of cognitive algorithms
               that accumulate knowledge in a dynamic and continuously evolving
               knowledgebase. In this way, we use machine learning to automate
               AI infrastructure management to minimize human engineering, and
               significantly accelerating application performance.",
  pages     = "5--8",
  month     =  may,
  year      =  2018,
  keywords  = "Machine learning;Computational modeling;Neural networks;Data
               models;Acceleration;Heuristic algorithms;Complexity theory;Deep
               learning;machine learning;cognitive infrastructure"
}

@INPROCEEDINGS{Curino2019-uu,
  title     = "Hydra: a federated resource manager for data-center scale
               analytics",
  booktitle = "16th {$\{$USENIX$\}$} Symposium on Networked Systems Design and
               Implementation ({$\{$NSDI$\}$} 19)",
  author    = "Curino, Carlo and Krishnan, Subru and Karanasos, Konstantinos
               and Rao, Sriram and Fumarola, Giovanni M and Huang, Botong and
               Chaliparambil, Kishore and Suresh, Arun and Chen, Young and
               Heddaya, Solom and {Others}",
  pages     = "177--192",
  year      =  2019,
  file      = "All Papers/C/Curino et al. 2019 - Hydra - a federated resource manager for data-center scale analytics.pdf",
  keywords  = "DataCenter"
}

@BOOK{Sutton2018-qr,
  title     = "Reinforcement Learning, second edition: An Introduction",
  author    = "Sutton, Richard S and Barto, Andrew G",
  abstract  = "The significantly expanded and updated new edition of a widely
               used text on reinforcement learning, one of the most active
               research areas in artificial intelligence.Reinforcement
               learning, one of the most active research areas in artificial
               intelligence, is a computational approach to learning whereby an
               agent tries to maximize the total amount of reward it receives
               while interacting with a complex, uncertain environment. In
               Reinforcement Learning, Richard Sutton and Andrew Barto provide
               a clear and simple account of the field's key ideas and
               algorithms. This second edition has been significantly expanded
               and updated, presenting new topics and updating coverage of
               other topics.Like the first edition, this second edition focuses
               on core online learning algorithms, with the more mathematical
               material set off in shaded boxes. Part I covers as much of
               reinforcement learning as possible without going beyond the
               tabular case for which exact solutions can be found. Many
               algorithms presented in this part are new to the second edition,
               including UCB, Expected Sarsa, and Double Learning. Part II
               extends these ideas to function approximation, with new sections
               on such topics as artificial neural networks and the Fourier
               basis, and offers expanded treatment of off-policy learning and
               policy-gradient methods. Part III has new chapters on
               reinforcement learning's relationships to psychology and
               neuroscience, as well as an updated case-studies chapter
               including AlphaGo and AlphaGo Zero, Atari game playing, and IBM
               Watson's wagering strategy. The final chapter discusses the
               future societal impacts of reinforcement learning.",
  publisher = "MIT Press",
  month     =  nov,
  year      =  2018,
  language  = "en",
  isbn      = "9780262352703"
}

@ARTICLE{Dean2008-hd,
  title     = "Approximating the Stochastic Knapsack Problem: The Benefit of
               Adaptivity",
  author    = "Dean, Brian C and Goemans, Michel X and Vondr{\'a}k, Jan",
  abstract  = "[We consider a stochastic variant of the NP-hard 0/1 knapsack
               problem, in which item values are deterministic and item sizes
               are independent random variables with known, arbitrary
               distributions. Items are placed in the knapsack sequentially,
               and the act of placing an item in the knapsack instantiates its
               size. Our goal is to compute a solution ``policy'' that
               maximizes the expected value of items successfully placed in the
               knapsack, where the final overflowing item contributes no value.
               We consider both nonadaptive policies (that designate a priori a
               fixed sequence of items to insert) and adaptive policies (that
               can make dynamic choices based on the instantiated sizes of
               items placed in the knapsack thus far). An important facet of
               our work lies in characterizing the benefit of adaptivity. For
               this purpose we advocate the use of a measure called the
               adaptivity gap: the ratio of the expected value obtained by an
               optimal adaptive policy to that obtained by an optimal
               nonadaptive policy. We bound the adaptivity gap of the
               stochastic knapsack problem by demonstrating a polynomial-time
               algorithm that computes a nonadaptive policy whose expected
               value approximates that of an optimal adaptive policy to within
               a factor of four. We also devise a polynomial-time adaptive
               policy that approximates the optimal adaptive policy to within a
               factor of 3 + $\epsilon$ for any constant $\epsilon$ > 0.]",
  journal   = "Math. Oper. Res.",
  publisher = "INFORMS",
  volume    =  33,
  number    =  4,
  pages     = "945--964",
  year      =  2008,
  file      = "All Papers/D/Dean et al. 2008 - Approximating the Stochastic Knapsack Problem - The Benefit of Adaptivity.pdf",
  keywords  = "Algorithms\_Theory",
  issn      = "0364-765X, 1526-5471"
}

@BOOK{Arrowsmith_undated-iv,
  title    = "Modelling Network Data Traffic",
  author   = "Arrowsmith, D K and Mondragon, R J",
  file     = "All Papers/A/Arrowsmith and Mondragon - Modelling Network Data Traffic.pdf",
  keywords = "NetworkTraffic"
}

@MISC{Taleb_undated-pm,
  title        = "{WHITE} {PAPER} {ON} {6G} {NETWORKING}",
  author       = "Taleb, Tarik ed",
  howpublished = "White paper of the 6G Flagship project",
  file         = "All Papers/T/Taleb - WHITE PAPER ON 6G NETWORKING.pdf",
  keywords     = "5G6G"
}

@INPROCEEDINGS{Sapio2021-vo,
  title     = "Scaling Distributed Machine Learning with {\{In-Network\}}
               Aggregation",
  booktitle = "18th {USENIX} Symposium on Networked Systems Design and
               Implementation ({NSDI} 21)",
  author    = "Sapio, Amedeo and Canini, Marco and Ho, Chen-Yu and Nelson,
               Jacob and Kalnis, Panos and Kim, Changhoon and Krishnamurthy,
               Arvind and Moshref, Masoud and Ports, Dan and Richtarik, Peter",
  pages     = "785--808",
  year      =  2021,
  file      = "All Papers/S/Sapio et al. 2021 - Scaling Distributed Machine Learning with {In-Network} Aggregation.pdf",
  keywords  = "ServicesDescription",
  isbn      = "9781939133212"
}

@MISC{Whalley2021-fl,
  title        = "[No title]",
  author       = "Whalley, David and Yuan, Xin and Liu, Xiuwen",
  month        =  aug,
  year         =  2021,
  howpublished = "\url{https://cacm.acm.org/magazines/2021/8/254308-the-domestic-computer-science-graduate-students-are-there-we-just-need-to-recruit-them/fulltext}",
  note         = "Accessed: 2021-12-22",
  keywords     = "Teaching"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Niebert2004-il,
  title     = "Ambient networks: an architecture for communication networks
               beyond {3G}",
  author    = "Niebert, N and Schieder, A and Abramowicz, H and {others}",
  abstract  = "In this article we present a new networking concept referred to
               as ambient networks, which aims to enable the cooperation of
               heterogeneous networks belonging to different operator or
               technology domains. We aim to provide a domain-structured
               edge-to-edge view for the …",
  journal   = "IEEE Wirel. Commun.",
  publisher = "prehofer.de",
  year      =  2004,
  file      = "All Papers/N/Niebert et al. 2004 - Ambient networks - an architecture for communication networks beyond 3G.pdf",
  keywords  = "MyPapers;ServicesDescription"
}

@ARTICLE{Bao2018-pj,
  title     = "Online job scheduling in distributed machine learning clusters",
  author    = "Bao, Y and Peng, Y and Wu, C and Li, Z",
  abstract  = "… distributed … scheduling the arriving jobs and deciding the
               adjusted numbers of concurrent workers and parameter servers for
               each job over its course, to maximize overall utility of all
               jobs …",
  journal   = "IEEE INFOCOM 2018-IEEE",
  publisher = "ieeexplore.ieee.org",
  year      =  2018,
  file      = "All Papers/B/Bao et al. 2018 - Online job scheduling in distributed machine learning clusters.pdf"
}

@INPROCEEDINGS{HasanzadeZonuzy2020-rl,
  title     = "Reinforcement Learning for {Multi-Hop} Scheduling and Routing of
               {Real-Time} Flows",
  booktitle = "2020 18th International Symposium on Modeling and Optimization
               in Mobile, Ad Hoc, and Wireless Networks ({WiOPT})",
  author    = "HasanzadeZonuzy, Aria and Kalathil, Dileep and Shakkottai,
               Srinivas",
  abstract  = "We consider the problem of serving real-time flows over a
               multi-hop wireless network. Each flow is composed of packets
               that have strict deadlines, and the goal is to maximize the
               weighted timely throughput of the system. Consistent with recent
               developments using mm-wave communications, we assume that the
               links are directional, but are lossy, and have unknown
               probabilities of successful packet transmission. An average link
               utilization budget (similar to a power constraint) constrains
               the system. We pose the problem in the form of a Constrained
               Markov Decision Process (CMDP) with an unknown transition
               kernel. We use a duality approach to decompose the problem into
               an inner unconstrained MDP with link usage costs, and an outer
               link-cost update step. For the inner MDP, we develop model-based
               reinforcement learning algorithms that sample links by sending
               packets to learn the link statistics. While the first algorithm
               type samples links at will at the beginning and constructs the
               model, the second type is an online approach that can only use
               packets from flows to sample links that they traverse. The
               approach to the outer problem follows gradient descent. We
               characterize the sample complexity (number of packets
               transmitted) to obtain near-optimal policies, to show that a
               basic online approach has a poorer sample complexity bound, it
               can be modified to obtain an online algorithm that has excellent
               empirical performance.",
  pages     = "1--8",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/H/HasanzadeZonuzy et al. 2020 - Reinforcement Learning for Multi-Hop Scheduling and Routing of Real-Time Flows.pdf",
  keywords  = "Complexity theory;Throughput;Routing;Real-time systems;Learning
               (artificial intelligence);Delays;Wireless
               networks;Wireless;ML;MLNetworking"
}

@MISC{Hassidim_undated-nw,
  title        = "Robust guarantees of stochastic greedy algorithms",
  author       = "Hassidim, Avinatan and Singer, Yaron",
  howpublished = "\url{http://proceedings.mlr.press/v70/hassidim17a/hassidim17a.pdf}",
  note         = "Accessed: 2021-5-30",
  file         = "All Papers/H/Hassidim and Singer - Robust guarantees of stochastic greedy algorithms.pdf",
  keywords     = "SurveysTutorials"
}

@INPROCEEDINGS{Huang2019-mn,
  title     = "{MLoC}: A Cloud Framework adopting Machine Learning for
               Industrial Automation",
  booktitle = "2019 12th Asian Control Conference ({ASCC})",
  author    = "Huang, Yu-Lun and Sun, Wen-Lin and Yeh, Kai-Wei",
  abstract  = "By leveraging the modern machine learning algorithms, we can
               build up more Artificial Intelligence (AI) systems, like
               self-driving cars, smart factories and financial analysis
               systems, to improve our daily life. In addition to building up
               an AI system, several prerequisites are required to drive the
               system, including data collection, data storage, machine
               learning models, training dataset, parameters tuning, and so on.
               To obtain the benefit of scalability and flexibility, most AI
               systems are built on a cloud platform, which shares resources
               with others in the same infrastructure. Though the above concept
               is trivial, the implementation faces big challenges when
               realizing it. In this paper, an easy-to-use cloud framework for
               machine learning as well as its implementation guideline is
               presented for building up a cloud-based development platform. We
               conduct several experiments on analyzing and monitoring the
               health condition of bearings of motors. We compare and analyze
               the feasibility of the proposed framework.",
  pages     = "1413--1418",
  month     =  jun,
  year      =  2019,
  keywords  = "Data models;Containers;Cloud computing;Sensors;Logic
               gates;Machine learning algorithms;Testing"
}

@INPROCEEDINGS{Xiao2020-ur,
  title     = "{\{AntMan\}}: Dynamic scaling on {\{GPU\}} clusters for deep
               learning",
  booktitle = "14th {USENIX} Symposium on Operating Systems Design and
               Implementation ({OSDI} 20)",
  author    = "Xiao, Wencong and Ren, Shiru and Li, Yong and Zhang, Yang and
               Hou, Pengyang and Li, Zhi and Feng, Yihui and Lin, Wei and Jia,
               Yangqing",
  pages     = "533--548",
  year      =  2020,
  file      = "All Papers/X/Xiao et al. 2020 - {AntMan} - Dynamic scaling on {GPU} clusters for deep learning.pdf",
  isbn      = "9781939133199"
}

@INCOLLECTION{Asanovic2019-ke,
  title     = "The {RISC-V} Instruction Set Manual",
  booktitle = "Privileged Architecture, Document Version
               {20190608-Priv-MSU-Ratified}",
  author    = "Asanovic, Krste and Waterman, Andrew",
  publisher = "RISC-V Foundation",
  volume    =  2,
  year      =  2019,
  keywords  = "GDS"
}

@BOOK{Karl2007-qf,
  title     = "Protocols and Architectures for Wireless Sensor Networks",
  author    = "Karl, Holger and Willig, Andreas",
  abstract  = "Learn all you need to know about wireless sensor networks!
               Protocols and Architectures for Wireless Sensor Networks
               provides a thorough description of the nuts and bolts of
               wireless sensor networks. The authors give an overview of the
               state-of-the-art, putting all the individual solutions into
               perspective with one and other. Numerous practical examples,
               case studies and illustrations demonstrate the theory,
               techniques and results presented. The clear chapter structure,
               listing learning objectives, outline and summarizing key points,
               help guide the reader expertly through the material. Protocols
               and Architectures for Wireless Sensor Networks: Covers
               architecture and communications protocols in detail with
               practical implementation examples and case studies. Provides an
               understanding of mutual relationships and dependencies between
               different protocols and architectural decisions. Offers an
               in-depth investigation of relevant protocol mechanisms. Shows
               which protocols are suitable for which tasks within a wireless
               sensor network and in which circumstances they perform
               efficiently. Features an extensive website with the
               bibliography, PowerPoint slides, additional exercises and worked
               solutions. This text provides academic researchers, graduate
               students in computer science, computer engineering, and
               electrical engineering, as well as practitioners in industry and
               research engineers with an understanding of the specific design
               challenges and solutions for wireless sensor networks. Check out
               www.wiley.com/go/wsn for accompanying course material! ``I am
               deeply impressed by the book of Karl \& Willig. It is by far the
               most complete source for wireless sensor networks...The book
               covers almost all topics related to sensor networks, gives an
               amazing number of references, and, thus, is the perfect source
               for students, teachers, and researchers. Throughout the book the
               reader will find high quality text, figures, formulas,
               comparisons etc. - all you need for a sound basis to start
               sensor network research.'' Prof. Jochen Schiller, Institute of
               Computer Science, Freie Universit{\"a}t Berlin",
  publisher = "John Wiley \& Sons",
  month     =  oct,
  year      =  2007,
  keywords  = "MyPapers",
  language  = "en",
  isbn      = "9780470519233"
}

@MISC{Wilson_undated-dt,
  title        = "A Historical View of Network Traffic Models",
  author       = "Wilson, Michael L",
  howpublished = "\url{https://www.cse.wustl.edu/~jain/cse567-06/ftp/traffic_models2/}",
  note         = "Accessed: 2021-1-22",
  file         = "All Papers/W/Wilson - A Historical View of Network Traffic Models.pdf",
  keywords     = "NetworkTraffic"
}

@TECHREPORT{Hubaux1999-bb,
  title    = "Terminodes: Toward self-organized mobile wide area networks",
  author   = "Hubaux, Jean-Pierre",
  abstract = "This document presents a new research domain in the area of
              communication systems. It is meant to be the proposal of the
              Communication Systems Division of EPFL and its Consortium to the
              National Pole of Competence of the Swiss National Fund. The
              expected (and desirable) characteristics of the proposed domain
              are as follows: 1.Be more future-looking than what is currently
              considered by the mobile communication industry 2.Have a positive
              impact on civilization and contain challenging societal and
              business issues 3.Be broad enough to encompass the research
              interest of all labs of the Communication Division of EPFL and of
              all the partners of the Consortium 4.Nevertheless, be focused
              enough to foster synergy within the Division and within the
              Consortium. Hubaux, Jean-Pierre",
  number   = "REP\_WORK",
  year     =  1999,
  keywords = "NCCR-MICS; NCCR-MICS/CL3"
}

@INPROCEEDINGS{Bonati2022-qn,
  title       = "{OpenRAN} Gym: An open toolbox for data collection and
                 experimentation with {AI} in {O-RAN}",
  booktitle   = "2022 {IEEE} Wireless Communications and Networking Conference
                 ({WCNC})",
  author      = "Bonati, Leonardo and Polese, Michele and D'Oro, Salvatore and
                 Basagni, Stefano and Melodia, Tommaso",
  pages       = "518--523",
  institution = "IEEE",
  year        =  2022
}

@BOOK{Bekkerman2012-jk,
  title     = "Scaling Up Machine Learning: Parallel and Distributed Approaches",
  author    = "Bekkerman, Ron and Bilenko, Mikhail and Langford, John",
  abstract  = "This book presents an integrated collection of representative
               approaches for scaling up machine learning and data mining
               methods on parallel and distributed computing platforms. Demand
               for parallelizing learning algorithms is highly task-specific:
               in some settings it is driven by the enormous dataset sizes, in
               others by model complexity or by real-time performance
               requirements. Making task-appropriate algorithm and platform
               choices for large-scale machine learning requires understanding
               the benefits, trade-offs, and constraints of the available
               options. Solutions presented in the book cover a range of
               parallelization platforms from FPGAs and GPUs to multi-core
               systems and commodity clusters, concurrent programming
               frameworks including CUDA, MPI, MapReduce, and DryadLINQ, and
               learning settings (supervised, unsupervised, semi-supervised,
               and online learning). Extensive coverage of parallelization of
               boosted trees, SVMs, spectral clustering, belief propagation and
               other popular learning algorithms and deep dives into several
               applications make the book equally useful for researchers,
               students, and practitioners.",
  publisher = "Cambridge University Press",
  year      =  2012,
  language  = "en",
  isbn      = "9780521192248"
}

@INPROCEEDINGS{McMahan2017-ts,
  title      = "{{Communication-Efficient} Learning of Deep Networks from
                Decentralized Data}",
  booktitle  = "Proceedings of the 20th International Conference on Artificial
                Intelligence and Statistics",
  author     = "McMahan, Brendan and Moore, Eider and Ramage, Daniel and
                Hampson, Seth and Arcas, Blaise Aguera y",
  editor     = "Singh, Aarti and Zhu, Jerry",
  abstract   = "Modern mobile devices have access to a wealth of data suitable
                for learning models, which in turn can greatly improve the user
                experience on the device. For example, language models can
                improve speech recognition and text entry, and image models can
                automatically select good photos. However, this rich data is
                often privacy sensitive, large in quantity, or both, which may
                preclude logging to the data center and training there using
                conventional approaches. We advocate an alternative that leaves
                the training data distributed on the mobile devices, and learns
                a shared model by aggregating locally-computed updates. We term
                this decentralized approach Federated Learning. We present a
                practical method for the federated learning of deep networks
                based on iterative model averaging, and conduct an extensive
                empirical evaluation, considering five different model
                architectures and four datasets. These experiments demonstrate
                the approach is robust to the unbalanced and non-IID data
                distributions that are a defining characteristic of this
                setting. Communication costs are the principal constraint, and
                we show a reduction in required communication rounds by 10-100x
                as compared to synchronized stochastic gradient descent.",
  publisher  = "PMLR",
  volume     =  54,
  pages      = "1273--1282",
  series     = "Proceedings of Machine Learning Research",
  year       =  2017,
  file       = "All Papers/M/McMahan et al. 2017 - Communication-Efficient Learning of Deep Networks from Decentralized Data.pdf",
  conference = "Conference on Artificial Intelligence and Statistics"
}

@INPROCEEDINGS{Qizhen_Weng_Wencong_Xiao_Yinghao_Yu_Wei_Wang_Cheng_Wang_Jian_He_Yong_Li_Liping_Zhang_Wei_Lin_and_Yu_Ding2022-af,
  title      = "{MLaaS} in the Wild: Workload Analysis and Scheduling in
                {Large-Scale} Heterogeneous {GPU} Clusters",
  author     = "{Qizhen Weng, Wencong Xiao, Yinghao Yu, Wei Wang, Cheng Wang,
                Jian He, Yong Li, Liping Zhang, Wei Lin, and Yu Ding}",
  month      =  apr,
  year       =  2022,
  annote     = "Alibaba trace program: job traces in a data center<div>can be
                used for CPU and GPU scheduling</div>",
  file       = "All Papers/Q/Qizhen Weng, Wencong Xiao, Yinghao Yu... 2022 - MLaaS in the Wild - Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters.pdf",
  conference = "19th USENIX Symposium on Networked Systems Design and
                Implementation"
}

@INPROCEEDINGS{Song2020-rr,
  title     = "{Meta-Scheduling} for the Wireless Downlink through Learning
               with Bandit Feedback",
  booktitle = "2020 18th International Symposium on Modeling and Optimization
               in Mobile, Ad Hoc, and Wireless Networks ({WiOPT})",
  author    = "Song, Jianhan and Veciana, Gustavo de and Shakkottai, Sanjay",
  abstract  = "In this paper, we study learning-assisted multi-user scheduling
               for the wireless downlink. There have been many scheduling
               algorithms developed that optimize for a plethora of performance
               metrics; however a systematic approach across diverse
               performance metrics and deployment scenarios is still lacking.
               We address this by developing a meta-scheduler-given a diverse
               collection of schedulers, we develop a learning-based overlay
               algorithm (meta-scheduler) that selects that ``best'' scheduler
               from amongst these for each deployment scenario. More formally,
               we develop a multi-armed bandit (MAB) framework for
               meta-scheduling that assigns and adapts a score for each
               scheduler to maximize reward (e.g., mean delay, timely
               throughput etc.). The meta-scheduler is based on a variant of
               the Upper Confidence Bound algorithm (UCB), but adapted to
               interrupt the queuing dynamics at the base-station so as to
               filter out schedulers that might render the system unstable. We
               show that the algorithm has a poly-logarithmic regret in the
               expected reward with respect to a genie that chooses the optimal
               scheduler for each scenario. Finally through simulation, we show
               that the meta-scheduler learns the choice of the scheduler to
               best adapt to the deployment scenario (e.g. load conditions,
               performance metrics).",
  pages     = "1--7",
  month     =  jun,
  year      =  2020,
  file      = "All Papers/S/Song et al. 2020 - Meta-Scheduling for the Wireless Downlink through Learning with Bandit Feedback.pdf",
  keywords  = "Wireless communication;Task analysis;Optimal
               scheduling;Throughput;Heuristic algorithms;Delays;wireless
               scheduling;multi-armed bandit;UCB;Wireless;NICCI"
}

@BOOK{Bertsekas2023-dh,
  title     = "A Course in Reinforcement Learning",
  author    = "Bertsekas, Dimitri",
  abstract  = "These lecture notes were prepared for use in the 2023 ASU
               research-oriented course on Reinforcement Learning (RL) that I
               have offered in each of the last five years. Their purpose is to
               give an overview of the RL methodology, particularly as it
               relates to problems of optimal and suboptimal decision and
               control, as well as discrete optimization. There are two major
               methodological RL approaches: approximation in value space,
               where we approximate in some way the optimal value function, and
               approximation in policy space, whereby we construct a (generally
               suboptimal) policy by using optimization over a suitably
               restricted class of policies.The lecture notes focus primarily
               on approximation in value space, with limited coverage of
               approximation in policy space. However, they are structured so
               that they can be easily supplemented by an instructor who wishes
               to go into approximation in policy space in greater detail,
               using any of a number of available sources, including the
               author's 2019 RL book.While in these notes we deemphasize
               mathematical proofs, there is considerable related analysis,
               which supports our conclusions and can be found in the author's
               recent RL and DP books. These books also contain additional
               material on off-line training of neural networks, on the use of
               policy gradient methods for approximation in policy space, and
               on aggregation.",
  publisher = "Athena Scientific",
  month     =  jun,
  year      =  2023,
  file      = "All Papers/B/Bertsekas 2023 - A Course in Reinforcement Learning.pdf",
  language  = "en",
  isbn      = "9781886529496"
}

@ARTICLE{Zhu_undated-oh,
  title  = "Benchmarking and Analyzing Deep Neural Network Training",
  author = "Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris,
            Andrew",
  file   = "All Papers/Z/Zhu et al. - Benchmarking and Analyzing Deep Neural Network Training.pdf"
}

@BOOK{Cai2014-mo,
  title     = "Optimal Stochastic Scheduling",
  author    = "Cai, Xiaoqiang and Wu, Xianyi and Zhou, Xian",
  abstract  = "Many interesting and important results on stochastic scheduling
               problems have been developed in recent years, with the aid of
               probability theory. This book provides a comprehensive and
               unified coverage of studies in stochastic scheduling. The
               objective is two-fold: (i) to summarize the elementary models
               and results in stochastic scheduling, so as to offer an
               entry-level reading material for students to learn and
               understand the fundamentals of this area and (ii) to include in
               details the latest developments and research topics on
               stochastic scheduling, so as to provide a useful reference for
               researchers and practitioners in this area.Optimal Stochastic
               Scheduling is organized into two parts: Chapters 1-4 cover
               fundamental models and results, whereas Chapters 5-10 elaborate
               on more advanced topics. More specifically, Chapter 1 provides
               the relevant basic theory of probability and then introduces the
               basic concepts and notation of stochastic scheduling. In
               Chapters 2 and 3, the authors review well-established models and
               scheduling policies, under regular and irregular performance
               measures, respectively. Chapter 4 describes models with
               stochastic machine breakdowns. Chapters 5 and 6 introduce,
               respectively, the optimal stopping problems and the multi-armed
               bandit processes, which are necessary for studies of more
               advanced subjects in subsequent chapters. Chapter 7 is focused
               on optimal dynamic policies, which allow adjustments of policies
               based on up-to-date information. Chapter 8 describes stochastic
               scheduling with incomplete information in the sense that the
               probability distributions of random variables contain unknown
               parameters, which can however be estimated progressively
               according to updated information. Chapter 9 is devoted to the
               situation where the processing time of a job depends on the time
               when it is started. Lastly, in Chapter 10 the authors look at
               several recent models beyond those surveyed in the previous
               chapters.",
  publisher = "Springer Science \& Business Media",
  month     =  mar,
  year      =  2014,
  language  = "en",
  isbn      = "9781489974051"
}

@MISC{Raffin_undated-pc,
  title       = "stable-baselines3: {PyTorch} version of Stable Baselines,
                 reliable implementations of reinforcement learning algorithms",
  author      = "Raffin, Antonin and Hill, Ashley and Gleave, Adam and
                 Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah",
  abstract    = "PyTorch version of Stable Baselines, reliable implementations
                 of reinforcement learning algorithms. - GitHub -
                 DLR-RM/stable-baselines3: PyTorch version of Stable Baselines,
                 reliable implementations of reinforcement learning algorithms.",
  institution = "Github",
  file        = "All Papers/R/Raffin et al. - stable-baselines3 - PyTorch version of Stable Baselines, reliable implementations of reinforcement learning algorithms.pdf",
  language    = "en"
}

@MISC{Zaharia_undated-pp,
  title        = "Resilient distributed datasets: A fault-tolerant abstraction
                  for in-memory cluster computing",
  author       = "Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and
                  Dave, Ankur and Ma, Justin and Mc Cauley, Murphy and
                  Franklin, Michael J and Shenker, Scott and Stoica, Ion",
  howpublished = "\url{http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf}",
  note         = "Accessed: 2021-5-19",
  file         = "All Papers/Z/Zaharia et al. - Resilient distributed datasets - A fault-tolerant abstraction for in-memory cluster computing.pdf",
  keywords     = "Distributed Systems"
}

@MISC{Ermert2023-ab,
  title        = "Missing Link: Auch Internetprotokolle haben ihren Lifecycle",
  booktitle    = "Heise Medien",
  author       = "Ermert, Monika",
  abstract     = "Deutsche Forscher dominieren in der Internet Engineering Task
                  Force (IETF). heise online hat mit zwei von Ihnen {\"u}ber
                  den ``Zoo von Protokollen'' gesprochen.",
  month        =  mar,
  year         =  2023,
  howpublished = "\url{https://heise.de/-7549017}",
  note         = "Accessed: 2023-3-20",
  keywords     = "ComputerNetworks"
}

@ARTICLE{noauthor_undated-qt,
  title    = "towards-achieving-high-performance-in-5g-mobile-packet-cores-user-plane-function.pdf",
  file     = "All Papers/Other/towards-achieving-high-performance-in... - towards-achieving-high-performance-in-5g-mobile-packet-cores-user-plane-function.pdf",
  keywords = "ProgrammableNetworks"
}

@MISC{noauthor_undated-yd,
  title       = "Gymnasium at v0.29.1",
  abstract    = "An API standard for single-agent reinforcement learning
                 environments, with popular reference environments and related
                 utilities (formerly Gym) - GitHub -
                 Farama-Foundation/Gymnasium at v0.29.1",
  institution = "Github",
  language    = "en"
}

@ARTICLE{noauthor_undated-vc,
  title = "A method to estimate the energy consumption of deep neural networks",
  file  = "All Papers/Other/A method to estimate the energy consu... - A method to estimate the energy consumption of deep neural networks.pdf"
}

@MISC{noauthor_undated-oq,
  title       = "openrlbenchmark",
  abstract    = "Contribute to openrlbenchmark/openrlbenchmark development by
                 creating an account on GitHub.",
  institution = "Github",
  language    = "en"
}

@MISC{noauthor_undated-wg,
  title = "{TPCX-AI\_v1.0.3.1.pdf}",
  file  = "All Papers/Other/TPCX-AI_v1.0.3.1.pdf - TPCX-AI_v1.0.3.1.pdf"
}
